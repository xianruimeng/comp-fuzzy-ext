\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}

\title{Constant Error Tolerance Computational Fuzzy Extractors}
%\author{Ran Canetti \and Benjamin Fuller\footnote{The Lincoln Laboratory portion of this
%work was sponsored by the Department of the Air Force under Air Force
%Contract
%\#FA8721-05-C-0002.  Opinions,
%interpretations, conclusions and recommendations are those of the author
%and
%are not necessarily endorsed by the United States Government.} \and Omer Paneth \and Leonid Reyzin}

\begin{document}
\maketitle


\begin{abstract}
Fuzzy extractors derive strong keys from noisy sources.  The goal is to reliably convert a high entropy source~(which may differ on repeated readings) to the same uniformly distributed key.  Traditionally, their security is defined information-theoretically.  %Fuzzy extractors have upper bounds on the length of the derived key.  

Traditionally, a fuzzy extractor works for any distribution with enough entropy.  %In this setting, the entropy of the source must be significantly higher than the logarithm of number of error patterns corrected~(Dodis et al., J. of Comp 2008).  
We call the difference between the starting entropy and the logarithm of the number of correctable error patterns the \emph{minimum usable entropy}.  Dodis et al.~(J. of Comp. 2008) show there is some source where the maximum key length cannot exceed the minimum usable entropy.  Therefore, fuzzy extractors that only consider the input entropy must output a key whose length is at most the minimum usable entropy.
For many practical noisy sources, like biometrics, this condition is not fulfilled, leaving reliable key derivation from these sources as an open problem.

%Traditional fuzzy extractors output a key roughly of length minimum usable entropy.  
To achieve meaningful security when the minimum usable entropy is negative, some restriction on the source is necessary.  Meaningful restrictions (and accompanying constructions) have proved elusive for information-theoretic fuzzy extractors.
Fuller, Meng, and Reyzin (Asiacrypt 2013) define a computationally-secure version of a fuzzy extractor.  It may be easier to construct computational fuzzy extractors for meaningful sources.

In this work, we construct the first (computational) fuzzy extractors that are secure for a large class of distributions where the minimum usable entropy is negative.

\textbf{Constructions:}  We construct computational fuzzy extractors from point function obfuscation.
Our constructions are inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~(Eurocrypt 2008).  Our two constructions are similar to spirit but optimize different parameters.  The first requires individual blocks of the source to have super-logarithmic entropy and corrects a constant fraction of errors.  The second construction uses sampling to reduce the required entropy of the source.  Unfortunately, sampling also reduces the effective error tolerance of the construction.  %The first construction works for high-entropy and high-error sources, while the second construction works for distributions with lower entropy and error.
\bnote{need something else to say here}
\end{abstract}


\section{Introduction}\label{sec:introduction}
Reliable key derivation is a cornerstone of authentication.  However, many sources with sufficient entropy for key derivation are noisy and provide similar but not identical values when the source is read multiple times~(examples include biometrics~\cite{daugman2004} and physically unclonable functions~\cite{pappu2002physical}). \bnote{add more examples here} To use noisy physical sources in applications, two problems must be addressed.  The first is ensuring the same value is obtained from every reading of the source.  This must be done in a way that does not eliminate all the entropy from the output~(known as information-reconciliation~\cite{bennett1988privacy}).  The second is converting the entropic source to a uniform random key~(privacy amplification~\cite{bennett1988privacy}).  Both of these problems have interactive and non-interactive versions.  For a single user~(trying to produce the same key from multiple readings of a physical source) non-interactive solutions are appropriate.  We focus on this setting.  %If both of these tasks are achieved a noisy physical source can be used like a uniformly random private key.

Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} perform both tasks non-interactively.  They consist of a pair of algorithms: \gen takes a source value $w$, and produces a key $r$ and a public helper value $p$.  The second algorithm \rep takes this helper value $p$ and a close $w'$ to reproduce the original key $r$.  Traditionally, a secure sketch~\cite{DBLP:journals/siamcomp/DodisORS08} performs information-reconciliation and a randomness extractor~\cite{nisan1993randomness} performs privacy amplification.

Unfortunately, fuzzy extractors have not yielded key derivation for all noisy sources.
This is because there is tension between security and error-tolerance.  The key should be uniform to an observer but nearby $w'$ should map to the same key.  By nearby, we mean that $\dis(w, w')\leq t$ for some metric $\dis$.  In the information-theoretic realm, increasing $t$ means a decrease in the strength of the key $r$.  This is because a fuzzy extractor can be converted into an error correcting code~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08}.  The length of the key $r$ is dictated by size of the best code that corrects $t$ errors.  Error-correcting codes are well studied and we have tight bounds on this quantity. % how many codewords may be in a code that corrects $t$ errors.

We consider the Hamming metric.  For the Hamming metric, there exists a distribution $W$ where the key derived from $W$ may be no longer than the difference between the starting entropy of the source and logarithm of the number of error patterns to be corrected~(and we have constructions of fuzzy extractors that nearly meet this bound). For the remainder of this paper we call this difference the minimum usable entropy, denoted $\Huse$.  This represents a practical problem as important biometrics have $\Huse<0$~(see \apref{sec:iris no key}).

 %let $|B_t|$ be the number of points within distance $t$.  The length of the key $|r| < \Hoo(W) - \log |B_t|$~(see~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08} for the precise conditions).  For the remainder of this paper, we denote this quantity by $\Huse = \Hoo(W) - \log |B_t|$.

To achieve security when $\Huse<0$, some restriction on the source distribution $W$ is necessary~(minimal conditions are discussed in~\apref{sec:minimal conditions}).  However, the tools for constructing information-theoretic fuzzy extractors have not produced natural restrictions.  Fuller, Meng, and Reyzin define a computationally-secure version of fuzzy extractors~\cite{fuller2013computational}.  Computational fuzzy extractors produce pseudorandom instead of truly random keys.  A computational fuzzy extractor may exist for a larger class of distributions with $\Huse<0$ than is possible information-theoretically.

In this work, we construct a computational fuzzy extractor when $\Huse<0$ for a large class of distributions.
In the computational setting, a key may be expanded once it is long enough to serve as input to a computational extractor.  We focus on providing a reasonable length key when $\Huse<0$.

Fuller, Meng, and Reyzin show that replacing an information-theoretic information-reconciliation component with a computational component is unlikely to be fruitful~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational}.  Instead, the authors suggest two alternatives: combine the information-reconciliation and privacy amplification components~(their approach) or produce a new consistent high-entropy secret instead of recovering $w$.  This is known as a fuzzy conductor~(the entropy is ``conducted'' from $w$ to a new distribution) and was introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}.  Here, we follow the second approach and use a \emph{computational} fuzzy conductor.  We present two constructions of computational fuzzy conductors for different types of source distributions $W$.  These constructions may be converted to computational fuzzy extractors using information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors~(\lemref{lem:cond and cext}).

Both of constructions are based on the obfuscation of point programs.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.  Obfuscation for this class of functions is achievable under particular number-theoretic assumptions~\cite{canetti1997towards} and from generic cryptographic hardness assumptions~\cite{wee2005obfuscating}.  We need a strong version of point obfuscation secure under composition, this is achievable under a particular number-theoretic assumption~\cite{bitansky2010strong}.  Both constructions are inspired by Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  



\textbf{\consref{cons:first construction}: }
Consider a source $W = W_1,..., W_\ell$ that is split into blocks~(over a large alphabet).  We would like to tolerate errors in $t$ individual blocks.  For each block $i$, we flip a coin $c_i$ and either obfuscate $I_{w_i}$ or pick a random point $r_i$ and obfuscate $I_{r_i}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rec$ then runs the obfuscated program $P_i$ on $w_i'$ and checks whether $P_i(w_i')=1$.  For most locations $i$, \rep can determine whether $w_i$ or a random value was the point obfuscated.  Thus, most bits of $c_i$ are recoverable. To tolerate errors we choose the coins $c$ from an error-correcting code.  The set of possible codewords forms a high entropy distribution.  This construction conducts entropy from $w$ to $c$ and is a computational fuzzy conductor.

In \consref{cons:first construction}, each block of $W$ is individually obfuscated.  This has two drawbacks, first an adversary may learn the value of some blocks which may be sensitive information.  Second, security only holds for sources where a large number of blocks have super-logarithmic entropy~(\defref{def:block guessable}).  Our second construction addresses these problems.

\textbf{\consref{cons:sampling}: }
As before we consider a source $W=W_1,...,W_\ell$ that is split into blocks.  However, instead of obfuscating individually
blocks individually, for each bit $c_i$ we randomly select a subset of blocks.  These blocks are concatenated and obfuscated together.  By sampling, we can significantly decrease the entropy requirement on individual blocks.  This paradigm is similar to the \emph{sample-then-extract} paradigm in the locally-computable extractor literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason, we call \consref{cons:sampling} \emph{sample-then-obfuscate}.

%Our construction provides several advantages over previous constructions of computational fuzzy extractors:
%\begin{itemize}
%\item The first computational fuzzy extractor where $\Huse$ may be negative for a large class of distributions.\item The first computational fuzzy extractor that allows for a constant fraction of errors.  Previous constructions could correct only  a logarithmic fraction of errors~($O(\frac{\log \ell}{\ell})$)
%\item The first computational fuzzy extractor where blocks are not required to be uniformly distributed.  Our construction simply requires a large number of blocks that are unguessable given equality queries.  This subsumes the distributions supported by~\cite[Construction 4.1]{fuller2013computational}.
%\end{itemize}

\textbf{Limitations: } In order to achieve security with $\Huse<0$, our alphabet~(for Hamming distance) must be super-polynomial in the security parameter.  Our constructions are applicable for sources where many bits are expected to err consecutively, followed by many consecutive correct bits~(known as burst errors~\cite{gilbert1960capacity}).  Constructing a computational fuzzy extractor with the above advantages and a small alphabet is an open problem.  Furthermore, we may leak sensitive information about the underlying source $W$.  In \consref{cons:first construction} it is possible to learn the value of individual blocks $W_i$.  Our two constructions are not noisy point obfuscation for any reasonable definition of obfuscation.  Dodis and Smith~\cite{DBLP:conf/stoc/DodisS05} construct noisy point obfuscation for $\Huse>>0$ using a information-reconciliation component that hides all partial information.  Constructing noisy point obfuscation for all sources remains an open problem.

The remainder of this paper is organized as follows: we cover notation and background on obfuscation and error correcting codes in \secref{sec:preliminaries}, describe computational fuzzy extractors in \secref{sec:fuzzy extractors}, and present our two constructions in Sections \ref{sec:construction} and \ref{sec:sampling} respectively.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote $X = X_1,..., X_\ell$ as the concatenation of $X_1$ to $X_\ell$.  For a set of indices $\mathcal{I}$,  $X_{\mathcal{I}}$ is the restriction of $X$ to the indices in $\mathcal{I}$.  The set $\mathcal{I}^c$ is the complement of $\mathcal{I}$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$,
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   Let $|W|$ be the size of the support of $W$ that is $|W| = |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric, for two vectors $x, y$ over $\mathcal{Z}^\ell$ the Hamming distance between $x,y$ is $d(x,y) = \{i | x_i \neq y_i\}$.  For the Hamming metric, $|B_t| = \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
Usually, we use capitalized letters for random variables and lowercase letters for samples from a random variable.

\subsection{Coding Theory}
\label{sec:coding theory}
We introduce some notions from binary coding theory.  Usually the standard class of errors  is all points within Hamming distance $t$, we will use the Hamming analog of the $Z$-channel~\cite{tallini2002capacity} where there are flips from $0\rightarrow 1$ but no bit flips from $1\rightarrow 0$.
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$.
\end{definition}

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \delta_{code})$-code if there exists an efficient procedure $\rec$ such that for all $c\in C, \forall c'\in \neigh(c), \Pr[\rec(c') \neq c] \leq \delta_{code}$.
\end{definition}

We note that for any code learning a few bits does not inform on the remainder of the bits~(the claim is a direct result of \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).

\begin{claim}
\label{cl:many locations ent}
Let $C$ be a binary code and let $\mathcal{I}^c$ be a set of indices of $C$.  Then $\Hav(C | C_{\mathcal{I}^c}) = \log |C| - |\mathcal{I}^c|$.
\end{claim}

\textbf{Notes:} 
Any code that corrects $t$ Hamming errors corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
 Codes where $\log |C| = \Theta(\ell)$ and $t = \Theta(\ell)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.
The class of errors we support in our source~($t$ Hamming errors over a large alphabet) and the class of errors corrected by our code~($t$ $0\rightarrow 1$ errors) are different.  See Constructions~\ref{cons:first construction} and~\ref{cons:sampling} for the translation between the error classes.

\subsection{Obfuscation}
Our construction uses obfuscation for a family of point functions $\mathtt{I} = \{I_w\}_{w \in \zo^*}$ defined as follows:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
The required notion of obfuscation is virtual grey-box (VGB) introduced in \cite{bitansky2010strong}. This notion is weaker then the standard notion of virtual black-box (\cite{barak2001possibility}) as it allows the simulator to run in unbounded time while making at most polynomial number of oracle queries to the function. In the following definition we also require that the obfuscation is composable and secure with respect to auxiliary input. Composable auxiliary-input VGB obfuscators for point functions are constructed in \cite[Theorem 6.1]{bitansky2010strong} from the Strong Vector Decision Diffie-Hellman assumption which is a generalization of the strong DDH assumption of \cite{canetti1997towards} for tuples of points.

\begin{definition}[$\ell$-composable obfuscation VGB obfuscation with auxiliary input \cite{bitansky2010strong}]
\label{def:obf} Let $\mathtt{I}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is a $\ell$-composable VGB obfuscator for $\mathtt{I}$ with auxiliary-input if the following conditions are met:
\begin{enumerate}
\item \emph{Functionality:} for every $ I \in \mathtt{I}$, $\mathcal{O}(I)$ is a circuit that computes the same function as $I$.
\item \emph{Virtual grey-box:}  For every PPT adversary $A$ and polynomial $p$, there exists a (possibly inefficient) simulator $S$ and a polynomial $q$ such that for all sufficiently large $n$, any  sequence of circuits $I^1,\dots,I^\ell \in \mathtt{I}_n$, (where $\ell=\poly(n)$) and for all auxiliary inputs $z\in \zo^*$:
\[
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^t)) = 1] - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | < \frac{1}{p(n)} \enspace,
\]
where $(I^1,\dots,I^\ell)[q(n)]$ is an oracle that answers at most $q(n)$ queries, and where every query of the form $(i,x)$ is answered by $I^i(x)$.
\end{enumerate}
\end{definition}

\section{Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

In this section we present our paradigm for constructing computational fuzzy extractors.  Definitions for information-theoretic fuzzy extractors can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$: if the error pattern between $w$ and $w'$ depends on the output of $\gen$ there is no guarantee about the probability of correctness. In both our constructions it is crucial that $w'$ is chosen independently of the outcome of \gen.
The definition of computational fuzzy extractors specifies a family of sources for which the fuzzy extractor works rather than the family of all sources of a given min-entropy $m$.  With $\Huse<0$ some restriction is necessary for any meaningful security~(see \apref{sec:minimal conditions}).  Instead of restricting the class of source distributions~(for which security is achieved), the definition could restrict the error model to errors that are ``likely'' to occur in the source.  We leave this as an open problem.

Fuller, Meng, and Reyzin~\cite{fuller2013computational} present two approaches for constructing a computational fuzzy extractor: analyzing the information-reconciliation and privacy amplifications components together or using a fuzzy conductor and a privacy amplification component.  We follow the second approach.
In \apref{sec:conductors}, we show that fuzzy conductors are subject to the same lower bounds on entropy loss as fuzzy extractors.  To overcome these bounds, we use a computational version of a fuzzy conductor.
We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition}
We now define a computational fuzzy conductor and a (computational)~randomness extractor.  A computational fuzzy conductor is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' ($\gen'$) and ``reproduce'' ($\rep'$) is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $X\in\{0,1\}^\ell$ and a helper string $SS\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $SS\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(X, SS)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', SS) = X] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $Y$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon_{cond}, s_{cond}}(Y |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}
A computational extractor is the adaption of a randomness extractor to the computational setting.  We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average-case:
\begin{definition}
Let $\chi$ be a finite set.
A function $\cext: \zo^\ell \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon_{ext}, s_{ext})$-average-case computational extractor} if for all pairs
of random variables $X, Y$ over $\zo^\ell, \chi$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{ext}}}((\cext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon_{ext}$.
\end{definition}

Combining a computational fuzzy conductor and an appropriate computational extractor yields a computational fuzzy extractor~(proof in \apref{sec:cond and cext}):

\begin{lemma}
\label{lem:cond and cext}
Let $\gen'$, $\rep'$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\ell$.  Let $\cext:\zo^\ell\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon_{ext}, s_{ext})$-average case computational extractor.  Define $(\gen, \rep)$ as:
\begin{itemize}
\item $\gen(w; seed):$ run $(x, ss)= \gen'(w)$ and set $r = \cext(x; seed)$, $p = (ss, seed)$.  Output $(r, p)$.
\item $\rep(w, (ss, seed)):$ recover $x = \rec'(w'; p')$ and output $r = \cext(x; seed)$.
\end{itemize}
Then $(\gen, \rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \min\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}


\section{A Computational Fuzzy Extractor when $\Huse<0$}
\label{sec:construction}
For the remainder of this work, we consider the Hamming metric over some alphabet $\mathcal{Z}$.  Our goal is to derive strong keys for a large class of sources where $0>\Huse = \Hoo(W) - \log|B_t|$.
In the computational setting a ``long enough'' key may be expanded using a computational extractor~(\lemref{lem:cond and cext}).  We focus on building a computational fuzzy conductor with meaningful output entropy.  Our first construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}.

\begin{construction}
\label{cons:first construction}
Let $n$ be a security parameter, let $\mathcal{Z}$ be an alphabet with $|\mathcal{Z}| \ge 2^n$ and let $W = W_1,..., W_\ell$ be a distribution over $\mathcal{Z}^\ell$.  Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for the family of point functions over $\mathcal{Z}$.  Let $t$ be the desired error-tolerance and let $C\subset \zo^\ell$ be a
$(\neigh_t, \delta_{code})$-error-correcting code.
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item If $c_i = 0$: $p_i = \mathcal{O}(I_{w_i})$.
\item Else: Sample $r_i \overset{\$}\leftarrow \mathcal{Z}$.
\subitem Let $p_i = \mathcal{O}(I_{r_i})$.
\end{enumerate}
\item Output $(c, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item If $p_i(w_i') = 1$: set $c_i' = 0$.
\item Else: set $c_i' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

The input $w$ is hidden in two different ways.  In locations where $c_i=1$ the block $w_i$ is information-theoretically unknown.
In locations where $c_i=0$ it is hard to find $w_i$ given the point obfuscation.  %As described in~\cite[Section 3.3]{fuller2013computational} some type of lossiness is necessary to avoid coding bounds.

There are two possible reasons for a bit $c_i'$ to be $1$.  Because the true value was $1$~(there is little chance of $1$ being incorrectly decoded as $0$ assuming $w_i'$ is independent of the sketch) and because $w_i \neq w_i'$.  However if a bit $c_i'$ is $0$ this likely means that $w_i=w_i'$ because collisions when the $c_i=0$ are unlikely~(occurring with probability $1/|\mathcal{Z}|$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips, instead of a code that corrects all Hamming weight $t$ errors.

\subsection{Security of Construction}
\label{sec:sec of construction}
\consref{cons:first construction} is secure if no distinguisher can tell whether they are working with random obfuscations or obfuscations of $W_i$.  By using the security of point obfuscation, anything learnable from the obfuscation is learnable from oracle access to the function.  This means we can consider what the adversary learns using adaptive equality queries to individual blocks.  Our construction is secure as long as enough blocks are unpredictable after adaptive queries~(we discuss minimal conditions for fuzzy extractor security in more depth in \apref{sec:minimal conditions}):

\begin{definition}
\label{def:block guessable}
Let $\mathcal{O}_{w_1,..., w_\ell}$ be an oracle that returns \[\mathcal{O}_{w_1,..., w_\ell}(i, w_i')=
\begin{cases}
1 & w_i = w_i'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1||...|W_\ell$ is a $(q, \alpha, \beta)$-\emph{unguessable block distribution} if there exists a set $\mathcal{I}\subset\{1,..., \ell\}$ of size at least $\ell -\beta$ such that for any unbounded adversary $A$ with oracle access to $\mathcal{O}$ making at most $q$ queries
\[
\forall i\in \mathcal{I}, \Hav(W_i |View(A^{\mathcal{O}_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}

%\noindent We argue security assuming that the adversary knows the value of all blocks outside of $\mathcal{I}$.
We discuss unguessable block distributions in \apref{sec:characterize}.  \consref{cons:first construction} is secure for unguessable block distributions:
\begin{theorem}
\label{thm:security of cons}
Let $\mathcal{O}$ be an $\ell$-composable VGB obfuscator with auxiliary inputs for point programs over $\mathcal{Z}$. 
Let $W$ be a $(s_{obf}, \alpha = \omega(\log n), \beta)$-unguessable block distribution.  For every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{obf}}( C | P ) \geq |C| - \beta$.% for $\epsilon' = 2\epsilon_{obf} + (\ell-\beta)2^{-(\alpha - 1)}$.
\end{theorem}

We give a brief outline of the proof here, the proof is in \apref{app:security of main cons}.
It is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))\le \ngl(n)$.  Let $\mathcal{I}$ be the set of indices that exists in \defref{def:block guessable}, the distribution $C'$ is defined as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $\mathcal{I}$.  We first note that $C'$ has sufficient entropy.  That is, $\Hav(C' |P) =\Hav(C' | C_{\mathcal{I}^c} = C'_{\mathcal{I}^c}) = |C| - |\mathcal{I}^c|$.  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P)) \le \ngl(n)$.
%Define the distribution $X$ as follows:
%\[X_i =
%\begin{cases}
%W_i & C_i = 0\\
%R_i & C_i = 1.
%\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$, since $P$ is a collection of obfuscated programs there exists a simulator, $S$~(outputting a single bit), such that $\Pr[D(C, P)=1]$ is close to $\Pr[S^{\mathcal{O}}(C)=1]$.
\item Show that even an unbounded $S$ making a polynomial number of queries to the stored points cannot distinguish between $C$ and $C'$.  That is, $\delta(S^{\mathcal{O}}(C),S^{\mathcal{O}}(C'))$ is small.
\item By the security of obfuscation, $\Pr[S^{\mathcal{O}}(C')=1]$ is close to $\Pr[D(C', P)=1]$.
\end{itemize}

%The prior lemma implies that the $View$ does not increase the probability of any codeword by much.
%\begin{lemma}
%\label{lem:no codeword high prob}
%For all $c\in C|C_{\mathcal{I}^c}$,
%\[
%\expe_{View(S)} \Pr[C=c | C_{\mathcal{I}^c} \wedge View(S)] \leq \Pr[C=c | C_{\mathcal{I}^c}]+2^{-(\alpha-1)}.
%\]
%\end{lemma}
%\begin{proof}
%We begin by noting there must exist some $c\in C|C_{\mathcal{I}^c}$ such that
%\[
%\expe_{view\leftarrow View}\Pr[C = c | C_{\mathcal{I}^c}] \le \frac{1}{| C| C_{\mathcal{I}^c}|}.
%\]
%Otherwise the sum of the probabilities of $c\in C | C_{\mathcal{I}^c}$ will be greater than $1$.  Let $c_{min}$ be one such value.
%Let $c^*$ be an arbitrary $c^*$ in the support of $C|C_{\mathcal{I}^c}$.  Suppose that
%\begin{align*}
%\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\Pr[C=c^* | C_{\mathcal{I}^c} \wedge view ] &\geq \Pr[C=c^* | C_{\mathcal{I}^c} ]+ 2^{-(\alpha-1)} \\
%\end{align*}
%This means that the difference between the expected probability of $c_{min}$ and $c^*$ is at least $2^{-\alpha+1}$.  Thus, there is an unbounded distinguisher that can tell between the two cases, violating the statistical distance property of   \lemref{lem:codewords in I close}.  This is a contradiction and completes the proof.
%\end{proof}
%\begin{corollary}
%\label{cor:avg min after view}
%$\Hav(C| View(S))\geq \Hav(C | View (A) \wedge C_{\mathcal{I}^c}) \geq \min \{ \Hav(C | C_{\mathcal{I}^c}), \alpha-1\} -1= \min\{ |C| - |\mathcal{I}^c|, \alpha-1\} -1$.
%\end{corollary}
%\begin{proof}
%%Let $e$ be the exponent of the particular negligible value above~(note $e = \omega(1)$).
%Taking the negative log of \lemref{lem:no codeword high prob} one has:
%\begin{align*}
%\Hav(C_i |  View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot)}))
% &= \\
% -\log \left(\expe_{view \leftarrow View}\max_{c\in C|C_{\mathcal{I}^c}}\Pr[C =c | view \wedge C_{\mathcal{I}^c} ]  \right) &\geq -\log (\Pr[C=c^* | C_{\mathcal{I}^c} ]+ 2^{-\alpha+1}) \\
% &\geq \min \{-\log P_c, -\log 2^{-\alpha+1}\} -1\\
% &= \min \{ \Hav(C | C_{\mathcal{I}^c}), \alpha-1\}-1
%\end{align*}
%\end{proof}

\subsection{Correctness of \consref{cons:first construction}}
\label{sec:correct first cons}
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \ell/2^n = \ngl(n)$.
\end{lemma}
\begin{proof}
Recall that $c$ contains at most $\ell$ $1$s.  Since $w'$ is chosen independently of the points $r_i$, this probability is the size of each block, that is $\Pr[r_i =w_i']  = 1/2^n$. Since the $r_i$ are chosen independently, one has,
\[
\Pr[\text{no $1\rightarrow 0$ flips}] \geq 1-\Pr[\text{some }r_i = w_i']\geq 1-\sum_{i=1}^\ell \Pr[r_i = w_i'] \geq 1-\ell \frac{1}{2^n}.
\]
\end{proof}

We now consider the number of possible $0\rightarrow 1$ bit flips in $c$.  A $0\rightarrow 1$ flip on index $i$ occurs when two conditions are met $w_i\neq w_i'$ and $c_i = 0$.  Since the first conditioned is only fulfilled at most $t$ times, we have the following lemma:

\begin{lemma}
\label{lem:correct of cons}
Let $C$ be a $(\neigh_t, \delta_{code})$-code.  When $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$
\[
\Pr[\rep( w', P) = R] \geq 1-\left(\frac{\ell}{2^n}+\delta_{code}\right).
\]
That is, \consref{cons:first construction} is correct with error at most $\ell/2^n+\delta_{code}$.
\end{lemma}

\textbf{Note: }By using a more structured type of code, the necessary error tolerance may be decreased from $t$ to  $(1/2+O(1))t$ by arguing that roughly half of the mismatches between $w, w'$ occur where $c_i =1$.  This requires a code where most codewords have Hamming weight close to $1/2$.

%\bnote{even if all $t$ are inserted into the bad block its okay right now.  My goal is to decrease the error tolerance of the code below $t$.}
%We now turn to the question of how many $0\rightarrow 1$ bits flips occur between $c, c'$.  Recall that $\dis(w, w')\leq t$ furthermore recall that $w, w'$ are selected independently of the fuzzy extractor, in particular, independently of $c$.  We break the indices of $c = c_1,..., c_\ell$ into two components those where $c_i=1$ and those $c_i = 0$.  Denote these two components by $c_{I_1}$ and $c_{I_0}$.  Our goal is to bound how many of the places where $w, w'$ are inserted into $c_{I_0}$.  Indices $i$,  where $w_i \neq w_i'$ and $c_i = 1$ will be ``corrected'' as $w_i'$ yield $c_i' = 1$ with overwhelming probably~(see \lemref{lem:no 1 to 0 flips}).  Since $w_{err}$ be set of indices where $w, w'$ differ then for all $j\in w_{err}, \Pr[j\in c_{I_0}] = 1/2$.  This is a binomial distribution with $p=1/2$ and $n=t$.  Denote by $X =|\{j | w_j \neq w_j' \wedge j\in c_{I_1}|$. Thus, $\expe[X] = t/2$.  Using Hoeffding's inequality~\cite{hoeffding1963probability}, one has that
%\begin{align*}
%\Pr[X\leq t(1/2+\alpha)] \leq 1-e^{-2\alpha^2 t}
%\end{align*}
%This leads us to the following lemma:
%\begin{lemma}
%\label{lem:correctness holds}
%If $t = \omega(\log n)$ and that $\dis(w, w')\leq t$ then for any constant $\alpha>0$, then $\Pr[\dis(c, c')\geq t(1/2+\alpha)] \leq 1-e^{-2\alpha t} = 1-\ngl(n)$.
%\end{lemma}
%\bnote{This analysis is done assuming that the two sets are the same size.  Need to worry about variation in the number of $1$s and $0$s in $c$.  Need to show that for most codes (linear?) the variance in Hamming weight is small.}
%\textbf{Note:} If $t = O(\log n)$ the only error tolerance for $C$ that results in decoding with overwhelming probability is $t$.  However, in most use cases, we expect $t=\omega(\log n)$.

Together, with the arguments in \secref{sec:sec of construction} we have the construction is secure and correct for unguessable block distributions:
\begin{theorem}
Let $\mathcal{W}$ be the family of $(q,\alpha= \omega(\log n),  \beta)$-unguessable distributions over $\mathcal{Z}^\ell$ for any $q = \poly(n)$.  Furthermore, let $C$ be a $(\neigh_t, \delta_{code})$-code.  Then for $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\mathcal{Z}^\ell, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta_{code} + \ell/2^n$ and resulting entropy $\tilde{m} =\log |C| - \beta$.
\end{theorem}
\begin{proof}
Security is a result of \thref{thm:security of cons}.  Correctness follows from \lemref{lem:correct of cons}.
\end{proof}


%\subsection{Supported Sources}
%\label{sec:supported sources}
%\bnote{need to complete this section and redo the proof in  \secref{sec:sec of construction} with this type of source.}
%
%We now describe a set of high entropy sources appropriate for which the proof outline given in \secref{sec:sec of construction} holds.  We need a distribution where after $q$-equality queries, a large number of blocks still have super-logarithmic entropy.
%
%As long as $\beta$ is small enough, we can show security of \consref{cons:first construction}~(replace \lemref{lem:blocks unguessable after queries} with an unguessable source, the remaining arguments follow).


\subsection{Discussion}
\label{sec:discussion}
Security of \consref{cons:first construction} relies on a large number of dimensions which are unpredictable.  Correction occurs for a small number of dimensions that are allowed to vary completely.  As described in \secref{sec:sec of construction}, \consref{cons:first construction} is not secure for an arbitrary min-entropy source, it must contain a significant number of dimensions that are hard to guess by equality queries.

To show that $\Huse$ can be negative for \consref{cons:first construction}, we first calculate the size of the Hamming ball.  We allow $t$ errors with an alphabet $\mathcal{Z}$ of size $2^n$.  This means that
\begin{align*}
\log |B_t| &= \log \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i\\
&> \log {\ell \choose t} (2^n-1)^t\\
& =\Theta(tn) + \log {\ell\choose t}
% \log (2^n)^{H_{2^n}(t/\ell)\ell - o(\ell)} =n( H_{2^n}(t/\ell)\ell -o(\ell) )
\end{align*}

We consider what entropy is necessary for security.  The simplest type of unguessable block distribution is where each block is independent and has super-logarithmic entropy~(\clref{cl:independent high ent}).  For this type of source the required entropy is $\Hoo(W) = \ell\omega(\log n)$.  This yields:
\[
\Huse = \Hoo(W) - \log |B_t| < \ell \omega(\log n) -\left( \Theta(t n) + \log {\ell \choose t}\right).
\]
When $t =\Theta(\ell)$ and the entropy of each block is $o(n)$, then $\Huse<0$ and the output entropy is $|C| -\beta$~(if $C$ is a constant rate, this is $\Theta(\ell)$).
%Thus, \consref{cons:first construction} achieves for security for unguessable block distributions with negative $\Huse$ when $t = \Theta(\ell)$.  %We achieve security when $\Huse<0$, constant error tolerance, and extract a constant fraction of $n$ when $t = \Theta(\ell)$ and $\ell = O(n)$.
%When $ \ell , t, n$ to be the same order provides a natural balance of parameters.

\textbf{Limitations of \consref{cons:first construction}:}  There are three major drawbacks to \consref{cons:first construction}.   First, we require blocks to be super-polynomial in size.  Second, we only obtain a single bit from each block.  Third, since each block is individually obfuscated, we leak information about individual blocks.  To compensate for this leakage, we need most blocks to be unguessable given equality queries.  Our second construction addresses this last weakness.

\section{Sample-then-Obfuscate}
\label{sec:sampling}
\consref{cons:first construction} is a computational fuzzy extractor with $\Huse<0$.  In this section, we present a generalization where blocks are not individually obfuscated, instead several blocks are obfuscated together~(these blocks are selected using an oblivious sampler).  The main advantage of this approach is reduces the required entropy in each block of the source.  Unfortunately, obfuscating multiple blocks together decreases the effective error tolerance.  Thus, we have a tradeoff between individually required entropy and supported error tolerance.  In ordering for sampling to work properly, we need a source where a constant fraction of blocks are likely to contribute some entropy.

\begin{definition}
\label{def:unordered source}
A distribution $W = W_1,..., W_\gamma$ is an $(\alpha, \beta)$-unordered block source if there exists a set of indices $\mathcal{I}$ where $|\mathcal{I}| \geq \gamma - \beta$ such that the following holds:
\[
\forall i\in \mathcal{I}, \forall w_1,..., w_\gamma \in W_1,..., W_\gamma, \Hoo(W_i | W_1 = w_1,..., W_{i-1}=w_{i-1}, W_{i+1}=w_{i+1},..., W_\gamma = w_\gamma) \geq \alpha.
\]
\end{definition}

\defref{def:unordered source} is a generalization of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}) where blocks must have entropy conditioned on the value of previous blocks.  An unordered block source is also a strengthening of an unguessable block distribution~(\defref{def:block guessable}).  An unordered block source requires there to be worst case entropy while an unguessable block distribution only requires average conditional entropy.  However, we achieve security for significantly lower entropy levels than for unguessable block distributions.

We use an algorithm that selects a fixed size random subset of $\{1,..., \gamma\}$.  Let $\sample_{\gamma, \eta}(\cdot)$ be an algorithm that outputs a random subset of $\{1,..., \gamma\}$ of size $\eta$ and let $r_{sam}$ the number of required coins for $\sample_{\gamma, \eta}$.
\begin{construction}
\label{cons:sampling}
Let $n$ be a security parameter.
Let $\mathcal{Z}$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source where each $W_i$ is over $\mathcal{Z}$ and $\gamma = \Omega(n)$.  Let $\ell = O(\poly(n)) = \omega(\log n)$ and let $C\subset \zo^\ell$ be a $(\neigh_{t'}, \delta_{code})$.   Let $\eta = \omega(\log n) = o(\gamma)$ and let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for the family of point functions over $ \mathcal{Z}^\eta$.  Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $c\leftarrow C$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Sample $\lambda_i\overset{\$}\leftarrow \zo^{r_{sam}}$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\eta,\gamma}( \lambda_i)$
\item If $c_i = 0$:
\subitem Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\subitem Set $\rho_i = \mathcal{O}(I_{v_i})$.
\subitem Set $p_i = \rho_i, \lambda_i$.
\item If $c_i = 1$: Sample $r_i \overset{\$}\leftarrow \mathcal{Z}^{\eta}$.
\subitem Let $p_i = \mathcal{O}(I_{r_i}), \lambda_i$.
\end{enumerate}
\item Output $(c, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Parse $p_i$ as $\rho_i, \lambda_i$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\gamma, \eta}(\lambda_i)$.
\item Set $v_i' = w_{j_{i, 1}},..., w_{j_{i, \eta}}$.
\item If $\rho_i(v_i') = 1$ set $c_i' = 0$.
\item Else set $c_i' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.37in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

The main change in \consref{cons:sampling} is that the obfuscated values are concatenated blocks symbols of $W$.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.  A crucial difference is the use of a computational primitive~(obfuscation) allows us to sample multiple times and only argue that each $v_i$ has individual entropy.  In the information-theoretic setting, it would be necessary to argue about the joint entropy of the obfuscated values.  Our construction uses a na\"{i}ve sampler that takes truly random samples, but the public randomness may be substantially decreased by using more sophisticated samplers.  Goldreich provides an introduction to samplers~\cite{goldreich1997sample}.

As before we show security and correctness of \consref{cons:sampling}.  For security, we argue that each of the $v_i$ is unguessable.  For correctness we show that the induced error rate in $v$ and $v'$ is a small constant~(with overwhelming probability) so that $c'$ will be corrected to $c$ with overwhelming probability.

\subsection{Security of \consref{cons:sampling}}
In order to show security \consref{cons:sampling}, we show that with overwhelming probability each of the selected $V_i$ has super-logarithmic entropy on its own~(samplers will select ``enough'' of the good blocks of $W$ each time).  We can then argue that $V_1,..., V_\ell$ forms a block-unguessable distribution.  Then \consref{cons:sampling} is just the first construction applied to $V_1,.., V_\ell$ and security follows by \thref{thm:security of cons}.  We begin by showing that each $V_i$ is statistically close to a high entropy distribution~(proof in \apref{sec:proof of sampling lemma}).

%\bnote{think about how to make this work when $\eta = \Theta(1)$.  Will make probabilities gross.  Probably not for submission.}
\begin{lemma}
\label{lem:sampling works}
Let $W = W_1,..., W_\gamma$ be a $(\alpha = \Omega(1), \beta\le \gamma(1- \Theta(1)))$-unordered block source and let $\eta = \omega(\log n) \le  o(\gamma)$.  Consider some fixed $i$, there exists $\epsilon_{sam} = O(e^{-\eta}) = \ngl(n)$ and $\alpha' = \alpha\eta(\gamma-\beta-\eta)/\gamma = \omega(\log n)$ such that 
\[
\Pr_{\lambda\leftarrow \Lambda}[\Hoo(V_i | \Lambda= \lambda) \geq \alpha'] \geq 1- \epsilon_{sam}.
\]
\end{lemma}
%
%\begin{lemma}
%
%Let $W = W_1,..., W_\gamma$ be a $(\alpha = \Omega(1), \beta \leq \gamma(1/2-\Theta(1)))$-unordered block source.  Let $\eta = \Omega(\log n)$. Consider some fixed $i$, the distribution $V_{i}, \Lambda_i$ is $(\ngl(n)+ 2^{-\Omega(n)})$-close to a distribution $V', U_{r_{sam}}$ where for all $u\in U_{r_{sam}}$ $V' | U_{r_{sam}}=u$, $\Hoo(V' | U_{r_{sam}}=u) \geq \Omega(\alpha\times (1-2\times \beta) \times \eta) = \omega(\log n) = \alpha'$.
%\end{lemma}
%\begin{proof}
%By~\cite[Lemma 9]{vadhan2003constructing}, noting that the na\"{i}ve sampler is optimum in necessary samples and we use $\eta = \omega(\log n)$ samples.  \bnote{need someone to check parameters, in the abstract, I am fairly sure the statement makes sense, but I could easily have messed something up}\bnote{even if this is right it needs to be expanded.  too much for a reader to digest.}
%\end{proof}

\noindent
We can then argue that all $V_i$ simultaneously have individual entropy with good probability:
\begin{corollary}
\label{cor:samp sec}
Let $W = W_1,..., W_\gamma$ be a $(\alpha = \Omega(1), \beta \leq \gamma(1-\Theta(1)))$-unordered block source.  Let $\eta = \omega(\log n) = o(\gamma)$ and let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works}.  Then $\Pr_{\lambda\leftarrow \Lambda}[\exists i, \Hoo(V_i | \Lambda = \lambda) \not \ge \alpha'] \leq 1-\ell\epsilon_{sam}$.
%[V_i(V, \Lambda)$ is $(\ell\epsilon_{sam})$-close to a distribution $(V', U_{\ell\times r_{sam}})$ where for $u\in U_{\ell\times r_{sam}}$ for all $i$, $\Hoo(V_i' | U_{\ell\times r_{sam}} =u)\geq \alpha'$.
\end{corollary}
\begin{proof}
Union bound over the probability in \lemref{lem:sampling works}.
%Hybrid argument over the statistical distance in \lemref{lem:sampling works}.
\end{proof}

In the \apref{sec:characterize} we show that any distribution where each individually block has super-logarithmic min-entropy forms a unguessable block distribution~\clref{cl:all blocks entropy}.  This allows us to conclude:
\begin{corollary}
\label{cor:v are unguessable}
Let $W, V, \Lambda$ be as in \corref{cor:samp sec}.
Let $q = \poly(n)$, for $\alpha'' =\alpha'-1-\log (q+1) =  \omega(\log n)$ the distribution $V , \Lambda$ is $\ell \epsilon_{sam}$ statistically close to a $(q, \alpha'', 0)$-unguessable block distribution.
\end{corollary}

Finally, since $V_1,..., V_\ell$ is statistically close~(conditioned on the randomness of $\sample$) to a unguessable block distribution, \consref{cons:sampling} is \consref{cons:first construction} applied to $V_1,..., V_\ell$.
\begin{corollary}
\label{cor:samp unguess}
Let $n, \mathcal{Z}, W, \epsilon_{sam}$ be as in \corref{cor:samp sec} and let $\mathcal{O}$ is an $\ell$-composable VGB obfuscator with auxiliary inputs for point programs over $\zo^{\beta\times \log |\mathcal{Z}|}$, then for every $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C | P) \geq |C|$. %for $\epsilon' = (2\epsilon_{obf} + \ell(\epsilon_{sam}+ 2^{-(\alpha''-1)})) = \ngl(n)$.
\end{corollary}
\begin{proof}
Result of \corref{cor:v are unguessable} and \thref{thm:security of cons} noting that $\ell \epsilon_{sam} = \ngl(n)$.
\end{proof}

\subsection{Correctness of \consref{cons:sampling}}
\label{sec:correct sampling}
The argument that $1\rightarrow 0$ flips are unlikely carries over from \consref{cons:first construction}.  This probability is at most $\ell/\mathcal{Z}^\eta$.  We now show that there are few enough $0\rightarrow 1$ flips with overwhelming probability.  Let $t'$ be the capacity of $C$.  We show $\Pr_{(v, v')\leftarrow (V, V')}[\dis(v, v')\ge t'] <\ngl(n)$.  The proof is in \apref{sec:sampling errors}.

\begin{lemma}
\label{lem:sampling errors}
Let $n, W$ be as above and Let $C$ be a code that corrects $t'$ $0\rightarrow 1$ bit errors where $t' =\Theta(\ell)$.  Let $\mu = -\log(1-t'/(2\ell))/2 = \Theta(1)$, if $t \leq \mu(\gamma - \eta)/\eta$, then $\Pr[\dis(v, v')\leq t']\geq 1-O(2^{-\ell})$ where the probability is over the coins of $\gen$.  %That is, for any $t\leq \mu(\gamma-\eta)/\eta$, \consref{cons:sampling} is correct with overwhelming probability.
\end{lemma}

\begin{corollary}
Let $\eta= \omega(\log n)$ and $\ell = O(\poly(n)) = \omega(\log n)$ let $C\subset\zo^\ell$ be a $(\neigh_{t'}, \delta_{code})$ for $t' = \Theta(\ell)$ and $\delta_{code} = \ngl(n)$.  Then when
\[
\dis(w, w')\le \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta} = \Theta(\frac{\gamma-\eta}{\eta}) = \Theta(\gamma/\eta)
\]
and $(R, P)\leftarrow \gen(w)$, then
\[
\Pr[\rep( w', P) = R] \geq 1-\frac{\ell}{|\mathcal{Z}|^\eta} - O(2^{-\ell}) -\delta_{code}= 1-\ngl(n).
\]
That is, \consref{cons:sampling} is correct with overwhelming probability.

\end{corollary}

Combining the last two sections, we have that \consref{cons:sampling} is secure and correct:
\begin{theorem}
Let $n$ be a security parameter.
Let $\mathcal{W}$ be the family of $(\alpha = \omega(1), \beta\leq \gamma(1/2-\Theta(1)))$-unordered block sources over $\mathcal{Z}^\gamma$ where $\gamma = \Omega(n)$.  Let $\eta = \omega(\log n) = o(\gamma)$ and $\ell = O(\poly(n)) = \omega(\log n)$.  Let $C$ be a $(\neigh_{t'}, \delta_{code})$ where $t' = \Theta(\ell)$.  Then for every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that \consref{cons:sampling} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ for
\begin{align*}
t&\le \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta} \\
\tilde{m} &=|C|\\
\delta &= O(\ell/|\mathcal{Z}|+2^{-\ell} +\delta_{code}).
\end{align*}
\end{theorem}

\subsection{Discussion of \consref{cons:sampling}}
\consref{cons:sampling} achieves $\Huse<0$ for an interesting setting of parameters.  The required entropy of  an unordered block source is $\alpha\times (1-\beta )\times \gamma = \Theta(\gamma)$.  We are able to correct $O(\gamma/\eta)$ errors.
This yields:
\begin{align*}
\Huse &= \Hoo(W) -\log |B_t| \\
&< \Theta(\gamma)- t \log |\mathcal{Z}|\\
&= \Theta(\gamma) - \Theta(\gamma/\eta) \log |\mathcal{Z}|
\end{align*}
That is, \consref{cons:sampling} achieves $\Huse<0$ when the starting alphabet is super polynomial~(noting that for super polynomial size $\mathcal{Z}$ we can set $\eta$ to be super logarithmic and $o(\log |\mathcal{Z}|)$.  If we are willing to accept $\Huse\geq 0$ then \consref{cons:sampling} works for polynomial size alphabet $\mathcal{Z}$.  However, in this setting $\Huse =\Omega(\gamma)$ and known information-theoretic fuzzy extractors provide superior performance.
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

\section{Use of Fuzzy Extractors for Biometrics}
Fuzzy extractors were developed to derive stable from noisy sources.  Biometrics represent an important noisy physical source.  Unlike physical unclonable functions, biometrics are fixed distributions.  Biometrics cannot be redesigned to  increase their entropy or reduce their error rates.  Many biometric systems have incorporated fuzzy extractors to improve system security~(see the work of Jain, Nadakumar, and Nagar for a survey~\cite{jain2008biometric}).  Unfortunately, most work measures system security using false accept rate vs. false reject rate.  This assumes an adversary that creates inputs to the \rec algorithm without looking at the public helper data.  False accept rate provides no bound on helper data information-leakage.  Indeed the helper data may completely reveal the original input.  In high security applications, measuring false accept rate vs. false reject rate is insufficient to argue system security.  

\label{sec:iris no key}
As an example, we focus on the human iris.  The iris is believed to be the best biometric for high security applications~\cite{prabhakar2003biometric}.  The best estimate for iris entropy is $250$ bits~\cite{daugman2004}.  Daugman uses specialized wavelets to derive a $2048$ bit string called an iris code.  The transform computes a phase angle at $1024$ locations, this angle is then quantized into a positive/negative value in the two dimensions. There has been considerable research since the work of Daugman~\cite{daugman2004}.  These works do not dramatically alter the entropy measurement or error rate necessary for a reasonable false reject rate.  We use the Daugman's parameters for our calculations.

The precise number of errors that must be tolerated depends on the desired false reject rate (how often the correct key is produced).  For a false reject rate of $\le 80\%$, a $t$ of approximately $205$ is required.%\footnote{Daugman  improves his true accept rate using a masking vector, where bits that are not considered ``trustworthy'' are excluded from the comparison.  The comparison between the two readings is then made using bits not excluded by either image.  It is not clear how to extend into the fuzzy extractor setting.  Even with the masking vector $\Huse$ is negative for irises.}  
We have the following calculation for $\Huse$:
\begin{align*}
\Huse &= \Hoo(W) - \log |B_t|\\
&= 249 - \log \sum_{i=0}^{205} {2048 \choose i} = -707.
\end{align*}
Thus, if we were to use a fuzzy extractor that worked for all distributions with the same entropy and error characteristics no key is possible~(see \apref{sec:minimal conditions} for minimal conditions for fuzzy extractor security).
Although irises are believed to be the strongest biometric a fuzzy extractor can provide no security without using some additional property of the iris distribution.

\section{Minimal Conditions for Fuzzy Extractor Security}
\label{sec:minimal conditions}

A necessary condition for fuzzy extractor security is that an adversary should not be able to learn the key simply by inputting any point into the \rep algorithm.  This means a negligible portion of the source distribution $W$ lies within any Hamming ball.  We make this intuition formal here:

\begin{definition}
\label{def:fuzzy min-ent}
A distribution $W$ in a metric space $(\mathcal{M}, \dis)$ has $(t, k)$-fuzzy min-entropy, denoted $\Hfuzz(W) \ge k$ if the following holds:
\[
\forall m\in \mathcal{M}, Pr_{w\in W}[\dis(w, m) \leq t] \leq 2^{-k}.
\]
\end{definition}
\begin{lemma}
\label{lem:fuzz necessary}
Let $n$ be a security parameter and let $W$ be a distribution over $(\mathcal{M}, \dis)$.
If $\Hfuzz (W) = \Theta(\log n)$ there is no $(\mathcal{M}, W, \kappa, t)$-computational fuzzy extractor that corrects is secure against nonuniform adversaries of size $\log \mathcal{M} + |\rep|$ for negligible $\epsilon$~(and thus no fuzzy extractor) for $\kappa =\omega(\log n)$.
\end{lemma}
\begin{proof}
Let $W$ be a distribution where $\Hfuzz(W) = \Theta(\log n)$.  This means that there exists a point $m\mathcal{M}$ such that $\Pr_{w\in W}[\dis (w, m)\leq t] \geq 1/\poly(n)$.  Consider the following distinguisher $D$:
\begin{itemize}
\item On input $r, p$.
\item If $\rep(m, p) = r$, output $1$.
\item Else output $0$.
\end{itemize}
First note that $|D|$ is of size $\log |\mathcal{M}|+ |\rep|$.  Clearly, $\Pr[D(R, P) = 1]\geq 1/\poly(n)$, while $\Pr[D(U_\kappa, P)=1 ]\leq 1/2^{-\kappa}$.  Thus, when $\kappa = \omega(\log n)$:
\[
\delta^D((R, P), (U_\kappa, P))\geq \frac{1}{\poly(n)} - \frac{1}{2^{-\kappa}} = 1/\poly(n).
\]
\end{proof}
\lemref{lem:fuzz necessary} generalizes to interactive protocols, $D$ only provides an input to the protocol and looks at the output.  This means that fuzzy min-entropy is also a necessary condition for interactive solution.  %The original fuzzy extractors paper of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08} separated the starting entropy of $W$ and the desired error tolerance.  
If we wish to support parameter regimes where $\Huse<0$ there is some distribution with $\Hfuzz(W)=0$~(consider some fixed point $m\in\mathcal{M}$ and let $W$ be the uniform distribution over points within distance $t$).  Thus, if the analysis of a fuzzy extractor is for all input distributions with a particular $\Huse$, no key can be output when $\Huse<0$.   This motivates our restriction to meaningful classes of source distributions with $\Huse<0$.
%This means we inherently must talk about the security and errors together.  

In this work we consider the Hamming distance over some alphabet $\mathcal{Z}$.  There are two minimal types of distributions where $\Hfuzz(W)\geq \omega(\log n)$, the first is where the $(t+1)$-st least entropic block has super-logarithmic entropy~(the $t$ most entropic blocks are essentially free to the adversary).  The other is that there $t+\omega(\log n)$ blocks that contribute some entropy~(there is a continuum between these two types).  Definitions~\ref{def:block guessable} and~\ref{def:unordered source} were developed for these types of sources respectively.  However, we do not achieve security when $\Hfuzz(W)= \omega(\log n)$ for either definition, this is an open problem. 

\section{Fuzzy Conductors}

\label{sec:conductors}
Fuzzy extractors are known to have strong upper bounds on remaining entropy based on the best error correcting codes~(if they provide the same guarantee for all input distributions with the same entropy and error tolerance).  Fuller, Meng, and Reyzin show that computational information-reconciliation techniques are subject to similar bounds~\cite{fuller2013computational}.  They suggest these bounds may be avoided by outputting a fresh random variable.  This is known as a fuzzy conductor~\cite{KanukurthiR09}.
\begin{definition}
A $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-\emph{fuzzy conductor} with error $\delta$ is a pair of randomized procedures, ``generate''~(\gen') and ``reproduce''~(\rep'), with the following properties:
\begin{enumerate}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $x\in\zo^*$ and a helper string $ss\in\zo^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $ss\in\zo^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w, w')\leq t$, if $X, SS$ were generated by $(X, SS)\leftarrow \gen'(w)$, then $\Pr[\rep(w', SS) = X]\geq 1-\delta$~(over the coins of $\gen', \rep'$).  If $\dis(w, w')>t$, then no guarantee is provided about the output of $\rep'$.
\item The security property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $X$ is close to a high entropy distribution $Y$.  That is, $\exists Y$ with $\Hav(Y | SS ) \geq \tilde{m}$ such that $\Delta((X, SS), (Y, SS))\leq \epsilon$.
\end{enumerate}
\end{definition}

\noindent In this section, we show that fuzzy conductors are subject to the same lower bounds as fuzzy extractors.  This motivates our use of a computational fuzzy conductor in \defref{def:comp fuzzy cond}.
We borrow the following notation from the work of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08}:
\begin{itemize}
\item A $(\mathcal{M}, K, t)$ code is a subset of $\mathcal{M}$ of size $K$ where a procedure exists that corrects $t$ errors.
%\item $K(\mathcal{M}, t)$ is the largest $K$ for which there exists an $(\mathcal{M}, K, t)$-code.
\item $K(\mathcal{M}, t, S)$ is the largest $K$ such that there exists an $(\mathcal{M}, K, t)$ code all of whose $K$ points belong to $S$.
\item $L(\mathcal{M}, t, m) = \log (\min_{|S| = 2^m} K(\mathcal{M}, t, S))$.  This is the performance of worst error-correcting code for an arbitrary subset of size $2^m$.
\end{itemize}
Intuitively, $K(\mathcal{M}, t, S)$ is the size of the best code that covers a given subset and $L(\mathcal{M}, t, m)$ is size of the code that covers the hardest subset in the metric space.  
\begin{lemma}
The existence of a $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-fuzzy conductor implies that $\tilde{m}\leq L(\mathcal{M}, t, m) +1 -\log (1-2\epsilon)$.
\end{lemma}
\begin{proof}  Assume $(\gen', \rep')$ is a fuzzy conductor with the parameters stated.  Let $A$ be a set of size $2^m$ in $\mathcal{M}$ and let $W$ be the uniform distribution over $A$.  Define $(X,SS) \leftarrow \gen'(W)$.  Then there must exist a distribution $Y$ with $\Hav(Y|SS) \geq \tilde{m}$ such that $\Delta((X,SS), (Y, SS))\leq \epsilon$.  By Markov's inequality, there exists some set $B_{SS}$ such that $\Pr[ss\in B_{SS}] \geq 1/2$ and $\forall s\in B_{SS}$ one has $\Delta(X | SS = s, s ), (Y | SS = s, s)<2\epsilon$.  Now applying Markov's inequality of $\max_{Y} \Pr[Y=y | SS=s]$, there exists a set $B_{SS'}$ such that $\Pr[SS\in B_{SS'}]>1/2$, and for all $s\in B_{SS'}$, $\Hoo(Y| SS =s ) \geq \tilde{m}-1$.  Denote by $s^*$ a value in $B_{SS}\cap B_{SS'}$~(one value must exist).  Then $\Delta((X | SS =s^* , s^*), (Y| SS = s^*, s^*))\leq 2\epsilon$ and $\Hoo(Y|SS=s^*)\geq \tilde{m}-1$.  Denote by the set $A'$ the possible values of $X$ when $SS=s^*$.  For the statistical distance property to hold, $|A'| \geq  (1-2\epsilon)2^{\tilde{m}-1}$.  Associate with every $x\in A'$ some $w\in S$ which could have produced $x$ with nonzero probability given $SS=s^*$, and call this map $C$.  $C$ defines an error correcting code with the required parameters.
\end{proof}

\section{Characterizing unguessable block distributions}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and thus a little unwieldy for a distribution.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.
The majority of the difficulty in characterizing \defref{def:block guessable} is that different blocks may be dependent, so an equality query on block $i$ may shape the distribution of block $j$.  Thus, we begin with the case of independent blocks.  In all of the examples that follow we denote the adversary by $S$ as we need security against the computationally unbounded adversary defined in VGB obfuscation~(\defref{def:obf}).

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,  ... , W_\ell$ be a source in which all blocks $W_i$  are mutually independent.  Let $\alpha$ be a parameter.  Let $\mathcal{I}\subset \{1,..., \ell\}$ be a set of indices such that for all $i\in\mathcal{I}$, $\Hoo(W_i ) =\alpha $.  Then for any $q$, $W$ is a $(q, \alpha - \log (q+1), \ell - |\mathcal{I}|)$-unguessable block distribution.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then $W$ is a $(q, \omega(\log n), \ell - |\mathcal{I}|)$-unguessable block distribution.
\end{claim}
\begin{proof}
It suffices to show that for all $i\in \mathcal{I}, \Hav(W_i |View(A^{\mathcal{O}_{W}(\cdot, \cdot)}) = \alpha -\log (q+1)$.
We can ignore queries for all blocks but the $i$th, as the blocks are independent. Furthermore, without loss of generality, we can assume that no duplicate queries are asked, and that the adversary is deterministic (we can hardwire the best coins as nonuniform advice). Let $A_1, A_2, \dots A_q$ be the random variables representing the oracle answers for an  adversary $S$ making $q$  queries about the $i$th block. Each $A_j$ is just a bit, and at most one of them  is equal to 1 (because duplicate queries are disallowed). Thus, the total number of possible responses is $q+1$. Thus, we have the following,
\begin{align*}
\Hav(W_i | View(A^{\mathcal{O}_{W}(\cdot, \cdot)}) &= \Hav(W_i| A_1, \dots, A_q)\\
&=\Hoo(W_i) - |A_1, \dots, A_q|\\
&=\alpha - \log (q+1)\,,
\end{align*}
where the second line follows from the first by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
\end{proof}
\noindent In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} show a construction for block-fixing sources, where each block is either uniform or a fixed symbol~(block fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} shows that \defref{def:block guessable} captures, in particular, this class of distributions.
However, \defref{def:block guessable} captures more distributions.  We now consider more complicated distributions where blocks are not independent.

\begin{claim}
\label{cl:each block from single seed}
Let $f:\zo^e \rightarrow \mathcal{Z}^\ell$ be a function.  Furthermore, let $f_i$ denote the restriction of $f$'s output to its $i$th coordinate.  If for all $i, f_i$ is injective then $W = f(U_e)$ is a $( q, e - \log (q+1), 0)$-unguessable block distribution.
\end{claim}
\begin{proof}
Since $f$ is injective on each block, $\Hav(W_i | View(A^{\mathcal{O}_{W}(\cdot, \cdot)})) = \Hav(U_e | View(A^{\mathcal{O}_{W}(\cdot, \cdot)})$.  Consider a query $q_j$ on block $i$.  There are two possibilities: either $q_j$ is not in the image of $f_i$,  or $q_j$ can be considered a query on the preimage $f_i^{-1}(q_j)$. Then (by assuming $S$ knows $f$) we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ (like in the 
proof of \clref{cl:independent high ent}),
 and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $i$, $\Hav(W_i | View(A^{\mathcal{O}_{W}(\cdot, \cdot)})) \geq \Hoo(W_i) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so this is a family of distributions with total entropy $\omega(\log n)$ for which \consref{cons:first construction} is secure.  For these distributions, all the coordinates are as dependent as possible: one determines all others.

We can prove a slightly weaker claim when the correlation between the coordinates $W_i$ is arbitrary:

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\ell$ be a source.  Suppose that for all $i$, $\Hoo(W_i)\geq \alpha$, and that $q 2^{-\alpha}\leq1/4$ (this holds asymptotically, in particular, if $q$ is polynomial and $\alpha$ is superlogarithmic). Then  $W$ is a $(q, \alpha-1-\log(q+1), 0)$-unguessable block distribution.
\end{claim}

\begin{proof}
Intuitively, the claim is true because the oracle is not likely to return 1 on any query. Formally, we proceed by induction on oracle queries,
using the same notation as in the proof of   \clref{cl:independent high ent}. Our inductive hypothesis is
that $\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0] \leq (j-1)2^{1-\alpha}$.  If the inductive hypothesis holds, then, for each $i$,  
\begin{equation}
\label{eq:cond-entropy}
\Hoo(W_i | A_1= \dots= A_{j-1}=0) \ge \alpha-1\,.
\end{equation}
This is true for $j=1$ by the condition of the theorem. It is true for $j>1$ because, as a consequence of the definition of $\Hoo$,
for any random variable $X$ and event $E$, $\Hoo(X|E)\ge \Hoo(X)+\log\Pr[E]$; and $(j-1) 2^{1-\alpha}\leq 2 q 2^{-\alpha} \leq 1/2$.  

We now show that $\Pr[A_1\neq 0 \vee \dots \vee A_{j}\neq 0] \leq j2^{1-\alpha}$, assuming that $\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0] \leq (j-1)2^{1-\alpha}$.
\begin{align*}
\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0 \vee A_j\neq 0] & = 
\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0]+\Pr[A_1=\dots = A_{j-1}=0 \wedge A_j=1]\\
& \le  (j-1)2^{1-\alpha}+\Pr[A_j=1\,|\,A_1=\dots = A_{j-1}=0]\\
& \le  (j-1)2^{1-\alpha}+\max_i 2^{-\Hoo(W_i | A_1=\dots =A_{j-1}=0)}\\
& \le  (j-1)2^{1-\alpha}+ 2^{1-\alpha}\\
& = j 2^{1-\alpha}
\end{align*}
(where the third line follows by considering that to get $A_j=1$, the adversary needs to guess some $W_i$, and the fourth line follows by~\eqref{eq:cond-entropy}).
Thus, using $j=q+1$ in~\eqref{eq:cond-entropy},
 we know $\Hoo(W_i | A_1= \dots= A_q=0) \ge \alpha-1$.  Finally this means that
\begin{align*}
\Hav(W_i | A_1,\dots, A_q) &\ge -\log \left( 2^{-\Hoo(W_i | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]+1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] \right)\\
& \ge -\log \left(  2^{-\Hoo(W_i | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\end{proof}

Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some blocks to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$, let $X = U_{\log p\times \ell}$ be a distribution and let $f_1,..., f_{\ell}$ be injective functions where $f_i:\zo^{i\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \ell \times \log p$, that is the source has a small number of blocks.}  Then define the distribution $W_1 = f_1(U_{1,...,\ell}), W_2 = f_2(U_{1,..., 2\ell}),...., W_\ell = f_\ell(U)$.  There is an adversary making $2^e\times \ell = \poly(n)$ queries such that $\Hav(W | View(A^{\mathcal{O}_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
We present an adversary $A$~(running in polynomial time) that completely determines the value $x$.  $A$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $A$ queries on $y_1,..., y_p$ exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $A$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered.  The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \ell)$.  Once $x$ has been recovered then $\Hav(W | View(A^{\mathcal{O}_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relied on an adversaries ability to completely determine a block from the previous blocks.  We formalize this notion next.  We defining the entropy jump of a block source as the remaining entropy when other blocks are known:

\begin{definition}
Let $W = W_1,..., W_\ell$ be a source under ordering $i_1,..., i_\ell$.  The \emph{jump} of a block $i_j$ is $J(i_j) = \max|W_{i_j} |$ conditioned on the values $W_{i_1},..., W_{i_{j-1}}$.
\end{definition}

\noindent
We now show that there must be a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\ell$ such that for all $j\le \ell-\beta +1$, $J(i_j) = \log q /\ell$, then $W$ is not $(q, 0, \beta)$-unguessable.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \ell$.  We describe an unbounded adversary that determines $W_1,..., W_{\ell-\beta+1}$.  As before $A$ queries the $q /\ell$ possible values for $W_1$ and determines $W_1$.  Then $A$ queries the $q/\ell$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\ell-\beta+1}$ is learned.  Note the optimum ordering for the adversary can be encoded nonuniformly but it may take an unbounded amount of time/space to construct the support of $W_i | W_1,.., W_{i-1}$.
\end{proof}

\section{Proof of \lemref{lem:cond and cext}}
\label{sec:cond and cext}
\begin{proof}
It suffices to show if there is some distinguisher $D'$ of size $s'$ where
\[\delta^{D'}((\cext(X; U_d), U_d, SS), (U_\kappa, U_d, SS))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D$ of size $s_{cond}$ where for all $Y$ where $\Hav(Y|SS) \geq \tilde{m}$ such that
 \[
 \delta^{D}((X, SS), (Y, SS))\geq \epsilon_{cond}.
 \]
Let $D'$ be such a distinguisher.  That is,
\[
\delta^{D'}(\cext(X, U_d)\times U_d \times SS, U_\kappa\times U_d\times SS)> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D$ as follows.  On input $(y, ss)$ sample $seed\leftarrow U_d$, compute $r\leftarrow \cext(y; seed)$ and output $D(r, seed, ss)$.  Note that $|D| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D}((X, SS), (Y, SS))&= \delta^{D'}((\cext(X, U_d), U_d, SS), \cext(Y, U_d), U_d, SS)\\
&\geq \delta^{D'}((\cext(X, U_d), U_d, SS), (U_\kappa\times U_d \times SS)) - \delta^{D'}((\cext(Y, U_d), U_d, SS), (U_\kappa\times U_d \times SS))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Where the last line follows by noting that $D'$ is of size at most $s_{ext}$.  Thus $D$ distinguishes $X$ from all $Y$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\section{Proof of \thref{thm:security of cons}}
\label{app:security of main cons}
\begin{proof}

Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for point programs over $\zo^n$.  Let $W$ be a $(q, \alpha = \omega(\log n), \beta)$-unguessable block distribution.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P)\geq |C|- \beta$. % for $\epsilon' = 2\epsilon_{obf} + (\ell - \beta)2^{-(\alpha-1)}$.
Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P) < |C|-\beta$.

By \defref{def:block guessable} there exists a set of indices $\mathcal{I}$ such that all blocks within $\mathcal{I}$ are unguessable.  Define by $C'$ the distribution of sampling a uniform codeword where all locations outside $\mathcal{I}$ are fixed.  Then $C | C_{\mathcal{I}^c} \overset{d}=C'$.  By \clref{cl:many locations ent}, we have that $\Hav(C|C_{\mathcal{I}^c} )= |C| -\beta$ and thus $\Hav(C'| C_{\mathcal{I}^c}) = |C| -\beta$.  
Let $D$ a distinguisher of size be some distinguisher of size at most $s_{sec}$ such that 
\[
| \expe[D(C, P)] - \expe[D(C', P)] > \epsilon_{sec} = 1/\poly(n).
\]  
Define the distribution $X$ as follows:
\[X_i =
\begin{cases}
W_i & C_i = 0\\
R_i & C_i = 1.
\end{cases}\]  By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(P_1,..., P_\ell, C)] - \expe [S^{\mathcal{O}_X(\cdot, \cdot)}(C)] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $C$ and $C'$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{\mathcal{O}_X(\cdot, \cdot)}(C), S^{\mathcal{O}_X(\cdot, \cdot)}(C')) \le (\ell-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $\mathcal{I}^c$, the statistical distance is at most $2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then,
\[
\Delta( S^{\mathcal{O}_X(\cdot, \cdot)}(c^*), S^{\mathcal{O}_X(\cdot, \cdot)}(c')) \le ( \ell -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $i\in \mathcal{I}$, $\Hav(W_i | View(S))\geq \alpha$.  The only information about the correct value of $c_i^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $i \in \mathcal{I}$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not, that is suppose, the probability of at least one nonzero response on index $i$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_i$, the probability of this happening when $c^*_i = 1$ is at most $q/2^n$ or equivalently $2^{-n+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }i]\nonumber \\
 &= \Pr[c_i^* =1]\Pr[\text{non zero response location }i\wedge c_i^*=1]\nonumber \\&+ \Pr[c_i^*=0] \Pr[\text{non zero response location }i \wedge c_i^*=0]\nonumber \\
&\le 1\times 2^{-n+\log q} + 1\times  \Pr[\text{non zero response location }i \wedge c_i^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that $n-\log q \geq \alpha$:
\begin{claim}
\label{cl:ent bounded away from n}
$n-\log q \geq \alpha$
\end{claim}
\begin{proof}
It suffices to show that there exists a simulator $S$ making $q$ queries such that the remaining entropy in that block is at most $n-\log q$.  Let $W_i$ be a distribution, consider $S$ that asks the $q$ most likely outcomes, the total probability of this block must be at least $2^{-n+\log q}$.  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_i | View(S)) &\leq  -\log \left(\Pr[\text{some }q_i=1]\times 1+ \Pr[\text{no }q_i=1]\times \Pr[\text{most likely outcome}|q_1,...,q_q]\right)\\
&\leq  -\log \left(\Pr[\text{some }q_i=1]\times 1\right)\\
&=-\log\left( 2^{-n+\log q} \right) = n-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }i \wedge c_i=0] &>2^{-\alpha+1} - 2^{-(n-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_i=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the block unguessability of $W$~(\defref{def:block guessable}).  By the union bound over the indices $i\in\mathcal{I}$ the total probability of a $1$ in $\mathcal{I}$ is at most $(\ell-\beta)2^{-\alpha+1}$. Thus, for all $c_1, c_2\in C| C_{\mathcal{I}^c}$ the statistical distance is at most $(\ell- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}
\end{proof}
By averaging over all points in $C'$ we conclude that $\Delta(S^{\mathcal{O}_X(\cdot, \cdot)}(C), S^{\mathcal{O}_X(\cdot, \cdot)}(C')) < (\ell -\beta)2^{-(\alpha+1)}$.  This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(P_1,..., P_\ell, C') ]- \expe [S^{\mathcal{O}_X(\cdot, \cdot)}(C')] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish}, we have
\begin{align*}
\delta^{D}(( P, C), (P, C'))&\leq |\expe \expe [D(P_1,..., P_\ell, C)] - \expe [S^{\mathcal{O}_X(\cdot, \cdot)}(C)]| \\
&+|\expe[S^{\mathcal{O}_X(\cdot, \cdot)}(C)] - \expe[S^{\mathcal{O}_X(\cdot, \cdot)}(C')] |\\
&+|\expe [S^{\mathcal{O}_X(\cdot, \cdot)}(C')] - \expe [D(P_1,..., P_\ell, C') ]|\\
&\leq \epsilon_{sec}/3+ (\ell-\beta)2^{-(\alpha-1)}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \thref{thm:security of cons}.
\end{proof}

\section{Proof of \lemref{lem:sampling works}}
\label{sec:proof of sampling lemma}
\begin{proof}
Consider some fixed $i$.  Let $\Lambda$ represent the random variable of all the coins used by $\sample$ and $\lambda$ be some particular outcome.
For each particular value of $w_1,..., w_\gamma$ there exists some set $\mathcal{I}$ of size $\gamma - \beta = \Theta(\gamma)$ such that each block $j\in \mathcal{I}, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}, W_{j+1}=w_{j+1},..., W_\gamma = w_\gamma) \geq \alpha$.  Since this is a worst case guarantee for all $\lambda$, 
\[
\Hoo(V_i |\lambda ) \geq \alpha \times |\{\text{indices in }\mathcal{I}\text{ sampled in }V_i\}|.
\]

Denote by $X = |\{\text{indices in }\mathcal{I}\text{ sampled in }V_i\}|$.  We now calculate $\expe[X]$.  Let $j_1,.., j_\eta$ be the elements selected by sampling algorithm.  The ``first'' element $j_1$ lands in $\mathcal{I}$ with probability $(\gamma-\beta)/\gamma$.  For each selected index $j_k$ there are least remaining $\gamma-\beta-k$ samples in $\mathcal{I}$, so $\Pr[j_k\in\mathcal{I}]>(\gamma-\beta-\eta)/\gamma$.  We can associate with each $j_k$ a Bernoulli random variable $X_k$ with success probability at least $(\gamma-\beta-\eta)/\gamma$ and note that 
\[
\expe[X] = \sum_{k=1}^\eta\expe[X_k] \geq \eta \expe[X_\eta]\geq \eta\frac{\gamma-\beta-\eta}{\gamma}.
\]
Using the Chernoff bound, $X$ rarely is far from its expected value:
\[
\Pr\left[ \frac{1}{\eta} \sum_{k=1}^\eta X_i \leq 1/2 \expe[X_i]\right]\leq 2e^{-\expe[X_i] \eta} = O(e^{-\eta})
\]
Where the last equality follows because 
\[
\expe[ X_i] = \frac{\gamma-\beta-\eta}{\gamma} \geq \frac{\Theta(\gamma) - \eta}{\gamma} = \frac{\Theta(\gamma) - o(\gamma)}{\gamma} = O(1).
\]
That is \[
\Pr[\Hoo(V_i ) \geq \alpha \times \frac{\eta(\gamma-\beta-\eta)}{2\gamma} = \alpha'] \geq 1- O(e^{-\eta}).
\]
\end{proof}

\section{Proof of \lemref{lem:sampling errors}}
\label{sec:sampling errors}

\begin{proof}
Let $t$ be as stated in the lemma.
We first consider calculate $\expe[\dis(V, V')]$ and then show that the distance between $v$ and $v'$ is rarely much more than this value.
Let $\mu = -\log(1-t'/(2\ell))/2$ then one has:
\begin{align*}
\expe \Pr[v_i = v_i'] &=\Pr[j_{i,1},..., j_{i,\eta}\text{ have no errors}]\\
&\geq \prod_{j=0}^{\eta-1}\left(1 - \frac{t}{\gamma-j}\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu(\gamma-\eta)/\eta}{\gamma-i}\right)\\
&\geq  \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu}{\eta}\left(\frac{\gamma-\eta}{\gamma-j}\right)\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu}{\eta}\right) \\
&\geq \left(1-\frac{\mu}{\eta}\right)^{\eta} =\left( \left(1-\frac{\mu}{\eta}\right)^{\eta/\mu}\right)^\mu\geq \frac{1}{4}^\mu\\
& = \left(\frac{1}{4}\right)^{-\log(1-\frac{t'}{2\ell})/2} = 1-\frac{t'}{2\ell}
\end{align*}

With each $i$ we can associate a Bernoulli random variable $X_i \overset{def}= V_i \overset{?}\neq V_i'$.  As stated above, we have that $\expe[X_i] \leq  t'/(2\ell)$. By the Chernoff bound, we have
\begin{align*}
\Pr\left[\frac{1}{\ell} \sum_{i=1}^\ell X_i\geq 2\expe[X_i]\right]\leq 2e^{-2\expe[X_i]\ell} = O(e^{- \ell}).
\end{align*}
Where the last equality holds because $t'/(2\ell) = \Theta(1)$.
\end{proof}




\end{document} 