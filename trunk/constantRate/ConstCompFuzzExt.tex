\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}  

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}

\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}

\title{Constant Error Tolerance Computational Fuzzy Extractors}
\author{Ran Canetti \and Benjamin Fuller\footnote{The Lincoln Laboratory portion of this
work was sponsored by the Department of the Air Force under Air Force
Contract
\#FA8721-05-C-0002.  Opinions,
interpretations, conclusions and recommendations are those of the author
and
are not necessarily endorsed by the United States Government.} \and Omer Paneth \and Leonid Reyzin}

\begin{document}
\maketitle


\begin{abstract} 
Fuzzy extractors derive strong keys from noisy sources.  The goal is to reliably convert a high entropy source~(which may have different values on repeated readings) to the same uniformly distributed key.  Traditionally, their security is defined information-theoretically.  Fuzzy extractors have upper bounds on the length of the derived key~(Dodis et al., J. of Comp 2008).  In particular, the starting entropy of the source must be significantly higher than the logarithm of number of error patterns corrected.  For many practical noisy sources, like biometrics, this condition is not fulfilled, leaving reliable key derivation from these sources as open problem.  

Fuller, Meng, and Reyzin (Asiacrypt 2013) hypothesize that these bounds can be overcome by only providing security against computationally-bounded adversaries. 
In this work, we provide the first construction of a fuzzy extractor for a large class of distributions where the starting entropy is smaller than the logarithm of the number of correctable error patterns.

\textbf{Construction:} We construct a computational fuzzy extractor based on obfuscation of point functions.  
Our construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~(Eurocrypt 2008).  
Our construction provides the following novel features:
\begin{itemize}
\item The source entropy is smaller than the log of the number of correctable error patterns.  
\item Constant error tolerance rate.  
\item Security for a large and meaningful class of distributions.  \bnote{this description needs to be better}.
\end{itemize}
\end{abstract}


\section{Introduction}\label{sec:introduction}
Reliable key derivation is a cornerstone of authentication.  However, many sources with sufficient entropy for key derivation are noisy and provide similar but not identical values when the source is read multiple times~(examples include biometrics~\cite{daugman2004} and physically unclonable functions~\cite{pappu2002physical}). \bnote{add more examples here} To use noisy physical sources in applications, two problems must be addressed.  The first is ensuring the same value is obtained from every reading of the source.  This must be done in a way that does not eliminate all the entropy from the output~(known as information-reconciliation~\cite{bennett1988privacy}).  The second is converting the entropic source to a uniform random key~(privacy amplification~\cite{bennett1988privacy}).  Both of these problems have interactive and non-interactive versions.  For a single user~(trying to produce the same key from multiple readings of a physical source) the non-interactive solutions are appropriate.  We focus on this setting.  %If both of these tasks are achieved a noisy physical source can be used like a uniformly random private key. 

Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} perform both tasks non-interactively.  They consist of a pair of algorithms: \gen takes a source value $w$, and produces a key $r$ and a public value $p$.  The second algorithm \rep takes this public value $p$ and a close $w'$ to reproduce the original key $r$.  Traditionally, a secure sketch~\cite{DBLP:journals/siamcomp/DodisORS08} performs information-reconciliation and a randomness extractor~\cite{nisan1993randomness} performs privacy amplification.

Unfortunately, fuzzy extractors have not yielded key derivation for all noisy sources.  
This is because there is tension between security and error-tolerance.  The key should be uniform to an observer but nearby $w'$ should map to the same key.  By nearby, we mean that $\dis(w, w')\leq t$ for some metric $\dis$.  In the information-theoretic realm, increasing $t$ means a decrease in the strength of the key $r$.  This is because a fuzzy extractor can be converted into an error correcting code~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08}.  The length of $r$ is dictated by size of the best code that corrects $t$ errors.  Error-correcting codes are well studied and we have tight bounds on this quantity. % how many codewords may be in a code that corrects $t$ errors.  

We consider the Hamming metric.  For the Hamming metric, the key may be no longer than the difference between the starting entropy of the source\footnote{We assume that the fuzzy extractor operates independently of the source $W$.  As we discuss later, it is possible to avoid these losses if the fuzzy extractor has additional knowledge about the distribution.} and logarithm of the number of error patterns to be corrected. For the remainder of this paper we denote this quantity by $gap$.  This represents a real problem, important biometrics, such as irises~\cite{daugman2004}, have $gap <0$ and current fuzzy extractors provide no security guarantee.
 
 %let $|B_t|$ be the number of points within distance $t$.  The length of the key $|r| < \Hoo(W) - \log |B_t|$~(see~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08} for the precise conditions).  For the remainder of this paper, we denote this quantity by $gap = \Hoo(W) - \log |B_t|$.

This does not need to be the case in the computational setting.  Fuller, Meng, and Reyzin define computational fuzzy extractors~\cite{fuller2013computational}.  Computational fuzzy extractors produce pseudorandom instead of truly random keys.  They hypothesize a computational fuzzy extractor may exist with a small or negative $gap$.  We resolve this question and construct a computational fuzzy extractor when $gap<0$.  This is impossible for an arbitrary min-entropy distribution as every point could be within distance $t$.  A necessary condition for security is that a negligible portion of the source's probability distribution lies within any Hamming ball of radius $t$.  We require a stronger condition that many blocks are unpredictable~(\defref{def:block guessable}).  

The goal of an information-theoretic fuzzy extractor is to output a maximal length key for a source $W$.  
In the computational setting, a key may be expanded once it is long enough to serve as input to a pseudorandom generator.  Our focus is on expanding the class of distributions for which we can provide meaningful security, and remember keys can be expanded once they are long enough.

\textbf{Overview of Construction:}  Fuller, Meng, and Reyzin~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational} show that replacing an information-theoretic information-reconciliation component with a computational component is unlikely to be fruitful.  Instead, the authors suggest two alternatives: combine the information-reconciliation and privacy amplification components~(their approach) or produce a new consistent high-entropy secret instead of recovering $w$.  This is known as a fuzzy conductor~(the entropy is ``conducted'' from $w$ to a new distribution) and was introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}.  Here, we follow the second approach and use a \emph{computational} fuzzy conductor.

Our construction~(\consref{cons:first construction}) is based on obfuscation of point programs.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.  Obfuscation for this class of functions is achievable under particular number-theoretic assumptions~\cite{canetti1997towards} and from generic cryptographic hardness assumptions~\cite{wee2005obfuscating}.  

Consider a source $W = W_1,..., W_\ell$ that is split into blocks~(over a large alphabet).  We would like to tolerate errors in $t$ individual blocks.  For each block $i$, we flip a coin $c_i$ and either obfuscate $I_{w_i}$ or pick a random point $r_i$ and obfuscate $I_{r_i}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rec$ then runs the obfuscated program $P_i$ on $w_i'$ and checks whether $P_i(w_i')=1$.  For most locations $i$, \rep can determine whether $w_i$ or a random value was the point obfuscated.  Thus, most bits of $c_i$ are recoverable. To tolerate errors we choose the coins $c$ from an error-correcting code.  The set of possible codewords forms a high entropy distribution.  This construction conducts entropy from $w$ to $c$ and is a computational fuzzy conductor.  It may be converted to a computational fuzzy extractor by either information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors.  Note, it is hard to recover $w$ from the value $c$.   This is necessary to overcome the information-theoretic lower bounds~\cite[Section 3.3]{fuller2013computational}.  
Our construction is inspired by Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  

Our construction provides several advantages over previous constructions of computational fuzzy extractors:
\begin{itemize}
\item The first computational fuzzy extractor where $gap$ may be negative for a large class of distributions.\footnote{There are examples where no loss is necessary in a fuzzy extractor.  If $W$ consists of points from an error-correcting code of sufficient minimum distance, then no public information is necessary and $gap$ may be negative.  This is not in the spirit of a fuzzy extractor as the distribution is already ``error-corrected.''  For arbitrary min-entropy sources, analysis of information-theoretic fuzzy extractors proceeds by arguing an adversary learns at most a bit for each bit necessary for error correction.   For an arbitrary distribution, this is at least the logarithm of the total number of error patterns.  The distributions supported by our construction are not error-correcting codes, we require unpredictability in many dimensions~(see \defref{def:block guessable}).}
\item The first computational fuzzy extractor that allows for a constant fraction of errors.  Previous constructions could correct only  a logarithmic fraction of errors~($O(\frac{\log \ell}{\ell})$)
\item The first computational fuzzy extractor where blocks are not required to be uniformly distributed.  Our construction simply requires a large number of blocks that are unguessable given equality queries.  This subsumes the distributions supported by~\cite[Construction 4.1]{fuller2013computational}.
\end{itemize}

\textbf{Limitations: } The main limitation of our construction is that each block is individually obfuscated.  This means that blocks much be drawn from distributions with super-logarithmic entropy.  Furthermore, our distance is Hamming distance over a large alphabet~(super-polynomial in the security parameter).  Our construction is applicable for physical sources where many bits are expected to err consecutively, followed by many consecutive correct bits~(known as burst errors~\cite{gilbert1960capacity}).  Constructing a computational fuzzy extractor with the above advantages and a small alphabet is an open problem.


\textbf{Connection with Noisy Point Obfuscation: } Ideally, our construction would be a noisy point obfuscation~(which is a stronger object than a computational fuzzy extractor).  Unfortunately, this is not the case.  Each block is individually obfuscated, so an adversary may ask equality queries on individual blocks.  If not all blocks have high entropy an adversary may learn significant information about the source.  Thus, our construction is not a virtual black box~(VBB) obfuscation~(where anything learnable by the circuit is simulatable using input/output behavior).  Dodis and Smith~\cite{DBLP:conf/stoc/DodisS05} construct noisy point obfuscation for $gap>>0$ using a information-reconciliation component that hides partial information.  Constructing VBB obfuscation for sources where $gap <0$ is an open problem.

The remainder is organized as follows: we cover notation and background on obfuscation and error correcting codes in \secref{sec:preliminaries} , describe computational fuzzy extractors in \secref{sec:fuzzy extractors}, present our construction in \secref{sec:construction} and discuss parameters and tradeoffs in \secref{sec:discussion}.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote $X = X_1,..., X_\ell$ as the concatenation of $X_1$ to $X_\ell$.  For a set of indices $\mathcal{I}$,  $X_{\mathcal{I}}$ is the restriction of $X$ to the indices in $\mathcal{I}$.  The set $\mathcal{I}^c$ is the complement of $\mathcal{I}$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   Let $|W|$ be the size of the support of $W$ that is $|W| = |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. 
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  For our sources, we consider the Hamming metric, for two vectors $x, y$ over $\mathcal{Z}^n$ the Hamming distance between $x,y$ is $d(x,y) = \{i | x_i \neq y_i\}$.  For the Hamming metric, $|B_t| = \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
%\bnote{this is weird here and doesn't fit}  However, we will use codes that only correct one-sided errors~(see \secref{sec:coding theory}).


Usually, we use bold letters for vectors or matrices, capitalized letters for random variables, and lowercase letters for elements in a vector or samples from a random variable. 

\subsection{Coding Theory}
\label{sec:coding theory}
We introduce some notions from the field of binary error correcting codes.  Usually the standard class of errors  is all points within Hamming distance $t$, we will use the Hamming analog of the $Z$-channel~\cite{tallini2002capacity} where there are flips from $0\rightarrow 1$ but no bit flips from $1\rightarrow 0$.  
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$. 
\end{definition}
\textbf{Notes:} Any code that corrects $t$ Hamming errors corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
The class of errors we support~($t$ Hamming errors over a large alphabet) in our source and the class of errors corrected by our code~($t$ $0\rightarrow 1$ errors) are different~(see \consref{cons:first construction} for details).  Codes where $\log |C| = \Theta(\ell)$ and $t = \Theta(\ell)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \epsilon_{code})$-code if there exists an efficient procedure $\rec$ such that for all $c\in C, \forall c'\in \neigh(c), \Pr[\rec(c') \neq c] \leq \epsilon_{code}$.
\end{definition}

We note that for any code learning a few bits does not inform on the remainder of the bits.  

\begin{claim} 
\label{cl:many locations ent}
Let $C$ be a binary code and let $\mathcal{I}^c$ be a set of indices of $C$.  Then $\Hav(C | C_{\mathcal{I}^c}) = \log |C| - |\mathcal{I}^c|$.
\end{claim}
This claim is a direct result of \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}


%need one special property of the code, namely that each output bit is set to $1$ with probability $1/2$.

%\bnote{I don't think we need this!}
%\begin{definition}
%A binary $(\neigh_t, \epsilon_{code})$ code is output equiprobable\bnote{Find the right name for this} $\forall i, |\{C | C_i =1\}| =|C|/2$.
%\end{definition}
%All binary linear codes~(without constant bits) are output equiprobable and .  This is true as long as $C$ contains no constant bits.  Usually, if a code has constant bits, we can truncate those bits before encoding and obtain a code with better parameters.  For the remainder of this work when we say a code, we assume there are no constant bits.
%
%\begin{lemma}
%\label{lem:linear codes independent}
%Let $C$ be a \emph{linear} binary code with no constant bits.  Then $C$ is output equiprobable.
%\end{lemma}
%Proof in \apref{sec:proof of linear indep}.
%
%

\subsection{Obfuscation}
Our construction uses virtual black-box obfuscation of point functions.  This is the following family of functions:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
Here we present the virtual black-box definition of obfuscation.  See \secref{sec:obfuscation} for a more detailed introduction to obfuscation.  This definition is known to be achievable for point functions under various assumptions~\cite{canetti1997towards, wee2005obfuscating}.  We also need the obfuscation to be $\ell$-composable~(\defref{def:obf comp}).

We present the notion of virtual black-box obfuscation with the weakening that the quality of the simulator is an arbitrarily small $1/\poly$ as all known point obfuscations satisfy this definition.

\begin{definition}~\cite{barak2001possibility, goldwasser2005impossibility}  
\label{def:obf} Let $\mathcal{C}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is an obfuscator for $\mathcal{C}$ with dependent auxiliary input if the following conditions are met:
\begin{enumerate}
\item \emph{Approximate Functionality:}  There exists a negligible function $\epsilon$ such that for every $n\in \mathbb{N}$, every circuit $C\in \mathcal{C}_n$, and every $x\in\zo^n$, 
\[
\Pr[\mathcal{O}(C; r)(x) = C(x)] > 1-\epsilon(n),
\]
where the probability is taken over the randomness $r$.  \emph{Almost exact functionality} is a stronger condition that requires $\mathcal{O}(C;r)\equiv C$ with overwhelming probability over the random coin tosses $r$.  Finally, if this probability always equals $1$, then $\mathcal{O}$ has \emph{exact functionality}.
\item \emph{Polynomial Slowdown:}  There exists a polynomial $\psi$ such that for every $n$, every circuit $C\in \mathcal{C}_n$, and every possible $r$, the circuit $\mathcal{O}(C; r)$ run-in time at most $\psi(n)$.
\item \emph{Virtual Black-box:}  For every PPT adversary $A$ and polynomial $\rho$, there exists a PPT simulator $S$ such that for all sufficiently large $n$, for all $C\in \mathcal{C}_n$, for all auxiliary inputs $z\in \zo^*$, 
and for all binary predicates $\pi$, 
\[
|\Pr[A(\mathcal{O}(C), z) = \pi(C, z)] - \Pr[S^C(1^n, z) = \pi(C, z)] | < \frac{1}{\rho(n)}
\]
where the first probability is taken over the coin tosses of $A$ and $\mathcal{O}$, and the second probability is taken over the coin tosses of $S$.  Furthermore, the runtime of $A$ and $S$ must be polynomial in the length of their first input.
\bnote{We never need to run our simulator so I think we can use virtual gray-box.}
\end{enumerate}
\end{definition}


\section{Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

We focus on computational fuzzy extractors introduced by Fuller, Meng, and Reyzin~\cite{fuller2013computational}.  Definitions for information-theoretic fuzzy extractors and a non-interactive information-reconciliation component, known as a secure sketch, can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.  
%\begin{definition}
%\label{def:fuzzy extractor}
%An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$, with the following properties: 
%\begin{enumerate}
%\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\ell$ and a helper string $p\in\{0,1\}^*$.
%\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w,w')\leq t$, if $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$ with probability~(over the coins of $\gen, \rep$) at least $1-\delta$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
%\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
%\end{enumerate}
%A fuzzy extractor is efficient if $\gen$ and $\rep$ run in expected polynomial time.
%\end{definition}
%
%Secure sketches are the main technical tool in the construction of fuzzy extractors.  Secure sketches produce a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
%\begin{definition}
%\label{def:secure sketch}
%An $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$, with the following properties:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|\sketch(W))\geq \tilde{m}$.
%\end{enumerate}
%A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time. 
%\end{definition}
%

%A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}. 
%
%\begin{lemma}
%\label{lem:fuzzy ext construction}
%Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext:\mathcal{M}\times \zo^d \rightarrow \zo^\ell$ be a $(\tilde{m}, \epsilon)$-average-case extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor with error $\delta$:
%\begin{itemize}
%\item $\gen(w):$ generate $x\leftarrow \zo^d$, set $p=(\sketch(w), x), r=\ext(w;x)$, and output $(r,p)$.
%\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $r=\ext(w;x)$.
%\end{itemize}
%\end{lemma}

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$ is known: if the error pattern between $w$ and $w'$ depends on the output of $\gen$ there is no guarantee about the probability of correctness.
The definition of computational fuzzy extractors specifies a family of sources for which the fuzzy extractor works rather than the family of all sources of a given min-entropy $m$.  With $gap<0$ there are distributions contained in a single Hamming ball, so restricting the family of sources is necessary.

Fuller, Meng, and Reyzin~\cite{fuller2013computational} present two approaches for constructing a computational fuzzy extractor: analyzing the information-reconciliation and privacy amplifications components together or using a fuzzy conductor and a privacy amplification component.  We follow the second approach.  

In \apref{sec:conductors}, we show that fuzzy conductors are subject to the same lower bounds on entropy loss as fuzzy extractors.  To overcome these bounds, we use a computational version of a fuzzy conductor.  
We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has 
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition}

We now define the two primitives for this approach: a computational fuzzy conductor and a (computational)~randomness extractor.  A computational fuzzy conductor is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' ($\gen'$) and ``reproduce'' ($\rep'$) is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon, s_{sec})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $Y\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(Y, P)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', P) = Y] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $Y$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon, s_{sec}}(Y |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}

A computational extractor is the natural adaption of a randomness extractor to the computational setting.  We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average-case:
%start with the standard notion of an average-case extractor. An average-case extractor is a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}) (in particular, Vadhan~\cite[Problem 6.8]{Vad12} showed that all strong extractors are average-case extractors with a slight loss of parameters):
\begin{definition}
Let $\chi$ be a finite set.
A function $\cext: \zo^\ell \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, s_{sec}, \epsilon)$-average-case computational extractor} if for all pairs
of random variables $X, Y$ over $\zo^\ell, \chi$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{sec}}}((\cext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon$.
\end{definition}

Combining a computational fuzzy conductor and an appropriate randomness extractor yields a computational fuzzy extractor~(proof in \secref{sec:cond and cext}):

\begin{lemma}
\label{lem:cond and cext}
Let $\gen'$, $\rep'$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\ell$.  Let $\cext:\zo^\ell\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, s_{ext}, \epsilon_{ext})$-average case computational extractor.  Define $\gen, \rep$ as:
\begin{itemize}
\item $\gen(w; r, x):$ run $(y, p')= \gen'(w; r)$ and set $r = \cext(y; x)$ and $p = (p', x)$.  Output $(r, p)$.
\item $\rep(w, (p', x)):$ recover $y = \rec'(w'; p')$ and output $r = \cext(y; x)$. 
\end{itemize}
Then $\gen, \rep$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \max\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}


\section{A Constant Error-Tolerance Computational Fuzzy Extractor}
\label{sec:construction}

Before describing our construction, we recall the problem we are trying address.  The goal is to derive strong keys from noisy sources.  As discussed in the introduction, our goal is to provide security when $gap = \Hoo(W) - \log|B_t|$ is small or even negative.
In the computational setting once a ``long enough'' key may be expanded using a pseudorandom generator.  Thus, we focus on providing enough security when $gap<0$.  We build a computational fuzzy conductor, which can be converted to a computational fuzzy extractor using a computational extractor~(\lemref{lem:cond and cext}).%If $gap> \omega(\log n)$~(ignoring losses due to randomness extraction), then a key of any length can be formed by using an information-theoretic fuzzy extractor~(which yields nearly this many bits using an optimal code) and expanding the output with a pseudorandom generator~(or using a secure sketch with a computational extractor).  Using an information-theoretic analysis of a fuzzy extractor this seems to be the best construction possible.  As described in Fuller, Meng, and Reyzin~\cite{fuller2013computational}, use of a computational fuzzy extractor may allow a construction when $gap = O(\log n)$.  Their construction is only applicable for high entropy input and yield a result when $gap$ is small.  We will provide the first construction where $gap$ is negative.  

\begin{construction}
\label{cons:first construction}
Let $n$ be a security parameter and let $W = W_1,..., W_\ell$ be a distribution over $\zo^{\ell n}$.  Let $\mathcal{O}$ be a $\ell$-composable obfuscator for the family of point functions over $\zo^n$.  Let $t$ be the desired error-tolerance and let $C\subset \zo^\ell$ be a
$(\neigh_t, \epsilon_{code})$-error-correcting code.  
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C$.
\item For $i=1,..., \ell$:
\subitem If $c_i = 0$: $p_i = \mathcal{O}(I_{w_i})$.
\subitem Else: Sample $r_i \leftarrow U_n$. 
\subsubitem Let $p_i = \mathcal{O}(I_{r_i})$.
\item Output $(c, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$ 
\item For $i=1,..., \ell$:
\subitem If $p_i(w_i') = 1$: set $c_i' = 0$.
\subitem Else: set $c_i' = 1$.
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage} 
\end{tabular}
\end{center}
\end{construction}

\textbf{Notes:}  We make several observations about the above construction:
\begin{itemize}
\item The input $w$ is hidden in two different ways.  In locations where $c_i=1$ the block $w_i$ is information-theoretically unknown.
In locations where $c_i=0$ it is hard to find $w_i$ given the point obfuscation.  As described in~\cite[Section 3.3]{fuller2013computational} some type of lossiness is necessary to avoid coding bounds.
\item There are two possible reasons for a bit $c_i'$ to be $1$.  Because the true value was $1$~(there is little chance of $1$ being incorrectly decoded as $0$ assuming $w_i'$ is independent of the sketch) and because $w_i \neq w_i'$.  However if a bit $c_i'$ is $0$ this likely means that $w_i=w_i'$ because collisions when the $c_i=0$ are unlikely~(occurring with probability roughly $2^{-n}$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips, instead of a code that corrects all Hamming weight $t$ errors.
\end{itemize}

\subsection{Security of Construction}
\label{sec:sec of construction}
\bnote{this paragraph needs to be rewritten}
Assuming virtual black-box obfuscation, we argue security in the presence of equality oracles for each block.  The adversary is provided with equality oracles for each block individually.  Security rests on an adversary's inability to tell whether they are working with obfuscations of $w_i$ or random points using \emph{adaptive} equality queries.  Our construction is secure as long as enough blocks are unpredictable after adaptive queries:

\begin{definition}
\label{def:block guessable}
Let $\mathcal{O}_{w_1,..., w_\ell}$ be an oracle that returns \[\mathcal{O}_{w_1,..., w_\ell}(i, w_i')=
\begin{cases}
1 & w_i = w_i'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1||...|W_\ell$ is a $(q, \alpha, \beta)$-\emph{unguessable block distribution} if for any unbounded adversary $A$ with oracle access to $\mathcal{O}$ making at most $q$ queries there exists a set $\mathcal{I}\subset\{1,..., \ell\}$ of size at least $\ell -\beta$ such that 
\[
\forall i\in \mathcal{I}, \Hav(W_i |View(A^{\mathcal{O}_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}

%\noindent We argue security assuming that the adversary knows the value of all blocks outside of $\mathcal{I}$.  
We show security of \consref{cons:first construction} for distributions satisfying \defref{def:block guessable}.  In \apref{sec:characterize} we provide discussion on the types of sources that satisfy \defref{def:block guessable}.
To show security it is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))$ are indistinguishable.  Let $\mathcal{I}$ be the set of indices that exists in \defref{def:block guessable}, the distribution $C'$ is defined as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $\mathcal{I}$.  First note that $\Hav(C' |P) =\Hav(C' | C_{\mathcal{I}^c} = C'_{\mathcal{I}^c}) = |C| - |\mathcal{I}^c|$~(\clref{cl:many locations ent}).  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))$.  
Define the distribution $X$ as follows:
\[X_i = 
\begin{cases}
W_i & C_i = 0\\
R_i & C_i = 1.
\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$, since $P$ is a collection of obfuscated programs there exists a simulator, $S$~(outputting a single bit), such that $D(C, P) \approx_c S^{\mathcal{O}_X(\cdot, \cdot)}(C)$~(\defref{def:obf}).
\item Show that even an unbounded $S$ with oracle access to $X_1,..., X_\ell$ cannot distinguish between $C$ and $C'$.  That is, $S^{\mathcal{O}_X(\cdot, \cdot)}(C) \approx S^{\mathcal{O}_X(\cdot, \cdot)}(C')$.
This is done in \lemref{lem:sim cannot distinguish}.
% the following steps:
%\begin{itemize}
%%\item Show that conditioned on the values of all other indices, $\mathcal{I}^c$, $C$ still has many possible values.  That is, $\Hav(C | C_{\mathcal{I}^c})$ is large~(\clref{cl:many locations ent}).
%%\item Assume that the simulator has knowledge of $C_{\mathcal{I}^c}$.
%\item For all codewords $c, c' \in C| C_{\mathcal{I}^c}$, the codewords $c, c'$ are indistinguishable given the simulator's view~(\lemref{lem:codewords in I close}).
%\item The two distributions $C$ and $C'$ are indistinguishable given the simulator's view~(\bnote{this needs to be written down.})
%\item The set of codewords $C| View(S) \wedge C_{\mathcal{I}^c}$ has conditional min-entropy~(and thus $C|View(S)$ has conditional min-entropy)~(\corref{cor:avg min after view}).  
%\item Any adversary given $C$ or a fresh $C'$ cannot statistically distinguish.
%\end{itemize}
\item By the security of obfuscation, $S^{\mathcal{O}}(C') \approx_c D(C', P)$~(\defref{def:obf}).
\end{itemize}

\noindent Together these arguments allow us to conclude:
\begin{theorem}
\label{thm:security of cons}
Let $\mathcal{O}$ be an $\ell$-composable virtual black box obfuscator with auxiliary inputs for point programs over $\zo^n$ that is $(s_{obf}, \epsilon_{obf})$-hard.  Let $W$ be a $(s_{obf}, \alpha = \omega(\log n), \beta)$-unguessable block distribution.  Then $H^{\hill}_{\epsilon', s_{obf}}( C | P ) \geq |C| - \beta$ for $\epsilon' = 2\epsilon_{obf} + (\ell-\beta)2^{-(\alpha - 1)}$.
\end{theorem}


%The prior lemma implies that the $View$ does not increase the probability of any codeword by much.
%\begin{lemma}
%\label{lem:no codeword high prob}
%For all $c\in C|C_{\mathcal{I}^c}$, 
%\[
%\expe_{View(S)} \Pr[C=c | C_{\mathcal{I}^c} \wedge View(S)] \leq \Pr[C=c | C_{\mathcal{I}^c}]+2^{-(\alpha-1)}.
%\]
%\end{lemma}
%\begin{proof}
%We begin by noting there must exist some $c\in C|C_{\mathcal{I}^c}$ such that 
%\[
%\expe_{view\leftarrow View}\Pr[C = c | C_{\mathcal{I}^c}] \le \frac{1}{| C| C_{\mathcal{I}^c}|}.
%\]
%Otherwise the sum of the probabilities of $c\in C | C_{\mathcal{I}^c}$ will be greater than $1$.  Let $c_{min}$ be one such value.
%Let $c^*$ be an arbitrary $c^*$ in the support of $C|C_{\mathcal{I}^c}$.  Suppose that  
%\begin{align*}
%\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\Pr[C=c^* | C_{\mathcal{I}^c} \wedge view ] &\geq \Pr[C=c^* | C_{\mathcal{I}^c} ]+ 2^{-(\alpha-1)} \\
%\end{align*}
%This means that the difference between the expected probability of $c_{min}$ and $c^*$ is at least $2^{-\alpha+1}$.  Thus, there is an unbounded distinguisher that can tell between the two cases, violating the statistical distance property of   \lemref{lem:codewords in I close}.  This is a contradiction and completes the proof.
%\end{proof}
%\begin{corollary}
%\label{cor:avg min after view}
%$\Hav(C| View(S))\geq \Hav(C | View (A) \wedge C_{\mathcal{I}^c}) \geq \min \{ \Hav(C | C_{\mathcal{I}^c}), \alpha-1\} -1= \min\{ |C| - |\mathcal{I}^c|, \alpha-1\} -1$.
%\end{corollary}
%\begin{proof}
%%Let $e$ be the exponent of the particular negligible value above~(note $e = \omega(1)$). 
%Taking the negative log of \lemref{lem:no codeword high prob} one has:
%\begin{align*}
%\Hav(C_i |  View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot)}))
% &= \\
% -\log \left(\expe_{view \leftarrow View}\max_{c\in C|C_{\mathcal{I}^c}}\Pr[C =c | view \wedge C_{\mathcal{I}^c} ]  \right) &\geq -\log (\Pr[C=c^* | C_{\mathcal{I}^c} ]+ 2^{-\alpha+1}) \\
% &\geq \min \{-\log P_c, -\log 2^{-\alpha+1}\} -1\\
% &= \min \{ \Hav(C | C_{\mathcal{I}^c}), \alpha-1\}-1 
%\end{align*}
%\end{proof}

\subsection{Correctness of \consref{cons:first construction}}
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \ell/2^n = \ngl(n)$.
\end{lemma}
\begin{proof}
\bnote{we need to come back to this if we use obfuscation with error}
Recall that $c$ contains at most $\ell$ $1$s.  Since $w'$ is chosen independently of the points $r_i$, this probability is the size of each block, that is $\Pr[r_i =w_i']  = 1/2^n$. Since the $r_i$ are chosen independently, one has,
\begin{align*}
\Pr[\text{no $1\rightarrow 0$ flips}] &\geq 1-\Pr[\text{some }r_i = w_i']\\
&\geq 1-\ell \frac{1}{2^n} = 1-\frac{\poly(n)}{2^n} = \ngl(n)
\end{align*}
\end{proof}

We now consider the number of possible $0\rightarrow 1$ bit flips in $c$.  A $0\rightarrow 1$ flip on index $i$ occurs when two conditions are met $w_i\neq w_i'$ and $c_i = 0$.  Since the first conditioned is only fulfilled at most $t$ times, we have the following lemma:

\begin{lemma}
\label{lem:correct of cons}
Let $C$ be a code that corrects $t$ $0\rightarrow 1$ bit errors.  Then when $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then 
\[
\Pr[\rep( w', P) = R] \geq 1-\frac{\ell}{2^n}.
\]
That is, \consref{cons:first construction} is correct with error at most $\ell/2^n$.
\end{lemma}

\textbf{Note: }By using a more structured type of code, the necessary error tolerance may be decreased from $t$ to  $(1/2+O(1))t$ by arguing that roughly half of the mismatches between $w, w'$ occur where $c_i =1$.  However, this requires a codeword where most codewords have Hamming weight close to $1/2$.

%\bnote{even if all $t$ are inserted into the bad block its okay right now.  My goal is to decrease the error tolerance of the code below $t$.}
%We now turn to the question of how many $0\rightarrow 1$ bits flips occur between $c, c'$.  Recall that $\dis(w, w')\leq t$ furthermore recall that $w, w'$ are selected independently of the fuzzy extractor, in particular, independently of $c$.  We break the indices of $c = c_1,..., c_\ell$ into two components those where $c_i=1$ and those $c_i = 0$.  Denote these two components by $c_{I_1}$ and $c_{I_0}$.  Our goal is to bound how many of the places where $w, w'$ are inserted into $c_{I_0}$.  Indices $i$,  where $w_i \neq w_i'$ and $c_i = 1$ will be ``corrected'' as $w_i'$ yield $c_i' = 1$ with overwhelming probably~(see \lemref{lem:no 1 to 0 flips}).  Since $w_{err}$ be set of indices where $w, w'$ differ then for all $j\in w_{err}, \Pr[j\in c_{I_0}] = 1/2$.  This is a binomial distribution with $p=1/2$ and $n=t$.  Denote by $X =|\{j | w_j \neq w_j' \wedge j\in c_{I_1}|$. Thus, $\expe[X] = t/2$.  Using Hoeffding's inequality~\cite{hoeffding1963probability}, one has that 
%\begin{align*}
%\Pr[X\leq t(1/2+\alpha)] \leq 1-e^{-2\alpha^2 t}
%\end{align*}
%This leads us to the following lemma:
%\begin{lemma}
%\label{lem:correctness holds}
%If $t = \omega(\log n)$ and that $\dis(w, w')\leq t$ then for any constant $\alpha>0$, then $\Pr[\dis(c, c')\geq t(1/2+\alpha)] \leq 1-e^{-2\alpha t} = 1-\ngl(n)$.
%\end{lemma}
%\bnote{This analysis is done assuming that the two sets are the same size.  Need to worry about variation in the number of $1$s and $0$s in $c$.  Need to show that for most codes (linear?) the variance in Hamming weight is small.}
%\textbf{Note:} If $t = O(\log n)$ the only error tolerance for $C$ that results in decoding with overwhelming probability is $t$.  However, in most use cases, we expect $t=\omega(\log n)$.  

Together, with the arguments in \secref{sec:sec of construction} we have the construction is secure and correctness for block unguessable distributions:
\begin{theorem}
Let $\mathcal{W}$ be a family of $(q,\alpha= \omega(\log n),  \beta)$-unguessable distributions over $\zo^{n\times \ell}$ for any $q = \poly(n)$.  Furthermore, let $C$ be a $(\neigh_t, \epsilon_{code})$-code.  Then for $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\zo^{n\times \ell}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon, s_{sec})$ with error $\epsilon_{code} + \ell/2^n$ and resulting entropy $\tilde{m} =\log |C| - \beta$.
\end{theorem}
\begin{proof}
Security is a result of \thref{thm:security of cons} because $2\epsilon_{obf} + (\ell-\beta)2^{-(\alpha-1)}$ is negligible when $\epsilon_{obf} = \ngl(n), \ell = \poly(n)$ and $\alpha =\omega(\log n)$.  Correctness follows from \lemref{lem:correct of cons} and the fact that $C$ is a $(\neigh_t, \epsilon_{code})$-code.
\end{proof}


%\subsection{Supported Sources}
%\label{sec:supported sources}
%\bnote{need to complete this section and redo the proof in  \secref{sec:sec of construction} with this type of source.}
%
%We now describe a set of high entropy sources appropriate for which the proof outline given in \secref{sec:sec of construction} holds.  We need a distribution where after $q$-equality queries, a large number of blocks still have super-logarithmic entropy.
%
%As long as $\beta$ is small enough, we can show security of \consref{cons:first construction}~(replace \lemref{lem:blocks unguessable after queries} with an unguessable source, the remaining arguments follow).


\subsection{Discussion}
\label{sec:discussion}
Security of \consref{cons:first construction} relies on a large number of dimensions which are unpredictable.  Correction occurs for a small number of dimensions that are allowed to vary completely.  As described in \secref{sec:sec of construction}, \consref{cons:first construction} is not secure for an arbitrary min-entropy source, it must contain a significant number of dimensions that are hard to guess by equality queries~(meaning they have min-entropy).

To show that $gap$ can be negative for \consref{cons:first construction}, we first calculate the size of the Hamming ball.  We allow $t$ errors with an alphabet of size $2^n$.  This means that
\begin{align*}
\log |B_t| &= \log \sum_{i=0}^t {\ell \choose i} (2^n-1)^i\\
&> \log {\ell \choose t} (2^n-1)^t\\
&=\Theta(tn) + \log {\ell\choose t}
% \log (2^n)^{H_{2^n}(t/\ell)\ell - o(\ell)} =n( H_{2^n}(t/\ell)\ell -o(\ell) )
\end{align*}
\bnote{Need a good bound on ${\ell \choose t}$.}
We now ask how much entropy is necessary for security.  The type of source we considered was independent blocks each with super-logarithmic min-entropy.  This yields 
\[
gap = \Hoo(W) - \log |B_t| < \ell \omega(\log n) -\left( \Theta(t n) + \log {\ell \choose t}\right). 
\]
When $t =\Theta(\ell)$, then $gap<0$ and the output entropy is $|C| - \ngl(n)$~(if $C$ is a constant rate, this is $\Theta(\ell)$).
Thus, \consref{cons:first construction} achieves negative $gap$ when $t = \Theta(\ell)$.  Furthermore, not every block must contribute entropy, so security is possible when the total starting entropy is $\omega(\log n)$.  For example, \consref{cons:first construction} is secure when each block is equal and drawn from a distribution of super-logarithmic min-entropy.  So there are supported sources with negative $gap$ for any value of $t$.  We achieve $gap<0$, constant error tolerance, and extract a constant fraction of $n$ when $t = \Theta(\ell)$ and $\ell = O(n)$.
%When $ \ell , t, n$ to be the same order provides a natural balance of parameters. 

\textbf{Limitations of \consref{cons:first construction}:}  There are three major drawbacks to our construction.   The first is that we leak equality information about particular blocks, this may be sensitive information in the presence of auxiliary information.  Second, we obtain only a single bit from each block of $W_i$ which seems sub optimum.  Lastly, Hamming distance is computed over an alphabet of exponential size, which precludes use in some applications. We begin to address this weakness in the next section.\bnote{come back to this linking language}

\section{Sample-then-Obfuscate}
\consref{cons:first construction} is a computational fuzzy extractor with $gap<0$.  In this section, we present a generalization of where the values being obfuscated are sampled from the blocks of the source, but blocks are not individually obfuscated.  This has two advantages, first it simplifies the type of sources we consider as we no longer need high entropy blocks.  Second, it allows us to decrease the size of each block from exponential to a super polynomial $2^{\log^c n}$ for some $c>0$.  Unfortunately, obfuscating multiple blocks together decreases the effective error tolerance.  In ordering for sampling to work properly, we need a source that is likely contribute some entropy from a randomly chosen block.  

\begin{definition}
\label{def:unordered source}
A distribution $W = W_1,..., W_\gamma$ is called a $(\alpha, \beta)$-unordered block source if there exists a set of indices $\mathcal{I}$ where $|\mathcal{I}| \geq \gamma - \beta$ such that the following holds:
\[
\forall i\in \mathcal{I}, \forall w_1,..., w_\gamma \in W_1,..., W_\gamma, \Hoo(W_i | W_1 = w_1,..., W_{i-1}=w_{i-1}, W_{i+1}=w_{i+1},..., W_\gamma = w_\gamma) \geq \alpha.
\]
\end{definition}

\defref{def:unordered source} is a generalization of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}), an unordered block source is one where most blocks contribute some entropy conditioned on the value of the other blocks.  This is a strengthening of an unguessable block distribution~(\defref{def:block guessable}), however we will achieve security for significantly lower entropy levels than were necessary for block unguessable distributions.  
\begin{construction}
\label{cons:sampling}
Let $n$ be a security parameter.
Let $Z$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source where each $W_i$ is over $Z$ and $\gamma = Omega(n )$.  Let $t$ be the desired error tolerance and let $C\subset \zo^\ell$ be a $(\neigh_t, \epsilon_{code})$ code.  Let $\sample_\beta(\gamma)$ output a uniform subset of indices of $1,..., \gamma$ of size $\beta$. Let $\eta = \omega(\log n)$ be some function such that $\eta \log (Z) = \omega(\log n)$.  Let $\mathcal{O}$ be a $\ell$-composable f for the family of point functions over $\zo^\eta$.  Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $c\leftarrow C$.
\item For $i=1,..., \ell$:
\begin{itemize}
\item Sample $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_\eta(\gamma)$
\item If $c_i = 0$: 
\subitem Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\subitem Set $\rho_i = \mathcal{O}(I_{w_i})$.
\subitem Set $p_i = \rho_i, j_{i, 1},..., j_{i, \eta}$.
\item If $c_i = 1$: Sample $r_i \leftarrow U_n$. 
\subitem Let $p_i = \mathcal{O}(I_{r_i}) j_{i,1},..., j_{i, \eta}$.
\end{itemize}
\item Output $(c, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$ 
\item For $i=1,..., \ell$:
\begin{itemize}
\item Parse $p_i$ as $\rho_i, j_{i, 1},..., j_{i, \eta}$.
\item Set $v_i = w_{j_{i, 1}},..., w_{j_{i, \eta}}$.
\item If $\rho_i(w_i') = 1$ set $c_i' = 0$.
\item Else set $c_i' = 1$.
\end{itemize}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.37in}
\end{minipage} 
\end{tabular}
\end{center}
\end{construction}

The main change in \consref{cons:sampling} is that the obfuscated values are concatenated alphabet symbols of $W$.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.  There is a crucial difference, our use of a computationally-secure primitive allows us to sample multiple times and only argue that each $v_i$ has sufficient has individual entropy.  In the information-theoretic setting, it would be necessary to argue about the joint entropy of the obfuscated values.  Our construction uses the na\"{i}ve sampler that takes truly random samples, the analysis is essentially the same for more sophisticated samplers that use fewer random bits.  Goldreich provides an accessible introduction to samplers~\cite{goldreich1997sample}.

As before we must show security and correctness of \consref{cons:sampling}.  For security, the main argument is that each of the $v_i$ has enough entropy to be unguessable.  For correctness we must show that the induced error rate in $v$ and $v'$ is a small constant so that $c'$ will be corrected to $c$ with overwhelming probability.

\subsection{Security of \consref{cons:sampling}}
In order to show security \consref{cons:sampling}, we show that with overwhelming probability each of the selected $v_i$ has super-logarithmic entropy on its own.  This means that  $V_1,..., V_\ell$ is a block-unguessable distribution~(by \clref{cl:all blocks entropy}) and security follows by \thref{thm:security of cons}.  Consider a source with entropy $\lambda$.  The expected number of selected blocks that have entropy at least $\alpha$ is $\frac{\gamma-\beta}{\gamma} \times \beta$.  A good sampler selects within a constant fraction of the expected number of these blocks.

\begin{lemma}
\label{lem:sampling works}
Let $W = W_1,..., W_\gamma$ be a $(\alpha = \Omega(1), \beta \leq \gamma(1/2-\Theta(1))$-unordered block source.  Let $\eta = \Omega(\log n)$. Consider some fixed $i$, the distribution $V_{i}, J_{i, 1},..., J_{i, \eta}$ is $(\ngl(n)+ 2^{-\Omega(n)})$-close to a distribution $V', U_{\beta\times \log \gamma}$ where for all $u\in U_{\beta \times \log \gamma}$ $V' | A=a$, $\Hoo(V' | A=a) \geq \Omega(\alpha\times (1-2\times \beta) \times \eta) = \omega(\log n) = \alpha'$.\bnote{need to come up with the right number of coins for $\sample$}
\end{lemma}
\begin{proof}
By~\cite[Lemma 9]{vadhan2003constructing}, noting that the truly random sampler is optimum in necessary samples and we take $\eta = \Omega(\log n)$ samples.  \bnote{need someone to check parameters, I am fairly sure of the quality of the statement}
\end{proof}
%\begin{align*}
%\expe[\Hoo(V_i)] &=\expe[\text{entropy of each uniform }w_j] (\text{number of }w_j\text{ selected})\\
%&=\frac{\lambda}{\gamma} \times \eta= \frac{\lambda \eta}{\gamma}.
%\end{align*}
%This value is super-logarithmic when $\lambda = \omega (\frac{\gamma \log n}{\eta})$.  Note this is independent of how the entropy is distributed between blocks.  In particular, we make no assumptions about particular blocks being unguessable, only about the overall entropy rate.  This is a much more natural condition for a source than \defref{def:block guessable}.  
%To show security we need the entropy of all $v_i$ to be $\omega(\log n)$ with overwhelming probability.  This follows by an argument from the locally computable extractor literature.

\begin{corollary}
\label{cor:samp sec}
Let $W = W_1,..., W_\gamma$ be a $(\alpha = \Omega(1), \beta \leq \gamma /2-\Omega(1))$-unordered block source.  Let $\eta = \Omega(\log n)$.  
The distribution $V, J$ is $\epsilon_{sam}(\ell (\ngl(n) +2^{-\Omega(n)})) = \ngl(n)$-close to a distribution $V', U_{\beta\times \ell \times \log \gamma}$ where for $u\in U_{\beta\times \ell\times \log \gamma}$ for all $i$, $\Hoo(V_i' | U_{\beta\times \ell \times \log \gamma}) \geq \alpha'$.
\end{corollary}
\begin{proof}
Union bound over the statistical distance in \lemref{lem:sampling works}.
\end{proof}

\begin{corollary}
\label{cor:v are unguessable}
Let $q = \poly(n)$, there exists some $\alpha'' = \omega(\log n)$ such that distribution $V' , U_{\beta \times \log \gamma}$ is a $(q, \alpha'', 0)$-block unguessable distribution.
\end{corollary}
\begin{proof}
Result of \clref{cl:all blocks entropy}.
\end{proof}

\begin{corollary}
\label{cor:samp unguess}
Let $n, Z, W, \epsilon_{sam}$ be as in \corref{cor:samp sec} and let $\mathcal{O}$ is an $\ell$-composable virtual black obfuscator with auxiliary inputs for point programs over $\zo^{\beta\times \log |Z|}$ that is $(s_{obf}, \epsilon_{obf})$-hard, then $H^{\hill}_{\epsilon', s_{obf}}(C | P) \geq |C|$ for $\epsilon' = (2\epsilon_{obf} + \epsilon_{sam}+ 2^{-(\alpha''-1)}) = \ngl(n)$.
\end{corollary}
\begin{proof}
Result of \corref{cor:v are unguessable} and \thref{thm:security of cons}.
\end{proof}


\subsection{Correctness of \consref{cons:sampling}}
The argument that $1\rightarrow 0$ flips are unlikely carries over from \consref{cons:first construction}.  Let $t'$ be the capacity of $C$.  To show correctness we must show that with overwhelming probability $\dis(v, v')\leq t'$.  Let $t$ be the number of supported errors between $w$ and $w'$.  Let $c>0$ and let $t = c(\gamma-\eta)/\eta$, for any  $i$ one has:
% In this section, we analyze what values of $t$ will yield a small constant likelihood of error between $v_i$ and $v_i'$ yielding a constant probability of bit flip $0\rightarrow 1$ between $c$ and $c'$.  Consider a single index $i$ and let $t$ be the number of errors between $w$ and $w'$.  Then 
\begin{align*}
\expe \Pr[v_i = v_i'] &=\Pr[j_{i,1},..., j_{i,\eta}\text{ have no errors}]\\
&\geq \prod_{j=0}^{\eta-1}\left(1 - \frac{t}{\gamma-j}\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{c(\gamma-\eta)/\eta}{\gamma-i}\right)\\
&\geq  \prod_{j=0}^{\eta-1}\left( 1-\frac{c}{\eta}\left(\frac{\gamma-\eta}{\gamma-j}\right)\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{c}{\eta}\right) \\
&\geq \left(1-\frac{c}{\eta}\right)^{\eta} =\left( \left(1-\frac{c}{\eta}\right)^{\eta/c}\right)^c\geq \frac{1}{4}^c
\end{align*} 
Thus, by setting $c = -\log(1-t'/\ell)/2$ we obtain in $\expe[\dis(v, v')\leq t']$.  We need a stronger condition that this occurs with overwhelming probability.

\begin{lemma}
Let $n, W$ be as above and Let $C$ be a code that corrects $t$ $0\rightarrow 1$ bit errors where $t =\Theta(\eta)$.  Let $c = -\log(1-t'/(2\ell))/2 = \Theta(1)$, when $t \leq c(\gamma - \eta)/\eta$, then $\Pr[\dis(v, v')\leq t']\geq 1-O(2^{-\ell})$ where the probability is over the coins of $\gen$.
\end{lemma}
\begin{proof}
With each $i$ we can associate a Bernoulli random variable $X_i \overset{def}= V_i \overset{?}\neq V_i'$.  As stated above, we have that $\expe[X_i] \leq 1-\frac{1}{4}^c = t'/(2\eta)$. By the Chernoff bound, we have
\begin{align*}
\Pr\left[\frac{1}{\eta} \sum_{i=1}^\eta X_i\geq 2\expe[X_i]\right]\leq 2e^{-2\expe[X_i]\eta} = O(e^{- \ell}) 
\end{align*}
\end{proof}

\begin{corollary}
Let $\ell = \omega(\log n)$ and let $C\subset\zo^\ell$ and  be a code that corrects $t' = \Theta(\ell)$ $0\rightarrow 1$ bit errors.  Then when 
\[
\dis(w, w')\leq \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta} = O(\frac{\gamma-\eta}{\eta}) = O(\gamma/\eta)
\] 
and $(R, P)\leftarrow \gen(w)$, then 
\[
\Pr[\rep( w', P) = R] \geq 1-\frac{\ell}{2^n} - O(2^{-\ell}) = 1-\ngl(n).
\]
That is, \consref{cons:sampling} is correct with error at most $\ngl(n)$.

\end{corollary}
\subsection{Discussion of \consref{cons:sampling}}
We now show that \consref{cons:sampling} achieves $gap<0$ for an interesting setting of parameters.  For security of an unordered block source we needed both $\alpha = \Theta(1)$ and $\beta\leq \gamma/2-\Omega(1)$.  This means that the overall required entropy of $\Hoo(W) = \Theta(\gamma)$.  We are able to correct $O(\gamma/\eta)$ errors
This yields:
\begin{align*}
gap &= \Hoo(W) -\log |B_t| \\
&< \Theta(\gamma)- t \log |Z|\\
&= \Theta(\gamma) - O(\gamma/\eta) \log |Z|\\
\end{align*}
That is, \consref{cons:sampling} achieves $gap<0$ when the starting alphabet is super polynomial~(noting that for any fixed $Z$ we can set $\eta = \omega(\log n)$ and $\eta = o(\log |Z|)$).  If we are willing to accept $gap\geq 0$ then \consref{cons:sampling} works for polynomial size alphabet $Z$.
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix
\section{Obfuscation}
\label{sec:obfuscation}
The standard notion of obfuscation is virtual black-box obfuscation~(introduced by Barak et al.~\cite{barak2001possibility}).  This notion is known to be impossible in a black-box way for all polynomial time programs.  Several variants~(best possible obfuscation~\cite{goldwasser2007best}, indistinguishability obfuscation~\cite{barak2001possibility}, differing inputs indistinguishability obfuscation~\cite{barak2001possibility}) have been presented~(see Varia~\cite{varia2010studies} for implications between various definitions).  Indistinguishability obfuscation has recently been shown to be constructible~\cite{garg2013candidate} using multilinear maps~\cite{garg2013multilinear}.  The definition of virtual black box obfuscation is presented in \defref{def:obf}.


In this work, we will use obfuscation of point-programs, 
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}
\]

We present the construction of Canetti~\cite{canetti1997towards}, as an illustration of how point functions can be obfuscated.  Let $G$ be a group:
\begin{enumerate}  
\item Input: string $w\in \zo^n$.
\item Choose a generator $g\overset{\$}\leftarrow G$
\item Compute $h\leftarrow g^w$, where $w$ is viewed as an element of $G$ is some canonical way.
\item Output: circuit that has $g, h$ hardwired, and on input $x$, accepts iff $g^x=h$.
\end{enumerate}
\begin{assumption}[Strong DDH assumption]\label{ass:strong ddh}.  Let $n$ be a security parameter and let $p = 2q+1$ be a randomly chosen $n$-bit safe prime.  Consider the group $Q$ of quadratic residues in $\mathbb{F}_p^*$.  For any $W$ with $\Hoo(W)= \omega(\log n)$ where the domain of $W$ is $\mathbb{F}_q$, for $g\overset{\$}\leftarrow Q, a\leftarrow W, b,c \overset{\$}\leftarrow \mathbb{F}_q$, the ensembles $\langle g, g^a, g^b, g^{ab}\rangle$ and $\langle g, g^a, g^b, g^c\rangle$ are computationally indistinguishable.
\end{assumption}
\begin{theorem}~\cite{canetti1997towards}
The construction above, when instantiated with the group $G = Q$ is a virtual black-box obfuscator for the family of point functions under \assref{ass:strong ddh}.
\end{theorem}

As mentioned above there exist other constructions of point function obfuscation under various assumptions~\cite{lynn2004positive, wee2005obfuscating}.

Lastly, we will need the notion of composable obfuscation.

\begin{definition}[Composable obfuscation~\cite{bitansky2010strong, canetti2008obfuscating,lynn2004positive}]
\label{def:obf comp}
A PPT algorithm $\mathcal{O}$ is a $t$-\emph{composable obfuscator} for the family $\mathcal{C}$ with dependent auxiliary input if functionality and polynomial slowdown hold as before, and the virtual black black-box property holds whenever the adversary and simulator are given up to $t$ circuits in $\mathcal{C}$.  That is, for every PPT $A$ and polynomial $\rho$, there exists a PPT $S$ such that for all sufficiently large $n$, and for all $C_1,..., C_t\in \mathcal{C}_n$, and for all auxiliary inputs $z\in \zo^*$, 
\[
|\Pr[A(\mathcal{O}(C_1), ..., \mathcal{O}(C_t), z) = 1] - \Pr[S^{C_1,..., C_t}(1^n, z) = 1]| < \frac{1}{\rho(n)},
\]
where the probabilities are taken over the random coins tosses of $A, S$ and $\mathcal{O}$.  The runtimes of $A$ and $S$ must be polynomial in the length of their first input.
\end{definition}

\section{Fuzzy Conductors}

\label{sec:conductors}
Fuzzy extractors are known to have strong upper bounds on remaining entropy based on the best error correcting codes.  Fuller, Meng, and Reyzin show that computational information-reconciliation techniques are subject to similar bounds~\cite{fuller2013computational}.  They suggest these bounds may be avoided by outputting a fresh random variable.  This is known as a fuzzy conductor~\cite{KanukurthiR09}.
\begin{definition}
A $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-\emph{fuzzy conductor} with error $\delta$ is a pair of randomized procedures, ``generate''~(\gen') and ``reproduce''~(\rep'), with the following properties:
\begin{enumerate}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $y\in\zo^*$ and a helper string $p\in\zo^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $p\in\zo^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w, w')\leq t$, if $Y, P$ were generated by $(Y, P)\leftarrow \gen'(w)$, then $\rep(w', P) = Y$ with probability~(over the coins of $\gen', \rep'$) at least $1-\epsilon$.  If $\dis(w, w')>t$, then no guarantee is provided about the output of $\rep'$.
\item The security property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $Y$ is close to a high entropy distribution $Z$.  That is, $\exists Z$ with $\Hav(Z | P ) \geq \tilde{m}$ such that $\mathbf{SD}((Y, P), (Z, P))\leq \epsilon$.
\end{enumerate}
\end{definition}

\noindent In this section, we show that fuzzy conductors are subject to the same lower bounds as fuzzy extractors.  This motivates our use of a computational fuzzy conductor in \secref{sec:fuzzy extractors}. 
We borrow the following notation from the work of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08}:
\begin{itemize}
\item A $(\mathcal{M}, K, t)$ code is a subset of $\mathcal{M}$ of size $K$ where a procedure exists that corrects $t$ errors.
%\item $K(\mathcal{M}, t)$ is the largest $K$ for which there exists an $(\mathcal{M}, K, t)$-code.
\item $K(\mathcal{M}, t, S)$ is the largest $K$ such that there exists an $(\mathcal{M}, K, t)$ code all of whose $K$ points belong to $S$.
\item $L(\mathcal{M}, t, m) = \log (\min_{|S| = 2^m} K(\mathcal{M}, t, S))$.  This is the performance of worst error-correcting code for an arbitrary subset of size $2^m$.
\end{itemize}

\begin{lemma}
The existence of a $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-fuzzy conductor implies that $\tilde{m}\leq L(\mathcal{M}, t, m) +1 -\log (1-2\epsilon)$.
\end{lemma}
\begin{proof}  Assume $(\gen', \rep')$ is a fuzzy conductor with the parameters stated.  Let $S$ be a set of size $2^m$ in $\mathcal{M}$ and let $W$ be the uniform distribution over $S$.  Define $(Y, P) \leftarrow \gen'(W)$.  Then there must exist a distribution $Z$ with $\Hav(Z|P) \geq \tilde{m}$ such that $\mathbf{SD}((Y, P), (Z, P))\leq \epsilon$.  By Markov's inequality, there exists some set $S_P$ such that $\Pr[P\in S_p] \geq 1/2$ and $\forall p\in S_P$ one has $\mathbf{SD}(Y | P = p, p ), (Z | P = p, p)<2\epsilon$.  Now applying Markov's inequality of $\max_{z} \Pr[Z=z | P=p]$, there exists a set $S_P'$ such that $\Pr[P\in S_P']>1/2$, and for all $p\in S_P'$, $\Hoo(Z| P =p ) \geq \tilde{m}-1$.  Denote by $p^*$ a value in $S_P\cap S_P'$~(one value must exist).  Then $\mathbf{SD}((Y | P =p^* , p^*), (Z| P = p^*, p^*))\leq 2\epsilon$ and $\Hoo(Z|P=p^*)\geq \tilde{m}-1$.  Denote by the set $T$ the possible values of $Y$ when $P=p^*$.  For the statistical distance property to hold, $|T| \geq  (1-2\epsilon)2^{\tilde{m}-1}$.  Associate with every $y\in T$ some $w\in S$ which could have produced $y$ with nonzero probability given $P=p^*$, and call this map $C$.  $C$ defines an error correcting code with the required parameters.

\end{proof}


%\section{Proof of \lemref{lem:linear codes independent}}
%\label{sec:proof of linear indep}
%\begin{proof}
%Suppose there is some $i$ that is not equiprobable.  Let $C_{i_1} = \{C | C_i =1\}$ and $C_{i_0} = \{C | C_i = 0\}$. First note that since $C$ does not have any constant bits, it dimension is at least $1$ yielding that $|C| = 2^k$ for some $k$. First suppose that $0<|C_{i_1}| < |C_{i_0}|$.  Let $c\in C_{i_1}$ then by linearity $\forall c'\in C_{i_0}$, $c\oplus c' \in C_{i_1}$.  Furthermore, for all distinct $c_1, c_2\in C_{i_0}$, $c_1\oplus c \neq c_2\oplus c$.  This means that $|C_{i_i}|\geq |C_{i_0}|$ as all $c'\oplus c$ must be distinct and contained in $C_{i_1}$.  
%Now suppose that $|C_{i_1}| > |C_{i_0}| >0$, denote the elements of $C_{i_1}$ by $c_1,..., c_{|C_{i_1}|}$, then by linearity $c_1\oplus c_2, c_1\oplus c_3,..., c_1\oplus c_{|C_{i_1}|}$ are all distinct elements of $C_{i_0}$ this means that $|C_{i_0}|\geq |C_{i_1}| -1 $, for the total size to be even this means that $|C_{i_0}|\geq |C_{i_1}|$.  This is a contradiction and completes the proof.
%\end{proof}

\section{Characterizing unguessable block distributions}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and thus a little unwieldy for a distribution.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.  
The majority of the difficulty in characterizing \defref{def:block guessable} is that different blocks may be dependent so a equality query on block $i$ may shape the distribution of block $j$.  Thus, we begin with the case of independent blocks.

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,..., W_\ell$ be a source where blocks are independent.  Let $\alpha$ be a parameter.  Let $\mathcal{I}$ be the set of blocks such that $\Hoo(W_i ) =\alpha $.  Then $W$ is a $(q, \alpha - \log (q+1), \ell - |\mathcal{I}|)$-block unguessable distribution.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then $W$ is a $(q, \omega(\log n), \ell - |\mathcal{I}|)$-block unguessable distribution.
\end{claim}
\begin{proof}
We ignore the blocks outside of $\mathcal{I}$ as queries on these blocks do not change the distribution of $W_i$ where $i\in \mathcal{I}$.  It suffices to show that for all $i\in \mathcal{I}, \Hav(W_i) = \alpha -\log q$.  
Consider a particular block $i$, we measure how an adversary's queries affect the entropy of this block~(we ignore all other blocks as they are independent).  
Let $q_w$ be a query asking if the stored value is $a_w$ and its corresponding response.  
Let $Q_{w_1},A_{w_1},..., Q_{w_q}, A_{w_q}$ be the random variables representing the queries and answers for an  adversary $\mathcal{A}$ making $q$ queries.  We assume a deterministic adversary~(we assume the best coins are in nonuniform advice).  The only dependence between $W_i$ and this view of the adversary is in $A_1,..., A_q$ so we can consider these binary responses.  Our goal is to count the total number of possible responses $A_{w_1},..., A_{w_q}$.  There are two basic cases for $A_{w_1},..., A_{w_q}$: the all zeros string and the case where some query returns $1$.  If some query $Q_{w*}$ returns $1$ then all other $Q_{w_i} = Q_{w*}$ will return $1$ but no other query returns $1$.  These responses can be removed as the response is deterministic.  Let $A'$ represent the sequence with all duplicate queries removed.  This sequence has at most a single $1$.  Thus, the total number of possible responses is $q+1$~(including the duplicates does not change the number of possible responses).  Thus, we have the following,
\begin{align*}
\Hav(W_i | View(S)) &= \Hav(W_i| Q_{w_1}, A_{w_1},..., Q_{w_q}, A_{w_q})\\
&=\Hav(W_i | A_{w_1},..., A_{w_q})\\
&=\Hav(W_i |A') \\
&=\Hoo(W_i) - \log |A'|\\
&=\alpha - \log |A'|\\
&=\alpha - \log (q+1)
\end{align*}
Where the fourth line follows from the third by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
This argument holds for all $i$.  This completes the proof.
\end{proof}
In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} consider block-fixing sources, where each block is either uniform or a fixed symbol~(block fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} captures this type of distribution.  
We now consider more complicated distributions where blocks are not independent.  



\begin{claim}
\label{cl:each block from single seed}
Let $e =\omega(\log n)$ and let $f:\zo^e \rightarrow \zo^{n\times \ell}$ be a function.  Furthermore, let $f_i$ denote the restriction of $f$'s output to the bits $[i\times n,..., i\times n+n-1]$.  If for all $i, f_i$ is injective then $W = f(U_e)$ is a $( q, e - \log (q+1), 0)$-block unguessable distribution.
\end{claim}
\begin{proof}
Since $f$ is injective on each block, $\Hav(W_i | View(A^{\mathcal{O}_{W}(\cdot, \cdot)})) = \Hav(U_e | View(A^{\mathcal{O}_{W}(\cdot, \cdot)}))$.  Consider a query $q_j$ on block $i$, there are two possibilities either $q_j$ is not in the image of $f_i$ on $q_j$ can be considered a query on the preimage $f_i^{-1}(q_j)$.  Thus, a query eliminates at most a single value of $U_e$, this holds for all blocks $i$.  Then similarly to the proof of \clref{cl:independent high ent}, we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $i$, $\Hav(W_i | View(A^{\mathcal{O}_{W}(\cdot, \cdot)})) \geq \Hoo(W_i) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so this is a family of distributions with total entropy $\omega(\log n)$ where \consref{cons:first construction} is secure.  We can prove a similar claim in the case where $f$ is not injectiveall blocks do not a source~(determined by $f$).

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\ell$ be a source.  Let $\alpha =\omega(\log n)$ and suppose that for all $i$ $\Hoo(W_i)\geq \alpha$, then for $q = \poly(n)$, $\alpha ' = \omega(\log n)$, $W$ is a $(q, \alpha', \ell)$-block unguessable distribution.
\end{claim}
\begin{proof}
Consider a particular block $W_i$, it suffices to show that $\Hav(W_i | Q_1,A_1,..., Q_q, A_q) \geq \omega(\log n)$.  By the definition of conditional min-entropy:
\begin{align*}
\Hav(W_i | Q_1, A_1,..., Q_q, A_q) &= -\log (\Pr[\text{some non zero response}] \max_{w\in W_i} \Pr[W_i | \text{non zero response}) \\&+ \Pr[\text{all zero responses}]\max_{w\in W_i} \Pr[W_i = w_i| \text{all zero responses}])\\
&\geq-\log (\Pr[\text{some non zero response}]\times 1 \\&+ \Pr[\text{all zero responses}]\max_{w\in W_i} \Pr[W_i = w_i | \text{all zero responses}]).
\end{align*}
It suffices to show that $\Pr[\text{some non zero response}] = \ngl(n)$ and that $\max_{w\in W_i} \Pr[W_i | \text{all zero responses}] = \ngl(n)$.  We show each of these is turn.

First note that clearly the 1st query returns $1$ with $2^{-\alpha}$ probability~(due to the min-entropy condition).  Conditioned on this response being $0$ the probability of all other events increases by at most negligible amount~(the statistical distance between the original distribution and the distribution conditioned on the first response being $0$ is $\ngl(n)$).  Thus, the total probability of a $1$ is at most:
\[
\Pr[\text{some non zero response}] \leq 2^{-\alpha} + (2^{-\alpha}+\ngl(n)) + ... +(2^{-\alpha}+q\times \ngl(n)) = \ngl(n).
\]
Now consider $\max_{w\in W_i} \Pr[W_i | \text{all zero responses}]$.  For each zero response, the probability of any $w_i$ increases by at most a negligible amount, this means that 
\[\max_{w\in W_i} \Pr[W_i | \text{all zero responses}] = 2^{-\alpha} + q\times \ngl(n) = \ngl(n).\]

Thus, we have the following:
\begin{align*}
\Hav(W_i | Q_1, A_1,..., Q_q, A_q) &\geq-\log (\Pr[\text{some non zero response}]\times 1 \\&+ \Pr[\text{all zero responses}]\max_{w\in W_i} \Pr[W_i = w_i | \text{all zero responses}])\\
& \geq -\log (\ngl(n) + (1-\ngl(n))\ngl(n)) = -\log (\ngl(n)) = \omega(\log (n)).
\end{align*}
This completes the proof.\bnote{someone check this proof!!!! also make it more precise and better}
\end{proof}

Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some blocks to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$, let $X = U_{\log p\times \ell}$ be a distribution and let $f_1,..., f_{\ell}$ be injective functions where $f_i:\zo^{i\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \ell \times \log p$, that is the source has a small number of blocks.}  Then define the distribution $W_1 = f_1(U_{1,...,\ell}), W_2 = f_2(U_{1,..., 2\ell}),...., W_\ell = f_\ell(U)$.  There is an adversary making $2^e\times \ell = \poly(n)$ queries such that $\Hav(W | View(A^{\mathcal{O}_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
We present an adversary $A$~(running in polynomial time) that completely determines the value $x$.  $A$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $A$ queries on $y_1,..., y_p$ exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $A$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered.  The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \ell)$.  Once $x$ has been recovered then $\Hav(W | View(A^{\mathcal{O}_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relied on an adversaries ability to completely determine a block from the previous blocks.  We formalize this notion next.  We defining the entropy jump of a block source as the remaining entropy when other blocks are known:

\begin{definition}
Let $W = W_1,..., W_\ell$ be a source under ordering $i_1,..., i_\ell$.  The \emph{jump} of a block $i_j$ is $J(i_j) = \max|W_{i_j} |$ conditioned on the values $W_{i_1},..., W_{i_{j-1}}$.
\end{definition}

\noindent
We now show that there must be a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\ell$ such that for all $j\le \ell-\beta +1$, $J(i_j) = \log q /\ell$, then $W$ is not $(q, 0, \beta)$-unguessable.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \ell$.  We describe an unbounded adversary that determines $W_1,..., W_{\ell-\beta+1}$.  As before $A$ queries the $q /\ell$ possible values for $W_1$ and determines $W_1$.  Then $A$ queries the $q/\ell$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\ell-\beta+1}$ is learned.  Note the optimum ordering for the adversary can be encoded nonuniformly but it may take an unbounded amount of time/space to construct the support of $W_i | W_1,.., W_{i-1}$.
\end{proof}

\section{Proof of \lemref{lem:cond and cext}}
\label{sec:cond and cext}
\begin{proof}
It suffices to show if there is some distinguisher $D$ of size $s'$ where 
\[\delta^D((\cext(W; X), U_d, P), (U_\kappa, U_d, P))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D'$ of size $s_{cond}$ where for all $Z$ where $\Hav(Z|P)\geq \tilde{m}$ such that
 \[
 \delta^{D'}((Y, P), (Z, P))\geq \epsilon_{cond}.
 \]
Let $D$ be such a distinguisher.  That is,
\[
\delta^D(\ext(X, U_d)\times U_d \times P, U_\kappa\times U_d\times P)> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D'$ as follows.  On input $y, p$ sample $x\leftarrow U_d$, compute $r\leftarrow \cext(y, x)$ and output $D(r, x, p)$.  Note that $|D'| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D'}((Y, P), (Z, P))&= \delta^D((\cext(Y, U_d), U_d, P), \cext(Z, U_d), U_d, P)\\
&\geq \delta^D((\cext(Y, U_d), U_d, P), (U_\kappa\times U_d \times P)) - \delta^D((\cext(Z, U_d), U_d, P), (U_\kappa\times U_d \times P))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Thus $D'$ distinguishes $Y$ from all $Z$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\section{Proof of \thref{thm:security of cons}}
\noindent
We now prove $S$ cannot distinguish between $C$ and $C'$.  
Let $\mathcal{I}$ be the set of all indices in \defref{def:block guessable}.  Recall that $C'$ is a uniformly distributed codeword conditioned on agreeing with $C$ in all coordinates of $C_{\mathcal{I}^c}$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{\mathcal{O}_X(\cdot, \cdot)}(C), S^{\mathcal{O}_X(\cdot, \cdot)}(C')) \le (\ell-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $\mathcal{I}^c$, the statistical distance is at most $2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then, 
\[
\Delta( S^{\mathcal{O}_X(\cdot, \cdot)}(c^*), S^{\mathcal{O}_X(\cdot, \cdot)}(c')) \le ( \ell -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $i\in \mathcal{I}$, $\Hav(W_i | View(S))\geq \alpha$.  The only information about the correct value of $c_i^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $i \in \mathcal{I}$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not, that is suppose, the probability of at least one nonzero response on index $i$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_i$, the probability of this happening when $c^*_i = 1$ is at most $q/2^n$ or equivalently $2^{-n+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }i]\nonumber \\
 &= \Pr[c_i^* =1]\Pr[\text{non zero response location }i\wedge c_i^*=1]\nonumber \\&+ \Pr[c_i^*=0] \Pr[\text{non zero response location }i \wedge c_i^*=0]\nonumber \\
&\le 1\times 2^{-n+\log q} + 1\times  \Pr[\text{non zero response location }i \wedge c_i^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that $n-\log q \geq \alpha$:
\begin{claim}
\label{cl:ent bounded away from n}
$n-\log q \geq \alpha$
\end{claim}
\begin{proof}
It suffices to show that there exists a simulator $S$ making $q$ queries such that the remaining entropy in that block is at most $n-\log q$.  Let $W_i$ be a distribution, consider $S$ that asks the $q$ most likely outcomes, the total probability of this block must be at least $2^{-n+\log q}$.  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_i | View(S)) &\leq  -\log \left(\Pr[\text{some }q_i=1]\times 1+ \Pr[\text{no }q_i=1]\times \Pr[\text{most likely outcome}|q_1,...,q_q]\right)\\
&\leq  -\log \left(\Pr[\text{some }q_i=1]\times 1\right)\\
&=-\log\left( 2^{-n+\log q} \right) = n-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }i \wedge c_i=0] &>2^{-\alpha+1} - 2^{-(n-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_i=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the block unguessability of $W$~(\defref{def:block guessable}).  By the union bound over the indices $i\in\mathcal{I}$ the total probability of a $1$ in $\mathcal{I}$ is at most $(\ell-\beta)2^{-\alpha+1}$. Thus, for all $c_1, c_2\in C| C_{\mathcal{I}^c}$ the statistical distance is at most $(\ell- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}
\end{proof}
By averaging over all points in $C'$ we conclude that $\Delta(S^{\mathcal{O}_X(\cdot, \cdot)}(C), S^{\mathcal{O}_X(\cdot, \cdot)}(C')) < (\ell -\beta)2^{-(\alpha+1)}$.  This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\end{document}