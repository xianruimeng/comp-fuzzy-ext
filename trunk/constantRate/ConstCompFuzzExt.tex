\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}

\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode} 

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}

\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}

\title{Constant Error Rate Computational Fuzzy Extractors\thanks{The work of B. Fuller is sponsored by Assistant Secretary of Defense for Research \& Engineering under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.}}
\author{Ran Canetti \and Benjamin Fuller \and Omer Paneth \and Leonid Reyzin}

\begin{document}
\maketitle


\begin{abstract} 
Fuzzy extractors derive strong keys from noisy sources.  Traditionally, their security is defined information-theoretically.  Fuzzy extractors have strong bounds on the length of the derived key.  In particular, the starting entropy of the source must be significantly higher than the logarithm of number of error patterns corrected.  For many practical noisy sources, like biometrics, this condition is not fulfilled, leaving reliable key derivation unachievable.  

The work of Fuller, Meng, and Reyzin (Asiacrypt 2013), posited these bounds could be overcome by only providing security against computationally-bounded adversaries.  %They provide a construction of a computational fuzzy extractor that corrects a small number of errors without any loss in key length.  
They show the feasibility of constructing a computational fuzzy extractor that does not suffer from the information-theoretic bounds.
%Unfortunately, their construction is limited to very high entropy distributions and does not significantly beat information-theoretic bounds.    In addition, we significantly improve on the allowable error rate and the required entropy rate of each block.  In particular we construct the following:
In this work, we provide the first construction of a fuzzy extractor for a large class of distributions where the starting entropy is smaller than the logarithm of the number of correctable error patterns.

\textbf{Construction:} We construct a computational fuzzy extractor based on obfuscation of point functions.  Let $W = W_1, ..., W_\ell$ be a source composed of blocks over a large alphabet. We flip a coin $c_i$ for each block and either obfuscate the block or a random point.  When a close value $w'$ is input, we can recover most of the coins $c_i$.  Error-tolerance is achieved by choosing $c = c_1,..., c_\ell$ from an error-correcting code.  Our construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~(Eurocrypt 2008).  Our construction is secure as long as a large number of dimensions are unpredictable given equality queries on individual blocks.  Our construction provides the following novel features:
\begin{itemize}
\item The source entropy is smaller than the log of the number of correctable error patterns.  
\item Constant error tolerance rate.  Previous constructions of computational fuzzy extractors corrected a logarithmic fraction of errors.  (Information-theoretic constructions support a constant fraction of errors but are subject to bounds described above.)
\item Security for a large class of distributions.  The main requirement for security is that most blocks have a super-logarithmic amount of entropy.  Previous computational fuzzy extractors required the source to be essentially uniform. 
\end{itemize}
\end{abstract}


\section{Introduction}\label{sec:introduction}
Authentication is a crucial component of system security.  Reliable key derivation significantly simplifies authentication.  However many sources with entropy are noisy, providing close values upon subsequent readings~(examples include biometrics~\cite{daugman2004} and physically unclonable functions~\cite{pappu2002physical}).

Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} derive stable keys from noisy data.  They are composed of two algorithms: \gen takes a source value $w_0$, and produces a key $r$ and a public value $p$, and \rep which takes a nearby value $w_1$ and $p$ producing the original key $r$.  Intuitively, they are two functions: information reconciliation~(to provide the same value with \emph{noisy} readings) and key derivation or privacy amplification~(to transform an entropic source into a uniform key).  Traditionally, these two functions were performed by two separate primitives: a secure sketch~\cite{DBLP:journals/siamcomp/DodisORS08} and a randomness extractor~\cite{nisan1993randomness}.

In fuzzy extractors, there are two competing parameters, security and error-tolerance.  The key should be uniform to an observer but a close  $w_1$ should map to the same key.  In the information-theoretic realm these two parameters are indeed at odds.  Let $t$ be the desired error-tolerance and let $|B_t(\cdot)|$ be the size of a Hamming ball of radius $t$.  Roughly, the length of the key $|r| < \Hoo(W) - \log |B_t(\cdot)|$~(see~\cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08} for the precise conditions, here we use the sphere packing bound as one bound for the best constructible code).  For the remainder of this paper, we denote this quantity by $gap = \Hoo(W) - \log |B_t(\cdot)|$.

However, this does not need to be the case in the computational setting.  Fuller, Meng, and Reyzin defined computational fuzzy extractors which produce pseudorandom instead of truly random keys~\cite{fuller2013computational}.  They posit it may be possible to build a computational fuzzy extractor when $gap$ is small or negative.  We resolve this question and construct a computational fuzzy extractor when $gap<0$.  This is impossible for an arbitrary min-entropy distribution as every point could be within distance $t$.  A necessary condition for security is that a negligible portion of the probability distribution lies within any Hamming ball of radius $t$.  We require a stronger condition that many blocks are unpredictable~(\defref{def:block guessable}).  

In the information-theoretic setting the goal of a fuzzy extractor is to build a key as long as possible.  
In the computational setting, a key may be expanded once it is long enough to run a pseudorandom generator.  Thus, our focus is building a reasonable length key for a class of distributions where nothing is possible information-theoretically.

\textbf{Overview of Construction:} The negative results of Fuller, Meng, and Reyzin~\cite{fuller2013computational} show that building a computational fuzzy extractor using a computational secure sketch and a privacy amplification element is unlikely to be fruitful.  They present two alternatives combining the information-reconciliation and privacy amplification components~(their approach) and using a computational version of a fuzzy conductor~(fuzzy conductors are defined by Kanukurthi and Reyzin~\cite{KanukurthiR09}).  We follow the second approach.

Our construction~(\consref{cons:first construction}) is based on obfuscation.  While black-box obfuscation of polynomial time functions is known to be impossible~\cite{barak2001possibility}, we use the obfuscation of point functions.  This class is known to be achievable under particular number-theoretic assumptions~\cite{canetti1997towards} and generically from generic cryptographic hardness assumptions~\cite{wee2005obfuscating}.  

Consider a source $w = w_1,..., w_\ell$ that is split into blocks~(over some large alphabet).  We would like to tolerate Hamming errors for individual blocks.  The main idea is for each block $i$ we flip a coin $c_i$ and either obfuscate $w_i$ or a random point $r_i$~(this is similar to the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}).  With a close value $w'$ the \rec can determine where random values were obfuscated and where $w_i$ was obfuscated~(with some errors).  Most of the bits $c_i$ are recoverable. To tolerate errors we choose the coins $c$ from an appropriate error-correcting code.  Then, $c$ forms a distribution of high entropy that can be extracted~(either information-theoretically~\cite{nisan1993randomness} or computationally~\cite{DBLP:conf/crypto/Krawczyk10}).  Note it is hard to recover $w$ from the value $c$.  As discussed in~\cite[Section 3.3]{fuller2013computational} this is necessary to overcome the information-theoretic lower bounds.  

Our construction provides several advantages over previous constructions of computational fuzzy extractors:
\begin{itemize}
\item The first computational fuzzy extractor where $gap$ may be negative for a large class of distributions\footnote{There are artificial examples where no loss in necessary in a fuzzy extractor.  If $W$ consists of points from an error-correcting code of distance $2t+1$, then no public information is necessary and $gap$ may be negative.  This violates the spirit of a fuzzy extractor as the distribution is already ``error-corrected.''  For arbitrary min-entropy sources, analysis of information-theoretic fuzzy extractors usually proceeds saying the residual entropy is at least $\Hoo(W)$ less the number of bits needed for error correcting.   For an arbitrary distribution, this is at least $\log |B_t(\cdot)|$.  The distributions supported by our construction are not error-correcting codes, we only require they are unpredictable in many dimensions.}.
\item The first computational fuzzy extractor that allows for a constant fraction of errors.  Previous constructions could correct only  a logarithmic fraction of errors~($O(\log \ell/ \ell)$)
\item The first computational fuzzy extractor where block are not required to be uniformly distributed.  Our construction simply requires a large number of blocks that are unguessable given equality queries.  This subsumes the distributions supported by~\cite[Construction 4.1]{fuller2013computational}.
\end{itemize}

\textbf{Connection with Noisy Point Obfuscation: } Ideally, our construction would be a noisy point obfuscation~(which is a strictly stronger object than a computational fuzzy extractor).  Unfortunately, this is not the case.  Each block is individual obfuscated, an adversary may ask equality on individual blocks.  If not all blocks have high entropy this may allow an adversary to learn significant information about the source.  Thus, our construction is not a virtual black box~(VBB) obfuscation~(where anything learnable by the circuit is simulatable using input/output behavior).  Dodis and Smith~\cite{DBLP:conf/stoc/DodisS05} construct noisy point obfuscation when $gap>>0$.  Constructing VBB obfuscation for sources where the error tolerance is smaller than the starting entropy is an important open question.

\textbf{Limitations: } The main limitation of our construction is that each block is individually obfuscated.  This means that each block much be drawn from a distribution with super-logarithmic entropy.  This also means that our distance is computed over a large alphabet~(exponential) in the security parameter.  Our construction is primarily applicable for burst error-models~\cite{gilbert1960capacity}.  Constructing a computational fuzzy extractor with the above parameters and a small alphabet is an important open problem.


\section{Preliminaries}
\label{sec:preliminaries}
For a random variable $X = X_1||...|| X_n$ where each $X_i$ is over some alphabet $\mathcal{Z}$, we denote by $X_{1,..., k} = X_1||...|| X_k$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.  
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. 
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t(\cdot)|$ the size of a ball of radius $t$.  For our sources, we consider the Hamming metric, for two vectors $x, y$ over $\mathcal{Z}^n$ the Hamming distance between $x,y$ is $d(x,y) = \{i | x_i \neq y_i\}$.  For the Hamming metric, $|B_t(\cdot)| = \sum_{i=0}^t {n \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  \bnote{this is weird here and doesn't fit}  However, we will use codes that only correct one-sided errors~(see \secref{sec:coding theory}).


Usually, we use bold letters for vectors or matrices, capitalized letters for random variables, and lowercase letters for elements in a vector or samples from a random variable. 

\subsection{Randomness Extractors and Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

Here we present relevant extractor definitions.  

We focus on computational fuzzy extractors introduced by Fuller, Meng, and Reyzin~\cite{fuller2013computational}.  Definitions for information-theoretic fuzzy extractors and secure sketches can be found in the work of Dodis et. al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error, as described in~\cite[Sections 8]{DBLP:journals/siamcomp/DodisORS08}.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.  

%\begin{definition}
%\label{def:fuzzy extractor}
%An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$, with the following properties: 
%\begin{enumerate}
%\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\ell$ and a helper string $p\in\{0,1\}^*$.
%\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w,w')\leq t$, if $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$ with probability~(over the coins of $\gen, \rep$) at least $1-\delta$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
%\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
%\end{enumerate}
%A fuzzy extractor is efficient if $\gen$ and $\rep$ run in expected polynomial time.
%\end{definition}
%
%Secure sketches are the main technical tool in the construction of fuzzy extractors.  Secure sketches produce a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
%\begin{definition}
%\label{def:secure sketch}
%An $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$, with the following properties:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|\sketch(W))\geq \tilde{m}$.
%\end{enumerate}
%A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time. 
%\end{definition}
%

We now present the definition of a computational fuzzy extractor~(from~\cite{fuller2013computational}):

%A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}. 
%
%\begin{lemma}
%\label{lem:fuzzy ext construction}
%Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext:\mathcal{M}\times \zo^d \rightarrow \zo^\ell$ be a $(\tilde{m}, \epsilon)$-average-case extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor with error $\delta$:
%\begin{itemize}
%\item $\gen(w):$ generate $x\leftarrow \zo^d$, set $p=(\sketch(w), x), r=\ext(w;x)$, and output $(r,p)$.
%\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $r=\ext(w;x)$.
%\end{itemize}
%\end{lemma}

\begin{definition}[Computational Fuzzy Extractor]\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
Any efficient fuzzy extractor is also a computational fuzzy extractor with the same parameters.  
Note that in the above definition of fuzzy extractors, the errors are chosen before $P$ is known: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness.
For notational convenience, we the definition of computational fuzzy extractors specifies a general class of sources for which the fuzzy extractor is designed to work, rather than the class of sources that consists of all sources of a given min-entropy $m$~(of course, this modification can also be applied to prior definitions of information-theoretic secure sketches and fuzzy extractors).

The work of Fuller, Meng, and Reyzin~\cite{fuller2013computational} presents two approaches for constructing a computational fuzzy extractor: analyzing the information-reconciliation and privacy amplifications components together or using a fuzzy conductor and a privacy amplification component.  We follow the second approach.  
%Our main question is: for what type of sources can we build fuzzy extractors?  Of particular interest are sources where information-theoretic bounds do not allow meaningful constructions~\cite[Lemmas C.1 and C.2]{DBLP:journals/siamcomp/DodisORS08}.  The main limiting factor is the difference between the starting entropy and the size of the ball to be corrected or $gap = \Hoo(W) - \log |B_t(\cdot)$.  When $gap$ is large, information-theoretic fuzzy extractors provide a good solution.  We ask whether security is possible when $gap$ is small or negative.
%In this paper, we ask whether a smaller entropy loss can be achieved by considering a computational fuzzy extractor with a computational security requirement.  %We therefore relax the security requirement of \defref{def:fuzzy extractor} to require a pseudorandom output instead of a truly random output.  

%As in the information-theoretic realm~\cite[Lemma 4.1]{DBLP:journals/siamcomp/DodisORS08}, a computational fuzzy extractor can be constructed by combining a correction component and an privacy-amplication component.  As shown in Fuller, Meng, and Reyzin~\cite[Corollary 3.8 and Theorem 3.10]{fuller2013computational}, the ``right'' correction component is probably not a computational version of a secure sketch~\cite[Definition 3]{DBLP:journals/siamcomp/DodisORS08}.  Instead, we will use a computational fuzzy conductor~(this is a computational analogue of the notion introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).  
We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has 
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition}

We now define the two primitives in this approach: a computational fuzzy conductor and a (computational)~randomness extractor.  A computational fuzzy conductor is the computational analogue of a fuzzy conductor introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}.
\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' (\gen') and ``reproduce'' (\rep') is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon, s_{sec})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $Y\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(Y, P)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', P) = Y] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $Y$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon, s_{sec}}(Y |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}

A computational extractor is the natural adaption of a randomness extractor to the computational setting.  We use the average case version of the notion introduced by Krawczyk~\cite{krawczyk2010cryptographic}:
%start with the standard notion of an average-case extractor. An average-case extractor is a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}) (in particular, Vadhan~\cite[Problem 6.8]{Vad12} showed that all strong extractors are average-case extractors with a slight loss of parameters):
\begin{definition}
Let $\chi_1$, $\chi_2$ be finite sets.
A function $\cext: \chi_1\times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, s_{sec}, \epsilon)$-average-case extractor} if for all pairs
of random variables $X, Y$ over $\chi_1, \chi_2$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{sec}}}((\cext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon$.
\end{definition}

Combining a computational fuzzy conductor and an appropriate randomness extractor yields a computational fuzzy extractor:

\begin{lemma}
\label{lem:cond and ext}
Let $\gen'$, $\rep'$ be a $(\mathcal{M}_1, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\mathcal{M}_2$.  Let $\cext:\mathcal{M}_2\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, s_{ext}, \epsilon_{ext})$-average case computational extractor.  Define $\gen, \rep$ as:
\begin{itemize}
\item $\gen(w; r, x):$ run $(y, p')= \gen'(w; r)$ and set $p = (p', x)$ and $r = \cext(y; x)$,  and output $(r, p)$.
\item $\rep(w, (p', x)):$ recover $y = \rec'(w'; p')$ and output $r = \cext(y; x)$. 
\end{itemize}
Then $\gen, \rep$ is a $(\mathcal{M}_1, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \max\{s_{cond} - |\cext|, s_{ext}\}$.
\end{lemma}
\begin{proof}
It suffices to show if there is some distinguisher $D$ of size $s'$ where 
\[\delta^D((\cext(W; X), U_d, P), (U_\kappa, U_d, P))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D'$ of size $s_{cond}$ where for all $Z$ where $\Hav(Z|P)\geq \tilde{m}$ such that
 \[
 \delta^{D'}((Y, P), (Z, P))\geq \epsilon_{cond}.
 \]
Let $D$ be such a distinguisher.  That is,
\[
\delta^D(\ext(X, U_d)\times U_d \times P, U_\kappa\times U_d\times P)> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D'$ as follows.  On input $y, p$ sample $x\leftarrow U_d$, compute $r\leftarrow \cext(y, x)$ and output $D(r, x, p)$.  Note that $|D'| \approx s' + |\cext| = s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D'}((Y, P), (Z, P))&= \delta^D((\cext(Y, U_d), U_d, P), \cext(Z, U_d), U_d, P)\\
&\geq \delta^D((\cext(Y, U_d), U_d, P), (U_\kappa\times U_d \times P)) - \delta^D((\cext(Z, U_d), U_d, P), (U_\kappa\times U_d \times P))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Thus $D'$ distinguishes $Y$ from all $Z$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\subsection{Coding Theory}
\label{sec:coding theory}
We introduce some notions from the field of binary error correcting codes.  Usually the standard class of errors  is all points within Hamming distance $t$, we will use the Hamming analog of the $Z$-channel where there are flips from $0\rightarrow 1$ but no bit flips from $1\not\rightarrow 0$.
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$. 
\end{definition}
\textbf{Note:} Any code that corrects $t$ Hamming errors corrects $t$ $0\rightarrow 1$ errors, but more efficient codes may be exist for these errors.

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \epsilon)$-code if there exists an efficient procedure $\rec$ such that for all $c\in C, \forall c'\in \neigh(c), \Pr[\rec(c') \neq c] \leq \epsilon$.
\end{definition}

We need one special property of the code, namely that each output bit is set $1$ with probability $1/2$.

\begin{definition}
A binary $(\neigh_t, \epsilon_{code})$ code is output equiprobable\bnote{Find the right name for this} $\forall i, |\{C | C_i =1\}| =|C|/2$.
\end{definition}
All binary linear codes~(without constant bits) are output equiprobable and there exist constant rate output equiprobable codes that correct errors for $t = O(\ell)$~(first constructed by Justesen~\cite{justesen1972class}).  This is true as long as $C$ contains no constant bits.  Usually, if a code has constant bits, we can truncate those bits before encoding and obtain a code with better parameters.  For the remainder of this work when we say a code, we assume there are no constant bits.

\begin{lemma}
Let $C$ be a \emph{linear} binary code with no constant bits.  Then $C$ is output equiprobable.
\end{lemma}
\begin{proof}
Suppose there is some $i$ that is not equiprobable.  Let $C_{i_1} = \{C | C_i =1\}$ and $C_{i_0} = \{C | C_i = 0\}$. First note that since $C$ does not have any constant bits, it dimension is at least $1$ yielding that $|C| = 2^k$ for some $k$. First suppose that $0<|C_{i_1}| < |C_{i_0}|$.  Let $c\in C_{i_1}$ then by linearity $\forall c'\in C_{i_0}$, $c\oplus c' \in C_{i_1}$.  Furthermore, for all distinct $c_1, c_2\in C_{i_0}$, $c_1\oplus c \neq c_2\oplus c$.  This means that $|C_{i_i}|\geq |C_{i_0}|$ as all $c'\oplus c$ must be distinct and contained in $C_{i_1}$.  
Now suppose that $|C_{i_1}| > |C_{i_0}| >0$, denote the elements of $C_{i_1}$ by $c_1,..., c_{|C_{i_1}|}$, then by linearity $c_1\oplus c_2, c_1\oplus c_3,..., c_1\oplus c_{|C_{i_1}|}$ are all distinct elements of $C_{i_0}$ this means that $|C_{i_0}|\geq |C_{i_1}| -1 $, for the total size to be even this means that $|C_{i_0}|\geq |C_{i_1}|$.  This is a contradiction and completes the proof.
\end{proof}



\subsection{Obfuscation}
The standard notion of obfuscation is virtual black-box obfuscation~(introduced by Barak et al.~\cite{barak2001possibility}).  This notion is known to be impossible in a black-box way for all polynomial time programs.  Several variants~(best possible obfuscation~\cite{goldwasser2007best}, indistinguishability obfuscation~\cite{barak2001possibility}, differing inputs indistinguishability obfuscation~\cite{barak2001possibility}) have been presented~(see Varia~\cite{varia2010studies} for implications between various definitions).  Indistinguishability obfuscation has recently been shown to be constructible~\cite{garg2013candidate} using multilinear maps~\cite{garg2013multilinear}.

We present the notion of virtual black-box obfuscation with the weakening that the quality of the simulator is an arbitrarily small $1/\poly$ as all known point obfuscations satisfy this definition.

\begin{definition}~\cite{barak2001possibility, goldwasser2005impossibility}  Let $\mathcal{C}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is an obfuscator for $\mathcal{C}$ with dependent auxiliary input if the following conditions are met:
\begin{enumerate}
\item \emph{Approximate Functionality:}  There exists a negligible function $\epsilon$ such that for every $n\in \mathbb{N}$, every circuit $C\in \mathcal{C}_n$, and every $x\in\zo^n$, 
\[
\Pr[\mathcal{O}(C; r)(x) = C(x)] > 1-\epsilon(n),
\]
where the probability is taken over the randomness $r$.  \emph{Almost exact functionality} is a stronger condition that requires $\mathcal{O}(C;r)\equiv C$ with overwhelming probability ver the random coin tosses $r$.  Finally, if this probability always equals $1$, then $\mathcal{O}$ has \emph{exact functionality}.
\item \emph{Polynomial Slowdown:}  There exists a polynomial $\psi$ such that for every $n$, every circuit $C\in \mathcal{C}_n$, and every possible $r$, the circuit $\mathcal{O}(C; r)$ run-in time at most $\psi(n)$.
\item \emph{Virtual Black-box:}  For every PPT adversary $A$ and polynomial $\rho$, there exists a PPT simulator $S$ such that for all sufficiently large $n$, for all $C\in \mathcal{C}_n$, for all auxiliary inputs $z\in \zo^*$, 
and for all binary predicates $\pi$, 
\[
|\Pr[A(\mathcal{O}(C), z) = \pi(C, z)] - \Pr[S^C(1^n, z) = \pi(C, z)] | < \frac{1}{\rho(n)}
\]
where the first probability is taken over the coin tosses of $A$ and $\mathcal{O}$, and the second probability is taken over the coin tosses of $S$.  Furthermore, the runtime of $A$ and $S$ must be polynomial in the length of their first input.
\end{enumerate}
\end{definition}

In this work, we will use obfuscation of point-programs, 
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}
\]

We provide the construction of Canetti~\cite{canetti1997towards}, as an illustration of how point functions can be obfuscated.  Let $G$ be a group:
\begin{enumerate}  
\item Input: string $w\in \zo^n$.
\item Choose a generator $g\overset{\$}\leftarrow G$
\item Compute $h\leftarrow g^w$, where $w$ is viewed as an element of $G$ is some canonical way.
\item Output: circuit that has $g, h$ hardwired, and on input $x$, accepts iff $g^x=h$.
\end{enumerate}
\begin{assumption}[Strong DDH assumption]\label{ass:strong ddh}.  Let $n$ be a security parameter and let $p = 2q+1$ be a randomly chosen $n$-bit safe prime.  Consider the group $Q$ of quadratic residues in $\mathbb{F}_p^*$.  For any $W$ with $\Hoo(W)= \omega(\log n)$ where the domain of $W$ is $\mathbb{F}_q$, for $g\overset{\$}\leftarrow Q, a\leftarrow W, b,c \overset{\$}\leftarrow \mathbb{F}_q$, the ensembles $\langle g, g^a, g^b, g^{ab}\rangle$ and $\langle g, g^a, g^b, g^c\rangle$ are computationally indistinguishable.
\end{assumption}
\begin{theorem}~\cite{canetti1997towards}
The construction above, when instantiated with the group $G = Q$ is a virtual black-box obfuscator for the family of point functions under \assref{ass:strong ddh}.
\end{theorem}

As described above there exist other constructions of point function obfuscates under various assumptions~\cite{lynn2004positive, wee2005obfuscating}.

Lastly, we will need the notion of composable obfuscation.

\begin{definition}[Composable obfuscation~\cite{bitansky2010strong, canetti2008obfuscating,lynn2004positive}].  A PPT algorithm $\mathcal{O}$ is a $t$-\emph{composable obfuscator} for the family $\mathcal{C}$ with dependent auxiliary input if functionality and polynomial slowdown hold as before, and the virtual black black-box property holds whenever the adversary and simulator are given up to $t$ circuits in $\mathcal{C}$.  That is, for every PPT $A$ and polynomial $\rho$, there exists a PPT $S$ such that for all sufficiently large $n$, and for all $C_1,..., C_t\in \mathcal{C}_n$, and for all auxiliary inputs $z\in \zo^*$, 
\[
|\Pr[A(\mathcal{O}(C_1), ..., \mathcal{O}(C_t), z) = 1] - \Pr[S^{C_1,..., C_t}(1^n, z) = 1]| < \frac{1}{\rho(n)},
\]
where the probabilities are taken over the random coins tosses of $A, S$ and $\mathcal{O}$.  The runtimes of $A$ and $S$ must be polynomial in the length of their first input.
\end{definition}

\section{A Constant Rate Computational Fuzzy Extractor}

Before describing our construction, we recall the problem we are trying address.  The goal is to derive strong keys from noisy sources.  As discussed in the introduction, the important parameter is the difference between the total entropy and the number of errors being corrected.  We call this value the entropy gap or $gap= \Hoo(W) - \log|B_t(\cdot)|$.  The results of Dodis et al.~\cite[Lemmas C.1 and C.2]{DBLP:journals/siamcomp/DodisORS08} show that gap is an upper bounded for the length of the derived key. 

If $gap> \omega(\log n)$~(ignoring losses due to randomness extraction), then a key of any length can be formed by using an information-theoretic fuzzy extractor~(which yields nearly this many bits using an optimal code) and expanding the output with a pseudorandom generator~(or using a secure sketch with a computational extractor).  Using an information-theoretic analysis of a fuzzy extractor this seems to be the best construction possible.  As described in Fuller, Meng, and Reyzin~\cite{fuller2013computational}, use of a computational fuzzy extractor may allow a construction when $gap = O(\log n)$.  Their construction is only applicable for high entropy input and yield a result when $gap$ is small.  We will provide the first construction where $gap$ is negative.  

\subsection{First $gap<0$ construction}
\begin{construction}
\label{cons:first construction}
Let $n$ be a security parameter and let $W = W_1,..., W_\ell$ be a distribution over $\zo^{\ell n}$.  Let $\mathcal{O}$ be a $\ell$-composable obfuscator for the family of point functions over $\zo^n$.  Let $t$ be the desired error-tolerance and let $C\subset \zo^\ell$ be an output equiprobable $(\neigh_t, \epsilon_{code})$-error-correcting code.  Let $\ext : \zo^n\times \zo^* \rightarrow \zo^r$ be a $(\tilde{m}, \epsilon_{ext})$-extractor where $\tilde{m}= XXX$. \bnote{fill this in}
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C, seed\leftarrow \zo^*$.
\item For $i=1,..., \ell$:
\subitem If $c_i = 0$: $p_i = \mathcal{O}(I_{w_i})$.
\subitem If $c_i = 1$: Sample $r_i \leftarrow U_n$. 
\subsubitem Let $p_i = \mathcal{O}(I_{r_i})$.
\item Let $r = \ext(c, seed)$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p, seed)$ 
\item For $i=1,..., \ell$:
\subitem If $p_i(w_i') = 1$ set $c_i' = 0$.
\subitem Else set $c_i' = 1$.
\item Set $c = \decode$.
\item Output $r  = \ext (c', seed)$.
\end{enumerate}
\vspace{0.45in}
\end{minipage} 
\end{tabular}
\end{center}
\label{cons:informal construction}
\end{construction}

\textbf{Notes:}  We make several observations about the above construction:
\begin{itemize}
\item If $C$ is output equiprobable, in expectation, half of $w$ is information-theoretically unknown conditioned on $p$.  As described in~\cite[Section 3.3]{fuller2013computational} some type of lossiness is necessary to avoid coding bounds.
\item There are two possible reasons for a bit of $c_i'$ to be $1$.  Because the true value was $1$~(there is little chance of $1$ being incorrectly decoded as $0$ assuming $w_i'$ is independent of the sketch) and because $w_i \neq w_i'$.  However if a bit of $c_i'$ is $0$ this likely means that $w_i=w_i'$ because collisions when the $c_i=0$ are unlikely~(occurring with probability roughly $2^{-n}$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips, instead of a code that corrects all Hamming weight $t$ errors.
\item The extractor can be removed from the construction leaving a computational fuzzy conductor~(\defref{def:comp fuzzy cond}).
\end{itemize}

\subsection{Security of Construction}
Assuming virtual black-box obfuscation, we can argue security in the presence of equality oracles for each block.  The adversary is provided with equality oracles for each block individually.  Security rests on an adversaries inability to learn the values using only \emph{adaptive} equality queries.  For illustrative purposes, we begin with the setting where blocks are independent and each have super-logarithmic entropy and then move to the setting of correlated blocks with some entropy deficient blocks.  Our proof for independent and high entropy blocks is more complicated than necessary, but the structure will hold for the more complicated setting.  The outline of the proof follows:

\begin{itemize}
\item Show that conditioned on the adversaries view there is a large set of blocks, $\mathcal{I}$, that still have a super logarithmic entropy.
\item Show that conditioned on the values of all indices $c_i, i\not \in \mathcal{I}$ there are many possible values of $c\in C$.
\item For all indices in $\mathcal{I}$, it is impossible to tell if you presented with an obfuscation of $W_i$ or a uniform random variable.
\item Any two codewords, $c_1, c_2$ differing in only these positions are statistically close given the adversaries view.
\item The set of codewords differing in these positions form a distribution with conditional entropy.  This is sufficient to show
\end{itemize}

\begin{lemma}
Let $n$ be a security parameter.  Let $W(n) = W = W_1,..., W_t$ be a distribution where each $W_i$ is independent of all $W_j$ and $\forall i, \Hoo(W_i) = \omega(\log n)$.  Then let $\mathcal{A}$ be an adversary making a polynomial number of oracle equality queries for blocks $i$.  %Let $C = c_1,.., c_\ell$ be as in \consref{cons:informal construction}. 
Then $\forall i, \Hav(W_i | View(A))  = \omega(\log n)$.
\end{lemma}
\begin{proof}
Consider a particular block $i$, we measure how an adversaries queries affect the entropy of this block~(we can ignore the other blocks as they are independent).  
Let $q_w$ be a query asking if the stored value is $a_w$ and its corresponding response.  
Let $Q_{w_1},A_{w_1},..., Q_{w_q}, A_{w_q}$ be the random variables representing the queries and answers for an  adversary $\mathcal{A}$ making $q$ queries.  We assume a deterministic adversary (the adversary is unbounded and thus can compute the best possible queries).  The only dependence between $W_i$ and this view of the adversary is in $A_1,..., A_q$ so we can consider these binary responses.  (There is dependency between $W_i$ and the queries but it is all contained in $A_{w_1},..., A_{w_q}$.)  Our goal is to count the total number of possible responses $A_{w_1},..., A_{w_q}$.  There are two basic cases for $A_{w_1},..., A_{w_q}$: the all zeros string and the case where some query returns $1$.  If some query $Q_{w*}$ returns $1$ then all other $Q_{w_i} = Q_{w*}$ will return $1$ but no other query returns $1$.  These responses can be removed as the response is deterministic.  Let $A'$ represent the sequence with all duplicate queries removed.  This sequence has at most a single $1$.  Thus, the total number of possible responses is $q+1$.  Thus, we have the following,
\begin{align*}
\Hav(W_i | View(A)) &= \Hav(W_i| Q_{w_1}, A_{w_1},..., Q_{w_q}, A_{w_q})\\
&=\Hav(W_i | A_{w_1},..., A_{w_q})\\
&=\Hav(W_i |A') \\
&=\Hoo(W_i) - \log |A'|\\
&= \omega(\log n) - \log |A'|\\
&= \omega(\log n) - \log (q+1) = \omega(\log n) - O(\log(n)) = \omega(\log n)
\end{align*}
Where the fourth line follows from the third by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
This argument holds for all $i$.  This completes the proof.
%There are two possible outcomes to an equality query $1$ in which case $\Hoo(W|Q = q_w) = 0$ and $0$ in which case $\Hoo(W| Q =q_w ) \geq -\log 2^{-\Hoo(W)}(1 - 2^{-\Hoo(W)})$~(every remaining outcome is scaled by the $w$ not being a possible outcome).  Thus,
%\begin{align*}
%\Hav(W | Q_w) &= -\log \left(\Pr[W=w]2^{-0} + \Pr[W\neq w] 2^{-\Hoo(W)}(1 - 2^{-\Hoo(W)})\right)\\
%&\geq -\log \left(2^{-k} + (1-2^{-k})2^{-k}(1-2^{-k})\right)\\
%&\geq -\log \left(2^{-(k)}(1+(1-2^{-k})^2)\right)\geq k-1
%\end{align*}
%Now consider the distribution $W' = W | W\neq w$, by the same argument asking a single query of $W'$ reduces it min-entropy by at most one bit~(we need only consider the case where previous responses were all zeros as the entropy is already $0$ if a previous response is $1$).  Thus, for some polynomial number of queries $n_q$ we have that $\Hav(W | Q_1,..., Q_{n_q}) = k - n_q$.  Thus for $k = \omega(\log n)$ and $n_q = \poly(n)$\bnote{this is too big a loss} %we have that $k-n_q = \omega(\log n)$.  This completes the proof of the lemma. 
\end{proof}
\begin{lemma}
\label{lem:super log insist by equal queries}
Let $W$ and $U$ be two distributions of super-logarithmic entropy.  Then for all adversaries $A$, $q = poly(n)$ queries, $\Delta(View(A^{\mathcal{O}_W(\cdot)}), View(A^{\mathcal{O}_U(\cdot)}))\leq \ngl(n)$.
\end{lemma}
\begin{proof}
It suffices to show that with the probability of all responses being $0$ is overwhelming.  Clearly, in the case when all responses are $0$ the views are identical, and thus the statistical distance is $0$.  \bnote{Finish this proof}
\end{proof}

\begin{lemma}
\label{lem:code bits statistically indist}
Let $X_i = W_i$ if $c_i = 1$, otherwise let $X_i = U$.  Then, for every $i$, $\Delta(View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)})| c_i = 1, View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)} | c_i =0) \leq \ngl(n)$.
\end{lemma}
\begin{proof}
\bnote{Do this}
\end{proof}
We can extend this lemma to the entire codeword $C$:
\begin{lemma}
For all $c_1, c_2 \in C$ we have the following: 
\[
\Delta(View(A^{\mathcal{O}_{(X_{c_{1_1}},..., X_{c_{2_\ell}})}(\cdot, \cdot)})| C = c_1, View(A^{\mathcal{O}_{(X_{c_{2_1}},..., X_{c_{2_\ell}})}(\cdot, \cdot)} | C = c_2)) \leq \ngl(n).
\]
\end{lemma}
\begin{lemma}
If $C$ is output then, for all $i$, $\Hav(C_i |View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)}))\geq 1 - \ngl(n)$.
\end{lemma}
\begin{proof}
Since $C$ is output equiprobable we know that for each $i$, $\Pr[C_i =1 ] = 1/2$.  Let $e$ be the exponent of the particular negligible value above~(note $e = \omega(1)$).  For $b\in\{0,1\}$, by \lemref{lem:code bits statistically indist}, we have that in expectation each for each $i$, $\Pr[C_i =1 ] = 1/2$. 
\begin{align*}
\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\Pr[C_i =b | view ] &\leq 1/2 + \ngl(n)\\
\end{align*}
If this was not true~(that is in expectation, there was some $b$ that was likely with probability greater than $1/2+1/\poly(n)$), there there would exist an unbounded adversary outputting that $b$, contradicting \lemref{lem:code bits statistically indist}).  
Taking the negative log of each side one has:
\begin{align*}
\Hav(C_i |  View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot)}))
 &=\\-\log \left(\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\max_{b\in\{0,1\}}\Pr[C_i =b | view ]  \right) &\geq -\log (1/2 + \ngl(n))\\
&\geq -\log(\frac{1}{2} + \frac{1}{n^{w}} )= -\log \left(\frac{n^{w}+2}{2n^{w}}\right)\\
%&=-\left(\log (n^w+2) -\log 2 -\log n^w \right)\\
&=1-\left(\log (1+\frac{2}{n^e}) \right)\\
&=1-\frac{1}{\ln 2}\left(\ln (1+\frac{2}{n^e}) \right)\\
&= 1- \frac{1}{\ln 2}\left(\sum_{i=1}^\infty \frac{(-1)^{i+1}}{i} (\frac{2}{n^e})^i\right) \\
&\geq 1 - \frac{1}{\ln 2} \frac{2}{n^e} = 1-\ngl(n)
\end{align*}
Here the second to last line is derived using the Taylor expansion for $\ln(1+x)$ and the last line is derived by noting it is a geometric series and thus all terms $i=3,....$ are contained in the second term $i=2$ which is positive.
\end{proof}
By similar reasoning we can extend this lemma to the entire codeword:
\begin{lemma}
If $C$ is output equiprobable then, $\Hav(C | View(A^{\mathcal{O}_{(X_{c_{2_1}},..., X_{c_{2_\ell}})}})) \geq |C| - \ngl(n)$.
\end{lemma}
%\begin{lemma}
%$\Hoo(C | View (A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)})) \geq \log |C| - o(\ell \log n)$.
%\end{lemma}
\begin{corollary}
\consref{cons:first construction} is a $XXXXX$ computational fuzzy extractor when $W = W_1,..., W_n$ all $W_i$ are independent and $\Hoo(W_i) = \omega(\log n)$.
\end{corollary}
\begin{proof}
Result of security of point obfuscation and \lemref{lem:cond and ext}.
\end{proof}

\subsection{Correctness of \consref{cons:first construction}}
Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \ell/2^n = \ngl(n)$.
\end{lemma}
\begin{proof}
\bnote{we need to come back to this if we use obfuscation with error}
Recall that $c$ contains at most $\ell$ $1$s.  Since $w'$ is chosen independently of the points $r_i$, this probability is the size of each block, that is $\Pr[r_i =w_i']  = 1/2^n$. Since the $r_i$ are chosen independently, one has,
\begin{align*}
\Pr[\text{no $1\rightarrow 0$ flips}] &\geq 1-\Pr[\text{some }r_i = w_i']\\
&\geq 1-\ell \frac{1}{2^n} = 1-\frac{\poly(n)}{2^n} = \ngl(n)
\end{align*}
\end{proof}
\bnote{even if all $t$ are inserted into the bad block its okay right now.  My goal is to decrease the error tolerance of the code below $t$.}
We now turn to the question of how many $0\rightarrow 1$ bits flips occur between $c, c'$.  Recall that $\dis(w, w')\leq t$ furthermore recall that $w, w'$ are selected independently of the fuzzy extractor, in particular, independently of $c$.  We break the indices of $c = c_1,..., c_\ell$ into two components those where $c_i=1$ and those $c_i = 0$.  Denote these two components by $c_{I_1}$ and $c_{I_0}$.  Our goal is to bound how many of the places where $w, w'$ are inserted into $c_{I_0}$.  Indices $i$,  where $w_i \neq w_i'$ and $c_i = 1$ will be ``corrected'' as $w_i'$ yield $c_i' = 1$ with overwhelming probably~(see \lemref{lem:no 1 to 0 flips}).  Since $w_{err}$ be set of indices where $w, w'$ differ then for all $j\in w_{err}, \Pr[j\in c_{I_0}] = 1/2$.  This is a binomial distribution with $p=1/2$ and $n=t$.  Denote by $X =|\{j | w_j \neq w_j' \wedge j\in c_{I_1}|$. Thus, $\expe[X] = t/2$.  Using Hoeffding's inequality~\cite{hoeffding1963probability}, one has that 
\begin{align*}
\Pr[X\leq t(1/2+\alpha)] \leq 1-e^{-2\alpha^2 t}
\end{align*}
This leads us to the following lemma:
\begin{lemma}
\label{lem:correctness holds}
If $t = \omega(\log n)$ and that $\dis(w, w')\leq t$ then for any constant $\alpha>0$, then $\Pr[\dis(c, c')\geq t(1/2+\alpha)] \leq 1-e^{-2\alpha t} = 1-\ngl(n)$.
\end{lemma}
\bnote{This analysis is done assuming that the two sets are the same size.  Need to worry about variation in the number of $1$s and $0$s in $c$.  Need to show that for most codes (linear?) the variance in Hamming weight is small.}
\textbf{Note:} If $t = O(\log n)$ the only error tolerance for $C$ that results in decoding with overwhelming probability is $t$.  However, in most use cases, we expect $t=\omega(\log n)$.  



\subsection{Supported Sources}
\label{sec:supported sources}
\bnote{need to complete this section and redo the proof in 3.2 with this type of source.}
We now describe a set of high entropy sources appropriate for our construction.  After giving the description of the distribution, we give examples of distributions that fit in this class and provide additional characterizations.

\begin{definition}
\label{def:block guessable}
Let $\mathcal{O}_{w_1,..., w_k}$ be an oracle that return $\mathcal{O}_{w_1,..., w_k}(i, w_i')=\left( w_i\overset{?}=w_i'\right)$.
A source $W = W_1||...|W_k$ is a $(q, \alpha, \ell)$-\emph{guessable block distribution} if for any adversary $A$~(not bounded in time or space) with oracle access to $\mathcal{O}$ making at most $q$ queries there exists a set $S$ of size at least $\ell$ such that 
\[
\forall i\in S, \Hav(W_i |View(A^{\mathcal{O}_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}
This type of source seems to be inherently adaptive and is difficult to formulate in a clean information-theoretic notion.  However, we can show necessary and sufficient types of sources for a guessable block distribution.  We begin by defining the entropy jump of a block source which will appear in our characterization.

\begin{definition}
Let $W = W_1,..., W_k$ be a source under ordering $i_1,..., i_k$.  The \emph{entropy jump} of a block $i_j$ is $J(i_j) = \Hav(W_{i_j} | W_{i_1},..., W_{i_{j-1}})$.
\end{definition}

\begin{claim}
Let $W$ be a $(q, \alpha, \ell)$-guessable block distribution.  Then for all orderings $i_1, ...., i_k$ there is some $j\leq k-\ell$ such that $J(i_j)> \log q + \alpha$.
\end{claim}
\begin{proof}
Suppose not, for convenience assume the ordering that violates this condition is $1,..., k$.  We describe a distribution $W$ where $\forall 1\leq j\leq k-\ell$, $J(j)\leq \log q +\alpha$. Define $W$ as follows $W_1 = c_1,...., W_{j-1} = c_{j-1}$ are constants.
\end{proof}

\subsection{Discussion}
Security of \consref{cons:first construction} relies on a large number of dimensions which are unpredictable.  Correction occurs in a small number of dimensions that are allowed to vary completely.  As described in \secref{sec:supported sources}, \consref{cons:first construction} is not secure for an arbitrary min-entropy source, it must contain a significant number of dimensions that are hard to guess by equality queries~(meaning they have min-entropy).

To show that $gap$ can be negative for \consref{cons:first construction}, we first calculate the size of the Hamming ball.  We allow $t$ errors with an alphabet of size $2^n$.  This means that
\begin{align*}
\log |B_t(\cdot)| &= \log \sum_{i=0}^t {\ell \choose i} (2^n-1)^i\\
&> \log {\ell \choose t} (2^n-1)^t\\
&=\Omega(tn) + \log {\ell\choose t}
% \log (2^n)^{H_{2^n}(t/\ell)\ell - o(\ell)} =n( H_{2^n}(t/\ell)\ell -o(\ell) )
\end{align*}
We now ask how much entropy is necessary for security?  The first presented type of source was where each block was independent and had super-logarithmic min-entropy.  This means that $\Hoo(W) \approx \ell \omega(\log n)$.  Thus in the setting where $t = \omega(\ell)$ we obtain that 
\[
gap = \Hoo(W) - \log |B_t(\cdot)| < \ell \omega(\log n) -\left( \omega(\ell n) + \log {\ell \choose t}\right)< 0. 
\]
This \consref{cons:first construction} is the first fuzzy extractor with a negative $gap$.\footnote{We can make gap smaller by recalling that note every block must contribute fresh entropy.  If each block is derived from a single source with super-logarithmic entropy then the starting entropy need only be $\omega(\log n)$.}

\textbf{Note:} If $t = o(\ell)$ then the value of $gap$ is not clear, there are some $\ell$ for which it will be negative.  However in the setting where $n>>\ell$ and a constant number of errors are being corrected $gap$ may be positive.  However, we extract from at most $\ell$ bits and thus need $\ell$ to be large enough for reasonable application security.  Setting $ \ell , t, n$ all of roughly the same size provides a natural balance of parameters.

\textbf{Limitations of \consref{cons:first construction}:}  There are three major drawbacks to our construction.  The first is that Hamming distance is over an alphabet of exponential size, which precludes use is several major settings.  The second is that we leak equality information about particular blocks, this may be sensitive information in the presence of auxiliary information.  Lastly, we obtain only a single bit from each block of $W_i$, in settings where entropy is at a premium this seems wasteful.
\section{Second construction}
\bibliographystyle{alpha}
\bibliography{crypto}

%\appendix
%\section{Computational Entropy}
%HILL entropy is a commonly used computational notion of entropy \cite{DBLP:journals/siamcomp/HastadILL99}.  It was extended to the conditional case by Hsiao, Lu, Reyzin~\cite{DBLP:conf/eurocrypt/HsiaoLR07}. 



\end{document}