\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}
\def\blinded{0}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\cond}{\ensuremath{\mathtt{cond}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\blinded=0
\newcommand{\blind}[1]{{#1}}
\else
\newcommand{\blind}[1]{}
\def\shownotes{0}
\fi


\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi


\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Vol}{\mathsf{Vol}}

\title{Fuzzy Extractors for Noisy Sources with More Errors Than Entropy}
%\author{Ran Canetti \and Benjamin Fuller \and Omer Paneth \and Leonid Reyzin \and Adam Smith}
\blind{
\author{Ran Canetti\footnote{Email: {\tt canetti@cs.bu.edu}. Boston University and Tel Aviv University.} \and Benjamin Fuller\footnote{Email: {\tt bfuller@cs.bu.edu}.  Boston University and MIT Lincoln Laboratory.} \and Omer Paneth\footnote{Email: {\tt paneth@cs.bu.edu}. Boston University.} \and Leonid Reyzin\footnote{Email: {\tt reyzin@cs.bu.edu}.  Boston University.} \and Adam Smith\footnote{Email: {\tt asmith@cse.psu.edu}.  Pennsylvania State University; work performed while at Boston University's Hariri Institute for Computing and RISCS Center, and Harvard University's
``Privacy Tools'' project.} }
}
\begin{document}
\maketitle


\begin{abstract}
Fuzzy extractors (Dodis et al., Eurocrypt 2004) convert repeated noisy readings of a high-entropy secret into the same uniformly distributed key. To eliminate noise, they require an initial enrollment phase that takes the first noisy reading of the secret and produces a nonsecret helper string to be used in subsequent readings. This helper string reduces  the entropy of the original secret---in the worst case, by as much as the logarithm of the number of tolerated error patterns. For many practical sources of secrets, reliability demands that the number of tolerated error patterns is large, making this loss greater than the original entropy of the secret. We say that such sources have \emph{more errors than entropy}.  Most known approaches for building fuzzy extractors cannot be used for such sources.

%\emph{Reusable} fuzzy extractors (Boyen, CCS 2004)
% remain secure even when the initial enrollment phase is repeated multiple times on correlated readings, producing multiple helper strings. Prior approaches to building reusable fuzzy extractors severely restricted allowed correlations among initial enrollment readings.
%   
%We provide constructions of fuzzy extractors for large classes of sources with more errors than entropy.  We also construct the first reusable fuzzy extractors that has no restrictions on correlations among multiple readings of the source. Our constructions exploit the structural properties of a source in addition to its entropy guarantees. One is information-theoretic; the other two rely computational assumptions. They differ in the types of sources they support.

We provide constructions of fuzzy extractors for large classes of sources with more errors than entropy.  Our constructions exploit the structural properties of a source in addition to its entropy guarantees. Some are made possible by relaxing the security requirement from information-theoretic to computational.

%One is information-theoretic; the other two rely computational assumptions. They differ in the types of sources they support.

\emph{Reusable} fuzzy extractors (Boyen, CCS 2004)
 remain secure even when the initial enrollment phase is repeated multiple times with the same or correlated secrets, producing multiple helper strings. By relying on computational security, we construct the first reusable fuzzy extractors that make no assumption about how multiple readings of the source are correlated.


\iffalse

We construct fuzzy extractors for large classes of such sources by exploiting  their structural properties in addition to their entropy guarantees.  
We show an information-theoretic construction and two computational constructions with additional features.  Our constructions are for the Hamming metric over a nonbinary alphabet.

The information-theoretic construction supports sources in which most symbols contributes fresh entropy. The first computational construction has the additional feature of \emph{reusability}~(Boyen, CCS 2004), allowing multiple correlated 

\begin{itemize} \item \textbf{Information-theoretic}  Our first construction supports sources where each symbol contributes fresh entropy and corrects a constant fraction of errors over a super constant size alphabet.  

\item \textbf{Computational}
%By adding computational techniques we obtain constructions with additional features.  
Our second construction is also secure when each symbol contributes fresh entropy.  
Additionally, it is a reusable fuzzy extractor~(Boyen, CCS 2004).  A reusable fuzzy extractor allows multiple readings of the source to be enrolled securely.  Our construction is reusable if each individual reading is a supported source.  Previous reusable fuzzy extractors required the correlation between repeated readings to be drawn from a small and unrealistic class.  This construction corrects a sub-constant fraction of errors over a super constant size alphabet.  %Computational techniques allow us to show a very sshows that computational assu computationaprovide requires a constant fraction of symbols to have a constant amount of entropy conditioned on prior symbols. It corrects a sub-constant fraction of errors and supports super-constant size alphabets.  In addition, it is first construction of a reusable fuzzy extractor without assumption on the form of errors that occurs between reuse.  This is impossible in the information-theoretic setting for an arbitrary number of reuses~(Boyen, CCS 2004).

Our third construction strengthens the class of supported sources and corrects a constant fraction of errors.  It is secure when individual symbols have high entropy but may be arbitrarily correlated.  This change requires a significantly larger alphabet.

The computational constructions can be implemented efficiently based on number-theoretic assumptions or assumptions on cryptographic hash functions.
\end{itemize}
\fi

\medskip

\textbf{Keywords} Fuzzy extractors, reusability, key derivation, error-correcting codes, computational entropy, point obfuscation.
\end{abstract}


\section{Introduction}\label{sec:introduction}

\paragraph{Fuzzy Extractors}
Cryptography relies on long-term secrets for key derivation and authentication. However, many sources with sufficient randomness to form long-term secrets provide similar but not identical values of the secret at repeated readings. Prominent examples include biometrics and other human-generated data~\cite{daugman2004,zviran1993comparison,brostoff2000passfaces,ellison2000protecting,mayrhofer2009shake,monrose2002password},
physically unclonable functions (PUFs)~\cite{pappu2002physical,tuyls2006puf,gassend2002silicon,suh2007physical},
and quantum information~\cite{bennett1988privacy}. Turning similar readings into identical values is known as \emph{information reconciliation}; further converting those values into uniformly random secret strings is known as \emph{privacy amplification}~\cite{bennett1988privacy}.
Both of these problems have interactive and non-interactive versions.  In this paper, we are interested in the non-interactive case, which is useful for a single user trying to produce the same key from multiple noisy readings of a secret at different times.
 A \emph{fuzzy extractor} is the primitive that accomplishes both information reconciliation and privacy amplification non-interactively; fuzzy extractors are defined in~\cite{DBLP:journals/siamcomp/DodisORS08}.


Fuzzy extractors consist of a pair of algorithms: \gen (used once, at ``enrollment'') takes a source value $w$, and produces a key $r$ and a public helper value $p$.  The second algorithm \rep (used subsequently) takes this helper value $p$ and a close $w'$ to reproduce the original key $r$.
The correctness guarantee is that $r$ will be correctly reproduced by \rep as long as $w'$ is no farther than $t$ from $w$ in some metric space. In this work, we consider the Hamming metric.
 The security guarantee is that $r$ produced by \gen is close to uniform (information-theoretically \cite{DBLP:journals/siamcomp/DodisORS08} or computationally \cite{fuller2013computational}), even given $p$. This guarantee holds as long as $w$ comes from a high-quality distribution, which
traditionally has been defined as \emph{any} distribution with sufficient min-entropy $m$. Note that in this paper we consider only passive attacks on $p$; the problem of protecting against modification of $p$ has been addressed in, e.g., \cite{Boyen05secureremote,DKKRS12}.  In particular, the random-oracle-based transform of \cite[Theorem 1]{Boyen05secureremote} can be applied to our constructions.

\paragraph{Limitations of Known Approaches}

Constructions of fuzzy extractors are limited by the tension between security and correctness guarantees: if we allow for higher error tolerance $t$, then we also need higher starting entropy $m$. The reason for this tension is simple: if an adversary who knows $p$ can guess some $w'$ within distance $t$ of $w$, then it will be able to easily obtain the true $r$ by running $\rep$.
%If $t$ is larger, then a single guess for $w'$ may be within distance $t$ of more $w$ values, thus increasing adversarial probability of winning.
In fact, if $t$ is high enough that there are $2^m$ points in a ball of radius $t$, then there exists a distribution of $w$ of min-entropy $m$  \emph{contained entirely in a single ball}.  For this distribution, an adversary can run $\rep$ on the center of this ball and always learn the key $r$.
Thus, if the security guarantee of a given fuzzy extractor holds for \emph{any} source of a given min-entropy $m$ and the correctness guarantees holds for any $t$ errors, then $m$ must   be greater than $\log |B_t|$, where $|B_t|$ denotes the number of points in a ball of radius $t$.  This condition on the source holds regardless of whether the fuzzy extractor is information-theoretic or computational, and extends even to the interactive setting.
If a source fails this condition, we will says that it has \emph{more errors than entropy}.



%More generally, for any $m$ and $t$, there is a distribution of min-entropy $m$ such that the adversary can guess a correct $w'$ with probability $1/\lceil( 2^m/B_t) \rceil\approx B_t 2^{-m}$: the distribution consists of the uniform distribution over all points in several non-overlapping balls of radius $t$ (the metric space must be large enough for these balls not to intersect). We thus call $m-\log B_t$ the \emph{minimum usable} entropy, denoted by $\Huse$. The previous paragraph shows that  no fuzzy extractor can handle all distributions of a given min-entropy $m$ if  $\Huse\le 0$.

%Prime candidate sources for authentication have $\Huse\le 0$.
Unfortunately, sources that have been proposed as prime candidates for authentication have more errors than entropy.
For example, the IrisCode~\cite{daugman2004}, which is the state of the art approach to handling what is believed to be the best biometric \cite{prabhakar2003biometric}, produces a source that more errors than entropy~\cite[Section 5]{blanton2009biometric}. PUFs with slightly nonuniform outputs suffer from similar problems~\cite{koeberl2014entropy}.

%As an example, the iris is believed to be the best biometric for high security applications~%\cite{prabhakar2003biometric}.  Daugman~\cite{daugman2004} designs a transform called IrisCode (using specialized wavelets) to derive a $2048$ bit string from an iris.  Let the outcome of this transform (on different irises) define a distribution $w$.  The precise number of errors that must be tolerated depends on the desired correctness.  For correctness of around $20\%$, a $t$ of approximately $205$ is required. Thus,
%\[
%\log |B_t|
%= \log \sum_{i=0}^{205} {2048 \choose i} \approx 956\,.
%\]
%In contrast, $w$ is estimated to have about $249$ bits of entropy~\cite{daugman2004}.
%There is considerable subsequent research~\cite{gentile2009slic,gentile2009efficient,rathgeb2011combining}, but it does not affect the above comparison dramatically.%\footnote{The work of Hao et al.~\cite[Section 4.3]{hao2006combining} provides a similar analysis but their calculation underestimates the number of possible error patterns; their calculation, which we cannot confirm, is $\Huse \approx 44$.}



\paragraph{Our Contributions: Handling More Errors than Entropy}
We provide the first constructions of fuzzy extractors that can be used for large classes of sources that have more errors than entropy.  Our constructions work for Hamming errors for strings $w$ of length $\gamma$ over some alphabet $\mathcal{Z}$. Naturally, as argued above, these constructions cannot work for all sources of a given entropy and for any $t$ errors. In this work, each construction takes advantage of the specific properties of a source, but still works for any $t$ errors. (It is an open problem to achieve our goal by restricting the error patterns instead of restricting the source distribution.)

Our first construction works for \emph{sources with high-entropy samples}: these are sources for which a randomly sampled substring of some superlogarithmic length is likely to have superlogarithmic  entropy. For example, a source can satisfy this condition if a constant fraction of symbols are almost $k$-wise independent for some superlogarithmic $k$ (note there is no total entropy requirement:  the total entropy of such a source may be not  much greater than the entropy of the sample). This construction uses digital lockers~\cite{canetti2008obfuscating}, which lock up the output $r$ using the sampled substring as the key to the locker. Such lockers can be constructed very efficiently via a single application of a cryptographic hash function (under appropriate assumptions) or, more generally, from point obfuscation, as we discuss below.

We augment the first construction with error-correcting codes to obtain our second construction, which works for \emph{sources with high-entropy marginals}: sources over large alphabets for which sufficiently many symbols have high entropy individually, but no independence among symbols is assumed. In this construction, we use single symbols of $w$ for locking up bits of a secret that we then transform into $r$. This construction tolerates a higher error rate than the first construction, but over a larger alphabet, where errors may be more likely.

Our third construction provides information-theoretic, rather than computational, security. It works for \emph{sparse block sources}. These are sources in which a sufficient fraction of the symbols have entropy conditioned on previous blocks. It uses symbols-by-symbol condensers to reduce the alphabet size while preserving most of the entropy, and then applies existing fuzzy extractors to the resulting string.

Our approach in all three constructions is different from most known constructions of fuzzy extractors, which put sufficient information in $p$ to recover the original $w$ from a nearby $w'$ during $\rep$ (this procedure is called a \emph{secure sketch}). We deliberately do not recover $w$, because known techniques for building secure sketches do not work for sources with more errors than entropy, since they lose at least $\log |B_t|$ bits of entropy regardless of the source. (This loss is necessary when the source is uniform~\cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08} or when reusability against a sufficiently rich class of correlations is desired~\cite[Theorem 11]{Boyen2004}; computational definitions of secure sketches suffer from similar problems~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational}.)

\paragraph{Our Contributions: Reusability}
An additional desirable security property of fuzzy extractors, introduced by Boyen~\cite{Boyen2004}, is called reusability. This property is necessary if a user enrolls the same or correlated values multiple times. For example, if the source is a biometric reading, the user may enroll the same biometric with different organizations.  Each of them will get a slightly different enrollment reading $w_i$, and will run $\gen(w_i)$ to get a key $r_i$ and a helper value $p_i$. Security for each $r_i$ should hold even when an adversary is given all the values $p_1, \dots, p_q$ (and, in case some organizations turn out to compromised or adversarial, a stronger security notion requires security for $r_i$ even in the presence of $r_j$ for $j\neq i$).  Many traditional fuzzy extractors are not reusable~\cite{Boyen2004,simoens2009privacy,blanton2012non,blanton2013analysis}. In fact, the only known construction of reusable fuzzy extractors \cite{Boyen2004} requires very particular relationships between $w_i$ values, which are unlikely to hold in any practical source.

Our first construction provides reusability (against computationally bounded adversaries).  
The reusability we obtain is very strong:  security holds even if the multiple readings $w_i$ used in $\gen$ are \emph{arbitrarily correlated}, as long as each $w_i$ \emph{individually} comes from a source with high-entropy samples. This construction is the first to provide reusability for a realistic class of correlated readings.
 




\begin{table}
\begin{center}
\begin{tabular}{l l l l l}
  & Source Structure & Error Rate  & Techniques\\
\hline
Cons. \ref{cons:sampling} (reusable)  & high-entropy samples & any subconstant &  Sample-and-obfuscate \\
Cons. \ref{cons:first construction}  & high-entropy  marginals & constant &  Error-correct-and-obfuscate\\
Cons. \ref{cons:info theoretic} (info-theoretic)   & sparse block source  & constant & Condense-and-fuzzy-extract\\
\hline\end{tabular}
\end{center}
%end centering
\caption{Summary of new constructions.  }
\label{tab:upper bounds}
\end{table}


%\begin{table}
%\begin{tabular}{l | l | l | l }
%Construction & Security & Main Feature  & Main Limitations \\
%\hline
%\consref{cons:info theoretic} & Info-theoretic & Exploit distribution & Symbols must add\\
%&&structure & fresh entropy \\\hline
%\consref{cons:sampling} & Computational  & Reusability & Sub-constant fraction \\
%&& & error tolerance\\\hline
%\consref{cons:first construction} & Computational & Allows correlated & Super-polynomial \\
%& &  symbols & size symbols
%\end{tabular}
%\caption{Summary of new constructions.  All constructions support families of distributions with more errors than entropy.}
%\label{tab:upper bounds}
%\end{table}


\paragraph{Relation to Obfuscation} 
Our constructions use simulation-secure obfuscation of digital lockers. The precise definition we require is not full-fledged virtual black-box obfuscation~\cite{barak2001possibility}. Instead, we rely on the relaxed notion of \emph{virtual grey-box} obfuscation, as long as it remains secure even when several digital lockers with correlated keys are composed~\cite{bitansky2010strong}. 

The following simple and efficient construction of digital lockers, proposed by Lynn, Prabhakaran, and Sahai~\cite[Section 4]{lynn2004positive}, provides the desired security in the random oracle model of~\cite{DBLP:conf/ccs/BellareR93}. Let $H$ be a cryptographic hash function, modeled as a random oracle. To lock up a value $y$ using a key $x$, output the pair $r, H(r, x)\oplus (y||0^k)$, where $r$ is a nonce, $||$ denotes concatenation, and $k$ is some security parameter. As long as the entropy of $x$ is superlogarithmic, the adversary has negligible probability of finding the correct $x$; and if the adversary doesn't find the correct $x$, then the adversarial knowledge about $x$ and $y$ is not significantly affected by this locker.  Concatenation with $0^k$ is used to make sure that it is possible to tell (with certainty $1-2^{-k}$) when the correct value is unlocked. 

It is seems plausible that in the real world (without random oracles), specific cryptographic hash functions, if used in this construction, will provide the necessary security~\cite[Section 3.2]{canetti2008obfuscating}, \cite[Section 8.2.3]{dakdoukThesis}. 
Moreover, Bitansky and Canetti~\cite{bitansky2010strong}, building on the work of~\cite{canetti2008obfuscating,CKVW10}, show how to obtain composable digital lockers based on a strong version of the Decisional Diffie-Hellman assumption without random oracles.


%Any procedure that converts a high-entropy input to a high-entropy output is known as a \emph{conductor} \cite{CRVW02}; if it's error-tolerant, then it's a \emph{fuzzy conductor}~\cite{KanukurthiR09}. Our first construction is a \emph{computational fuzzy conductors}.
%This may be converted to a computational fuzzy extractors using information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors~(\lemref{lem:cond and cext}).

%Both constructions are based on  obfuscation of point programs~\cite{canetti1997towards}.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.
%We need a strong version of point obfuscation that remains secure even when several obfuscations of correlated points are composed. While the standard definition of obfuscation \cite{barak2001possibility} does not imply security under composition, we can base our construction on the relaxed notion of \emph{virtual grey-box} obfuscation introduced in~\cite{bitansky2010strong}. For this notion, \cite{bitansky2010strong} construct composable obfuscation of point programs under particular number-theoretic assumptions. Additionally, such obfuscation can be made very efficient under a strong assumption on cryptographic hash functions~\cite{canetti1997towards}.

%Both of our constructions use techniques from Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  Let $w=w_1 \dots w_\ell$, for $w_j\in \mathcal{Z}$. In the first construction (\consref{cons:first construction}), for each $j$, $\gen$ flips a coin $c_j$ and either obfuscates $I_{w_j}$ or picks a random point $r_j$ and obfuscates $I_{r_j}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rep$ then runs the obfuscated program $P_j$ on $w_j'$ and checks whether $P_j(w_j')=1$.  For most locations $j$, \rep can determine whether $w_j$ or a random value was obfuscated.  Thus, most bits of $c_j$ are recoverable. To tolerate errors,  the set of coins $c_1\dots c_\ell$  is chosen at random from the codewords of an error correcting code. This construction conducts entropy from $w$ to $c$.

%Obfuscation of point functions provides no security if a point can be guessed; thus, in order for the first construction to be secure, sufficiently many coordinates of $w$ have to be unguessable (even to an adversary who can make equality queries for the values of other coordinates). We relax this requirement in our second construction (\consref{cons:sampling}), called \emph{sample-then-obfuscate}: it transforms $w$ into a string of blocks and then obfuscates each string of blocks. $\gen$ randomly samples several coordinates of $w$ and concatenates them to form a block. This reduces the entropy requirement on the individual symbols, but lowers the error-tolerance. We limit the degradation of error-tolerance by using digital lockers in place of point obfuscation~(sufficiently composable point obfuscation implies digital lockers).  This approach is similar to the sample-then-extract paradigm for building locally computable extractors~\cite{lu2002hyper,vadhan2003constructing}.  Unlike in locally computable extractors, we can form multiple blocks sampling from the same value $w$ and only argue about their individual entropy, because correlations among blocks are allowed in the first construction. Computational, rather than information-theoretic, analysis seems crucial for achieving this property.


We also note that fuzzy extractors for sources with more errors than entropy can be trivially constructed from virtual grey-box obfuscation for the class of {\em proximity point programs}. A proximity point program $I_w(x)$ tests if $x$ is within distance $t$ of $w$. 
Recently, Bitansky et al.~\cite{BitanskyCKP14} constructed such obfuscation for large classes of programs, including proximity point programs, based on the strong assumption of {\em semantically secure graded encodings} \cite{PassTS13}.
However, in contrast to obfuscated digital lockers, the  more general construction of Bitansky et al. is based on stronger assumptions and is highly impractical in terms of efficiency.  
It is not known how to construct \emph{reusable} fuzzy extractors from general obfuscation, because known obfuscation of proximity point programs is not known to be composable.


%\paragraph{Open Problems}
%\lnote{can we kill this paragraph? I don't see how it helps our cause. And in any case the grouping construction supports constant size alphabets}
%All of our constructions support more errors than entropy by using two metrics spaces.  Errors are tolerated in one metric space and corrected in a second.  To handle more errors than entropy,  we map to a metric space where multiple error patterns are grouped together.  All our constructions require the first metric space to have a super-constant size alphabet.  An alternative approach to the problem may support constant size alphabets.

%Our first construction achieves a constant fraction error tolerance but at the cost of a large alphabet size.  Our second construction reduces the alphabet size significantly but only achieves a sub-constant error tolerance.  It remains an open question to support a constant fraction of errors on a small alphabet.  More generally, the problem of constructing a secure fuzzy extractor whenever a negligible fraction lies in any ``ball'' remains an open problem.  %Both constructions require a large alphabet $\mathcal{Z}$---one whose size is more than polynomial in the security parameter.\footnote{Codes over large alphabets are often used to correct burst errors~\cite{gilbert1960capacity}.}  It is possible to tweak the sample-then-obfuscate construction for use with a small alphabet.  However, distributions with $\Huse \le 0$ are supported only for large alphabets.  For small alphabets, $\Huse>0$ and there are good information-theoretic constructions known~\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08}.
%Constructing a computational fuzzy extractor when $\Huse\le 0$ and a small alphabet is an open problem.  \bnote{Should this just be removed?}

%Using information-theoretic fuzzy extractors with additional privacy properties, Dodis and Smith~\cite[Section 5]{DBLP:conf/stoc/DodisS05} construct program obfuscators for the program $I_w(x)$ that tests if $x$ is within Hamming distance $t$ of $w$. The obfuscation is secure as long as $w$ comes from a distribution of sufficient min-entropy; in particular, the entropy must be high enough so that $\Huse>0$. Our constructions do not provide obfuscators for proximity queries, because they leak more information than whether $x$ is within distance $t$ of $w$ (for example, they may provide some information about the actual distance or about which coordinates agree). Constructing an efficient obfuscator for proximity queries when $\Huse\le 0$ is an open problem.

%In this work we restrict the distribution of the original reading $w$ and allow $w'$ to be an arbitrary point within distance $t$.  An alternative approach is to restrict the set of $w'$ where $\gen$ produces the correct key. \lnote{move this somewhere that makes more sense, probably above where we talk about a ball around a $w'$ and the obvious impossibility}%  A meaningful restriction of correctable errors is an open problem. 
%\fi

\medskip
\paragraph{Organization }
The remainder of this paper is organized as follows: we cover notation and fuzzy extractors in \secref{sec:preliminaries}.  We present our information-theoretic construction in \secref{sec:info theory cons} and our two computational constructions in Sections \ref{sec:sampling} and \ref{sec:cor construction}.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote by $X = X_1,..., X_\gamma$  the tuple $(X_1,\dots, X_\gamma)$.  For a set of indices $J$, $X_{J}$ is the restriction of $X$ to the indices in $J$.  The set $J^c$ is the complement of $J$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$,
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   For a random variable $W$, let $H_0(W)$ be the logarithm of the size of the support of $W$,  that is $H_0(W) = \log |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$ we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$ (we extend it to a class of distinguishers $\mathcal{D}$ by taking the maximum over all distinguishers $D\in\mathcal{D}$).  We denote by $\mathcal{D}_{s}$ the class of randomized circuits which output a single bit and have size at most $s$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric over vectors in $\mathcal{Z}^\gamma$, defined via $\dis(x,y) = \{i | x_i \neq y_i\}$.  For this metric, $|B_t| = \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
Usually, we use capitalized letters for random variables and corresponding lowercase letters for their samples.

\subsection{Fuzzy Extractors}
\label{sec:fuzzy extractors}

In this section we define computational fuzzy extractors.  Definitions for information-theoretic fuzzy extractors can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  %Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' ($\gen$) and ``reproduce'' ($\rep$) is an $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\kappa$ and a helper string $p\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(r, p)\leftarrow \gen(w)$, then $\Pr[\rep( w', p) = r] \geq 1-\delta$, where the probability is over the randomness of $(\gen, \rep)$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $r$ is pseudorandom conditioned on $p$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon_{sec}$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness. In Constructions~\ref{cons:sampling} and~\ref{cons:first construction} it is crucial that $w'$ is chosen independently of the outcome of \gen.

Information-theoretic fuzzy extractors are obtained by replacing computational distance by statistical distance.  We do make a second definitional modification.  The standard definition of information-theoretic fuzzy extractors considers $\mathcal{W}$  consisting of all distributions of a given entropy.  As described in the introduction, it is impossible to provide security for all distributions with more errors than entropy.  In both the computational and information-theoretic settings we consider a family of distributions $\mathcal{W}$.

\subsubsection{Reusable Fuzzy Extractors}
\label{sec:reusable}

An additional desirable feature of fuzzy extractors is reusability~\cite{Boyen2004}. Inuitively, it is the ability to support multiple independent enrollments of the same value, allowing users to reuse the same biometric or PUF, for example, with multiple noncommunicating providers. More precisely, the algorithm $\gen$ may be run multiple times on correlated readings $w_1,..., w_q$ of a given source. Each time, $\gen$ will produce a different pair of values $(r_1, p_1),..., (r_q, p_q)$. Security for each extracted string $r_i$ should hold even in the presence of all the helper strings $p_1, \dots, p_q$ (the reproduction procedure $\rep$ at the $i$th provider still obtains only a single $w'_i$ close to $w_i$ and uses a single helper string $p_i$). Because the multiple providers may not trust each other, a stronger security feature (which we satisfy) ensures that each $r_i$ is secure even when all $r_j$ for $j\neq i$ are also given to the adversary.

 Our ability to construct reusable fuzzy extractors depends on the types of correlations allowed among $w_1, \dots, w_q$. Boyen \cite{Boyen2004} showed how to do so when each $w_i$ is a shift of $w_1$ by a value that is oblivious to the value of $w_1$ itself (formally, $w_i$ is a result of a transitive isometry applied to $w_1$). Boyen also showed that even for this weak class of correlations, any secure sketch must lose at least $\log |B_t|$ entropy~\cite[Theorem 11]{Boyen2004}.
  

We modify the definition of Boyen~\cite[Definition 6]{Boyen2004} for the computational setting.  We discuss the our definition and definitions due to Boyen in \apref{sec:comparing reusability}.

\begin{definition}[Reusable Fuzzy Extractors]
\label{def:outsider fuzz ext}
Let $\mathcal{W}$ be a family of distributions over $\mathcal{M}$.  Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$.
Fix some $W_1 \in \mathcal{W}$.  Let $f_2,.., f_q , D$ be a split adversary.  Define the following game for all $j=1,..., q$:
\begin{itemize}%[leftmargin=-.6in]
\item \textbf{Sampling} The challenger samples $w_1\leftarrow W_1, u\leftarrow \zo^\kappa$.
\item \textbf{Perturbation} For $i=2,..., q$: the challenger computes $(r_i, p_i)\leftarrow \gen(w_i)$.  Set $w_{i+1} = f_i(w_1, p_1,..., p_i)$.\bnote{should we allow keys to the perturbation adversary?}
\item \textbf{Distinguishing} The advantage of $D$ is
\begin{align*}
\text{Adv}(D)&\overset{def}= \Pr[D(r_1,..., r_{j-1}, r_j, r_{j+1},..., r_q, p_1,..., p_q)=1]\\ &- \Pr[D(r_1,..., r_{j-1}, u, r_{j+1},..., r_q, p_1,..., p_q)=1].
\end{align*}
\end{itemize}
$(\gen, \rep)$ is $(q, \epsilon_{sec}, s_{sec}, f_2,..., f_q)$-reusable if for all $D\in\mathcal{D}_{s_{sec}}$ the advantage is at most $\epsilon_{sec}$.
\end{definition}

The definition is parameterized by $f_2,..., f_q$.  This adversary implicitly defines distributions $W_2,..., W_q$ (which depend on $W_1$ and the public values $P_1,... P_i$).  Security seems hopeless if fuzzy extractor is not secure on each of these distributions on their own.  This is the only requirement we make on these functions.  We call these types of functions admissible:

\begin{definition}
Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$.  In the reusability game above, we say a set of functions $f_2,..., f_q$ are \emph{admissible} if for all $W_1\in \mathcal{W}$ for all $w_1\in W_1$ and $\forall p_1,..., p_q$ that are the public outputs of $\gen$ the distribution $W_{i,w_1,p_1,..., p_{i-1}} = f_i(w_1,p_1,..., p_{i-1})$ is a member of $\mathcal{W}$.
\end{definition}

\subsection{Obfuscation}
Our constructions will use obfuscation for two types of circuits: point functions and digital lockers. The family of point functions $\mathtt{I}_n = \{I_w\}_{w \in \zo^n}$ defined as follows:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
and the class of digital lockers is $\mathtt{I}_n = \{I_{w, r}\}_{w \in \zo^n, r\in\zo^\kappa}$ defined as follows:
\[
I_{w, r}(x):\begin{cases} r & x=w\\\perp & \text{otherwise}\end{cases}.
\]
The required notion of obfuscation is virtual grey-box (VGB) introduced in \cite{bitansky2010strong}. This notion is weaker then the standard notion of virtual black-box (\cite{barak2001possibility}), as it allows the simulator to run in unbounded time while making at a polynomial number of oracle queries to the function. VGB obfuscators for point functions and digital lockers are constructible from specific number-theoretic assumptions or by strong assumptions on hash functions.  We provide more details and a formal definition in \apref{sec:obfuscation def}.

\section{Supporting more errors than entropy}
\label{sec:info theory cons}
In this section we show an information-theoretic fuzzy extractor that supports more errors than entropy.
%Information-theoretic fuzzy conductors can be converted to fuzzy extractors using a randomness extractor~(using the information-theoretic analogue of \lemref{lem:cond and cext}).
The construction first condenses entropy from each block of the source and then applies a different fuzzy extractor to the condensed blocks. We'll denote the fuzzy extractor on the smaller alphabet as $(\gen', \rep')$.  A condenser is like a randomness extractor but the output is allowed to be slightly entropy deficient.  Condensers are known with smaller entropy loss than possible for randomness extractors~(e.g.~\cite{dodis2014key}).
\begin{definition}
\label{def:conductor}
A function $\cond : \mathcal{Z}\times \zo^d\rightarrow \mathcal{Y}$ is a $(m, \tilde{m}, \epsilon)$-randomness condenser if whenever $\Hoo(W)\ge m$, then there exists a distribution $Y$ with $\Hoo(Y)\ge \tilde{m}$ and $(\cond(W, seed), seed) \approx_\epsilon (Y, seed)$.
\end{definition}

The main idea of the construction is that errors are ``corrected'' on the large alphabet~(before condensing) while the entropy loss for the error correction is incurred on a smaller alphabet~(after condensing).

\begin{construction}
\label{cons:info theoretic}
Let $\mathcal{Z}$ be an alphabet and let $W=W_1,..., W_\gamma$ be a distribution over $\mathcal{Z}^\gamma$.  We describe $\gen, \rep$ as follows:
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item Sample $seed_i\leftarrow \zo^d$.
\item Set $v_i = \cond(w_i, seed_i)$.
\end{enumerate}
\item Set $(r, p') \leftarrow \gen'(v_1,..., v_\gamma)$.
\item Set $p = (p', seed_1,..., seed_\gamma)$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p = (p', seed_1,..., seed_\gamma))$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item Set $v_i' = \cond(w_i', seed_i)$.
\end{enumerate}
\item Output $r = \rep'(v', p')$.
\end{enumerate}
\vspace{0.7in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
For \consref{cons:info theoretic} to be secure we need most blocks to contribute some entropy to the output.  We call this notion a partial block source.

\begin{definition}
\label{def:partial source}
A distribution $W = W_1,..., W_\gamma$ is an $(\alpha, \beta)$-partial block source if there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_{j-1} \in W_1,..., W_{j-1}, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
\end{definition}
\defref{def:partial source} is a weakening of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}), as only some blocks are required to have entropy conditioned on the past.  The choice of conditioning on the past is arbitrary: a more general sufficient condition is that there exists some ordering of indices where most items have entropy conditioned on all previous items in this ordering~(for example, a ``partial'' reverse block source~\cite{vadhan2003constructing}).  This construction is secure and it supports distributions with more errors than entropy~(proof is in~\apref{sec:info theory sec}).

\begin{lemma}
\label{lem:info theory sec}
%Let $n$ be a security parameter and let $\gamma = \omega(\log n)$.
Let $\mathcal{W}$ be the family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-partial block sources over $\mathcal{Z}^\gamma$ and let $\cond: \mathcal{Z} \times \zo^d\rightarrow \mathcal{Y}$ be a $(\alpha, \tilde{\alpha}, \epsilon_{cond})$-randomness conductor.  Define $\mathcal{V}$ as the family of all distributions with min-entropy at least $\tilde{\alpha}(\gamma-\beta)$ and let $(\gen', \rep')$ be $(\mathcal{Y}^\gamma, \mathcal{V}, \kappa, t, \epsilon_{fext})$-fuzzy extractor with error $\delta$.\footnote{We actually need $(\gen', \rep')$ to be an average case fuzzy extractor~(see \cite[Definition 4]{DBLP:journals/siamcomp/DodisORS08} and the accompanying discussion).  Most known constructions of fuzzy extractors are average-case fuzzy extractors.  For simplicity we refer to $\gen', \rep'$ as simply a fuzzy extractor.}  Then $(\gen, \rep)$ is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t, \gamma\epsilon_{cond}+\epsilon_{fext})$-fuzzy extractor with error $\delta$.

\end{lemma}

\subsection{More errors than entropy}

In this section we show that \consref{cons:info theoretic} supports partial block sources with more errors than entropy.  The structure of a partial block source implies that  $\Hoo(W) \ge \alpha (\gamma-\beta ) = \Theta(\gamma)$.  We assume that $\Hoo(W) = \Theta(\gamma)$. The condenser of Dodis et al~\cite{dodis2014key} has a constant entropy loss, so $\alpha-\tilde{\alpha} = \Theta(1)$. This means that the input entropy to $(\gen', \rep')$ is $\Theta(\gamma)$.   We assume that the new alphabet $\mathcal{Y}$ is of constant size.  Standard fuzzy extractors on constant size alphabets correct a constant fraction of errors at a entropy loss of $\Theta(\gamma)$, yielding $\kappa = \Theta(\gamma)$.  Thus, our construction is secure for distributions with more errors than entropy whenever $|\mathcal{Z}| = \omega(1)$.
More formally:
\[
\text{\# Errors} - \text{Entropy} = \log |B_t| - \Hoo(W) \ge  t \log |\mathcal{Z}| - \Theta(\gamma)-= \Theta(\gamma) \log |\mathcal{Z}| - \Theta(\gamma)  > 0
\]
That is, there exists a super-constant alphabet size for which \consref{cons:info theoretic} is secure with more errors than entropy.

\section{Adding reusability}
\label{sec:sampling}
In the previous section, we showed it was possible to construct a fuzzy extractor for a family of distributions with more errors than entropy.  Using computational techniques we are able to retain many of the advantages of \consref{cons:info theoretic} and achieve a reusable fuzzy extractor.

The construction samples a random subset of blocks $W_{j_1},..., W_{j_\eta}$ and obfuscates the concatenation of these blocks.  Denote this concatenated value by $V_1$.  This process is repeated to produce $V_1,..., V_\ell$ where at least one $V_i$ should be correct to ``unlock'' the correct key.
Let $\sample_{\gamma, \eta}(\cdot)$ be an algorithm that  outputs a random subset of $\{1,..., \gamma\}$ of size $\eta$ given let $r_{sam}$ bits of randomness.

\begin{construction}[Sample-then-Obfuscate]
\label{cons:sampling}
Let $\mathcal{Z}$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source where each $W_j$ is over $\mathcal{Z}$.
Let $\eta$ be a parameter, and $\mathcal{O}$ be an obfuscator for the family of digital lockers with $\kappa$-bit outputs.  Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $r \overset{\$}\leftarrow \zo^\kappa$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Select $\lambda_i\overset{\$}\leftarrow \zo^{r_{sam}}$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\gamma,\eta}( \lambda_i)$
\item Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\item Set $\rho_i = \mathcal{O}(I_{v_i, r})$.
\item Set $p_i = \rho_i, \lambda_i$.
\end{enumerate}
\item Output $(r, p)$, where $p=p_1\dots p_\ell$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w'=w_1',..., w_\gamma', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Parse $p_i$ as $\rho_i, \lambda_i$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\gamma, \eta}(\lambda_i)$.
\item Set $v_i' = w_{j_{i, 1}}',..., w_{j_{i, \eta}}'$.
\item Set $\rho_i(v_i') = r_i$.  If $r_i\neq \perp$ output $r_i$.
\end{enumerate}
\item Output $\perp$.
\end{enumerate}
\vspace{0.37in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
%There are three main differences between this construction and \consref{cons:first construction}.  They are as follows:
%\begin{itemize}
%\item Multiple blocks are concatenated together.  This comes at a cost to error tolerance but allows us to significant decrease the required entropy in each block.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.
%\item Instead of encoding a bit of the key with each obfuscated value we encode the entire key with each obfuscated value.  This requires the use of digital lockers in place of point obfuscations.  Instead of having to open ``most'' of the obfuscations, it is only necessary for us to open a single obfuscation.  \consref{cons:first construction} only hid part of the key in each obfuscation.  This allowed some blocks in the distribution to be ``weak.''  However, sampling smoothes out $W$ so that all $V_i$ are ``good'' simultaneously.
%\item \consref{cons:first construction} required a large alphabet as blocks were individually obfuscated.  This construction works for an arbitrary size alphabet.  We show it supports $\Huse (W)\le 0$ when the alphabet size is super-constant in the security parameter.
%\end{itemize}
The use of a computational primitive~(obfuscation of digital lockers) allows us to sample multiple times, because we need to argue only about individual entropy of $V_i$, as opposed to the information-theoretic setting, where it would be necessary to argue about the entropy of the joint variable $V$.  This is the property that allows reusability.

This construction uses a na\"{i}ve sampler that takes truly random samples, but the public randomness may be substantially decreased by using more sophisticated samplers. (See Goldreich~\cite{goldreich2011sample} for an introduction to samplers.)

\begin{theorem}
\label{thm:sampling}
Let $\mathcal{Z}$ be an alphabet.  Let $n$ be a security parameter.  Let $\mathcal{W}$ be the family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-partial block sources over $\mathcal{Z}^\gamma$ where $\gamma =\Omega(n)$.  Let $\eta$ be such that $\eta = \omega(\log n)$ and $\eta = o(\gamma)$, and let $c> 1$ be a constant and $\ell$ be such that $\ell = n^c$.  Let $\mathcal{O}$ be an $\ell$-composable VGB obfuscator for digital lockers~(with $\kappa$ bit outputs) with auxiliary inputs.  Then for every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that \consref{cons:sampling} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ for
\begin{align*}
t&\leq -\frac{(c-1)}{2} \frac{(\gamma-\eta)\log n}{\eta} = o(\gamma)\\
\delta &= e^{-n}
\end{align*}
\end{theorem}
 
\subsection{Security of \consref{cons:sampling}}
\label{ssec:sec cons sampling}
In this section we outline security of \consref{cons:sampling}.  A proof of security appears in \apref{sec:analysis sampling}.
With overwhelming probability, at each of the $\ell$ iterations, the sampler will choose enough coordinates of $W$ that have high entropy, making $V_i$ have sufficient entropy.   Once each of the $V_1,..., V_\ell$ have high entropy the obfuscations are unlikely to return a value other than $\perp$ to an adversary.
% forms a block-unguessable distribution.  Then security essentially follows from the security of \consref{cons:first construction}. %follows very similarly to \lemref{lem:security of cons}~(used to show security of \consref{cons:first construction}).
%Essentially, the argument is that an adversary will never be able to open a digital locker and thus they learn no information about the key.  %By the same argument of Then \consref{cons:sampling} is just \consref{cons:first construction} applied to $V_1,.., V_\ell$, and security follows by \lemref{lem:security of cons}.
We begin by showing that each $V_i$ is statistically close to a high entropy distribution.   Let $\Lambda$ represent the random variable of all the coins used by $\sample$ and $\lambda=\lambda_1 \dots \lambda_\ell$
be some particular outcome.

\begin{lemma}
\label{lem:sampling works}
Let all variables be as in \thref{thm:sampling}.
There exists $\epsilon_{sam} = O(e^{-\eta}) = \ngl(n)$ and $\alpha' = \alpha\eta(\gamma-\beta-\eta)/\gamma = \omega(\log n)$ such that for each $i$,
\[
\Pr_{\lambda\leftarrow \Lambda}[\Hoo(V_i | \Lambda= \lambda) \geq \alpha'] \geq 1- \epsilon_{sam}.
\]
\end{lemma}

\noindent
We can then argue that all $V_i$ simultaneously have individual entropy with good probability~(by union bound):
\begin{corollary}
\label{cor:samp sec}
 Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works}, and all the other variables be as in \thref{thm:sampling}.  Then $\Pr_{\lambda\leftarrow \Lambda}[\forall i, \Hoo(V_i | \Lambda = \lambda)  \ge \alpha'] \geq 1-\ell\epsilon_{sam}$.
%[V_i(V, \Lambda)$ is $(\ell\epsilon_{sam})$-close to a distribution $(V', U_{\ell\times r_{sam}})$ where for $u\in U_{\ell\times r_{sam}}$ for all $i$, $\Hoo(V_i' | U_{\ell\times r_{sam}} =u)\geq \alpha'$.
\end{corollary}
%\begin{proof}
%Union bound over the probability in \lemref{lem:sampling works}.
%%Hybrid argument over the statistical distance in \lemref{lem:sampling works}.
%\end{proof}

Once all $V_i$ all simultaneously have good entropy, the adversary only sees $\perp$ as an output from the obfuscations~(with overwhelming probability).  If the adversary only sees $\perp$ from the obfuscations, they have no information about the key.

\begin{lemma}[Proof in \apref{sec:analysis sampling}]
\label{lem:samp unguess}
Let all the variables be as in \thref{thm:sampling}.
For every $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} = \ngl(n)$ such that $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_{\kappa}, P))< \epsilon_{sec}$.
\end{lemma}

\subsection{Correctness of \consref{cons:sampling}}
\label{sec:correct sampling}
We encode the entire key in each obfuscation.  For correctness, at least one of the repeated readings must be correct with overwhelming probability.  Let $V_i$ represent one of the initial readings and $V_i'$ represent a repeated reading.  For showing correctness we must show that $\Pr[\forall i, V_i \neq V_i'] < \ngl(n)$.



\begin{lemma}[Proof in \apref{sec:analysis sampling}]
\label{lem:sampling errors}
Let all the variables be as in \thref{thm:sampling}.
 Then $\Pr[\forall i, v_i\neq v_i'] < \ngl(n)$, where the probability is over the coins of $\gen$.
\end{lemma}

\subsection{Reusability of \consref{cons:sampling}}
The reusability of \consref{cons:sampling} follows from the security of the VGB obfuscator with auxiliary input.  We consider a bounded $q = \poly(n)$ number of reuses.  For some fixed $i\in \{1,..., q\}$ we will treat the remaining keys as auxiliary input to the adversary, and the simulator still performs comparably to a distinguisher with access to the obfuscations.  Thus, given sufficiently strong reusability we achieve the following result:

\begin{theorem}
\label{thm:reusability}
Let $q = \poly(n)$, and let all the variables be as in \thref{thm:sampling}, except that $\mathcal{O}$ be an $\ell\times q$-composable VGB obfuscator for digital lockers~(with $\kappa$ bit outputs) with auxiliary inputs.  For any admissible $f_2,..., f_q$, for all $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $(\gen, \rep)$ is $(q, \epsilon_{sec}, s_{sec}, f_2,..., f_q)$-reusable fuzzy extractor.
\end{theorem}
\begin{proof}
The only modification to the outline presented in \secref{ssec:sec cons sampling} is in \lemref{lem:samp unguess} with the other keys $R_1,..., R_{i-1}, R_{i+1}, ..., R_q$ treated as additional auxiliary input to the adversary/simulator.  The simulator in the definition of composable obfuscation is required to function for arbitrary circuits in the family even if the choice of these circuits depends on the previous obfuscations.  Thus allows reading $w_i$ to be chosen depending on public values $p_1,..., p_{i-1}$.
\bnote{say more here?}
\end{proof}

\subsection{More errors than entropy?}
In this section, we show when \consref{cons:sampling} supports partial block sources with more errors than entropy.  The structure of the partial block source implies that $\Hoo(W) \ge \alpha (\gamma-\beta ) = \Theta(\gamma)$.  We assume that $\Hoo(W) = \Theta(\gamma)$.  We are able to correct $o(\gamma)$ errors.
This yields:
\[
\text{\# Errors} - \text{Entropy} =  \log |B_t| -\Hoo(W) \ge t \log |\mathcal{Z}| - \Theta(\gamma)= o(\gamma) \log |\mathcal{Z}| - \Theta(\gamma)
\]
That is, there exists a super-constant alphabet size for which \consref{cons:sampling} is secure with more errors than entropy.  

\textbf{Notes:} \consref{cons:sampling} works for an arbitrary size alphabet; however, for a constant size alphabet, the required entropy is greater than the number of corrected error patterns.  However, \consref{cons:sampling} is reusability for an arbitrary size alphabet.

In the analysis of \consref{cons:sampling} we restricted our attention to partial block sources, to allow for an easy comparison with \consref{cons:info theoretic}.  However,  in fact \consref{cons:sampling}  is secure for any source where sampling produces a high entropy string~(entropy $\omega(\log n)$) with overwhelming probability. For example, it is secure for sources with symbols that are $\omega(\log n)/\log |\mathcal{Z}|$-wise independent. 


\section{Allowing Correlated Symbols}
\label{sec:cor construction}
In the previous two sections, we presented constructions that supported restricted classes of distributions with more errors than entropy.  Unfortunately, both Constructions~\ref{cons:info theoretic} and \ref{cons:sampling} required each symbol to contribute ``fresh'' entropy.  In this section, we present a computational construction that allows for correlation between symbols while still supporting more errors than entropy and correcting a constant fraction of errors.
This construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}.  Instead of having large parts of the string $w$ unlock $r$, we have individual symbols unlock bits of the output.  

Before presenting the construction we provide some definitions from error correcting codes.
We use error-correct codes over $\{0,1\}^\gamma$ which correct up to $t$ bit flips from $0$ to $1$ but no bit flips from $1$ to $0$ (this is the Hamming analog of the $Z$-channel~\cite{tallini2002capacity}).\footnote{Any code that corrects $t$ Hamming errors also corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
Codes with $2^{\Theta(\gamma)}$ codewords and $t = \Theta(\gamma)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.
The class of errors we support in our source~($t$ Hamming errors over a large alphabet) and the class of errors for which we need codes~($t$ $0\rightarrow 1$ errors) are different.
}
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\gamma$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$.
\end{definition}

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\gamma$) is a $(\neigh_t, \delta_{code})$-code if there exists an efficient procedure $\decode$ such that $\Pr_{c\in C}[\exists c'\in \neigh_t(c) \text{ s.t. } \decode(c') \neq c] \leq \delta_{code}$.
\end{definition}
\bnote{these are quite strange.  changing them for dissertation.  update if accepted.}

\begin{construction}
\label{cons:first construction}
Let $\mathcal{Z}$ be an alphabet and let $W = W_1,..., W_\gamma$ be a distribution over $\mathcal{Z}^\gamma$.  Let $\mathcal{O}$ be an obfuscator for point functions with points from $\mathcal{Z}$.  Let  $C\subset \zo^\gamma$ be an error-correcting code.
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $c\leftarrow C$.
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item If $c_j = 0$: $p_j = \mathcal{O}(I_{w_j})$.
\item Else: $r_j \overset{\$}\leftarrow \mathcal{Z}$.
\subitem Let $p_j = \mathcal{O}(I_{r_j})$.
\end{enumerate}
\item Output $(c, p)$, where $p=p_1\dots p_\gamma$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item If $p_j(w_j') = 1$: set $c_j' = 0$.
\item Else: set $c_j' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

%\bnote{shorten this discussion?}
%The input $w$ is hidden in two different ways.  In locations where $c_j=1$, the block $w_j$ is information-theoretically unknown.
%In locations where $c_j=0$, it is hard to find $w_j$ given access to the point obfuscation.
%There are two possible reasons for a bit $c_j'$ to be $1$: because the true value was $1$ and because $w_j \neq w_j'$.  However, if a bit $c_j'$ is $0$, this likely means that $w_j=w_j'$ because collisions when $c_j=0$ are unlikely~(occurring with probability $1/|\mathcal{Z}|$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips.

\consref{cons:first construction} is secure if no distinguisher can tell whether it is working with random obfuscations or obfuscations of $W_j$.  By the security of point obfuscation, anything learnable from the obfuscation is learnable from oracle access to the function. Therefore, our construction is secure as long as enough blocks are unpredictable even after adaptive queries to equality oracles for individual symbols. This restriction on the distribution is captured in the following definition.

\begin{definition}
\label{def:block guessable}
Let $I_w (\cdot, \cdot)$ be an oracle that returns \[I_w(j, w_j')=
\begin{cases}
1 & w_j = w_j'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1| ... |W_\gamma$ is a $(q, \alpha, \beta)$-\emph{unguessable block source} if there exists a set $J\subset\{1,..., \gamma\}$ of size at least $\gamma -\beta$ such that for any unbounded adversary $S$ with oracle access to $I_w$ making at most $q$ queries
\[
\forall j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}

We show some examples of unguessable block sources in \apref{sec:characterize}.  In particular, any source $W$ where for all $j$, $\Hoo(W_j) \geq \omega(\log n)$~(but all blocks may arbitrarily correlated) is an unguessable block source~(\clref{cl:all blocks entropy}).

Unfortunately, \consref{cons:first construction} is not a computational fuzzy extractor.  The codewords $c$ are not uniformly distributed and it is possible to learn some bits of $c$~(for the symbols of $W$ without much entropy).  However, we can show that $c$ looks like it has entropy.  We use the notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon_{sec}, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon_{sec}$.
\end{definition}
We now define a weaker object that outputs computational entropy~(instead of a pseudorandom key).  We call this object a computational fuzzy conductor.  It is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\begin{definition}
\label{def:comp fuzzy cond}
A pair of randomized procedures ``generate'' ($\gen$) and ``reproduce'' ($\rep$) is an $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ if $\gen$ and $\rep$ satisfy \defref{def:comp fuzzy extractor}, except the last condition is replaced with the following weaker condition:
\begin{itemize}
\item for any distribution $W\in \mathcal{W}$, the string $r$ has high HILL entropy conditioned on $P$.  That is $H^{\hill}_{\epsilon_{sec}, s_{sec}}(R |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}
Computational fuzzy conductors can be converted to computational fuzzy extractors using standard techniques~(see \apref{sec:further defs}).
The following theorem states the construction is a computational fuzzy conductor~(proof in \apref{sec:construction analysis}).
\begin{theorem}
\label{thm:main thm first cons}
Let $n$ be a security parameter. Let $\mathcal{Z}$ be an alphabet where $|\mathcal{Z}| \ge 2^{ \omega(\log(n))}$.
Let $\mathcal{W}$ be a family of $(q,\alpha= \omega(\log n),  \beta)$-unguessable block sources over $\mathcal{Z}^\gamma$, for any $q = \poly(n)$.  Furthermore, let $C$ be a $(\neigh_t, \delta_{code})$-code over $\mathcal{Z}^\gamma$.  Let $\mathcal{O}$ be an $\gamma$-composable VGB obfuscator for point functions with auxiliary inputs. Then for any $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec}=\ngl(n)$ such that \consref{cons:first construction} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \tilde{m}=H_0(C)-\beta, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta_{code} + \gamma/|\mathcal{Z}|$.
\end{theorem}

\subsection{More errors than entropy?}
\label{sec:discussion}
In this section, we show that \consref{cons:first construction} can support distributions with more errors than entropy.
We first calculate the size of the Hamming ball.
\[
\log |B_t| = \log \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i> \log {\gamma \choose t} (|\mathcal{Z}|-1)^t =\Theta(t\log |\mathcal{Z}|) + \log {\gamma\choose t}
\]
The simplest type of unguessable block source is where each block is independent and has super-logarithmic entropy~(\clref{cl:independent high ent}).  For this type of source the entropy is $\Hoo(W) = \gamma\omega(\log n)$.  This yields:
\[
\text{\# errors} - \text{entropy} = \log |B_t| -  \Hoo(W)  >\left( \Theta(t\log |\mathcal{Z}|) + \log {\gamma \choose t}\right) -  \gamma \omega(\log n) .
\]
When $t =\Theta(\gamma)$ and the entropy of each block is $o(\log |\mathcal{Z}|)$, then the construction supports more errors than entropy. Furthermore, the output entropy is $H_0(C) -\beta$~(if $C$ is a constant rate code, this is $\Theta(\gamma)$).

\paragraph{Improvements}  If most codewords have Hamming weight close to $1/2$, we can decrease the error tolerance needed from the code from $t$ to  about $t/2$, because roughly half of the mismatches between $w$ and $w'$ occur where $c_j =1$.

If $\gamma$ is not long enough to get a sufficiently long output, the construction can be run multiple times with the same input and independent randomness.

\blind{
\section*{Acknowledgements}
The authors are grateful to Nishanth Chandran, Sharon Goldberg, Gene Itkis, Bhavana Kanukurthi, and Mayank Varia for helpful discussions, creative ideas, and important references.

Ran Canetti is supported by the NSF MACS project, an NSF Algorithmic foundations grant 1218461, the Check Point Institute for Information Security, and  ISF grant 1523/14.
Omer Paneth is additionally supported by the Simons award for graduate students in theoretical computer science.
The work of Benjamin Fuller is sponsored in part by US NSF grants 1012910 and 1012798 and  the United States Air Force under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government. 
Leonid Reyzin is supported in part by US NSF grants 0831281, 1012910, 1012798, and 1422965.  Adam Smith is supported in part by NSF awards 0747294 and 0941553. 
}
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

\section{Definitions}
\subsection{Computational Fuzzy Conductors and Computational Extractors}
\label{sec:further defs}
In this section, we show that a computational fuzzy conductor~\defref{def:comp fuzzy cond} can be transformed to a computational fuzzy extractor~\defref{def:comp fuzzy extractor}.  The transformation uses a computational extractor.
A computational extractor is the adaption of a randomness extractor to the computational setting.  Any information-theoretic randomness extractor is also a computational extractor; however, unlike information-theoretic extractors, computational extractors can expand their output arbitrarily via pseudorandom generators once a long-enough output is obtained. We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average case:
\begin{definition}
A function $\cext: \zo^\gamma \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon_{sec}, s_{sec})$-average-case computational extractor} if for all pairs
of random variables $X, Y$ (with $X$ over $\zo^\gamma$) such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{sec}}}((\cext(X; U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon_{sec}$.
\end{definition}

Combining a computational fuzzy conductor and a computational extractor yields a computational fuzzy extractor:

\begin{lemma}
\label{lem:cond and cext}
Let $(\gen'$, $\rep')$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\gamma$.  Let $\cext:\zo^\gamma\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon_{ext}, s_{ext})$-average case computational extractor.  Define $(\gen, \rep)$ as:
\begin{itemize}
\item $\gen(w; seed)$ (where $seed\in \zo^d$): run $(r', p')= \gen'(w)$ and output $r = \cext(r'; seed)$, $p = (p', seed)$.
\item $\rep(w', (p', seed)):$ run $r' = \rep'(w'; p')$ and output $r = \cext(r'; seed)$.
\end{itemize}
Then $(\gen, \rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \min\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}

\begin{proof}
It suffices to show if there is some distinguisher $D'$ of size $s'$ where
\[\delta^{D'}((\cext(X; U_d), U_d, P'), (U_\kappa, U_d, P'))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D$ of size $s_{cond}$ such that for all $Y$ with $\Hav(Y|P') \geq \tilde{m}$,
 \[
 \delta^{D}((X,P'), (Y, P'))\geq \epsilon_{cond}.
 \]
Let $D'$ be such a distinguisher.  That is,
\[
\delta^{D'}(\cext(X, U_d)\times U_d \times P', U_\kappa\times U_d\times P')> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D$ as follows.  On input $(y, p')$ sample $seed\leftarrow U_d$, compute $r\leftarrow \cext(y; seed)$ and output $D(r, seed, p')$.  Note that $|D| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D}((X, P'), (Y, P'))&= \delta^{D'}((\cext(X, U_d), U_d, P'), \cext(Y, U_d), U_d, P')\\
&\geq \delta^{D'}((\cext(X, U_d), U_d, P'), (U_\kappa\times U_d \times P')) \\
&- \delta^{D'}((U_\kappa\times U_d \times P'), (\cext(Y, U_d), U_d, P'))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Where the last line follows by noting that $D'$ is of size at most $s_{ext}$.  Thus $D$ distinguishes $X$ from all $Y$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\subsection{Reusability Fuzzy Extractors}
\label{sec:comparing reusability}

The goal of a reusable fuzzy extractor is to allow enrollment of a source across multiple services.  A service $i$ sees an reading of the source $w_i$.
Boyen considers two versions of reusable fuzzy extractors, first where the adversary sees $p_1,..., p_q$~(outsider security~\cite[Definition 6]{Boyen2004}) and tries to learn about the values $w_1,..., w_q$ or the keys $r_1,..., r_q$.  Second, where the adversary controls some subset of the servers and can key generation on arbitrary $p_i'$~(insider security~\cite[Definition 7]{Boyen2004}).   This allows the adversary to learn a subset of keys $r_i$~(by performing key generation on the valid $p_i$).  This definition makes sense when servers are compromised~(after enrollment) and act maliciously.  In both definitions, the adversary creates a perturbation function $_i$ after seeing $p_1,..., p_{i-1}$~(and generated keys for outsider security) and the challenger generates $w_i = f_i(w_1)$.  The definition is parameterized by the class of allowed perturbation functions.

Boyen constructs a outsider reusable cryptographic fuzzy extractor for unbounded $q$ when the perturbation family is a transitive isometric permutation groups. Boyen transforms this construction to insider security using random oracles. %In both constructions, the correlation between  $w_1,..., w_q$ is limited to a transitive isometric permutation group.  

Insider security strengthens outsider security in two ways.  First, it allows the adversary to see some subset of keys, second it allows the adversary to perform key generation on arbitrary $p_i$.  This mixes two properties of a fuzzy extractor:  reusability and robustness~\cite{dkrs2006}.  Robust fuzzy extractors provide security against modified $p$.  
In this work, we show reusability when $r_i$ are observed but do not handle the issue of robustness.  That is, we assume keys may be exposed but servers keep honest state.  Our definition lies between outsider and insider security.

We adapt the definition of Boyen to the computational setting~(\defref{def:outsider fuzz ext}).  
The definition of Boyen considers a single adversary.  We split the adversary into two parts, one of which is information-theoretic and another that is computationally bounded.  The functions $f_2,..., f_q$ can be thought of as a single adversary that sees all prior state.  However, to provide meaningful security in the computational setting, we cannot have communication between these adversaries.\footnote{An alternative would be to have a single computationally bounded adversary.  \consref{cons:sampling} satisfies this alternative adaption as well.}
Because these two adversaries do not communicate we strengthen the definition by allowing the perturbation functions,  $f_i$, to see the original sample $w_1$.  This was not allowed in the definition of Boyen as it would make security impossible.

%Finally, we allow the distinguishing adversary to see other keys \lnote{we do? then why do we call our definition ``outsider''?}.  This is strictly stronger than the definition of Boyen \lnote{you mean than the outsider security definition of Boyen?}.  However, it is weaker than the notion of insider reusable fuzzy extractors where the adversary sees key generation on arbitrary $p$.  This \lnote{what is ``this''?} is a mixture of reusability and robustness~\cite{dkrs2006} of the fuzzy extractor.  Our reusable construction~(\consref{cons:sampling}) is not insider secure \lnote{according to Boyen's notion, right? Why not say ``not robust'' and/or clarify that $r_i$ can leak but $p_i$ cannot be modified.}.

\subsection{Obfuscation}
\label{sec:obfuscation def}

In this section, we give a formal definition of the required notion of obfuscation.
We require that the obfuscation is composable and secure with respect to auxiliary input. Composable auxiliary-input VGB obfuscators for point functions and digital lockers are constructed in \cite[Theorem 6.1]{bitansky2010strong} (following \cite{canetti2008obfuscating,CKVW10}) from the Strong Vector Decision Diffie-Hellman assumption, which is a generalization of the strong DDH assumption of \cite{canetti1997towards} for tuples of points. As described in the Introduction, they can also be constructed by assuming strong properties of cryptographic hash functions~\cite{canetti1997towards}, \cite[Section 4]{lynn2004positive}, \cite[Section 3.2]{canetti2008obfuscating}, \cite[Section 8.2.3]{dakdoukThesis}.

\begin{definition}[composable obfuscation VGB obfuscation with auxiliary input \cite{bitansky2010strong}]
\label{def:obf} A PPT algorithm $\mathcal{O}$ is an $\ell$-composable VGB obfuscator for $\mathtt{I}_{n}$~(resp. $\mathtt{I}_{n+\kappa}$) with auxiliary-input if the following conditions are met:
\begin{enumerate}
\item \emph{Functionality:} for every $n$ and $I \in \mathtt{I}_n$, $\mathcal{O}(I)$ is a circuit that computes the same function as $I$.
\item \emph{Virtual grey-box:}  For every PPT adversary $A$ and polynomial $p$, there exists a (possibly inefficient) simulator $S$ and a polynomial $q$ such that for all sufficiently large $n$, any  sequence of circuits $I^1,\dots,I^\ell \in \mathtt{I}_n$, (where $\ell=\poly(n)$) and for all auxiliary inputs $z\in \zo^*$:
\[
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^\ell)) = 1] - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | < \frac{1}{p(n)} \enspace,
\]
where $(I^1,\dots,I^\ell)[q(n)]$ is an oracle that answers at most $q(n)$ queries, and where every query of the form $(i,x)$ is answered by $I^i(x)$.
\end{enumerate}
\end{definition}
For notational convenience, when we use point function obfuscation, we denote the oracle provided to the simulator as $I_w(\cdot, \cdot)$ where $w = w_1,..., w_\gamma$ is the vector of obfuscated points.  When we use digital lockers we denote the oracle provided to the simulator as $I_{v, r}(\cdot, \cdot)$ where $v = v_1,..., v_\ell$ is the vector of obfuscated points and $r$ is the hidden value~(we will hide the same value in each obfuscation).

%\consref{cons:sampling} is the fuzzy extractor that allows for arbitrary correlation between $w_1,..., w_q$ as long as each $w_i$ comes from an admissible distribution.  %Our construction can be made secure information-theoretically for fixed values of $q$ or computationally for an arbitrary value $q = \poly(n)$.  Our construction is based on a modification of the computational fuzzy extractor of Canetti et al.~\cite[Construction 5.2]{canetti2014key}.  Their construction is secure for a restricted class of distributions.\footnote{This is necessary as they secure distributions with more errors than entropy.  In order to provide any meaningful security guarantee, the source distribution must be restricted.  See the introduction of~\cite{canetti2014key} for discussion.}  In addition to showing the reusability of their construction, we provide two significant improvements:
\section{Characterizing unguessable block sources}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and a little unwieldy.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.
The majority of the difficulty in characterizing \defref{def:block guessable} is that different blocks may be dependent, so an equality query on block $i$ may reshape the distribution of block $j$.  In the examples that follow we denote the adversary by $S$ as we consider security against computationally unbounded adversaries defined in VGB obfuscation~(\defref{def:obf}).  We first show some sources that are unguessable block sources~(\secref{sec:positive ex}) and then show distributions with high overall entropy that are not unguessable block sources~(\secref{sec:negative ex}).

\subsection{Positive Examples}
\label{sec:positive ex}
We begin with the case of independent blocks.

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,  ... , W_\gamma$ be a source in which all blocks $W_j$  are mutually independent.  Let $\alpha$ be a parameter.  Let $J\subset \{1,..., \gamma\}$ be a set of indices such that for all $j\in J$, $\Hoo(W_j ) =\alpha $.  Then for any $q$, $W$ is a $(q, \alpha - \log (q+1), \gamma - |J|)$-unguessable block source.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then $W$ is a $(q, \omega(\log n), \gamma - |J|)$-unguessable block source.
\end{claim}
\begin{proof}
It suffices to show that for all $j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}) = \alpha -\log (q+1)$.
We can ignore queries for all blocks but the $j$th, as the blocks are independent. Furthermore, without loss of generality, we can assume that no duplicate queries are asked, and that the adversary is deterministic ($S$ can calculate the best coins). Let $A_1, A_2, \dots A_q$ be the random variables representing the oracle answers for an  adversary $S$ making $q$  queries about the $i$th block. Each $A_k$ is just a bit, and at most one of them  is equal to 1 (because duplicate queries are disallowed). Thus, the total number of possible responses is $q+1$. Thus, we have the following,
\begin{align*}
\Hav(W_j | View(S^{\mathcal{O}_{W}(\cdot, \cdot)}) &= \Hav(W_j| A_1, \dots, A_q)\\
&=\Hoo(W_j) - |A_1, \dots, A_q|\\
&=\alpha - \log (q+1)\,,
\end{align*}
where the second line follows from the first by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
\end{proof}
\noindent In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} show a construction for block-fixing sources, where each block is either uniform or a fixed symbol~(block fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} shows that \defref{def:block guessable} captures, in particular, this class of distributions.
However, \defref{def:block guessable} captures more distributions.  We now consider more complicated distributions where blocks are not independent.

\begin{claim}
\label{cl:each block from single seed}
Let $f:\zo^e \rightarrow \mathcal{Z}^\gamma$ be a function.  Furthermore, let $f_j$ denote the restriction of $f$'s output to its $j$th coordinate.  If for all $j$, $f_j$ is injective then $W = f(U_e)$ is a $( q, e - \log (q+1), 0)$-unguessable block source.
\end{claim}
\begin{proof}
Since $f$ is injective on each block, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) = \Hav(U_e | View(S^{I_{W}(\cdot, \cdot)}))$.  Consider a query $q_k$ on block $j$.  There are two possibilities: either $q_k$ is not in the image of $f_j$,  or $q_k$ can be considered a query on the preimage $f_j^{-1}(q_k)$. Then (by assuming $S$ knows $f$) we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ (like in the
proof of \clref{cl:independent high ent}),
 and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $j$, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) \geq \Hoo(W_j) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so there is a family of distributions with total entropy $\omega(\log n)$ for which \consref{cons:first construction} is secure.  For these distributions, all the coordinates are as dependent as possible: one determines all others.
We can prove a slightly weaker claim when the correlation between the coordinates $W_j$ is arbitrary:

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\gamma$ be a source.  Suppose that for all $j$, $\Hoo(W_j)\geq \alpha$, and that $q \le 2^{\alpha}/4$ (this holds asymptotically, in particular, if $q$ is polynomial and $\alpha$ is super-logarithmic). Then  $W$ is a $(q, \alpha-1-\log(q+1), 0)$-unguessable block source.
\end{claim}

\begin{proof}
Intuitively, the claim is true because the oracle is not likely to return 1 on any query. Formally, we proceed by induction on oracle queries,
using the same notation as in the proof of   \clref{cl:independent high ent}. Our inductive hypothesis is
that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.  If the inductive hypothesis holds, then, for each $j$,
\begin{equation}
\label{eq:cond-entropy}
\Hoo(W_j | A_1= \dots= A_{k-1}=0) \ge \alpha-1\,.
\end{equation}
This is true for $k=1$ by the condition of the theorem. It is true for $k>1$ because, as a consequence of the definition of $\Hoo$,
for any random variable $X$ and event $E$, $\Hoo(X|E)\ge \Hoo(X)+\log\Pr[E]$; and $(k-1) 2^{1-\alpha}\leq 2 q 2^{-\alpha} \leq 1/2$.

We now show that $\Pr[A_1\neq 0 \vee \dots \vee A_{k}\neq 0] \leq k 2^{1-\alpha}$, assuming that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.
\begin{align*}
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0 \vee A_k\neq 0] & =
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0]+\Pr[A_1=\dots = A_{k-1}=0 \wedge A_k=1]\\
& \le  (k-1)2^{1-\alpha}+\Pr[A_k=1\,|\,A_1=\dots = A_{k-1}=0]\\
& \le  (k-1)2^{1-\alpha}+\max_j 2^{-\Hoo(W_j | A_1=\dots =A_{k-1}=0)}\\
& \le  (k-1)2^{1-\alpha}+ 2^{1-\alpha}\\
& = k 2^{1-\alpha}
\end{align*}
(where the third line follows by considering that to get $A_k=1$, the adversary needs to guess some $W_j$, and the fourth line follows by~\eqref{eq:cond-entropy}).
Thus, using $k=q+1$ in~\eqref{eq:cond-entropy},
 we know $\Hoo(W_j | A_1= \dots= A_q=0) \ge \alpha-1$.  Finally this means that
\begin{align*}
\Hav(W_j | A_1,\dots, A_q) &\ge -\log \left( 2^{-\Hoo(W_j | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]+1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] \right)\\
& \ge -\log \left(  2^{-\Hoo(W_j | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\end{proof}

\subsection{Negative Examples}
\label{sec:negative ex}
Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some blocks to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$ and let $f_1,..., f_{\gamma}$ be injective functions where $f_j:\zo^{j\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \gamma \times \log p$, that is the source has a small number of blocks.}  Then define the distribution $W_1 = f_1(U_{1,...,\gamma}), W_2 = f_2(U_{1,..., 2\gamma}),...., W_\gamma = f_\gamma(U)$.  There is an adversary making $p\times \gamma = \poly(n)$ queries such that $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
Let $x$ be the true value for $U_{p\times \gamma}$.
We present an adversary $S$ that completely determines $x$.  $S$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $S$ queries on $(1, y_1),..., (1, y_p)$, exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $S$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered~(and thus $w$).  %The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \gamma)$.  Once $x$ has been recovered then $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relies on an adversaries ability to determine a block from the previous blocks.  We formalize this notion next.  We define the entropy jump of a block source as the remaining entropy when other blocks are known:

\begin{definition}
Let $W = W_1,..., W_\gamma$ be a source under ordering $i_1,..., i_\gamma$.  The \emph{jump} of a block $i_j$ is $\mathtt{Jump}(i_j) = \max_{w_{i_1},..., w_{i_{j-1}}} H_0 (W_{i_j} | W_{i_1} = w_{i_1} ,..., W_{i_{j-1}} = w_{i_{j-1}})$.
\end{definition}

If an adversary can learn blocks in succession they can eventually recover the entire secret.  In order for a source to be block unguessable the adversary must get ``stuck'' early enough in their recovery process.  This translates to having a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\gamma$ such that for all $j\le \gamma-\beta +1$, $\mathtt{Jump}(i_j) = \log q /(\gamma-\beta+1)$, then $W$ is not $(q, 0, \beta)$-unguessable block source.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \gamma$.  We describe an unbounded adversary that determines $W_1,..., W_{\gamma-\beta+1}$.  As before $S$ queries the $q /\gamma$ possible values for $W_1$ and determines $W_1$.  Then $S$ queries the (at most)~$q/(\gamma-\beta+1)$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\gamma-\beta+1}$ is learned.
\end{proof}

Presenting a sufficient condition for security is more difficult as $S$ may interleave queries to different blocks.  It seems like the optimum strategy is to focus on a single block at a time but it is unclear how to formalize this intuition.

\section{Analysis of \consref{cons:info theoretic}}
\label{sec:info theory sec}
\begin{proof}[Proof of \lemref{lem:info theory sec}]
Let $W\in \mathcal{W}$.
It suffices to argue correctness and security.  We first argue correctness.
When $w_i = w_i'$, then $\cond(w_i , seed_i) = \cond(w_i', seed_i)$ and thus $v_i = v_i'$.  Thus, for all $w, w'$ where $\dis(w, w')\le t$, then $\dis (v, v')\le t$.  Then by correctness of $(\gen', \rep')$, $\Pr[(r, p)\leftarrow \gen'(v) \wedge r'\leftarrow \rep(v',p) \wedge r' = r]\ge 1-\delta$.

We now argue security.  Denote by $seed$ the random variable consisting of all $\gamma$ seeds and $V$ the entire string of generated $V_1,..., V_\gamma$.  To show that $R | P, seed \approx_{\gamma \epsilon_{cond} + \epsilon_{fext}} U | P, seed$, it suffices to show that $\Hav(V | seed)$ is $\gamma \epsilon_{cond}$ close to a distribution with average min-entropy $\tilde{\alpha}(\gamma - \beta)$.  The lemma then follows by the security of $(\gen', \rep')$.\footnote{Note, again, that $(\gen', \rep')$ must be an average-case fuzzy extractor.  Most known constructions are average-case and we omit this notation.}

We now argue that there exists a distribution $Y$ where $\Hav(Y | seed)\ge \tilde{\alpha}(\gamma - \beta)$ and $(V, seed_1,..., seed_\gamma)\approx (Y, seed_1,.., seed_\gamma)$.  First note since $W$ is $(\alpha, \beta)$-partial block distribution that
there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_{j-1} \in W_1,..., W_{j-1}, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
Then consider the first element of $j_1\in J$, $\forall w_1,..., w_{j_1-1}\in W_1,..., W_{j_1-1}$,
\[\Hoo(W_{j_1} | W_1 = w_1,..., W_{j_1-1} = w_{j_1-1})\ge \alpha.\]

%\[(\ext (W_1, seed_1), seed_1) \approx_{\epsilon} (U_{\mathcal{Y}}, seed_1)\]
%Then since $W$ is a block-source,
%\[ \forall w_1\in W_1, \Hoo(W_2 | w_1)\ge k.\]
\noindent
Thus, there exists a distribution $Y_{j_1}$ with $\Hav(Y_{j_1} | seed_{j_1}) \ge \tilde{\alpha}$ such that
\[(\cond (W_{j_1}, seed_{j_1}), seed_{j_1}, W_1,..., W_{j_1-1}) \approx_{\epsilon_{cond}} (Y_{j_1}, seed_{j_1}, W_1,..., W_{j_1-1})\]
and since $(seed_1,..., seed_{j_1})$ are independent of these values
\[(\cond (W_{j_1},seed_{j_1}), W_{j_1-1},..., W_1, seed_{j_1}, ..., seed_{1}) \approx_{\epsilon_{cond}} (Y_{j_1}, W_{j_1-1},..., W_1, seed_{j_1}, , ...,  seed_{1})\]
consider the random variable $Z_{j_1} =( Y_{j_1}, \cond(W_{j_1-1},seed_{j_1-1}),..., \cond(W_{1}, seed_{1}))$ and note that \[\Hav(Z_{j_1} | seed_1,...,seed_{j_1})\ge \alpha'.\]
Applying a deterministic function does not increase statistical distance and thus,
\begin{align*}
(\cond (W_{j_1}, seed_{j_1}), \cond(W_{j_1-1}, seed_{j_1-1}),..., \cond(W_1, seed_1), seed_{j_1},..., seed_{1}) \\\approx_{\gamma \epsilon_{cond}} (Z_{j_1}, seed_{j_1},..., seed_1)
\end{align*}

\noindent
By a hybrid argument there exists a distribution $Z$ with $\Hav(Z | seed) \ge \tilde{\alpha}(\gamma -\beta)$ where
\[
(\cond(W_\gamma, seed_\gamma), ..., \cond(W_1, seed_1), seed_\gamma,..., seed_1) \approx_{\gamma \epsilon_{cond}} (Z, seed_\gamma,...,  seed_1).\]
%By the security of $(\sketch, \rec)$ we know that $\Hav(Z | seed, ss) \ge \tilde{m}$.  %Note that $U_{\mathcal{Y}}^\gamma$ is independent of $seed_1,..., seed_\gamma$ and so $\Hav(U_{\mathcal{Y}}^\gamma, seed_1,..., seed_\gamma, p)\ge \tilde{m}$.
%That is,
%\[
%(V, seed_1,..., seed_\gamma, p) \approx_{\epsilon\gamma} (U_{\mathcal{Y}}^\gamma, seed_1,..., seed_\gamma, p).\]
This completes the proof.
\end{proof}

\section{Analysis of \consref{cons:sampling}}
\label{sec:analysis sampling}
\subsection{Security}
The proof of security for \consref{cons:sampling} uses the definition of block unguessable sources~(\defref{def:block guessable}).  This definition is adaptive and discussed in \apref{sec:characterize}.
We show the security of \consref{cons:sampling}:
\begin{itemize}
\item \lemref{lem:sampling works}: Show that sampling is successful with overwhelming probability.
\item \corref{cor:samp sec}: The outputs $V_1,.., V_\ell$ have high individual entropy with good probability.
\item The outputs $V_1,..., V_\ell$ are a block unguessable source.  This is made formal in the following corollary:
\begin{corollary}
\label{cor:v are unguessable}
Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works},  and all the other variables be as in \thref{thm:sampling}. Take any $q=\poly(n)$.  For $\alpha'' =\alpha'-1-\log (q+1) =  \omega(\log n)$, with  probability $1-\ell \epsilon_{sam}$ over the choice of $\Lambda=\lambda$, the distribution $V| \Lambda=\lambda$ is a $(q, \alpha'', 0)$-unguessable block source.
\end{corollary}
\item \lemref{lem:samp unguess}: An adversary is unlikely to receive any information about the key for a block unguessable source.
\end{itemize}

\noindent
We now present the proofs of Lemmas \ref{lem:sampling works} and \ref{lem:samp unguess}.

\begin{proof}[{\large Proof of \lemref{lem:sampling works}}]
Consider some fixed $i$.
Recall that there a set $J$  of size $\gamma - \beta = \Theta(\gamma)$ such that each $w$ and  block $j\in J$, $\Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}, W_{j+1}=w_{j+1},..., W_\gamma = w_\gamma) \geq \alpha$.  Since this is a worst case guarantee, the entropy of $V_i$ can be deduced from the number of symbols in $V_i$ that come from $J$. Namely, Denote by $X= |\{j_{i, 1},..., j_{i, \eta}\}\cap J|$.

\begin{claim}
\label{cl:vi have entropy}
\[
\Hoo(V_i |\Lambda = \lambda ) \geq \alpha X.
\]
\end{claim}
\begin{proof}
Denote by $j_1,..., j_\eta$ the indices selected by the randomness $\lambda_i$.  We begin by noting that $\Hoo(V_i |\Lambda = \lambda ) = -\log \max_{v\in V_i} \Pr[ V_i =v | \Lambda =\lambda] = -\log \max_{w_{j_1}, ..., w_{j_\eta}} \Pr[W_{j_1} = w_{j_1} \wedge \dots \wedge W_{j_\eta} w_{j_\eta}] $.  Then
\begin{align*}
\max_{w_{j_1},..., w_{j_\eta}} \Pr[ W_{j_1}=w_{j_1} \wedge \dots \wedge W_{j_\eta} = w_{j_\eta}]
&= \max_{w_{j_1},..., w_{j_\eta}} \prod_{k=1}^\eta \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le \prod_{k=1}^\eta \max_{w_{j_1},..., w_{j_\eta}} \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le\prod_{k=1}^\eta \max_{w_1,..., w_\gamma} \Pr[W_{j_k} = w_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1} ]\\
\end{align*}
Taking the negative logarithm of both sides we have that
\begin{align*}
\Hoo(V_i | \Lambda = \lambda) &\ge \sum_{k=1}^\eta \min_{w_1,..., w_\gamma} \Hoo(W_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1})\\
&\ge \sum_{j_k\in J} \alpha = \alpha X
\end{align*}
This completes the proof of \clref{cl:vi have entropy}.
\end{proof}


We note that $X$ is distributed according to the hypergeometric distribution,
and that $\expe[X]=\eta(\gamma-\beta)/\gamma$. Using the tail bounds from~\cite{chvatal1979tail,scala2009hypergeometric}, we can conclude that $\Pr[X\le \expe[X]/2]\le e^{-2((\gamma-\beta)/2\gamma)^2 \eta}=O(e^{-\eta})$.

\bnote{this can also be tightened.}
Thus, setting $\alpha'=\frac{\alpha \eta(\gamma-\beta)}{2\gamma}$ and applying \clref{cl:vi have entropy}, we conclude that
 \[
\Pr[\Hoo(V_i ) \geq \alpha'] \geq 1- O(e^{-\eta}).
\]
\end{proof}

\begin{proof}[{\large Proof of \lemref{lem:samp unguess}}]

\lnote{I need to check this}
Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for digital lockers over $\mathcal{Z}$\footnote{In this proof we only consider the case where the sampling has produced a block unguessable source.  The negligible portion of the time when this does not happen in included in the security of \thref{thm:sampling}}  .  Let $W$ be a $(q, \alpha'' = \omega(\log n), 0)$-unguessable block source.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U, P))\le \epsilon_{sec}$.

Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U, P))> \epsilon_{sec}$.
Let $D$ be such a distinguisher of size at most $s_{sec}$.  That is,
\[
| \expe[D(R, P)] - \expe[D(U, P)] > \epsilon_{sec} = 1/\poly(n).
\]
Define the oracle $I_{v_1, ..., v_\ell, r}(\cdot, \cdot)$ as follows:
\[I_{v_1,..., v_\ell, r}(x, i) =
\begin{cases}
r & v_i = x\\
\perp & \text{otherwise.}
\end{cases}\]
By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(R, P_1,..., P_\ell)] - \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $R$ and $U$.
\begin{lemma}
\label{lem:sim cannot distinguish samp}
$\Delta(S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|}), S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})) \le \ell 2^{-\alpha''}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two values in $\zo^\kappa$, the statistical distance is at most $\ell 2^{-\alpha''}$.
\begin{lemma}
\label{lem:codewords in I close samp}
Let $r$ be true value encoded in $I$ and let $u\in \zo^\kappa$.  Then,
\[
\Delta( S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(r, 1^{\ell \log |Z|}), S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(u, 1^{\ell \log |Z|})) \le \ell 2^{-\alpha''}.
\]
\end{lemma}
\begin{proof}
Recall that for all $j$, $\Hav(V_j | View(S))\geq \alpha''$.  The only information about the correct value of $r$ is contained in the query responses.  When all responses are $\perp$ the view of $S$ is identical when presented with $r$ or $u$.  We now show that for any value of $r$ all queries return $\perp$ with probability $1-2^{-\alpha''}$.  Suppose not, that is suppose, the probability of at least one nonzero response is $> 2^{-(\alpha'')}$.

 When there is a response other than $\perp$ for some $j$ this means that there is no remaining min-entropy in $V_j$.  If this occurs with over $2^{-\alpha''}$ probability this violates the block unguessability of $V$~(\defref{def:block guessable}).  By the union bound over the indices $j$ the total probability of a response other than $\perp$ is at most $\ell 2^{-\alpha''}$. Thus, for all $r, u$ the statistical distance is at most $\ell 2^{-\alpha''}$.  This concludes the proof of \lemref{lem:codewords in I close samp}.
\end{proof}
By averaging over all points in $\zo^\kappa$ we conclude that
\[\Delta(S^{I_{v_1, ..., v_\ell, r}X(\cdot, \cdot)}(R, 1^{\ell \log |Z|}), S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})) < \ell 2^{-\alpha''}.\]  This completes the proof of \lemref{lem:sim cannot distinguish samp}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(R, P_1,..., P_\ell) ]- \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish samp}, we have
\begin{align*}
\delta^{D}((R, P), (U, P))&\leq |\expe [D(R, P_1,..., P_\ell)] - \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})]| \\
&+|\expe[S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})] - \expe[S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})] |\\
&+|\expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})] - \expe [D(U, P_1,..., P_\ell) ]|\\
&\leq \epsilon_{sec}/3+ \ell 2^{-\alpha''}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:samp unguess}.
\end{proof}

%\end{proof}
\subsection{Correctness}
\label{sec:sampling errors}

%\section{Proof of \lemref{lem:sampling errors}}
\bnote{can be tightened to $\omega(n^c \log n)$.}
\begin{proof}[Proof of \lemref{lem:sampling errors}]

Recall that $\dis(w, w')\leq t$ and that the locations of the errors is independent of the selected locations.  Denote by $\mu = -\frac{(c-1)\log n}{2}$.  Since $\eta = \omega(\log n)$, we will assume
$\eta\ge 2\mu$.  We begin by computing the probability that a single $v_i = v_i'$.
\begin{align*}
\Pr[v_i = v_i'] &= \Pr[w\text{ and }w'\text{ agree on positions }j_{i,1},..., j_{i,\eta}]\\
&\ge \prod_{j=0}^{\eta-1} \left( 1- \frac{t}{\gamma -j }\right) \ge \prod_{j=0}^{\eta-1}\left(1-\frac{\mu(\gamma-\eta)/\eta}{\eta-j}\right)\\
&\ge \prod_{j=0}^{\eta-1} \left( 1- \frac{\mu}{\eta}\left(\frac{\gamma-\eta}{\gamma -j }\right)\right)\ge \prod_{j=0}^{\eta-1}\left(1-\frac{\mu}{\eta}\right)\\
&= \left(1-\frac{\mu}{\eta}\right)^{\eta} =\left( \left(1-\frac{\mu}{\eta}\right)^{\eta/\mu}\right)^\mu\geq \left(\frac{1}{2}\right)^{2\mu}\\
&\ge \left(\frac{1}{2}\right)^{(c-1) \log n}= \frac{1}{n^{c-1}}.
\end{align*}
We then have the probability that all $v_i\neq v_i'$ as:
\begin{align*}
\Pr[\forall i, v_i \neq v_i'] &= \left(1-\Pr[v_i= v_i']\right)^\ell\\
&=\left( 1- \frac{1}{n^{c-1}}\right)^\ell =\left(\left( 1- \frac{1}{n^{c-1}}\right)^{n^{c-1}}\right)^{\ell /n^{c-1}}\\
&\le \left(\frac{1}{e}\right)^{n^c/n^{c-1}} = \frac{1}{e^n}.
\end{align*}
This completes the proof of \lemref{lem:sampling errors}.
\end{proof}


\section{Analysis of \consref{cons:first construction}}
\label{sec:construction analysis}
\subsection{Security}
Security of \consref{cons:first construction} is similar to the security of \consref{cons:sampling}.  However, security is more complicated, the main difficulty is that the definition of block unguessable sources~(\defref{def:block guessable}) allows for certain weak blocks that can easily be guessed.  This means we must limit our indistinguishable distribution to blocks that are difficult to guess.  Security is proved via the following lemma:

\begin{lemma}
\label{lem:security of cons}
Let all variables be as in \thref{thm:main thm first cons}.  For every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}( C | P ) \geq H_0(C) - \beta$.
\end{lemma}

We give a brief outline of the proof, followed by the proof.
It is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))\le \ngl(n)$.  Let $J$ be the set of indices that exists according to \defref{def:block guessable}. Define the distribution $C'$ as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $J$.  We first note that $C'$ has sufficient entropy, because $\Hav(C' |P) = \Hav(C' | C_{J^c}) \ge \Hoo(C', C_{J^c}) - H_0(C_{J^c})  = H_0(C) - |J^c|$ (the second step is by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P)) \le \ngl(n)$.
%Define the distribution $X$ as follows:
%\[X_i =
%\begin{cases}
%W_i & C_i = 0\\
%R_i & C_i = 1.
%\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$. Since $P$ is a collection of obfuscated programs, there exists a simulator $S$~(outputting a single bit), such that $\Pr[D(C, P)=1]$ is close to $\Pr[S^{\mathcal{O}}(C)=1]$.
\item Show that even an unbounded $S$ making a polynomial number of queries to the stored points cannot distinguish between $C$ and $C'$.  That is, $\Delta(S^{\mathcal{O}}(C),S^{\mathcal{O}}(C'))$ is small.
\item By the security of obfuscation, $\Pr[S^{\mathcal{O}}(C')=1]$ is close to $\Pr[D(C', P)=1]$.
\end{itemize}
\begin{proof}[Proof of \lemref{lem:security of cons}]
\label{app:security of main cons}

\lnote{I need to check this}
Let $\mathcal{O}$ be a $\gamma$-composable VGB obfuscator with auxiliary input for point programs over $\mathcal{Z}$.  Let $W$ be a $(q, \alpha = \omega(\log n), \beta)$-unguessable block source.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P)\geq H_0(C)- \beta$. % for $\epsilon' = 2\epsilon_{obf} + (\gamma - \beta)2^{-(\alpha-1)}$.
Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P) < H_0(C)-\beta$.
By \defref{def:block guessable} there exists a set of indices $J$ such that all blocks within $J$ are unguessable.  Define by $C'$ the distribution of sampling a uniform codeword where all locations outside $J$ are fixed.  Then
$\Hav(C' | C_{J^c}) \ge \Hoo(C', C_{J^c}) - H_0(C_{J^c})  = H_0(C) - \beta$ (by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).

Let $D$ a distinguisher of size at most $s_{sec}$ such that
\[
| \expe[D(C, P)] - \expe[D(C', P)] > \epsilon_{sec} = 1/\poly(n).
\]
Define the distribution $X$ as follows:
\[X_j =
\begin{cases}
W_j & C_j = 0\\
R_j & C_j = 1.
\end{cases}\]  By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(P_1,..., P_\gamma, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $C$ and $C'$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})) \le (\gamma-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $J^c$, the statistical distance is at most $(\gamma-\beta)2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then,
\[
\Delta( S^{I_X(\cdot, \cdot)}(c^*, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(c', 1^{\gamma \log |Z|})) \le ( \gamma -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $j\in J$, $\Hav(W_j | View(S))\geq \alpha$.  The only information about the correct value of $c_j^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $j \in J$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not, that is suppose, the probability of at least one nonzero response on index $j$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_j$, the probability of this happening when $c^*_j = 1$ is at most $q/\mathcal{Z}$ or  equivalently $2^{-\log |\mathcal{Z}|+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }j]\nonumber \\
 &= \Pr[c_j^* =1]\Pr[\text{non zero response location }j\wedge c_j^*=1]\nonumber \\&+ \Pr[c_j^*=0] \Pr[\text{non zero response location }j \wedge c_j^*=0]\nonumber \\
&\le 1\times 2^{-\log|\mathcal{Z}|+\log q} + 1\times  \Pr[\text{non zero response location }j \wedge c_j^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that for an unguessable block source the remaining entropy $\alpha\leq \log |\mathcal{Z}|-\log q $:
\begin{claim}
\label{cl:ent bounded away from n}
If $W$ is a $(q, \alpha, \beta)$-block unguessable source over $\mathcal{Z}$ then $\alpha \le \log |\mathcal{Z}|-\log q$.
\end{claim}
\begin{proof}
\bnote{Leo I changed some of this under you.  It wasn't clear or probably right as written.}
Let $W$ be a $(q, \alpha, \beta)$-block unguessable source.  Let $J\subset\{1,..., \gamma\}$ the set of good indices.
It suffices to show that there exists an $S$ making $q$ queries such that for some $j\in J, \Hav(W_j | S^{I_{W}(\cdot, \cdot)})\le \log |\mathcal{Z}| - \log q$.  Let $j\in J$ be some arbitrary element of $J$ and denote by $w_{j,1}, ..., w_{j,q}$ the $q$ most likely outcomes of $W_j$~(breaking ties arbitrarily).  Then $\sum_{i=1}^q \Pr[W_j = w_{j,i}]\geq q/|\mathcal{Z}|$.  Suppose not, this means that there is some $w_{j,i}$ with probability $\Pr[W_j = w_{j,i}] < 1/|\mathcal{Z}|$.  Since there are $\mathcal{Z} - q $ remaining possible values of $W_j$ for their total probability to be at least $1-q/|\mathcal{Z}|$ at least of these values has probability at least $1/\mathcal{Z}$.  This contradicts the statement $w_{j,1},..., w_{j,q}$ are the most likely values.  Consider $S$ that queries its oracle on $(j, w_{j,1}),.., (j, w_{j,q})$.  Denote by $Bad$ the random variable when $W_j\in \{w_{j,1},.., w_{j,q}\}$  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_j | S^{J_W(\cdot, \cdot)}) &=  -\log \left(\Pr[Bad=1]\times 1+ \Pr[Bad=0]\times \max_{w}\Pr[W_j = w| Bad =0]\right)\\
&\leq  -\log \left(\Pr[Bad=1]\times 1\right)\\
&=-\log\left( \frac{q}{|\mathcal{Z}|} \right) = \log|\mathcal{Z}|-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }j \wedge c_j=0] &>2^{-\alpha+1} - 2^{-(\log |\mathcal{Z}|-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_j=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the block unguessability of $W$~(\defref{def:block guessable}).  By the union bound over the indices $j\in J$ the total probability of a $1$ in $J$ is at most $(\gamma-\beta)2^{-\alpha+1}$. Recall that $c^*, c'$ match on all indices outside of $J$. Thus, for all $c^*, c'$ the statistical distance is at most $(\gamma- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}.
\end{proof}
By averaging over all points in $C'$ we conclude that $\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})) < (\gamma -\beta)2^{-(\alpha+1)}$.  This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(P_1,..., P_\gamma, C') ]- \expe [S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish}, we have
\begin{align*}
\delta^{D}(( P, C), (P, C'))&\leq |\expe [D(P_1,..., P_\gamma, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})]| \\
&+|\expe[S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})] - \expe[S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] |\\
&+|\expe [S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] - \expe [D(P_1,..., P_\gamma, C') ]|\\
&\leq \epsilon_{sec}/3+ (\gamma-\beta)2^{-(\alpha-1)}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:security of cons}.
\end{proof}

\subsection{Correctness}
We now argue correctness of \consref{cons:first construction}.
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
Let all variables be as in \thref{thm:main thm first cons}.
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \le \gamma/|\mathcal{Z}| = \ngl(n)$.
\end{lemma}
\begin{proof}
Consider a coordinate $j$ for which $c_j=1$. Since $w'$ is chosen independently of the points $r_j$, and $r_j$ is uniform, $\Pr[r_j =w_j']  = 1/|\mathcal{Z}|$. The lemma follows by the union bound, since there are at most $\gamma$ such coordinates.
\end{proof}

Since there are most $t$ locations for which $w_j\neq w_j'$ there are at most $t$ $0\rightarrow 1$ bit flips in $c$, which the code will correct with probability $1-\delta_{code}$, because $c$ was chosen uniformly.
Therefore, \consref{cons:first construction} is correct with error at most $\gamma/|\mathcal{Z}|$.





\end{document} 