\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\cond}{\ensuremath{\mathtt{cond}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Vol}{\mathsf{Vol}}

\title{Key Derivation From Noisy Sources With More Errors Than Entropy}
%\author{Ran Canetti \and Benjamin Fuller\footnote{The Lincoln Laboratory portion of this
%work was sponsored by the Department of the Air Force under Air Force
%Contract
%\#FA8721-05-C-0002.  Opinions,
%interpretations, conclusions and recommendations are those of the author
%and
%are not necessarily endorsed by the United States Government.} \and Omer Paneth \and Leonid Reyzin \and Adam Smith}

\begin{document}
\maketitle


\begin{abstract}
Fuzzy extractors convert a noisy source of entropy into a consistent uniformly-distributed key.  In the process of eliminating noise, they  lose some of the entropy of the original source---in the worst case, as much as the logarithm of the number of correctable error patterns. For many practical sources, the error tolerance necessary to achieve reliable key derivation is larger than the entropy of the source.  Unfortunately, it is impossible to provide security for all sources with this property.  Most known approaches for building fuzzy extractors work in the worst case and cannot be used when a source has \emph{more errors than entropy}.  

In this work, we exploit source structure to construct fuzzy extractors for large classes of sources with more errors than entropy.  Our constructions are for the Hamming metric over symbols of some alphabet $\mathcal{Z}$.  We present both information-theoretic and computational constructions.

Our first construction is information-theoretic and corrects a constant fraction of Hamming errors over a super-constant size alphabet.  In this construction, each symbol must contribute fresh entropy.  

Our second and third constructions switch to computational security to provide additional features.  Our second construction supports the same type of sources as the first construction but also provides reusability.  A reusable fuzzy extractor can be safely reused across multiple noisy readings of the source~(Boyen, CCS 2004).  Previous reusable fuzzy extractors only provide security when the noise between repeated readings comes from a small and unrealistic class. Our construction is secure as long as the fuzzy extractor is secure on each individual reading. Unfortunately, this construction only corrects a sub-constant fraction of errors.  %Computational techniques allow us to show a very sshows that computational assu computationaprovide requires a constant fraction of symbols to have a constant amount of entropy conditioned on prior symbols. It corrects a sub-constant fraction of errors and supports super-constant size alphabets.  In addition, it is first construction of a reusable fuzzy extractor without assumption on the form of errors that occurs between reuse.  This is impossible in the information-theoretic setting for an arbitrary number of reuses~(Boyen, CCS 2004).

Our first two constructions require each symbol of the source to contribute fresh entropy.  Our third construction removes this requirement and allows symbols to be dependent.  This change requires a significantly larger alphabet.

The computational constructions can be implemented efficiently based on number-theoretic assumptions or assumptions on cryptographic hash functions.

\bigskip

\textbf{Keywords} Fuzzy extractors, key derivation, error-correcting codes, computational entropy, point obfuscation.
\end{abstract}


\section{Introduction}\label{sec:introduction}

\paragraph{Fuzzy Extractors}
Cryptography relies on long-term secrets for key derivation and authentication. However, many sources with sufficient randomness to form long-term secrets provide similar but not identical values at repeated readings~(prominent examples include biometrics and other human-generated data~\cite{daugman2004,zviran1993comparison,brostoff2000passfaces,ellison2000protecting,mayrhofer2009shake,monrose2002password},
physically unclonable functions~\cite{pappu2002physical,tuyls2006puf,gassend2002silicon,suh2007physical},
and quantum information~\cite{bennett1988privacy}). Turning similar readings into identical values is known as \emph{information reconciliation}; further converting those values into uniformly random secret strings is known as \emph{privacy amplification}~\cite{bennett1988privacy}.
Both of these problems have interactive and non-interactive versions.  In this paper, we are interested in the non-interactive case, which is useful for a single user trying to produce the same key from multiple readings of a physical source at different times.
 A \emph{fuzzy extractor} is the primitive that accomplishes both information reconciliation and privacy amplification non-interactively; fuzzy extractors are defined information-theoretically in~\cite{DBLP:journals/siamcomp/DodisORS08}.


Fuzzy extractors consist of a pair of algorithms: \gen (used once, at ``enrollment'') takes a source value $w$, and produces a key $r$ and a public helper value $p$.  The second algorithm \rep (used subsequently) takes this helper value $p$ and a close $w'$ to reproduce the original key $r$. 
The correctness guarantee is that $r$ will be correctly reproduced by \rep as long as $w'$ is no farther than $t$ from $w$ in some metric space (in this paper, we focus on the Hamming metric on length $\gamma$ strings over some alphabet $\mathcal{Z}$).
 The security guarantee is that $r$ produced by \gen is close to uniform (information-theoretically \cite{DBLP:journals/siamcomp/DodisORS08} or computationally \cite{fuller2013computational}), even given $p$. This guarantee holds as long as $w$ comes from a high-quality distribution, which
traditionally has been defined as \emph{any} distribution with sufficient min-entropy $m$. 

\paragraph{Reusable Fuzzy Extractors}
An additional desirable security property of fuzzy extractors, introduced by Boyen~\cite{Boyen2004}, is called reusability. This property is necessary if a user enrolls the same or correlated values multiple times. For example, if the source is a biometric reading, the user may enroll the same biometric with different organizations.  Each of them will get a slightly different enrollment reading $w_i$, and will run $\gen(w_i)$ to get a key $r_i$ and a helper value $p_i$. Security for each $r_i$ should hold even when an adversary is given all the values $p_1, \dots, p_q$ (and, in case some organizations turn out to compromised or adversarial, a stronger security notion requires security for $r_i$ even in the presence of $r_j$ for $j\neq i$).  Many traditional fuzzy extractors are not reusable~\cite{Boyen2004,simoens2009privacy,blanton2012non,blanton2013analysis}.


\paragraph{Limitations of Known Approaches}

Constructions of fuzzy extractors are limited by the tension between security and correctness guarantees: if we allow for higher error tolerance $t$, then we also need higher starting entropy $m$. The reason for this tension is simple: if an adversary who knows $p$ can guess any $w'$ within distance $t$ of $w$, then it will be able to easily obtain the true $r$ by running $\rep$.
%If $t$ is larger, then a single guess for $w'$ may be within distance $t$ of more $w$ values, thus increasing adversarial probability of winning. 
In fact, if $t$ is high enough that there are $2^m$ points in a ball of radius $t$, then there exists a distribution of $w$ of min-entropy $m$  \emph{contained entirely in a single ball}.  For this distribution, an adversary can run $\rep$ on the center of this ball and always learn the key $r$.
Thus, if the security guarantee of a given fuzzy extractor holds for \emph{any} source of a given min-entropy $m$ and the correctness guarantees holds for any $t$ errors, then $m$ must   be greater than $\log |B_t|$, where $|B_t|$ denotes the number of points in a ball of radius $t$.  This condition on the source is necessary regardless of whether the fuzzy extractor is information-theoretic or computational, and extends even to the interactive setting.
If a source fails this condition, we will says that it has \emph{more errors than entropy}.



%More generally, for any $m$ and $t$, there is a distribution of min-entropy $m$ such that the adversary can guess a correct $w'$ with probability $1/\lceil( 2^m/B_t) \rceil\approx B_t 2^{-m}$: the distribution consists of the uniform distribution over all points in several non-overlapping balls of radius $t$ (the metric space must be large enough for these balls not to intersect). We thus call $m-\log B_t$ the \emph{minimum usable} entropy, denoted by $\Huse$. The previous paragraph shows that  no fuzzy extractor can handle all distributions of a given min-entropy $m$ if  $\Huse\le 0$.

%Prime candidate sources for authentication have $\Huse\le 0$.  
Unfortunately, prime candidate sources for authentication have more errors than entropy. 
For example, the IrisCode~\cite{daugman2004}, which is the state of the art approach to handling what is believed to be the best biometric \cite{prabhakar2003biometric}, produces a source that more errors than entropy~\cite[Section 5]{blanton2009biometric}. Physical unclonable problems with slightly nonuniform outputs suffer from similar problems~\cite{koeberl2014entropy}. 

%As an example, the iris is believed to be the best biometric for high security applications~%\cite{prabhakar2003biometric}.  Daugman~\cite{daugman2004} designs a transform called IrisCode (using specialized wavelets) to derive a $2048$ bit string from an iris.  Let the outcome of this transform (on different irises) define a distribution $w$.  The precise number of errors that must be tolerated depends on the desired correctness.  For correctness of around $20\%$, a $t$ of approximately $205$ is required. Thus,
%\[
%\log |B_t|
%= \log \sum_{i=0}^{205} {2048 \choose i} \approx 956\,.
%\]
%In contrast, $w$ is estimated to have about $249$ bits of entropy~\cite{daugman2004}.
%There is considerable subsequent research~\cite{gentile2009slic,gentile2009efficient,rathgeb2011combining}, but it does not affect the above comparison dramatically.%\footnote{The work of Hao et al.~\cite[Section 4.3]{hao2006combining} provides a similar analysis but their calculation underestimates the number of possible error patterns; their calculation, which we cannot confirm, is $\Huse \approx 44$.}  


The situation with reusability is even worse: the only known construction of reusable fuzzy extractors \cite{Boyen2004} requires very particular relationships between $w_i$ values, which are unlikely to hold in any practical source. \bnote{negative results stated in our contributions. keep there?}


\paragraph{Our Contributions}
We provide the first constructions of fuzzy extractors that can be used for large classes of sources that have more errors than entropy.  Our constructions work for Hamming errors for strings $w$ of length $\gamma$ over some alphabet $\mathcal{Z}$. Naturally, as argued above, these constructions cannot work for all sources of a given entropy; each construction comes with a constraint on the sources for which it is secure.  Table~\ref{tab:upper bounds} summarizes our constructions.

Our first construction provides information-theoretic security.  It can correct a constant fraction of errors, but requires that a constant fraction of the symbols contribute fresh entropy, even conditioned on previous symbols. %It requires an alphabet size that is super constant in the security parameter. \lnote{keep this last sentence?}

We switch to computational security to obtain constructions with additional features. Boyen showed information-theoretic reusability fuzzy extractors incur entropy loss proportional to the number of corrected error patterns~\cite[Theorem 11]{Boyen2004}.  For sources with more errors than entropy, information-theoretic reusability is impossible~(see \secref{sec:reusable} for discussion).  

Our second construction, like the first, also requires that a constant fraction of symbols contribute fresh entropy. It provides reusability against \emph{computational} adversaries. 
The reusability we obtain is very strong:  security holds even if the multiple readings $w_i$ used in $\gen$ are \emph{arbitrarily correlated}, as long as each $w_i$ \emph{individually} satisfies the above conditions as a distribution. 
 This construction, however, requires that the fraction of errors is subconstant. \bnote{reasonable?}

Our third construction removes the need for fresh entropy in the symbols. 
It is secure when symbols in $w$
each have individual super-logarithmic min-entropy, even if they are arbitrarily correlated. Moreover,
a constant fraction of symbols in $w$ may have little entropy, as long as knowledge of their values does not reduce the entropy of the high-entropy symbols too much (see \defref{def:block guessable}).  This construction can correct a constant fraction of errors.  However, this construction requires a large alphabet size~(super-polynomial in the security parameter).

%\bnote{rewrite this!}
%Our first construction is secure when symbols in $w$ each have individual super-logarithmic min-entropy, even if they are arbitrarily correlated. Moreover, a constant fraction of symbols in $w$ may have little entropy, as long as knowledge of their values does not reduce the entropy of the high-entropy symbols too much (see \defref{def:block guessable}).  This construction can correct a constant fraction of errors~($\Theta(\ell)$).  However, this construction requires a large alphabet size~(super-polynomial in the security parameter).
%
%We improve the entropy and alphabet size requirements in the second construction, which requires only a constant fraction of the symbols $w$ to have constant min-entropy conditioned on the previous symbols.  Our second construction supports an arbitrary alphabet size~(though it only achieves $\Huse\le 0$ when the alphabet size is super-constant).  However, these improvements come at a price to error-tolerance: it tolerates a sub-constant fraction of errors~($o(\ell)$).  
%
%\consref{cons:sampling} comes with an additional feature: reusability.  Boyen~\cite{Boyen2004} defined reusable fuzzy extractors: multiple readings $w_1,..., w_q$ are enrolled and the adversary sees the corresponding public values $p_1,..., p_q$ and possibly some of the derived keys.  The enrolled values $w_i$ and $w_j$ are not assumed to be equal, and so information leakage may add up.  Indeed, Boyen showed that with an unbounded number of reuses it is only possible to support limited correlations between $w_i$ and $w_j$~\cite[Theorem 11]{Boyen2004}.  Our second construction allows for arbitrary correlation between $w_i$ and $w_j$ against computational adversaries\footnote{Our restriction to computational adversaries implies that the adversary only sees a polynomially bounded number of reuses.  It remains an open problem to construct a polynomially reusable fuzzy extractor against unbounded adversaries.} as long as each marginal $W_i$ is an admissible distribution.  %We do not considering the joint distribution $W_1,..., W_q$.

\begin{table}
\begin{tabular}{l | l | l | l }
Construction & Security & Features & Limitations \\
\hline
\consref{cons:info theoretic} & Info-theoretic & Exploit distribution & Symbols must add\\
&&structure & fresh entropy \\\hline
\consref{cons:sampling} & Computational  & Reusability & Sub-constant fraction \\
&& & error tolerance\\\hline
\consref{cons:first construction} & Computational & Allows correlated & Super-polynomial \\
& &  symbols & size symbols
\end{tabular}
\caption{Summary of new constructions.  All constructions support families of distributions with more errors than entropy.}
\label{tab:upper bounds}
\end{table}

\paragraph{Our Approach}
%\bnote{rewrite this!}
%Our constructions are computationally secure.  Known techniques for proving security of information-theoretic  fuzzy extractors work for all distributions for a given $m, t$, and thus cannot be used with $\Huse\le 0$ (because they would prove security for distributions contained within a single ball of radius of $t$). 



%Fuller, Meng, and Reyzin show that replacing secure sketches with a similar computational component is unlikely to be fruitful~\cite[Corollary 3.8, Theorem 3.10].  Instead, they suggest two alternatives: produce a new consistent secret with computational entropy instead of recovering $w$~(our first construction) or combine the information-reconciliation and privacy amplification components~(our second construction).

Most known constructions of fuzzy extractors put sufficient information in $p$ to recover the original $w$ from a nearby $w'$ during $\rep$ (this procedure is called a \emph{secure sketch}), and then apply a randomness extractor to $w$ to get $r$.
Unfortunately, the current techniques for building secure sketches do not work for sources with more errors than entropy, because they lose at least $\log |B_t|$ bits of entropy regardless of the source (this loss is necessary when the source is uniform~\cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08}). Moreover, relaxing the security definition to computational will not improve this approach~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational}. Thus, we take a different approach and do not attempt to recover $w$.

Our first construction reduces the alphabet size by hashing each input symbol (which comes from a large alphabet) into a much smaller set, so that the resulting hash value has low entropy deficiency.
The intuition behind this approach is that it reduces the size of $B_t$ by reducing the alphabet size, but preserves a sufficient portion of the input entropy.  The resulting string no longer has more errors than entropy.
 We then apply any information-theoretic fuzzy extractor to the resulting string.

Our second construction, which is computationally secure, is based on obfuscated digital lockers~\cite{canetti2008obfuscating}. Digital lockers output a secret value only when given the correct input to ``unlock'' the secret. An obfuscated digital locker does not provide information about the locked value or how to unlock it.  The main idea of the construction is to pick a random $r$ and lock $r$ in a digital locker that is unlocked by a random subset of the symbols of $w$. To tolerate errors in the input, this process is repeated several times, so that at least one digital locker can be unlocked using $w'$. We use obfuscation in a way that does not leak partial information; this is crucial to arguing reusability.

Finally, our third construction tolerates more errors than the second because it uses digital lockers that are unlocked by single symbols of $w$. Since we do not assume that every symbol has high individual entropy, hiding an entire $r$ in every locker then becomes too risky, Instead, we hide a single bit per locker (our actual construction uses point obfuscations, which are simpler to construct than digital lockers). To tolerate errors, these bits come from an error correcting code. To ensure an  adversary who learns some bits doesn't learn anything useful about $r$, we don't encode $r$ in the error-correcting code, but rather extract $r$ (using an information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractor) from the decoded string.


We emphasize that the obfuscation techniques we use are much weaker than general obfuscation, and can be instantiated quite efficiently under appropriate assumptions.
We need a strong version of point obfuscation/digital lockers that remain secure even when several obfuscations of correlated points are composed. The required notion is 
%While the standard definition of obfuscation \cite{barak2001possibility} does not imply security under composition, we can base our construction on the relaxed notion \lnote{something is weird --- standard definition is not ok for composition , but a relaxed one is ok?} \bnote{In neither definition is composition implied by standalone security.  However it is known how to construct composable VGB obfuscation for the class of functions we need.} 
composable \emph{virtual grey-box} obfuscation introduced in~\cite{bitansky2010strong}. Bitanski and Canetti construct composable obfuscation of point programs under particular number-theoretic assumptions~\cite{bitansky2010strong}. 
Additionally, such obfuscation can be made very efficient under a strong assumption on cryptographic hash functions~\cite{canetti1997towards}.



%Any procedure that converts a high-entropy input to a high-entropy output is known as a \emph{conductor} \cite{CRVW02}; if it's error-tolerant, then it's a \emph{fuzzy conductor}~\cite{KanukurthiR09}. Our first construction is a \emph{computational fuzzy conductors}.
%This may be converted to a computational fuzzy extractors using information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors~(\lemref{lem:cond and cext}). 

%Both constructions are based on  obfuscation of point programs~\cite{canetti1997towards}.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.
%We need a strong version of point obfuscation that remains secure even when several obfuscations of correlated points are composed. While the standard definition of obfuscation \cite{barak2001possibility} does not imply security under composition, we can base our construction on the relaxed notion of \emph{virtual grey-box} obfuscation introduced in~\cite{bitansky2010strong}. For this notion, \cite{bitansky2010strong} construct composable obfuscation of point programs under particular number-theoretic assumptions. Additionally, such obfuscation can be made very efficient under a strong assumption on cryptographic hash functions~\cite{canetti1997towards}.

%Both of our constructions use techniques from Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  Let $w=w_1 \dots w_\ell$, for $w_j\in \mathcal{Z}$. In the first construction (\consref{cons:first construction}), for each $j$, $\gen$ flips a coin $c_j$ and either obfuscates $I_{w_j}$ or picks a random point $r_j$ and obfuscates $I_{r_j}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rep$ then runs the obfuscated program $P_j$ on $w_j'$ and checks whether $P_j(w_j')=1$.  For most locations $j$, \rep can determine whether $w_j$ or a random value was obfuscated.  Thus, most bits of $c_j$ are recoverable. To tolerate errors,  the set of coins $c_1\dots c_\ell$  is chosen at random from the codewords of an error correcting code. This construction conducts entropy from $w$ to $c$.

%Obfuscation of point functions provides no security if a point can be guessed; thus, in order for the first construction to be secure, sufficiently many coordinates of $w$ have to be unguessable (even to an adversary who can make equality queries for the values of other coordinates). We relax this requirement in our second construction (\consref{cons:sampling}), called \emph{sample-then-obfuscate}: it transforms $w$ into a string of blocks and then obfuscates each string of blocks. $\gen$ randomly samples several coordinates of $w$ and concatenates them to form a block. This reduces the entropy requirement on the individual symbols, but lowers the error-tolerance. We limit the degradation of error-tolerance by using digital lockers in place of point obfuscation~(sufficiently composable point obfuscation implies digital lockers).  This approach is similar to the sample-then-extract paradigm for building locally computable extractors~\cite{lu2002hyper,vadhan2003constructing}.  Unlike in locally computable extractors, we can form multiple blocks sampling from the same value $w$ and only argue about their individual entropy, because correlations among blocks are allowed in the first construction. Computational, rather than information-theoretic, analysis seems crucial for achieving this property.

\paragraph{Connection to General Obfuscation}
 \onote{write this!!!}
We note that fuzzy extractors for the setting where $\Huse\le 0$ can be trivially constructed from a strong form of obfuscation, specifically, virtual black-box obfuscation for the class {\em proximity point programs}, $I_w(x)$ that tests if $x$ is within Hamming distance $t$ of $w$. However, currently we do not know if such obfuscation exists, let alone whether it can be made as efficient as our constructions. Recent works constructed candidate indistinguishability obfuscators \cite{GargGH0SW13,PassTS13} and virtual black-box obfuscators in an idealized model \cite{brakerski2014virtual,barak2014protecting} for all programs. However, these notions of obfuscation do not imply virtual-black-box obfuscation for proximity point programs in the plain model. The work of \cite{BarakBCKPS13} suggests an average-case virtual-black-box obfuscator for the family of {\em evasive} functions that test if a low-degree multi-variant arithmetic circuit evaluates to zero. However, we do not know if these functions include proximity point programs.



\paragraph{Open Problems}
All of our constructions support more errors than entropy by using two metrics spaces.  Errors are tolerated in one metric space and corrected in a second.  To handle more errors than entropy,  we map to a metric space where multiple error patterns are grouped together.  All our constructions require the first metric space to have a super-constant size alphabet.  An alternative approach to the problem may support constant size alphabets.

%Our first construction achieves a constant fraction error tolerance but at the cost of a large alphabet size.  Our second construction reduces the alphabet size significantly but only achieves a sub-constant error tolerance.  It remains an open question to support a constant fraction of errors on a small alphabet.  More generally, the problem of constructing a secure fuzzy extractor whenever a negligible fraction lies in any ``ball'' remains an open problem.  %Both constructions require a large alphabet $\mathcal{Z}$---one whose size is more than polynomial in the security parameter.\footnote{Codes over large alphabets are often used to correct burst errors~\cite{gilbert1960capacity}.}  It is possible to tweak the sample-then-obfuscate construction for use with a small alphabet.  However, distributions with $\Huse \le 0$ are supported only for large alphabets.  For small alphabets, $\Huse>0$ and there are good information-theoretic constructions known~\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08}.
%Constructing a computational fuzzy extractor when $\Huse\le 0$ and a small alphabet is an open problem.  \bnote{Should this just be removed?}

%Using information-theoretic fuzzy extractors with additional privacy properties, Dodis and Smith~\cite[Section 5]{DBLP:conf/stoc/DodisS05} construct program obfuscators for the program $I_w(x)$ that tests if $x$ is within Hamming distance $t$ of $w$. The obfuscation is secure as long as $w$ comes from a distribution of sufficient min-entropy; in particular, the entropy must be high enough so that $\Huse>0$. Our constructions do not provide obfuscators for proximity queries, because they leak more information than whether $x$ is within distance $t$ of $w$ (for example, they may provide some information about the actual distance or about which coordinates agree). Constructing an efficient obfuscator for proximity queries when $\Huse\le 0$ is an open problem.

In this work we restrict the distribution of the original reading $w$ and allow $w'$ to be an arbitrary point within distance $t$.  An alternative approach is to restrict the set of $w'$ where $\gen$ produces the correct key.%  A meaningful restriction of correctable errors is an open problem.

\medskip
\paragraph{Organization }
The remainder of this paper is organized as follows: we cover notation and fuzzy extractors in \secref{sec:preliminaries}.  Present our information-theoretic construction in \secref{sec:info theory cons} and our two computational constructions in Sections \ref{sec:sampling} and \ref{sec:cor construction}.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote by $X = X_1,..., X_\gamma$  the tuple $(X_1,\dots, X_\gamma)$.  For a set of indices $J$, $X_{J}$ is the restriction of $X$ to the indices in $J$.  The set $J^c$ is the complement of $J$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$,
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   For a random variable $W$, let $H_0(W)$ be the logarithm of the size of the support of $W$,  that is $H_0(W) = \log |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$ we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$ (we extend it to a class of distinguishers $\mathcal{D}$ by taking the maximum over all distinguishers $D\in\mathcal{D}$).  We denote by $\mathcal{D}_{s}$ the class of randomized circuits which output a single bit and have size at most $s$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric over vectors in $\mathcal{Z}^\gamma$, defined via $\dis(x,y) = \{i | x_i \neq y_i\}$.  For this metric, $|B_t| = \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
Usually, we use capitalized letters for random variables and corresponding lowercase letters for their samples.

\subsection{Fuzzy Extractors}
\label{sec:fuzzy extractors}

In this section we define computational fuzzy extractors.  Definitions for information-theoretic fuzzy extractors can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  %Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' ($\gen$) and ``reproduce'' ($\rep$) is an $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s)$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\kappa$ and a helper string $p\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(r, p)\leftarrow \gen(w)$, then $\Pr[\rep( w', p) = r] \geq 1-\delta$, where the probability is over the randomness of $(\gen, \rep)$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $r$ is pseudorandom conditioned on $p$, that is $\delta^{\mathcal{D}_s}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness. In Constructions~\ref{cons:sampling} and~\ref{cons:first construction} it is crucial that $w'$ is chosen independently of the outcome of \gen.  

Information-theoretic fuzzy extractors are obtained by replacing computational distance by statistical distance.  We do make a second definitional modification.  The standard definition of information-theoretic fuzzy extractors considers $\mathcal{W}$  consisting of all distributions of a given entropy.  As described in the introduction, it is impossible to provide security for all distributions with more errors than entropy.  In both the computational and information-theoretic settings we consider a family of distributions $\mathcal{W}$.

\subsubsection{Reusable Fuzzy Extractors}
\label{sec:reusable}

Ideally, a fuzzy extractor allows a user to authenticate to multiple different services/devices.  That is, instead of creating a single pair $(r, p)$ from an initial reading $w$, we read the noisy source multiple times~($w_1,..., w_q$) and produce $(r_1, p_1),..., (r_q, p_q)$.  This problem is known as a reusable fuzzy extractor~\cite{Boyen2004}.  There are two primary parameters of the definition: 1) $q$, the reusability of the fuzzy extractor and 2) what class of correlation is allowed between readings $w_i$ and $w_j$.  

Boyen considers two versions of the problem, first where the adversary sees $p_1,..., p_q$~(outsider security).  Second where the adversary controls some subset of the servers and sees key generation on arbitrary $p$~(insider security).  
Boyen constructs a outsider reusable cryptographic fuzzy extractor for unbounded $q$ when the correlation between readings is limited to a transitive isometric permutation groups. Boyen transforms this construction to insider security using random oracles. In both constructions, the correlation between  $w_1,..., w_q$ is limited to a transitive isometric permutation group.  Unfortunately, this class of correlation is relatively weak and unrealistic.
%\paragraph{Previous Negative Results}

Boyen~\cite{Boyen2004} considers information-theoretic fuzzy extractors with unbounded reusability.  
However, even for this relatively weak class of correlations, Boyen shows that reusability comes at a real cost~\cite[Theorem 11]{Boyen2004}.  If the correlation between readings contains a set that generates a transitive isometric permutation group, any secure sketch must lose entropy proportional to the number of errors corrected.\footnote{This result strengthens the bounds of~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08}, who show this loss is necessary for uniform $W_1$.} This means achieving reusability for sources with more errors than entropy is impossible in this setting.
%\bnote{old stuff}
%Their construction is secure when the correlations between $w_1,.., w_q$ come from a subgroup of permutations used in the construction of the fuzzy extractor.  They then show information-theoretic fuzzy extractors secure with unbounded reuse can be transformed into their construction~\cite[Theorem 11]{Boyen2004}.  This shows that correlation between $w_1,..., w_q$ must be restricted to this limited class.  There is no reason to expect that noise between repeated readings comes from a well-behaved subgroup of permutations.  Their result can be interpreted as saying that reusability is impractical.
%
%However, this result is inherently information-theoretic, the adversary must exhaust all randomness of the fuzzy extractor and computes statistical distance over exponential size strings.  There are two  restrictions to avoid this result: 1) limit the number of reuses of the fuzzy extractor 2) provide security against computational adversaries.  The second restriction implies the adversary will only observe a polynomial number of invocations of the fuzzy extractor.

To overcome this result, we consider computationally bounded adversaries.
%We present a non-adaptive definition and note that our results extend to the adaptive definition presented by Boyen~\cite[Definition 7]{Boyen2004}.\bnote{We need to come back to justifying this or provide a proof that our scheme works in the adaptive setting.}
%\begin{definition}[Insider Reusable Fuzzy Extractors]
%\label{def:reusable fuzz}
%Let $\mathcal{W}$ be a family of distributions over $\mathcal{M}$.  Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon, s)$-hard with error $\delta$.  Let $W_1,..., W_q$ be a joint distribution such that $\forall i, W_i\in\mathcal{W}$.  Let $(R_i, P_i)$ be the distribution formed by running $\gen$ on the distribution $W_i$.  We say that $(\gen, \rep)$ is $q$-reusable if for all $i$
%\begin{align*}
%\delta^{\mathcal{D}_{s}}(&(R_1, P_1), ..., (R_i, P_i),..., (R_q, P_q), \\
%&(R_1, P_1),..., (U_\kappa, P_i),..., (R_q, P_q)) < \epsilon.
%\end{align*}
%\end{definition}
We modify the definition of Boyen~\cite[Definition 7]{Boyen2004}.  We discuss the differences in \apref{sec:comparing reusability}.

\begin{definition}[Outsider Reusable Fuzzy Extractors]
\label{def:outsider fuzz ext}
Let $\mathcal{W}$ be a family of distributions over $\mathcal{M}$.  Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon, s)$-hard with error $\delta$.
Fix some $W_1 \in \mathcal{W}$.  Let $f_2,.., f_q , D$ be a split adversary.  Define the following game for all $i=1,..., q$:
\begin{enumerate}%[leftmargin=-.6in]
\item[Sampling] The challenger samples $w_1\leftarrow W_1$.
\item[Perturbation] For $i=2,..., q$: the challenger computes $(r_i, p_i)\leftarrow \gen(w_i)$.  Set $w_{i+1} = f_i(w_1, p_1,..., p_i)$.% The output $p_i$ is provided to $\mathcal{A}$, who selects $\delta_i:\mathcal{M}\rightarrow \mathcal{M}$.  The challenger sets $w_{i+1} = \delta_i(w_1)$.
\item[Distinguishing] The advantage of $D$ is
\begin{align*}
\text{Adv}(D)&\overset{def}= \Pr[D(r_1,..., r_{i-1}, r_i, r_{i+1},..., r_q, p_1,..., p_q)=1]\\ &- \Pr[D(r_1,..., r_{i-1}, u, r_{i+1},..., r_q, p_1,..., p_q)=1].
\end{align*}
where $u$ is a uniformly random string.
\end{enumerate}
$(\gen, \rep)$ is $(q, s_{sec}, \epsilon_{sec}, f_2,..., f_q)$-reusable if for all $D\in\mathcal{D}_{s_{sec}}$ the advantage is at most $\epsilon_{sec}$.
\end{definition}

The definition is parameterized by $f_2,..., f_q$.  This adversary implicitly defines distributions $W_2,..., W_q$~(which depend on $W_1$ and the public values $P_1,... P_i$).  Security seems hopeless if fuzzy extractor is not secure on each of these distributions on their own.  This is the only requirement we make on these functions.  We call these types of functions admissible:

\begin{definition}
Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon, s)$-hard with error $\delta$.  In the reusability game above, we say a set of functions $f_2,..., f_q$ are \emph{admissible} if for all $W_1\in \mathcal{W}$ for all $w_1\in W_1$ and $\forall p_1,..., p_q$ that are the public outputs of $\gen$ the implicit distribution $W_{i,w_1,p_1,..., p_{i-1}} = f_i(w_1,p_1,..., p_{i-1})$ is a member of $\mathcal{W}$.   
\end{definition}

\subsection{Obfuscation}
Our constructions will use obfuscation for two types of circuits: point functions and digital lockers. The family of point functions $\mathtt{I}_n = \{I_w\}_{w \in \zo^n}$ defined as follows:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
and the class of digital lockers is $\mathtt{I}_n = \{I_{w, r}\}_{w \in \zo^n, r\in\zo^\kappa}$ defined as follows:
\[
I_{w, r}(x):\begin{cases} r & x=w\\\perp & \text{otherwise}\end{cases}.
\]
The required notion of obfuscation is virtual grey-box (VGB) introduced in \cite{bitansky2010strong}. This notion is weaker then the standard notion of virtual black-box (\cite{barak2001possibility}), as it allows the simulator to run in unbounded time while making at a polynomial number of oracle queries to the function. VGB obfuscators for point functions and digital lockers are constructible from specific number-theoretic assumptions or by strong assumptions on hash functions.  We provide more details and a formal definition in \apref{sec:obfuscation def}. 

\section{Supporting more errors than entropy}
\label{sec:info theory cons}
In this section we show an information-theoretic fuzzy extractor that supports more errors than entropy.
%Information-theoretic fuzzy conductors can be converted to fuzzy extractors using a randomness extractor~(using the information-theoretic analogue of \lemref{lem:cond and cext}).
The construction first condenses entropy from each block of the source and then applies a different fuzzy extractor to the condensed blocks. We'll denote the fuzzy extractor on the smaller alphabet as $(\gen', \rep')$.  A condenser is like a randomness extractor but the output is allowed to be slightly entropy deficient.  Condensers are known with smaller entropy loss than possible for randomness extractors~(e.g.~\cite{dodis2014key}).  
\begin{definition}
\label{def:conductor}
A function $\cond : \mathcal{Z}\times \zo^d\rightarrow \mathcal{Y}$ is a $(m, \tilde{m}, \epsilon)$-randomness condenser if whenever $\Hoo(W)\ge m$, then there exists a distribution $Y$ with $\Hoo(Y)\ge \tilde{m}$ and $(\cond(W, seed), seed) \approx_\epsilon (Y, seed)$.
\end{definition}

The main idea of the construction is that errors are ``corrected'' on the large alphabet~(before condensing) while the entropy loss for the error correction is incurred on a smaller alphabet~(after condensing).  

\begin{construction}
\label{cons:info theoretic}
Let $\mathcal{Z}$ be an alphabet and let $W=W_1,..., W_\gamma$ be a distribution over $\mathcal{Z}^\gamma$.  We describe $\gen, \rep$ as follows:
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item Sample $seed_i\leftarrow \zo^d$.
\item Set $v_i = \cond(w_i, seed_i)$.
\end{enumerate}
\item Set $(r, p') \leftarrow \gen(v_1,..., v_\gamma)$.
\item Set $p = (p', seed_1,..., seed_\gamma)$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p = (p', seed_1,..., seed_\gamma))$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item Set $v_i = \cond(w_i, seed_i)$.
\end{enumerate}
\item Output $r = \rep'(v', ss)$.
\end{enumerate}
\vspace{0.7in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
For \consref{cons:info theoretic} to be secure we need most blocks to contribute some entropy to the output.  We call this notion a partial block source.

\begin{definition}
\label{def:partial source}
A distribution $W = W_1,..., W_\gamma$ is an $(\alpha, \beta)$-partial block source if there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_\gamma \in W_1,..., W_\gamma, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
\end{definition}
\defref{def:partial source} is a weakening of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}), as only some blocks are required to have entropy conditioned on the past.  The choice of conditioning on the past is arbitrary: a more general sufficient condition is that there exists some ordering of indices where most items have entropy conditioned on all previous items in this ordering~(for example, a ``partial'' reverse block source~\cite{vadhan2003constructing}).  This construction is secure and it supports distributions with more errors than entropy~(proof is in~\apref{sec:info theory sec}).

\begin{lemma}
\label{lem:info theory sec}
%Let $n$ be a security parameter and let $\gamma = \omega(\log n)$.
Let $\mathcal{W}$ be the family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-partial block sources over $\mathcal{Z}^\gamma$ and let $\cond: \mathcal{Z} \times \zo^d\rightarrow \mathcal{Y}$ be a $(\alpha, \tilde{\alpha}, \epsilon_{cond})$-randomness conductor.  Let $(\gen', \rep')$ be $(\mathcal{Y}^\gamma, \tilde{\alpha}(\gamma - \beta), \kappa, t, \epsilon_{fext})$-fuzzy extractor with error $\delta$.\footnote{We actually need $(\gen', \rep')$ to be an average case fuzzy extractor~(see \cite[Definition 4]{DBLP:journals/siamcomp/DodisORS08} and the accompanying discussion).  Most known constructions of fuzzy extractors are average-case fuzzy extractors.  For simplicity we refer to $\gen', \rep'$ as simply a fuzzy extractor.}  Then $(\gen, \rep)$ is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t, \gamma\epsilon_{cond}+\epsilon_{fext})$-fuzzy extractor with error $\delta$.

\end{lemma}

\subsection{More errors than entropy}

In this section we show that \consref{cons:info theoretic} supports partial block sources with more errors than entropy.  The required entropy of partial block source is $\alpha (\gamma-\beta ) = \Theta(\gamma)$.  The condenser of Dodis et al~\cite{dodis2014key} has a constant entropy loss, so $\alpha-\tilde{\alpha} = \Theta(1)$. This means that the input entropy to the fuzzy extractor is $\Theta(\gamma)$.   We assume that the new alphabet $\mathcal{Y}$ is of constant size.  Standard fuzzy extractors on constant size alphabets correct a constant fraction of errors at a entropy loss of $\Theta(\gamma)$, yielding $\kappa = \Theta(\gamma)$.  Thus, our construction is secure for distributions with more errors than entropy whenever $|\mathcal{Z}| = \omega(1)$.
More formally:
\[
\text{\# Errors} - \text{Entropy} = \log |B_t| - \Hoo(W) <  t \log |\mathcal{Z}| - \Theta(\gamma)-= \Theta(\gamma) \log |\mathcal{Z}| - \Theta(\gamma)  > 0
\]
That is, there exists a super-constant alphabet size for which \consref{cons:info theoretic} is secure with more errors than entropy.

\section{Adding reusability}
\label{sec:sampling}
In the previous section, we showed it was possible to construct a fuzzy extractor for a family of distributions with more errors than entropy.  Using computational techniques we are able to retain many of the advantages of \consref{cons:info theoretic} and achieve a reusable fuzzy extractor.  

The construction samples a random subset of blocks $W_{j_1},..., W_{j_\eta}$ and obfuscates the concatenation of these blocks.  Denote this concatenated value by $V_1$.  This process is repeated to produce $V_1,..., V_\ell$ where at least one $V_i$ should be correct to ``unlock'' the correct key.  
Let $\sample_{\gamma, \eta}(\cdot)$ be an algorithm that  outputs a random subset of $\{1,..., \gamma\}$ of size $\eta$ given let $r_{sam}$ bits of randomness.

\begin{construction}[Sample-then-Obfuscate]
\label{cons:sampling}
Let $\mathcal{Z}$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source where each $W_j$ is over $\mathcal{Z}$. 
Let $\eta$ be a parameter, and $\mathcal{O}$ be an obfuscator for the family of digital lockers with $\kappa$-bit outputs.  Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $r \overset{\$}\leftarrow \zo^\kappa$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Select $\lambda_i\overset{\$}\leftarrow \zo^{r_{sam}}$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\eta,\gamma}( \lambda_i)$
\item Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\item Set $\rho_i = \mathcal{O}(I_{v_i, r})$.
\item Set $p_i = \rho_i, \lambda_i$.
\end{enumerate}
\item Output $(r, p)$, where $p=p_1\dots p_\ell$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Parse $p_i$ as $\rho_i, \lambda_i$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\gamma, \eta}(\lambda_i)$.
\item Set $v_i' = w_{j_{i, 1}},..., w_{j_{i, \eta}}$.
\item Set $\rho_i(v_i') = r_i$.  If $r_i\neq \perp$ output $r_i$.
\end{enumerate}
\item Output $\perp$.
\end{enumerate}
\vspace{0.37in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
%There are three main differences between this construction and \consref{cons:first construction}.  They are as follows:
%\begin{itemize}
%\item Multiple blocks are concatenated together.  This comes at a cost to error tolerance but allows us to significant decrease the required entropy in each block.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.  
%\item Instead of encoding a bit of the key with each obfuscated value we encode the entire key with each obfuscated value.  This requires the use of digital lockers in place of point obfuscations.  Instead of having to open ``most'' of the obfuscations, it is only necessary for us to open a single obfuscation.  \consref{cons:first construction} only hid part of the key in each obfuscation.  This allowed some blocks in the distribution to be ``weak.''  However, sampling smoothes out $W$ so that all $V_i$ are ``good'' simultaneously.
%\item \consref{cons:first construction} required a large alphabet as blocks were individually obfuscated.  This construction works for an arbitrary size alphabet.  We show it supports $\Huse (W)\le 0$ when the alphabet size is super-constant in the security parameter.
%\end{itemize}
The use of a computational primitive~(obfuscation) allows us to sample multiple times, because we need to argue only about individual entropy of $V_i$, as opposed to the information-theoretic setting, where it would be necessary to argue about the entropy of the joint variable $V$.  This is the property that allows reusability.

This construction uses a na\"{i}ve sampler that takes truly random samples, but the public randomness may be substantially decreased by using more sophisticated samplers. Goldreich provides an introduction to samplers in~\cite{goldreich2011sample}.

\begin{theorem}
\label{thm:sampling}
Let $\mathcal{Z}$ be an alphabet.  Let $n$ be a security parameter.  Let $\mathcal{W}$ be the family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-partial block sources over $\mathcal{Z}^\gamma$ where $\gamma =\Omega(n)$.  Let $\eta$ be such that $\eta = \omega(\log n)$ and $\eta = o(\gamma)$, and let $c> 1$ be a constant and $\ell$ be such that $\ell = n^c$.  Let $\mathcal{O}$ be an $\ell$-composable VGB obfuscator for digital lockers~(with $\kappa$ bit outputs) with auxiliary inputs.  Then for every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that \consref{cons:sampling} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ for 
\begin{align*}
t&\leq -\frac{(c-1)}{2} \frac{(\gamma-\eta)\log n}{\eta} = o(\gamma)\\
\delta &= e^{-n}
\end{align*}
\end{theorem}

\subsection{Security of \consref{cons:sampling}}
\label{ssec:sec cons sampling}
In this section we outline security of \consref{cons:sampling}.  A proof of security appears in\apref{sec:analysis sampling}.  
With overwhelming probability, at each of the $\ell$ iterations, the sampler will choose enough coordinates of $W$ that have high entropy, making $V_i$ have sufficient entropy.   Once each of the $V_1,..., V_\ell$ have high entropy the obfuscations are unlikely to return a value other than $\perp$ to an adversary.
% forms a block-unguessable distribution.  Then security essentially follows from the security of \consref{cons:first construction}. %follows very similarly to \lemref{lem:security of cons}~(used to show security of \consref{cons:first construction}).  
%Essentially, the argument is that an adversary will never be able to open a digital locker and thus they learn no information about the key.  %By the same argument of Then \consref{cons:sampling} is just \consref{cons:first construction} applied to $V_1,.., V_\ell$, and security follows by \lemref{lem:security of cons}.  
We begin by showing that each $V_i$ is statistically close to a high entropy distribution.   Let $\Lambda$ represent the random variable of all the coins used by $\sample$ and $\lambda=\lambda_1 \dots \lambda_\ell$
be some particular outcome.

\begin{lemma}
\label{lem:sampling works}
Let all variables be as in \thref{thm:sampling}.
There exists $\epsilon_{sam} = O(e^{-\eta}) = \ngl(n)$ and $\alpha' = \alpha\eta(\gamma-\beta-\eta)/\gamma = \omega(\log n)$ such that for each $i$,
\[
\Pr_{\lambda\leftarrow \Lambda}[\Hoo(V_i | \Lambda= \lambda) \geq \alpha'] \geq 1- \epsilon_{sam}.
\]
\end{lemma}

\noindent
We can then argue that all $V_i$ simultaneously have individual entropy with good probability~(by union bound):
\begin{corollary}
\label{cor:samp sec}
 Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works}, and all the other variables be as in \thref{thm:sampling}.  Then $\Pr_{\lambda\leftarrow \Lambda}[\forall i, \Hoo(V_i | \Lambda = \lambda)  \ge \alpha'] \leq 1-\ell\epsilon_{sam}$.
%[V_i(V, \Lambda)$ is $(\ell\epsilon_{sam})$-close to a distribution $(V', U_{\ell\times r_{sam}})$ where for $u\in U_{\ell\times r_{sam}}$ for all $i$, $\Hoo(V_i' | U_{\ell\times r_{sam}} =u)\geq \alpha'$.
\end{corollary}
%\begin{proof}
%Union bound over the probability in \lemref{lem:sampling works}.
%%Hybrid argument over the statistical distance in \lemref{lem:sampling works}.
%\end{proof}

Once all $V_i$ all simultaneously have good entropy, the adversary only sees $\perp$ as an output from the obfuscations~(with overwhelming probability).  If the adversary only sees $\perp$ from the obfuscations, they have no information about the key.  

\begin{lemma}[Proof in \apref{sec:analysis sampling}]
\label{lem:samp unguess}
Let all the variables be as in \thref{thm:sampling}.
For every $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} = \ngl(n)$ such that $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_{\kappa}, P))< \epsilon_{sec}$.% \geq H_0(C)$. %for $\epsilon' = (2\epsilon_{obf} + \ell(\epsilon_{sam}+ 2^{-(\alpha''-1)})) = \ngl(n)$.
\end{lemma}
%\begin{proof}
%Result of \corref{cor:v are unguessable} and \lemref{lem:security of cons} noting that $\ell \epsilon_{sam} = \ngl(n)$.
%\end{proof}

\subsection{Correctness of \consref{cons:sampling}}
\label{sec:correct sampling}
We encode the entire key in each obfuscation.  For correctness, at least one of the repeated readings must be correct with overwhelming probability.  Let $V_i$ represent one of the initial readings and $V_i'$ represent a repeated reading.  For showing correctness we must show that $\Pr[\forall i, V_i \neq V_i'] < \ngl(n)$.



\begin{lemma}[Proof in \apref{sec:analysis sampling}]
\label{lem:sampling errors}
Let all the variables be as in \thref{thm:sampling}.
 Then $\Pr[\forall i, v_i\neq v_i'] < \ngl(n)$, where the probability is over the coins of $\gen$.  
\end{lemma}

\subsection{Reusability of \consref{cons:sampling}}
The reusability of \consref{cons:sampling} follows from the security of the VGB obfuscator with auxiliary input.  We consider a bounded $q = \poly(n)$ number of reuses.  For some fixed $i\in \{1,..., q\}$ we will treat the remaining keys as auxiliary input to the adversary, and the simulator still performs comparably to a distinguisher with access to the obfuscations.  Thus, given sufficiently strong reusability we achieve the following result:

\begin{theorem}
\label{thm:reusability}
Let $q = \poly(n)$, and let all the variables be as in \thref{thm:sampling}, except that $\mathcal{O}$ be an $\ell\times q$-composable VGB obfuscator for digital lockers~(with $\kappa$ bit outputs) with auxiliary inputs.  For any admissible $f_2,..., f_q$, for all $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $(\gen, \rep)$ is $(q, s_{sec}, \epsilon_{sec}, f_2,..., f_q)$-reusable fuzzy extractor.
\end{theorem}
\begin{proof}
The only modification to the outline presented in \secref{ssec:sec cons sampling} is in \lemref{lem:samp unguess} with the other keys $R_1,..., R_{i-1}, R_{i+1}, ..., R_q$ treated as additional auxiliary input to the adversary/simulator.  The simulator in the definition of composable obfuscation is required to function for arbitrary circuits in the family even if the choice of these circuits depends on the previous obfuscations.
\bnote{say more here?}
\end{proof}

\subsection{More errors than entropy?}
In this section, we show when \consref{cons:sampling} supports partial block sources with more errors than entropy.  The required entropy of partial block source is $\alpha (\gamma-\beta ) = \Theta(\gamma)$.  We are able to correct $o(\gamma)$ errors.
This yields:
\[
\text{\# Errors} - \text{Entropy} =  \log |B_t| -\Hoo(W) < t \log |\mathcal{Z}| - \Theta(\gamma)= o(\gamma) \log |\mathcal{Z}| - \Theta(\gamma)  
\]
That is, there exists a super-constant alphabet size for which \consref{cons:sampling} is secure with more errors than entropy.  We note that \consref{cons:sampling} works for an arbitrary size alphabet; however, for a constant size alphabet, the required entropy is greater than the number of corrected error patterns.  However, \consref{cons:sampling} is reusability for an arbitrary size alphabet.

\section{Allowing Correlated Symbols}
\label{sec:cor construction}
In the previous two sections, we presented constructions that supported restricted classes of distributions with more errors than entropy.  Unfortunately, both Constructions~\ref{cons:info theoretic} and \ref{cons:sampling} required each symbol to contribute ``fresh'' entropy.  In this section, we present a computational construction that allows for correlation between symbols while still supporting more errors than entropy and correcting a constant fraction of errors.
This construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}.  

Before presenting the construction we provide some definitions from error correcting codes.
We use slightly nonstandard error-correct codes over $\{0,1\}^\gamma$, which correct up to $t$ bit flips from $0$ to $1$ but no bit flips from $1$ to $0$ (this is the Hamming analog of the $Z$-channel~\cite{tallini2002capacity}).\footnote{Any code that corrects $t$ Hamming errors also corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
Codes with $2^{\Theta(\gamma)}$ codewords and $t = \Theta(\gamma)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.
The class of errors we support in our source~($t$ Hamming errors over a large alphabet) and the class of errors for which we need codes~($t$ $0\rightarrow 1$ errors) are different.  
}
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\gamma$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$.
\end{definition}

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\gamma$) is a $(\neigh_t, \delta_{code})$-code if there exists an efficient procedure $\decode$ such that $\Pr_{c\in C}[\exists c'\in \neigh_t(c) \text{ s.t. } \decode(c') \neq c] \leq \delta_{code}$.
\end{definition}

\begin{construction}
\label{cons:first construction}
Let $\mathcal{Z}$ be an alphabet and let $W = W_1,..., W_\gamma$ be a distribution over $\mathcal{Z}^\gamma$.  Let $\mathcal{O}$ be an obfuscator for point functions with points from $\mathcal{Z}$.  Let  $C\subset \zo^\gamma$ be an error-correcting code.
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $c\leftarrow C$.
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item If $c_j = 0$: $p_j = \mathcal{O}(I_{w_j})$.
\item Else: $r_j \overset{\$}\leftarrow \mathcal{Z}$.
\subitem Let $p_j = \mathcal{O}(I_{r_j})$.
\end{enumerate}
\item Output $(c, p)$, where $p=p_1\dots p_\gamma$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item If $p_j(w_j') = 1$: set $c_j' = 0$.
\item Else: set $c_j' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

%\bnote{shorten this discussion?}
%The input $w$ is hidden in two different ways.  In locations where $c_j=1$, the block $w_j$ is information-theoretically unknown.
%In locations where $c_j=0$, it is hard to find $w_j$ given access to the point obfuscation.
%There are two possible reasons for a bit $c_j'$ to be $1$: because the true value was $1$ and because $w_j \neq w_j'$.  However, if a bit $c_j'$ is $0$, this likely means that $w_j=w_j'$ because collisions when $c_j=0$ are unlikely~(occurring with probability $1/|\mathcal{Z}|$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips.

\consref{cons:first construction} is secure if no distinguisher can tell whether it is working with random obfuscations or obfuscations of $W_j$.  By the security of point obfuscation, anything learnable from the obfuscation is learnable from oracle access to the function. Therefore, our construction is secure as long as enough blocks are unpredictable even after adaptive queries to equality oracles for individual symbols. This restriction on the distribution is captured in the following definition.

\begin{definition}
\label{def:block guessable}
Let $I_w (\cdot, \cdot)$ be an oracle that returns \[I_w(j, w_j')=
\begin{cases}
1 & w_j = w_j'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1| ... |W_\gamma$ is a $(q, \alpha, \beta)$-\emph{unguessable block distribution} if there exists a set $J\subset\{1,..., \gamma\}$ of size at least $\gamma -\beta$ such that for any unbounded adversary $S$ with oracle access to $I_w$ making at most $q$ queries
\[
\forall j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}

We show some examples of unguessable block distributions in \apref{sec:characterize}.  In particular, any source $W$ where for all $j$, $\Hoo(W_j) \geq \omega(\log n)$~(but all blocks may arbitrarily correlated) is an unguessable block distribution~(\clref{cl:all blocks entropy}).

Unfortunately, \consref{cons:first construction} is not a computational fuzzy extractor.  The codewords $c$ are not uniformly distributed and it is possible to learn some bits of $c$~(for the symbols of $W$ without much entropy).  However, we can show that $c$ looks like it has entropy.  We use the notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s}} ((W, S),(X,S))\leq \epsilon$.
\end{definition} 
We now define a weaker object that outputs computational entropy~(instead of a pseudorandom key).  We call this object a computational fuzzy conductor.  It is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\begin{definition}
\label{def:comp fuzzy cond}
A pair of randomized procedures ``generate'' ($\gen$) and ``reproduce'' ($\rep$) is an $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon, s)$-hard with error $\delta$ if $\gen$ and $\rep$ satisfy \defref{def:comp fuzzy extractor}, except the last condition is replaced with the following weaker condition:
\begin{itemize}
\item for any distribution $W\in \mathcal{W}$, the string $r$ has high HILL entropy conditioned on $P$.  That is $H^{\hill}_{\epsilon, s}(R |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}
Computational fuzzy conductors can be converted to computational fuzzy extractors using standard techniques~(see \apref{sec:further defs}).
The following theorem states the construction is a computational fuzzy conductor~(proof in \apref{sec:construction analysis}).  
\begin{theorem}
\label{thm:main thm first cons}
Let $n$ be a security parameter. Let $\mathcal{Z}$ be an alphabet where $|\mathcal{Z}| \ge 2^{ \omega(\log(n))}$.
Let $\mathcal{W}$ be a family of $(q,\alpha= \omega(\log n),  \beta)$-unguessable block distributions over $\mathcal{Z}^\gamma$, for any $q = \poly(n)$.  Furthermore, let $C$ be a $(\neigh_t, \delta_{code})$-code over $\mathcal{Z}^\gamma$.  Let $\mathcal{O}$ be an $\gamma$-composable VGB obfuscator for point functions with auxiliary inputs. Then for any $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \tilde{m}=H_0(C)-\beta, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta_{code} + \gamma/|\mathcal{Z}|$.
\end{theorem}

\subsection{More errors than entropy?}
\label{sec:discussion}
In this section, we show that \consref{cons:first construction} can support distributions with more errors than entropy.
We first calculate the size of the Hamming ball.  
\[
\log |B_t| = \log \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i> \log {\gamma \choose t} (|\mathcal{Z}|-1)^t =\Theta(t\log |\mathcal{Z}|) + \log {\gamma\choose t}
\]
The simplest type of unguessable block distribution is where each block is independent and has super-logarithmic entropy~(\clref{cl:independent high ent}).  For this type of source the required entropy is $\Hoo(W) = \gamma\omega(\log n)$.  This yields:
\[
\text{\# errors} - \text{entropy} = \log |B_t| -  \Hoo(W)  <\left( \Theta(t\log |\mathcal{Z}|) + \log {\gamma \choose t}\right) -  \gamma \omega(\log n) .
\]
When $t =\Theta(\gamma)$ and the entropy of each block is $o(\log |\mathcal{Z}|)$, then the construction supports more errors than entropy. Furthermore, the output entropy is $H_0(C) -\beta$~(if $C$ is a constant rate code, this is $\Theta(\gamma)$).

\paragraph{Improvements}  If most codewords have Hamming weight close to $1/2$, we can decrease the error tolerance needed from the code from $t$ to  about $t/2$, because roughly half of the mismatches between $w$ and $w'$ occur where $c_j =1$. 

If $\gamma$ is not long enough to get a sufficiently long output, the construction can be run multiple times with the same input and independent randomness. 


\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

\section{Definitions}
\subsection{Computational Fuzzy Conductors}
\label{sec:further defs}
A computational extractor is the adaption of a randomness extractor to the computational setting.  Any information-theoretic randomness extractor is also a computational extractor; however, unlike information-theoretic extractors, computational extractors can expand their output arbitrarily via pseudorandom generators once a long-enough output is obtained. We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average case:
\begin{definition}
A function $\cext: \zo^\gamma \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon, s)$-average-case computational extractor} if for all pairs
of random variables $X, Y$ (with $X$ over $\zo^\gamma$) such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s}}((\cext(X; U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon$.
\end{definition}

Combining a computational fuzzy conductor and a computational extractor yields a computational fuzzy extractor:

\begin{lemma}
\label{lem:cond and cext}
Let $(\gen'$, $\rep')$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\gamma$.  Let $\cext:\zo^\gamma\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon_{ext}, s_{ext})$-average case computational extractor.  Define $(\gen, \rep)$ as:
\begin{itemize}
\item $\gen(w; seed)$ (where $seed\in \zo^d$): run $(r', p')= \gen'(w)$ and output $r = \cext(r'; seed)$, $p = (p', seed)$. 
\item $\rep(w', (p', seed)):$ run $r' = \rep'(w'; p')$ and output $r = \cext(r'; seed)$.
\end{itemize}
Then $(\gen, \rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \min\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}

\begin{proof}
It suffices to show if there is some distinguisher $D'$ of size $s'$ where
\[\delta^{D'}((\cext(X; U_d), U_d, P'), (U_\kappa, U_d, P'))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D$ of size $s_{cond}$ such that for all $Y$ with $\Hav(Y|P') \geq \tilde{m}$,
 \[
 \delta^{D}((X,P'), (Y, P'))\geq \epsilon_{cond}.
 \]
Let $D'$ be such a distinguisher.  That is,
\[
\delta^{D'}(\cext(X, U_d)\times U_d \times P', U_\kappa\times U_d\times P')> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D$ as follows.  On input $(y, p')$ sample $seed\leftarrow U_d$, compute $r\leftarrow \cext(y; seed)$ and output $D(r, seed, p')$.  Note that $|D| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D}((X, P'), (Y, P'))&= \delta^{D'}((\cext(X, U_d), U_d, P'), \cext(Y, U_d), U_d, P')\\
&\geq \delta^{D'}((\cext(X, U_d), U_d, P'), (U_\kappa\times U_d \times P')) \\
&- \delta^{D'}((U_\kappa\times U_d \times P'), (\cext(Y, U_d), U_d, P'))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Where the last line follows by noting that $D'$ is of size at most $s_{ext}$.  Thus $D$ distinguishes $X$ from all $Y$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\subsection{Defining Reusability in the Computational Setting}
\label{sec:comparing reusability}

The definition of Boyen considers a single adversary.  We split the adversary into two parts, one of which is information-theoretic and another that is computationally bounded.  The functions $f_2,..., f_q$ can be thought of as a single adversary that sees all prior state.  However, to provide meaningful security in the computational setting, we cannot have communication between these adversaries.\footnote{An alternative would be to require the entire adversary to be computationally bounded.  Our construction satisfies this alternative adaption as well.}  

Because these two adversaries do not communicate we strengthen the definition by allowing the perturbation adversary~($f_i$) to see the original sample $w_1$.  This was not allowed in the definition of Boyen as it would make security impossible.  

Finally, we allow the distinguishing adversary to see other keys.  This is strictly stronger than the definition of Boyen.  However, it is weaker than the notion of insider reusable fuzzy extractors where the adversary sees key generation on arbitrary $p$.  This is a mixture of reusability and robustness~\cite{dkrs2006} of the fuzzy extractor.  Our reusable construction~(\consref{cons:sampling}) is not insider secure.

\subsection{Obfuscation}
\label{sec:obfuscation def}

In this section, we give a formal definition of the required notion of obfuscation.  
We require that the obfuscation is composable and secure with respect to auxiliary input. Composable auxiliary-input VGB obfuscators for point functions and digital lockers are constructed in \cite[Theorem 6.1]{bitansky2010strong} from the Strong Vector Decision Diffie-Hellman assumption, which is a generalization of the strong DDH assumption of \cite{canetti1997towards} for tuples of points. They can also be constructed by assuming strong properties of cryptographic hash functions~\cite{canetti1997towards}.

\begin{definition}[composable obfuscation VGB obfuscation with auxiliary input \cite{bitansky2010strong}]
\label{def:obf} A PPT algorithm $\mathcal{O}$ is an $\ell$-composable VGB obfuscator for $\mathtt{I}_{n}$~(resp. $\mathtt{I}_{n+\kappa}$) with auxiliary-input if the following conditions are met:
\begin{enumerate}
\item \emph{Functionality:} for every $n$ and $I \in \mathtt{I}_n$, $\mathcal{O}(I)$ is a circuit that computes the same function as $I$.
\item \emph{Virtual grey-box:}  For every PPT adversary $A$ and polynomial $p$, there exists a (possibly inefficient) simulator $S$ and a polynomial $q$ such that for all sufficiently large $n$, any  sequence of circuits $I^1,\dots,I^\ell \in \mathtt{I}_n$, (where $\ell=\poly(n)$) and for all auxiliary inputs $z\in \zo^*$:
\[
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^\ell)) = 1] - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | < \frac{1}{p(n)} \enspace,
\]
where $(I^1,\dots,I^\ell)[q(n)]$ is an oracle that answers at most $q(n)$ queries, and where every query of the form $(i,x)$ is answered by $I^i(x)$.
\end{enumerate}
\end{definition}
For notational convenience, when we use point function obfuscation, we denote the oracle provided to the simulator as $I_w(\cdot, \cdot)$ where $w = w_1,..., w_\gamma$ is the vector of obfuscated points.  When we use digital lockers we denote the oracle provided to the simulator as $I_{v, r}(\cdot, \cdot)$ where $v = v_1,..., v_\ell$ is the vector of obfuscated points and $r$ is the hidden value~(we will hide the same value in each obfuscation).

%\consref{cons:sampling} is the fuzzy extractor that allows for arbitrary correlation between $w_1,..., w_q$ as long as each $w_i$ comes from an admissible distribution.  %Our construction can be made secure information-theoretically for fixed values of $q$ or computationally for an arbitrary value $q = \poly(n)$.  Our construction is based on a modification of the computational fuzzy extractor of Canetti et al.~\cite[Construction 5.2]{canetti2014key}.  Their construction is secure for a restricted class of distributions.\footnote{This is necessary as they secure distributions with more errors than entropy.  In order to provide any meaningful security guarantee, the source distribution must be restricted.  See the introduction of~\cite{canetti2014key} for discussion.}  In addition to showing the reusability of their construction, we provide two significant improvements:
\section{Characterizing unguessable block distributions}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and a little unwieldy.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.
The majority of the difficulty in characterizing \defref{def:block guessable} is that different blocks may be dependent, so an equality query on block $i$ may reshape the distribution of block $j$.  In the examples that follow we denote the adversary by $S$ as we consider security against computationally unbounded adversaries defined in VGB obfuscation~(\defref{def:obf}).  We first show some sources that are unguessable block distributions~(\secref{sec:positive ex}) and then show distributions with high overall entropy that are not unguessable block distributions~(\secref{sec:negative ex}).

\subsection{Positive Examples}
\label{sec:positive ex}
We begin with the case of independent blocks.  

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,  ... , W_\gamma$ be a source in which all blocks $W_j$  are mutually independent.  Let $\alpha$ be a parameter.  Let $J\subset \{1,..., \gamma\}$ be a set of indices such that for all $j\in J$, $\Hoo(W_j ) =\alpha $.  Then for any $q$, $W$ is a $(q, \alpha - \log (q+1), \gamma - |J|)$-unguessable block distribution.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then $W$ is a $(q, \omega(\log n), \gamma - |J|)$-unguessable block distribution.
\end{claim}
\begin{proof}
It suffices to show that for all $j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}) = \alpha -\log (q+1)$.
We can ignore queries for all blocks but the $j$th, as the blocks are independent. Furthermore, without loss of generality, we can assume that no duplicate queries are asked, and that the adversary is deterministic ($S$ can calculate the best coins). Let $A_1, A_2, \dots A_q$ be the random variables representing the oracle answers for an  adversary $S$ making $q$  queries about the $i$th block. Each $A_k$ is just a bit, and at most one of them  is equal to 1 (because duplicate queries are disallowed). Thus, the total number of possible responses is $q+1$. Thus, we have the following,
\begin{align*}
\Hav(W_j | View(S^{\mathcal{O}_{W}(\cdot, \cdot)}) &= \Hav(W_j| A_1, \dots, A_q)\\
&=\Hoo(W_j) - |A_1, \dots, A_q|\\
&=\alpha - \log (q+1)\,,
\end{align*}
where the second line follows from the first by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
\end{proof}
\noindent In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} show a construction for block-fixing sources, where each block is either uniform or a fixed symbol~(block fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} shows that \defref{def:block guessable} captures, in particular, this class of distributions.
However, \defref{def:block guessable} captures more distributions.  We now consider more complicated distributions where blocks are not independent.

\begin{claim}
\label{cl:each block from single seed}
Let $f:\zo^e \rightarrow \mathcal{Z}^\gamma$ be a function.  Furthermore, let $f_j$ denote the restriction of $f$'s output to its $j$th coordinate.  If for all $j$, $f_j$ is injective then $W = f(U_e)$ is a $( q, e - \log (q+1), 0)$-unguessable block distribution.
\end{claim}
\begin{proof}
Since $f$ is injective on each block, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) = \Hav(U_e | View(S^{I_{W}(\cdot, \cdot)}))$.  Consider a query $q_k$ on block $j$.  There are two possibilities: either $q_k$ is not in the image of $f_j$,  or $q_k$ can be considered a query on the preimage $f_j^{-1}(q_k)$. Then (by assuming $S$ knows $f$) we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ (like in the
proof of \clref{cl:independent high ent}),
 and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $j$, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) \geq \Hoo(W_j) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so there is a family of distributions with total entropy $\omega(\log n)$ for which \consref{cons:first construction} is secure.  For these distributions, all the coordinates are as dependent as possible: one determines all others.
We can prove a slightly weaker claim when the correlation between the coordinates $W_j$ is arbitrary:

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\gamma$ be a source.  Suppose that for all $j$, $\Hoo(W_j)\geq \alpha$, and that $q \le 2^{\alpha}/4$ (this holds asymptotically, in particular, if $q$ is polynomial and $\alpha$ is super-logarithmic). Then  $W$ is a $(q, \alpha-1-\log(q+1), 0)$-unguessable block distribution.
\end{claim}

\begin{proof}
Intuitively, the claim is true because the oracle is not likely to return 1 on any query. Formally, we proceed by induction on oracle queries,
using the same notation as in the proof of   \clref{cl:independent high ent}. Our inductive hypothesis is
that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.  If the inductive hypothesis holds, then, for each $j$,
\begin{equation}
\label{eq:cond-entropy}
\Hoo(W_j | A_1= \dots= A_{k-1}=0) \ge \alpha-1\,.
\end{equation}
This is true for $k=1$ by the condition of the theorem. It is true for $k>1$ because, as a consequence of the definition of $\Hoo$,
for any random variable $X$ and event $E$, $\Hoo(X|E)\ge \Hoo(X)+\log\Pr[E]$; and $(k-1) 2^{1-\alpha}\leq 2 q 2^{-\alpha} \leq 1/2$.

We now show that $\Pr[A_1\neq 0 \vee \dots \vee A_{k}\neq 0] \leq k 2^{1-\alpha}$, assuming that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.
\begin{align*}
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0 \vee A_k\neq 0] & =
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0]+\Pr[A_1=\dots = A_{k-1}=0 \wedge A_k=1]\\
& \le  (k-1)2^{1-\alpha}+\Pr[A_k=1\,|\,A_1=\dots = A_{k-1}=0]\\
& \le  (k-1)2^{1-\alpha}+\max_j 2^{-\Hoo(W_j | A_1=\dots =A_{k-1}=0)}\\
& \le  (k-1)2^{1-\alpha}+ 2^{1-\alpha}\\
& = k 2^{1-\alpha}
\end{align*}
(where the third line follows by considering that to get $A_k=1$, the adversary needs to guess some $W_j$, and the fourth line follows by~\eqref{eq:cond-entropy}).
Thus, using $k=q+1$ in~\eqref{eq:cond-entropy},
 we know $\Hoo(W_j | A_1= \dots= A_q=0) \ge \alpha-1$.  Finally this means that
\begin{align*}
\Hav(W_j | A_1,\dots, A_q) &\ge -\log \left( 2^{-\Hoo(W_j | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]+1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] \right)\\
& \ge -\log \left(  2^{-\Hoo(W_j | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\end{proof}

\subsection{Negative Examples}
\label{sec:negative ex}
Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some blocks to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$ and let $f_1,..., f_{\gamma}$ be injective functions where $f_j:\zo^{j\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \gamma \times \log p$, that is the source has a small number of blocks.}  Then define the distribution $W_1 = f_1(U_{1,...,\gamma}), W_2 = f_2(U_{1,..., 2\gamma}),...., W_\gamma = f_\gamma(U)$.  There is an adversary making $p\times \gamma = \poly(n)$ queries such that $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
Let $x$ be the true value for $U_{p\times \gamma}$.
We present an adversary $S$ that completely determines $x$.  $S$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $S$ queries on $(1, y_1),..., (1, y_p)$, exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $S$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered~(and thus $w$).  %The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \gamma)$.  Once $x$ has been recovered then $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relies on an adversaries ability to determine a block from the previous blocks.  We formalize this notion next.  We define the entropy jump of a block source as the remaining entropy when other blocks are known:

\begin{definition}
Let $W = W_1,..., W_\gamma$ be a source under ordering $i_1,..., i_\gamma$.  The \emph{jump} of a block $i_j$ is $\mathtt{Jump}(i_j) = \max_{w_{i_1},..., w_{i_{j-1}}} H_0 (W_{i_j} | W_{i_1} = w_{i_1} ,..., W_{i_{j-1}} = w_{i_{j-1}})$.
\end{definition}

If an adversary can learn blocks in succession they can eventually recover the entire secret.  In order for a distribution to be block unguessable the adversary must get ``stuck'' early enough in their recovery process.  This translates to having a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\gamma$ such that for all $j\le \gamma-\beta +1$, $\mathtt{Jump}(i_j) = \log q /(\gamma-\beta+1)$, then $W$ is not $(q, 0, \beta)$-unguessable block distribution.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \gamma$.  We describe an unbounded adversary that determines $W_1,..., W_{\gamma-\beta+1}$.  As before $S$ queries the $q /\gamma$ possible values for $W_1$ and determines $W_1$.  Then $S$ queries the (at most)~$q/(\gamma-\beta+1)$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\gamma-\beta+1}$ is learned.  
\end{proof}

Presenting a sufficient condition for security is more difficult as $S$ may interleave queries to different blocks.  It seems like the optimum strategy is to focus on a single block at a time but it is unclear how to formalize this intuition.

\section{Analysis of \consref{cons:info theoretic}}
\label{sec:info theory sec}
\begin{proof}[Proof of \lemref{lem:info theory sec}]
Let $W\in \mathcal{W}$.
It suffices to argue correctness and security.  We first argue correctness.  
When $w_i = w_i'$, then $\cond(w_i , seed_i) = \cond(w_i', seed_i)$ and thus $v_i = v_i'$.  Thus, for all $w, w'$ where $\dis(w, w')\le t$, then $\dis (v, v')\le t$.  Then by correctness of $(\gen', \rep')$, $\Pr[(r, p)\leftarrow \gen'(v) \wedge r'\leftarrow \rep(v',p) \wedge r' = r]\ge 1-\delta$.  

We now argue security.  Denote by $seed$ the random variable consisting of all $\gamma$ seeds and $V$ the entire string of generated $V_1,..., V_\gamma$.  To show that $R | P, seed \approx_{\gamma \epsilon_{cond} + \epsilon_{text}} U | P, seed$, it suffices to show that $\Hav(V | seed)$ is $\gamma \epsilon_{cond}$ close to a distribution with average min-entropy $\tilde{\alpha}(\gamma - \beta)$.  The lemma then follows by the security of $(\gen', \rep')$.\footnote{Note, again, that $(\gen', \rep')$ must be an average-case fuzzy extractor.  Most known constructions are average-case and we omit this notation.}

We now argue that there exists a distribution $Y$ where $\Hav(Y | seed)\ge \tilde{\alpha}(\gamma - \beta)$ and $(V, seed_1,..., seed_\gamma)\approx (Y, seed_1,.., seed_\gamma)$.  First note since $W$ is $(\alpha, \beta)$-partial block distribution that
there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_\gamma \in W_1,..., W_\gamma, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
Then consider the first element of $j_1\in J$, $\forall w_1,..., w_{j_1-1}\in W_1,..., W_{j_1-1}$, 
\[\Hoo(W_{j_1} | W_1 = w_1,..., W_{j_1-1} = w_{j_1-1})\ge \alpha.\]
 
%\[(\ext (W_1, seed_1), seed_1) \approx_{\epsilon} (U_{\mathcal{Y}}, seed_1)\]
%Then since $W$ is a block-source, 
%\[ \forall w_1\in W_1, \Hoo(W_2 | w_1)\ge k.\]
\noindent 
Thus, there exists a distribution $Y_{j_1}$ with $\Hav(Y_{j_1} | seed_{j_1}) \ge \tilde{\alpha}$ such that 
\[(\cond (W_{j_1}, seed_{j_1}), seed_{j_1}, W_1,..., W_{j_1-}) \approx_{\epsilon_{cond}} (Y_{j_1}, seed_{j_1}, W_1,..., W_{j_1})\]
and since $(seed_1,..., seed_{j_1})$ are independent of these values
\[(\cond (W_{j_1},seed_{j_1}), W_{j_1-1},..., W_1, seed_{j_1}, ..., seed_{1}) \approx_{\epsilon_{cond}} (Y_{j_1}, W_{j_1-1},..., W_1, seed_{j_1}, , ...,  seed_{1})\]
consider the random variable $Z_{j_1} =( Y_{j_1}, \cond(W_{j_1-1},seed_{j_1-1}),..., \cond(W_{1}, seed_{1}))$ and note that \[\Hav(Z_{j_1} | seed_1,...,seed_{j_1})\ge \alpha'.\]
Applying a deterministic function does not increase statistical distance and thus,
\begin{align*}
(\cond (W_{j_1}, seed_{j_1}), \cond(W_{j_1-1}, seed_{j_1-1}),..., \cond(W_1, seed_1), seed_{j_1},..., seed_{1}) \\\approx_{\gamma \epsilon_{cond}} (Z_{j_1}, seed_{j_1},..., seed_1)
\end{align*}

\noindent
By a hybrid argument there exists a distribution $Z$ with $\Hav(Z | seed) \ge \tilde{\alpha}(\gamma -\beta)$ where 
\[
(\cond(W_\gamma, seed_\gamma), ..., \cond(W_1, seed_1), seed_\gamma,..., seed_1) \approx_{\gamma \epsilon_{cond}} (Z, seed_\gamma,...,  seed_1).\]
%By the security of $(\sketch, \rec)$ we know that $\Hav(Z | seed, ss) \ge \tilde{m}$.  %Note that $U_{\mathcal{Y}}^\gamma$ is independent of $seed_1,..., seed_\gamma$ and so $\Hav(U_{\mathcal{Y}}^\gamma, seed_1,..., seed_\gamma, p)\ge \tilde{m}$. 
%That is, 
%\[
%(V, seed_1,..., seed_\gamma, p) \approx_{\epsilon\gamma} (U_{\mathcal{Y}}^\gamma, seed_1,..., seed_\gamma, p).\]  
This completes the proof.
\end{proof}

\section{Analysis of \consref{cons:sampling}}
\label{sec:analysis sampling}
\subsection{Security}
The proof of security for \consref{cons:sampling} uses the definition of block unguessable distributions~(\defref{def:block guessable}).  This definition is adaptive and discussed in \apref{sec:characterize}.  
We show the security of \consref{cons:sampling}:
\begin{itemize}
\item \lemref{lem:sampling works}: Show that sampling is successful with overwhelming probability.
\item \corref{cor:samp sec}: The outputs $V_1,.., V_\ell$ have high individual entropy with good probability.
\item The outputs $V_1,..., V_\ell$ are a block unguessable distribution.  This is made formal in the following corollary:
\begin{corollary}
\label{cor:v are unguessable}
Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works},  and all the other variables be as in \thref{thm:sampling}. Take any $q=\poly(n)$.  For $\alpha'' =\alpha'-1-\log (q+1) =  \omega(\log n)$, with  probability $1-\ell \epsilon_{sam}$ over the choice of $\Lambda=\lambda$, the distribution $V| \Lambda=\lambda$ is a $(q, \alpha'', 0)$-unguessable block distribution.
\end{corollary}
\item \lemref{lem:samp unguess}: An adversary is unlikely to receive any information about the key for a block unguessable distribution.
\end{itemize}

\noindent
We now present the proofs of Lemmas \ref{lem:sampling works} and \ref{lem:samp unguess}.

\begin{proof}[{\large Proof of \lemref{lem:sampling works}}]
Consider some fixed $i$.
Recall that there a set $J$  of size $\gamma - \beta = \Theta(\gamma)$ such that each $w$ and  block $j\in J$, $\Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}, W_{j+1}=w_{j+1},..., W_\gamma = w_\gamma) \geq \alpha$.  Since this is a worst case guarantee, the entropy of $V_i$ can be deduced from the number of symbols in $V_i$ that come from $J$. Namely, Denote by $X= |\{j_{i, 1},..., j_{i, \eta}\}\cap J|$.

\begin{claim}
\label{cl:vi have entropy}
\[
\Hoo(V_i |\Lambda = \lambda ) \geq \alpha X.
\]
\end{claim}
\begin{proof}
Denote by $j_1,..., j_\eta$ the indices selected by the randomness $\lambda_i$.  We begin by noting that $\Hoo(V_i |\Lambda = \lambda ) = -\log \max_{v\in V_i} \Pr[ V_i =v | \Lambda =\lambda] = -\log \max_{w_{j_1}, ..., w_{j_\eta}} \Pr[W_{j_1} = w_{j_1} \wedge \dots \wedge W_{j_\eta} w_{j_\eta}] $.  Then
\begin{align*}
\max_{w_{j_1},..., w_{j_\eta}} \Pr[ W_{j_1}=w_{j_1} \wedge \dots \wedge W_{j_\eta} = w_{j_\eta}]
&= \max_{w_{j_1},..., w_{j_\eta}} \prod_{k=1}^\eta \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le \prod_{k=1}^\eta \max_{w_{j_1},..., w_{j_\eta}} \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le\prod_{k=1}^\eta \max_{w_1,..., w_\gamma} \Pr[W_{j_k} = w_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1} ]\\
\end{align*}
Taking the negative logarithm of both sides we have that
\begin{align*}
\Hoo(V_i | \Lambda = \lambda) &\ge \sum_{k=1}^\eta \min_{w_1,..., w_\gamma} \Hoo(W_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1})\\
&\ge \sum_{j_k\in J} \alpha = \alpha X
\end{align*}
This completes the proof of \clref{cl:vi have entropy}.
\end{proof}


We note that $X$ is distributed according to the hypergeometric distribution,
and that $\expe[X]=\eta(\gamma-\beta)/\gamma$. Using the tail bounds from~\cite{chvatal1979tail,scala2009hypergeometric}, we can conclude that $\Pr[X\le \expe[X]/2]\le e^{-2((\gamma-\beta)/2\gamma)^2 \eta}=O(e^{-\eta})$.

\bnote{this can also be tightened.}
Thus, setting $\alpha'=\frac{\alpha \eta(\gamma-\beta)}{2\gamma}$ and applying \clref{cl:vi have entropy}, we conclude that
 \[
\Pr[\Hoo(V_i ) \geq \alpha'] \geq 1- O(e^{-\eta}).
\]
\end{proof}

\begin{proof}[{\large Proof of \lemref{lem:samp unguess}}]

\lnote{I need to check this}
Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for digital lockers over $\mathcal{Z}$\footnote{In this proof we only consider the case where the sampling has produced a block unguessable distribution.  The negligible portion of the time when this does not happen in included in the security of \thref{thm:sampling}}  .  Let $W$ be a $(q, \alpha'' = \omega(\log n), 0)$-unguessable block distribution.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U, P))\le \epsilon_{sec}$.

Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U, P))> \epsilon_{sec}$.
Let $D$ be such a distinguisher of size at most $s_{sec}$.  That is,
\[
| \expe[D(R, P)] - \expe[D(U, P)] > \epsilon_{sec} = 1/\poly(n).
\]
Define the oracle $I_{v_1, ..., v_\ell, r}(\cdot, \cdot)$ as follows:
\[I_{v_1,..., v_\ell, r}(x, i) =
\begin{cases}
r & v_i = x\\
\perp & \text{otherwise.}
\end{cases}\]  
By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(R, P_1,..., P_\ell)] - \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $R$ and $U$.
\begin{lemma}
\label{lem:sim cannot distinguish samp}
$\Delta(S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|}), S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})) \le \ell 2^{-\alpha''}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two values in $\zo^\kappa$, the statistical distance is at most $\ell 2^{-\alpha''}$.
\begin{lemma}
\label{lem:codewords in I close samp}
Let $r$ be true value encoded in $I$ and let $u\in \zo^\kappa$.  Then,
\[
\Delta( S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(r, 1^{\ell \log |Z|}), S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(u, 1^{\ell \log |Z|})) \le \ell 2^{-\alpha''}.
\]
\end{lemma}
\begin{proof}
Recall that for all $j$, $\Hav(V_j | View(S))\geq \alpha''$.  The only information about the correct value of $r$ is contained in the query responses.  When all responses are $\perp$ the view of $S$ is identical when presented with $r$ or $u$.  We now show that for any value of $r$ all queries return $\perp$ with probability $1-2^{-\alpha''}$.  Suppose not, that is suppose, the probability of at least one nonzero response is $> 2^{-(\alpha'')}$. 

 When there is a response other than $\perp$ for some $j$ this means that there is no remaining min-entropy in $V_j$.  If this occurs with over $2^{-\alpha''}$ probability this violates the block unguessability of $V$~(\defref{def:block guessable}).  By the union bound over the indices $j$ the total probability of a response other than $\perp$ is at most $\ell 2^{-\alpha''}$. Thus, for all $r, u$ the statistical distance is at most $\ell 2^{-\alpha''}$.  This concludes the proof of \lemref{lem:codewords in I close samp}.
\end{proof}
By averaging over all points in $\zo^\kappa$ we conclude that 
\[\Delta(S^{I_{v_1, ..., v_\ell, r}X(\cdot, \cdot)}(R, 1^{\ell \log |Z|}), S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})) < \ell 2^{-\alpha''}.\]  This completes the proof of \lemref{lem:sim cannot distinguish samp}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(R, P_1,..., P_\ell) ]- \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish samp}, we have
\begin{align*}
\delta^{D}((R, P), (U, P))&\leq |\expe [D(R, P_1,..., P_\ell)] - \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})]| \\
&+|\expe[S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(R, 1^{\ell \log |Z|})] - \expe[S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})] |\\
&+|\expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}(U, 1^{\ell \log |Z|})] - \expe [D(U, P_1,..., P_\ell) ]|\\
&\leq \epsilon_{sec}/3+ \ell 2^{-\alpha''}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:samp unguess}.
\end{proof}

%\end{proof}
\subsection{Correctness}
\label{sec:sampling errors}

%\section{Proof of \lemref{lem:sampling errors}}
\bnote{can be tightened to $\omega(n^c \log n)$.}
\begin{proof}[Proof of \lemref{lem:sampling errors}]

Recall that $\dis(w, w')\leq t$ and that the locations of the errors is independent of the selected locations.  Denote by $\mu = -\frac{(c-1)\log n}{2}$.  Since $\eta = \omega(\log n)$, we will assume
$\eta\ge 2\mu$.  We begin by computing the probability that a single $v_i = v_i'$.  
\begin{align*}
\Pr[v_i = v_i'] &= \Pr[w\text{ and }w'\text{ agree on positions }j_{i,1},..., j_{i,\eta}]\\
&\ge \prod_{j=0}^{\eta-1} \left( 1- \frac{t}{\gamma -j }\right) \ge \prod_{j=0}^{\eta-1}\left(1-\frac{\mu(\gamma-\eta)/\eta}{\eta-j}\right)\\
&\ge \prod_{j=0}^{\eta-1} \left( 1- \frac{\mu}{\eta}\left(\frac{\gamma-\eta}{\gamma -j }\right)\right)\ge \prod_{j=0}^{\eta-1}\left(1-\frac{\mu}{\eta}\right)\\
&= \left(1-\frac{\mu}{\eta}\right)^{\eta} =\left( \left(1-\frac{\mu}{\eta}\right)^{\eta/\mu}\right)^\mu\geq \left(\frac{1}{2}\right)^{2\mu}\\
&\ge \left(\frac{1}{2}\right)^{(c-1) \log n}= \frac{1}{n^{c-1}}.
\end{align*}
We then have the probability that all $v_i\neq v_i'$ as:
\begin{align*}
\Pr[\forall i, v_i \neq v_i'] &= \left(1-\Pr[v_i= v_i']\right)^\ell\\
&=\left( 1- \frac{1}{n^{c-1}}\right)^\ell =\left(\left( 1- \frac{1}{n^{c-1}}\right)^{n^{c-1}}\right)^{\ell /n^{c-1}}\\
&\le \left(\frac{1}{e}\right)^{n^c/n^{c-1}} = \frac{1}{e^n}.
\end{align*}
This completes the proof of \lemref{lem:sampling errors}.
\end{proof}


\section{Analysis of \consref{cons:first construction}}
\label{sec:construction analysis}
\subsection{Security}
Security of \consref{cons:first construction} is similar to the security of \consref{cons:sampling}.  However, security is more complicated, the main difficulty is that the definition of block unguessable distributions~(\defref{def:block guessable}) allows for certain weak blocks that can easily be guessed.  This means we must limit our indistinguishable distribution to blocks that are difficult to guess.  Security is proved via the following lemma:

\begin{lemma}
\label{lem:security of cons}
Let all variables be as in \thref{thm:main thm first cons}.  For every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}( C | P ) \geq H_0(C) - \beta$.
\end{lemma}

We give a brief outline of the proof here; the proof follows.

\noindent \textbf{Outline:}  
It is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))\le \ngl(n)$.  Let $J$ be the set of indices that exists according to \defref{def:block guessable}. Define the distribution $C'$ as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $J$.  We first note that $C'$ has sufficient entropy, because $\Hav(C' |P) = \Hav(C' | C_{J^c}) \ge \Hoo(C', C_{J^c}) - H_0(C_{J^c})  = H_0(C) - |J^c|$ (the second step is by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P)) \le \ngl(n)$.
%Define the distribution $X$ as follows:
%\[X_i =
%\begin{cases}
%W_i & C_i = 0\\
%R_i & C_i = 1.
%\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$. Since $P$ is a collection of obfuscated programs, there exists a simulator $S$~(outputting a single bit), such that $\Pr[D(C, P)=1]$ is close to $\Pr[S^{\mathcal{O}}(C)=1]$.
\item Show that even an unbounded $S$ making a polynomial number of queries to the stored points cannot distinguish between $C$ and $C'$.  That is, $\Delta(S^{\mathcal{O}}(C),S^{\mathcal{O}}(C'))$ is small.
\item By the security of obfuscation, $\Pr[S^{\mathcal{O}}(C')=1]$ is close to $\Pr[D(C', P)=1]$.
\end{itemize}
\begin{proof}[Proof of \lemref{lem:security of cons}]
\label{app:security of main cons}

\lnote{I need to check this}
Let $\mathcal{O}$ be a $\gamma$-composable VGB obfuscator with auxiliary input for point programs over $\mathcal{Z}$.  Let $W$ be a $(q, \alpha = \omega(\log n), \beta)$-unguessable block distribution.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P)\geq H_0(C)- \beta$. % for $\epsilon' = 2\epsilon_{obf} + (\gamma - \beta)2^{-(\alpha-1)}$.
Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P) < H_0(C)-\beta$.
By \defref{def:block guessable} there exists a set of indices $J$ such that all blocks within $J$ are unguessable.  Define by $C'$ the distribution of sampling a uniform codeword where all locations outside $J$ are fixed.  Then 
$\Hav(C' | C_{J^c}) \ge \Hoo(C', C_{J^c}) - H_0(C_{J^c})  = H_0(C) - \beta$ (by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}). 

Let $D$ a distinguisher of size at most $s_{sec}$ such that
\[
| \expe[D(C, P)] - \expe[D(C', P)] > \epsilon_{sec} = 1/\poly(n).
\]
Define the distribution $X$ as follows:
\[X_j =
\begin{cases}
W_j & C_j = 0\\
R_j & C_j = 1.
\end{cases}\]  By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(P_1,..., P_\gamma, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $C$ and $C'$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})) \le (\gamma-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $J^c$, the statistical distance is at most $(\gamma-\beta)2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then,
\[
\Delta( S^{I_X(\cdot, \cdot)}(c^*, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(c', 1^{\gamma \log |Z|})) \le ( \gamma -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $j\in J$, $\Hav(W_j | View(S))\geq \alpha$.  The only information about the correct value of $c_j^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $j \in J$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not, that is suppose, the probability of at least one nonzero response on index $j$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_j$, the probability of this happening when $c^*_j = 1$ is at most $q/\mathcal{Z}$ or  equivalently $2^{-\log |\mathcal{Z}|+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }j]\nonumber \\
 &= \Pr[c_j^* =1]\Pr[\text{non zero response location }j\wedge c_j^*=1]\nonumber \\&+ \Pr[c_j^*=0] \Pr[\text{non zero response location }j \wedge c_j^*=0]\nonumber \\
&\le 1\times 2^{-\log|\mathcal{Z}|+\log q} + 1\times  \Pr[\text{non zero response location }j \wedge c_j^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that for an unguessable block source the remaining entropy $\alpha\leq \log |\mathcal{Z}|-\log q $:
\begin{claim}
\label{cl:ent bounded away from n}
If $W$ is a $(q, \alpha, \beta)$-block unguessable distribution over $\mathcal{Z}$ then $\alpha \le \log |\mathcal{Z}|-\log q$.
\end{claim}
\begin{proof}
\bnote{Leo I changed some of this under you.  It wasn't clear or probably right as written.}
Let $W$ be a $(q, \alpha, \beta)$-block unguessable distribution.  Let $J\subset\{1,..., \gamma\}$ the set of good indices.
It suffices to show that there exists an $S$ making $q$ queries such that for some $j\in J, \Hav(W_j | S^{I_{W}(\cdot, \cdot)})\le \log |\mathcal{Z}| - \log q$.  Let $j\in J$ be some arbitrary element of $J$ and denote by $w_{j,1}, ..., w_{j,q}$ the $q$ most likely outcomes of $W_j$~(breaking ties arbitrarily).  Then $\sum_{i=1}^q \Pr[W_j = w_{j,i}]\geq q/|\mathcal{Z}|$.  Suppose not, this means that there is some $w_{j,i}$ with probability $\Pr[W_j = w_{j,i}] < 1/|\mathcal{Z}|$.  Since there are $\mathcal{Z} - q $ remaining possible values of $W_j$ for their total probability to be at least $1-q/|\mathcal{Z}|$ at least of these values has probability at least $1/\mathcal{Z}$.  This contradicts the statement $w_{j,1},..., w_{j,q}$ are the most likely values.  Consider $S$ that queries its oracle on $(j, w_{j,1}),.., (j, w_{j,q})$.  Denote by $Bad$ the random variable when $W_j\in \{w_{j,1},.., w_{j,q}\}$  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_j | S^{J_W(\cdot, \cdot)}) &=  -\log \left(\Pr[Bad=1]\times 1+ \Pr[Bad=0]\times \max_{w}\Pr[W_j = w| Bad =0]\right)\\
&\leq  -\log \left(\Pr[Bad=1]\times 1\right)\\
&=-\log\left( \frac{q}{|\mathcal{Z}|} \right) = \log|\mathcal{Z}|-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }j \wedge c_j=0] &>2^{-\alpha+1} - 2^{-(\log |\mathcal{Z}|-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_j=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the block unguessability of $W$~(\defref{def:block guessable}).  By the union bound over the indices $j\in J$ the total probability of a $1$ in $J$ is at most $(\gamma-\beta)2^{-\alpha+1}$. Recall that $c^*, c'$ match on all indices outside of $J$. Thus, for all $c^*, c'$ the statistical distance is at most $(\gamma- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}.
\end{proof}
By averaging over all points in $C'$ we conclude that $\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})) < (\gamma -\beta)2^{-(\alpha+1)}$.  This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(P_1,..., P_\gamma, C') ]- \expe [S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish}, we have
\begin{align*}
\delta^{D}(( P, C), (P, C'))&\leq |\expe [D(P_1,..., P_\gamma, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})]| \\
&+|\expe[S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})] - \expe[S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] |\\
&+|\expe [S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] - \expe [D(P_1,..., P_\gamma, C') ]|\\
&\leq \epsilon_{sec}/3+ (\gamma-\beta)2^{-(\alpha-1)}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:security of cons}.
\end{proof}

\subsection{Correctness}
We now argue correctness of \consref{cons:first construction}.
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
Let all variables be as in \thref{thm:main thm first cons}.
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \le \gamma/|\mathcal{Z}| = \ngl(n)$.
\end{lemma}
\begin{proof}
Consider a coordinate $j$ for which $c_j=1$. Since $w'$ is chosen independently of the points $r_j$, and $r_j$ is uniform, $\Pr[r_j =w_j']  = 1/|\mathcal{Z}|$. The lemma follows by the union bound, since there are at most $\gamma$ such coordinates.
\end{proof}

Since there are most $t$ locations for which $w_j\neq w_j'$ there are at most $t$ $0\rightarrow 1$ bit flips in $c$, which the code will correct with probability $1-\delta_{code}$, because $c$ was chosen uniformly.
Therefore, \consref{cons:first construction} is correct with error at most $\gamma/|\mathcal{Z}|$.





\end{document} 