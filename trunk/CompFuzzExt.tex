\documentclass[11pt]{article}
\def\shownotes{1}

\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode} 

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\textbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\D}{\mathcal{D}}

\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\xnote}[1]{{\authnote{Xianrui}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}

\title{\textbf{Computational Fuzzy Extractors}}
\author{Benjamin Fuller%\footnote{The work of Benjamin Fuller is sponsored by the United States Air Force under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.}
 \and Xianrui Meng \and Leonid Reyzin}
\begin{document}
\maketitle

\begin{abstract} 
\lnote{This is an old abstract, no longer correct}
One-message information reconciliation protocols called secure sketches remove noise
from secrets without revealing too much about them.  They are a crucial ingredient in
fuzzy extractors, which provide an information-theoretic mechanism for reliable key derivation from noisy sources.

Secure sketches necessarily reveal some information about the noisy secret, and thus the derived key in fuzzy extractors has lower entropy than the source.  The minimal entropy loss is dictated by bounds on error-correcting codes.

We overcome these bounds by considering computational, rather than information-theoretic, secure sketches and fuzzy extractors, and building them based on the Learning with Errors (LWE) problem.  We thus obtain the first secure sketch with no loss in entropy.   More specifically, our contributions are as follows:

\begin{itemize}
\item \textbf{New Definitions:} In seeking the right computational relaxation of the information-theoretic definition of a secure sketch, we rule out most natural one, which uses HILL pseudoentropy.  We do so by demonstrating that a secure sketch defined via HILL pseudoentropy is subject to the same coding-based  bounds as the information-theoretic secure sketch.  We then find a different relaxation, using unpredictability entropy, and show that it can be used to construct computational fuzzy extractors.


\item \textbf{New Construction:} We build a lossless secure sketch for uniform sources and certain other sources of high entropy (namely, symbol-fixing sources, in which each dimension is either uniform or fixed),  based on the hardness of the LWE problem.  This construction requires us to show that the decision version of LWE is secure when a small number of dimensions have no error---a result that may be of independent interest.
\end{itemize}
\end{abstract}

\newcommand{\M}{\mathcal{M}}
\section{Introduction}\label{sec:introduction}

Authentication generally require a secret drawn from some high-entropy source.  One of the primary building blocks for authentication is reliable key derivation.  Unfortunately, many sources that contain sufficient entropy to derive a key are  noisy, and provide similar, but not identical secret values at each reading (examples of such sources include biometrics~\cite{daugman2004}, human memory~\cite{zviran1993comparison}, pictorial passwords~\cite{brostoff2000passfaces}, measurements of capacitance~\cite{tuyls2006puf}, timing~\cite{suh2007physical}, motion~\cite{castelluccia2005shake},  quantum information~\cite{bennett1988privacy} etc.).  %Information reconciliation protocols~\cite{bennett1988privacy} remove the noise without revealing the secrets.

Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} achieve reliable key derivation from noisy sources.  (\bnote{Are these references for fuzzy extractors or secure sketches?} as well as in other contexts, e.g.,~\cite{Boyen05secureremote,dodisWichs2009,chandran2010privacy}).  The setting 
%We will specifically focus on a one-round information reconciliation  mechanism called ``secure sketch,''  which, aside from its immediate application to authentication, is also
%a crucial ingredient in the construction of fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} (as well as in other contexts, e.g.,~\cite{Boyen05secureremote,dodisWichs2009,chandran2010privacy}).
consists of  two algorithms: Generate (used once) and Reproduce (used subsequently).  The Generate ($\gen$) algorithm takes an input $w$ and produces a key $r$ and a public value $p$.  This information allows
the Reproduce ($\rep$) algorithm to reproduce $r$ given $p$ and some value $w'$ that is close to $w$ (according to some predefined metric, such as Hamming distance). 
Crucially for security,  knowledge of $p$ should not reveal $r$; that is, $r$ should be uniformly distributed conditioned on $p$.  This feature is needed because $p$ is not secret: for example, in a single-user setting (where the user wants to reproduce the key $r$ from a subsequent reading $w'$), it would be stored in the clear; and in a key agreement application~\cite{Boyen05secureremote} (where two parties have $w$ and $w'$, respectively), it would be transmitted between the parties.

%As defined in \cite{DBLP:journals/siamcomp/DodisORS08}, 
The entropy loss of a fuzzy extractor is the difference between the entropy of $w$ and the length of the derived key $r$.  A goal of fuzzy extractor constructions is to minimize the entropy loss, increasing the security of the resulting application.  The resulting secret key may be too short to be useful if the entropy loss is too high.  Traditionally, fuzzy extractors are defined as information-theoretic objects.  In the information-theoretic setting, some entropy loss is necessary as the value $p$ contains enough information to reproduce $r$ from any close value $w'$.  

The main novel tool in constructing fuzzy extractors is the secure sketch.  The secure sketch produces a public value $s$ that allows recovery of $w$ from any close value $w'$.  The security requirement is that $w$ has high min-entropy conditioned on $s$.  The key $r$ is formed by applying a randomness extractor~\cite{nisan1993randomness} to $w$.
%Secret entropy of convenient sources is often at a premium; a goal of secure sketch constructions is to minimize the entropy loss, increasing the security of the resulting application. When used in a fuzzy extractor, the resulting secret key may be too short to be useful if the entropy loss is too high. 
The minimum entropy loss for secure sketches has been quantified in past work.  Even if $\rep$ is allowed to produce an incorrect value with small probability~\cite[Section 8]{DBLP:journals/siamcomp/DodisORS08}, 
%Even if we don't require perfect correctness (i.e., allow $\rep$ to err with small probability~\cite[Section 8]{DBLP:journals/siamcomp/DodisORS08}), 
the entropy loss of a secure sketch that can reconcile $t$ errors
 %with high probability 
 is bounded by the redundancy of
the best code that can correct $t$ random errors with high probability~\cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}.  Constructions of fuzzy extractors that use a secure sketch inherit this bound.
%(the argument is the same as in \cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08}\lnote{probably we should double-check that this is correct}).

\paragraph {Our Contributions}
To see if entropy loss in fuzzy extractors can be improved, we explore fuzzy extractors and secure sketches with a secrecy requirement that is computational rather than information-theoretic.  Our goal is to decrease the entropy loss in a fuzzy extractor by allowing the key $r$ to be pseudorandom conditioned on $p$.

On the negative side, we show that the paradigm of secure sketch and extraction from~\cite{DBLP:journals/siamcomp/DodisORS08} is unlikely to be fruitful.  We show that some entropy loss is necessary for secure sketches.
The most natural relaxation of the secrecy requirement is to require HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99}~(namely, that the distribution of $w$ conditioned on $s$ be \emph{indistinguishable} from a high-min-entropy distribution).  Under this definition, using the paradigm for constructing fuzzy extractors from~\cite{DBLP:journals/siamcomp/DodisORS08} would yield a pseudorandom key.  We prove in Theorem~\ref{thm:impSketchArbitraryW} that the entropy loss of such secure sketches is subject to the same coding bounds as the ones that constrain information-theoretic secure sketches.  
%Furthermore, we have (asymptotic)~constructions that meet these bounds.

Another possible relaxation is to require that the value $w$ is unpredictable conditioned on $s$.  Using an extractor with a reconstruction would yield a pseudorandom key, see~\cite[Lemma 6]{DBLP:conf/eurocrypt/HsiaoLR07}.  We show lower bounds on the unpredictability loss of sketches in the Hamming metric.  We prove in \thref{thm:imp of unp entropy} that the unpredictability loss must be at least the volume of the ball of radius $t$.  

Both of these impossibility results arise because a secure sketch functions like a decoder of an error-correcting code.  To avoid these impossibility results, we give up on building secure sketches and focus directly on the entropy loss in fuzzy extractors.  

%to focus on producing a consistent string $\recout$  each time $\rec(w', s)$ is called.  This string $\recout$ is  required to have high HILL entropy even conditioned on $s$. Because this object \emph{conducts} the entropy from $w$ to $\recout$ and tolerates noise in its input, following \cite{CRVW02,KanukurthiR09}, we call it a \emph{computational fuzzy conductor}.
By considering a computational secrecy requirement, we are able to construct the first \emph{lossless} (computational) fuzzy extractors, where the derived key $r$ is as long as the entropy of the source $w$\footnote{A trivial construction is possible for very high entropy sources.  An information-theoretic fuzzy extractor can be used to derive a key $r$ and $r$ can be expanded using a pseudorandom generator.  However, when the source is low entropy, the entropy loss due to the information theoretic fuzzy extractor may make it impossible to securely run a pseudorandom generator.  We present a comparison between the two methods in \secref{sec:prg based comparison}}. % without conditioning on anything---i.e., before $\gen$ is run (see~\thref{thm:lossless secure conductor log})).  
Our construction of lossless fuzzy extractors~(\consref{cons:informal construction}) is based on the Learning with Errors~(LWE) assumption due to Regev~\cite{regev2005LWE, regevLWEsurvey},  which says that decoding a random linear code is computationally difficult.  
%We show that this relaxation of the security requirement is  useful.  Specifically, we also define the notion of computational fuzzy extractors, in which the extracted key is required to be pseudorandom---i.e., to look random to any computationally bounded observer.
%We show that our relaxed notion of secure sketches is sufficient to construct computational  fuzzy extractors via a simple and efficient construction, utilizing reconstructive extractors~\cite{barak-computational}.  The resulting fuzzy extractor produces a key that is as long as the entropy of $w$, minus only the loss necessary in any extractor construction.
%\lnote{it would be good to find another application} 
Our lossless fuzzy extractor is built via the code-offset construction~\cite{JW99},\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} used in prior work.  To apply the LWE assumption, we correct errors of a random linear code which limits our decoding algorithm to reconciling a logarithmic number of differences.  

The recent result of D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} shows the hardness of LWE when the error vector comes from the uniform distribution, with each coordinate ranging over a small interval.  This allows us to directly use our source $w$ as the LWE error vector.  To utilize the result of~\cite{dottling2012}, we need to assume that the input comes from the uniform distribution. We are able to relax this limitation and allow $w$ to come from a symbol-fixing source~\cite{KZ07} (each dimension is either uniform or fixed). This relaxation requires new results about the hardness of LWE when samples have a fixed~(and adversarially known) error vector, which may be of independent interest~(\thref{thm:blockLWE}).

%Authentication generally requires secrets.  Unfortunately, many useful  sources that contain sufficient entropy for a secret  are also noisy, and provide similar, but not identical secret values at each invocation (examples of such sources include biometrics~\cite{daugman2004}, human memory~\cite{zviran1993comparison}, pictorial passwords~\cite{brostoff2000passfaces}, measurements of capacitance~\cite{tuyls2006puf}, timing~\cite{suh2007physical}, motion~\cite{castelluccia2005shake},  quantum information~\cite{bennett1988privacy} etc.).  Information reconciliation protocols~\cite{bennett1988privacy} remove the noise without revealing the secrets.
%
%We will specifically focus on a one-round information reconciliation  mechanism called ``secure sketch,''  which, aside from its immediate application to authentication, is also
%a crucial ingredient in the construction of fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} (as well as in other contexts, e.g.,~\cite{Boyen05secureremote,dodisWichs2009,chandran2010privacy}).
%It consists of  two algorithms: Sketch (used once) and Recover (used subsequently).  The Sketch ($\sketch$) algorithm takes an input $w$ and produces a sketch $s$.  This information allows
%the Recover ($\rec$) algorithm to recover $w$ given $s$ and some value $w'$ that is close (according to some predefined metric, such as Hamming distance) to $w$. 
%Crucially for security,  knowledge of $s$ should not reveal $w$; that is, $w$ should still have entropy  conditioned on $s$.  This feature is needed because $s$ is not secret: for example, in a single-user setting (where the user wants to recover the original reading $w$ from a subsequent reading $w'$), it would be stored in the clear; and in a key agreement application~\cite{Boyen05secureremote} (where two parties have $w$ and $w'$, respectively), it would be transmitted between the parties.
%As defined in \cite{DBLP:journals/siamcomp/DodisORS08}, the entropy loss of a secure sketch is the difference in the entropy of $w$ and the entropy of $w$ conditioned on $s$.  
%
%Secret entropy of convenient sources is often at a premium; a goal of secure sketch constructions is to minimize the entropy loss, increasing the security of the resulting application. For example, in the application of secure sketches to fuzzy extractors (whose goal is to reliably produce a uniformly random secret key from a noisy input), the resulting secret key may be too short to be useful if the entropy loss is too high. However, because secure sketches are defined as information-theoretic objects, some entropy loss is inherent in any secure sketch construction, because $s$ contains enough information to recover $w$ from any value $w'$ that is close to it.  
%
%The minimum necessary  entropy loss has been quantified in past work.  Even if we don't require perfect correctness (i.e., allow $\rec$ to err with small probability~\cite[Section 8]{DBLP:journals/siamcomp/DodisORS08}), the entropy loss of a secure sketch that can reconcile $t$ errors with high probability is bounded by the redundancy of
%the best code that can correct $t$ random errors with high probability~\cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}
%%(the argument is the same as in \cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08}\lnote{probably we should double-check that this is correct}).
%
%\paragraph {Our Contributions}
%To see if entropy loss can be improved, we explore secure sketches with the relaxed secrecy requirement that is computational rather than information-theoretic.  
%The natural relaxation of the secrecy requirement is to require HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99}~(namely, that the distribution of $w$ conditioned on $s$ be \emph{indistinguishable} from a high-min-entropy distribution).  Under this definition, using the paradigm for constructing fuzzy extractors from~\cite{DBLP:journals/siamcomp/DodisORS08} would yield a pseudorandom key.  
%
%On the negative side, we show defining sketches using HILL entropy is unlikely to be fruitful. We prove in Theorem~\ref{thm:impSketchArbitraryW} that the entropy loss of such secure sketches is subject to the same coding bounds as the ones that constrain information-theoretic secure sketches.
%%Furthermore, we have (asymptotic)~constructions that meet these bounds.
%
%On the positive side, we show that a different relaxation of the definition can in fact lead to a \emph{lossless} construction. The idea is to give up on reconstructing $w$ and, instead, to focus on producing a consistent string $\recout$  each time $\rec(w', s)$ is called.  This string $\recout$ is  required to have high HILL entropy even conditioned on $s$. Because this object \emph{conducts} the entropy from $w$ to $\recout$ and tolerates noise in its input, following \cite{CRVW02,KanukurthiR09}, we call it a \emph{computational fuzzy conductor}.
%
%Given this relaxation, we are able to build \emph{lossless} computational fuzzy conductors, in which the HILL entropy of $\recout$ conditioned on $s$ is the same as the entropy of $w$ without conditioning on anything---i.e., before $\sketch$ is run (see Theorem~%s~%\ref{thm:lossless secure sketch} and
%~\ref{thm:lossless secure conductor log})).  We demonstrate how to build such lossless conductors based on the Learning with Errors~(LWE) assumption due to Regev~\cite{regev2005LWE, regevLWEsurvey},  which says that decoding a random linear code is computationally difficult. 
%
%We show that this relaxation of the security requirement is  useful.  Specifically, we also define the notion of computational fuzzy extractors, in which the extracted key is required to be pseudorandom---i.e., to look random to any computationally bounded observer.
%We show that our relaxed notion of secure sketches is sufficient to construct computational  fuzzy extractors via a simple and efficient construction, utilizing reconstructive extractors~\cite{barak-computational}.  The resulting fuzzy extractor produces a key that is as long as the entropy of $w$, minus only the loss necessary in any extractor construction.
%\lnote{it would be good to find another application} 
%
%Both our lossless fuzzy conductor and the resulting fuzzy extractor are built via the code-offset construction~\cite{JW99},\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} already used in prior work.  To be able to apply the LWE assumption, we use a random linear code (unfortunately, our decoding algorithm can only reconcile a logarithmic number of differences).  We utilize the recent result of D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} that shows security of LWE when the error vector comes from the uniform distribution, with each coordinate ranging over a small interval.  The LWE error vector, in our case, is the input $w$ itself.  To utilize the result of~\cite{dottling2012}, we need to assume that the input comes from the uniform distribution. We are able to relax this limitation somewhat and allow $w$ to come from a symbol-fixing source~\cite{KZ07} (where each dimension is either uniform or fixed). This relaxation requires new results about the hardness of LWE when samples have a fixed error vector, which may be of independent interest~(\thref{thm:blockLWE}).
\section{Preliminaries}
\label{sec:preliminaries}
For a random variable $X = X_1||...|| X_n$ where each $X_i$ is over some alphabet $Z$, we denote by $X_{1,..., k} = X_1||...|| X_k$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
%the {\em (worst-case) conditional min-entropy} of $X$ given $Y$ is  $\Hoo(X|Y) = -\log(\max_{x,y} \Pr[X=x|Y=y])$, and
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.  
%The {\em collision probability} of  $X$ is 
%$\col(X) = \sum_{x} \Pr[X=x]^2$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. %We write $X \approx_{\epsilon} Y$ if $\Delta(X,Y) \leq \epsilon$, and when $\epsilon$ is negligible (in the appropriate parameter, as clear from the context) then we say $X$ and $Y$ are \emph{statistically close}.  
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.
For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t(\cdot)|$ the size of a ball of radius $t$.  For the Hamming metric over $Z^n$ the size of $B_t(\cdot) = \sum_{i=0}^t {n \choose t} (|Z|-1)^i $, for clarity we express this value as $B_t(\cdot)$.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.
During the presentation, we denote bold letters to be vectors or matrix, capitalized letters to be random variables, and lower case letter to be the elements in the vector. 

\subsection{Fuzzy Extractors and Secure Sketches}
\label{sec:fuzzy extractors}

We now recall definitions and lemmas from the work of Dodis et. al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}, adapted to allow for a small probability of error, as discussed in \cite[Sections 8]{DBLP:journals/siamcomp/DodisORS08}.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.% \lnote{$d$ is overloaded -- it's also a seed length for an extractor and also a constant in the parameter setting.  can we change this $d$ to $\mathsf{dis}$?}	


\begin{definition}%\protect{\cite[Definition 5]{DBLP:journals/siamcomp/DodisORS08}}
\label{def:fuzzy extractor}
An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$, with the following properties:
\begin{enumerate}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w,w')\leq t$, if $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$ with probability at least $1-\delta$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
\end{enumerate}
\end{definition}

Secure sketches are the main technical tool in the construction of fuzzy extractors.  Secure sketches produce a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
\begin{definition}%\protect{\cite[Definition 3]{DBLP:journals/siamcomp/DodisORS08}}
\label{def:secure sketch}
An $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$, with the following properties:
\begin{enumerate}
\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|\sketch(W))\geq \tilde{m}$.
\end{enumerate}
A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time. 
\end{definition}

Note that in the above definition of secure sketches (resp., fuzzy extractors), the errors are chosen before $s$ (resp., $P$) is known: if the error pattern between $w$ and $w'$ depends the output of $\sketch$ (resp., $\gen$), then there is no guarantee about the probability of correctness.


A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}. An average-case extractor is simply a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}):

\begin{definition}
Let $\chi_1$, $\chi_2$ be finite sets.
A function $\ext: \chi_1\times \{0,1\}^d \rightarrow \{0,1\}^m$ a \emph{$(k, \epsilon)$-average-case extractor} if for all pairs
of random variables $X, Y$ over $\chi_1, \chi_2$ such that
$\tilde{H}_\infty(X|Y) \ge k$, we have $\Delta((\ext(X, U_d), U_d, Y), U_m\times
U_d \times Y) \le \epsilon$.
\end{definition}


%In \secref{sec:defCompFuzzyExtractors} we will use a slightly weaker notion, in which \rec is allowed to fail with small probability.\lnote{Can we add a comment that this corresponds to errors that can depend on the input but not the sketch? It's not obvious from the way this is written.} \lnote{can we just have a single definition --- the one below --- instead of two?}
%\begin{definition}
%We say that pair of procedures $(\sketch, \rec)$ is $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$ is an \emph{approximately correct secure sketch} if \defref{def:secure sketch} holds with correctness replaced with the following property, $\forall w, w'$ such that $\dis(w, w')\leq t$:
%\[
%\Pr[\rec(w', \sketch(w)) = w]\geq 1-\epsilon.
%\] where the probability is taken over the coins of \sketch and \rec.
%\end{definition}

\begin{lemma}%\protect{\cite[Lemma 4.1]{DBLP:journals/siamcomp/DodisORS08}}
\label{lem:fuzzy ext construction}
Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext$ be an average-case $(n, \tilde{m}, \ell, \epsilon)$-strong extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor with error $\delta$:
\begin{itemize}
\item $\gen(w):$ set $P=(\sketch(w), x), R=\ext(w;x)$, and output $(R,P)$.
\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $R=\ext(w;x)$.
\end{itemize}
\end{lemma}
The main parameter we will be concerned with is the entropy loss of the construction.  In this paper, we ask whether a smaller entropy loss can be achieved by considering a fuzzy extractor with a computational security requirement.  We relax the security requirement of \defref{def:fuzzy extractor} to require a pseudorandom output instead of a truly random output.

\begin{definition}[Computational Fuzzy Extractor]\label{def:comp fuzzy extractor}
A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, m, \ell, t)$-\emph{computational fuzzy extractor} is that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.  %%Furthermore \rep is computable by a circuit of size at most $s_{rec}$.  
If $d(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\ell, P))\leq \epsilon$.
\end{itemize}
\end{definition}
Any efficient fuzzy extractor is also a computational fuzzy extractor with the same parameters.% ($s_{rec}$ is the size of the circuit that computes \rec and $s_{sec}$ is unbounded).  

It seems natural to construct a computational fuzzy extractor using a sketch that retains computational entropy and then apply a randomness extractor~(extractors convert distributions with high computational entropy to pseudorandom).  In the next section, we show this paradigm is difficult to realize. 

%\textbf{Note: }A computational fuzzy extractor could also be constructed by making the extractor computational~(using a standard extractor and then running a pseudorandom generator on its output).  We compare the two approaches in \secref{sec:prg based comparison} after presenting our construction.
%, specifically, the entropy loss in the definition of the secure sketch~($m-\tilde{m}$).  %The cause for the entropy loss in the secure sketch is the additional bits added for error correction.  In the definition of a fuzzy extractor this is the input min-entropy less the size of the output: $m-\ell$.   
%The entropy loss in the fuzzy extractor is due to the use of an extractor (where the loss is inversely proportional to the statistical distance of the output distribution from uniform) and the use of the secure sketch.


%=============================================================
\section{Impossibility of Computational Secure Sketches}
\label{sec:defCompFuzzyExtractors}
In this section, we consider whether it is possible in build a secure sketch that retains significantly more computational entropy than information theoretic entropy.  %We will not provide formal definitions for these sketches but use the notation HILL/unpredictability~$(\mathcal{M}, m, \tilde{m}, t)$-secure sketch to denote a sketch that retains $\tilde{m}$ bits of HILL~(resp. unpredictability) entropy when given a source with $m$ bits of entropy.  
In \secref{sec:imp HILL sketch} we show that a sketch that retains HILL entropy implies a sketch that retains nearly the same amount of min-entropy.  In \secref{sec:imp unp sketch}, we show that the unpredictability of a sketch must decrease with the size of a ball in the metric space \lnote{not the most clear sentence}.  
%These results are strongest for high entropy sources.  The results get weaker as we consider sources with lower starting entropy.  If a source contains points from a good error code, then the sketch procedure does not need to output any public value so no drop occurs in entropy.  

\subsection{Bounds on Secure Sketches using HILL entropy}
\label{sec:imp HILL sketch}
We recall a commonly used computational notion of entropy \cite{DBLP:journals/siamcomp/HastadILL99}, as extended to the conditional case by Hsiao, Lu, Reyzin~\cite{DBLP:conf/eurocrypt/HsiaoLR07}.

\begin{definition}
Let $(X, Z)$ be a pair of random variables.  $X$ has 
\emph{conditional HILL entropy} at least $k$ conditioned on $Z$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(X|Z)\geq k$ if there exists a collection of
distributions $Y_z$ for each $z\in Z$, giving rise to a joint distribution
$(Y, Z)$, such that $\tilde{H}_\infty(Y|Z)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((X, Z),(Y,Z))\leq \epsilon$.
\end{definition}

\lnote{is there any reason we can't work for the more relaxed notion of HILL, where the condition can be changed, too?}Intuitively, conditional HILL entropy is as good as average min-entropy for all computationally-bounded observers.  Thus, redefining secure sketches using HILL entropy is a  natural relaxation of the original information-theoretic definition; in particular, the construction in \lemref{lem:fuzzy ext construction} would yield pseudorandom outputs if the secure sketch ensured high HILL entropy.  However, as we show in Section~\ref{sec:imp HILL sketch}, such secure sketches cannot  perform much better than the information-theoretic ones. \lnote{this paragraph needs rephrasing and better gluing with the next one}

%Instead, in \secref{sec:unp secure sketch}, we relax the definition by allowing by outputting a new random variable that can be recovered when $\dis (w, w')\leq t$.  This object is the computational analogue of a fuzzy conductor~\cite{KanukurthiR09}.%This is a noisy computational conductor define computational secure sketches using unpredictability, and work with that definition for the rest of the paper.  
%  In \secref{sec:comp fuzzy extractors}, we show that computational fuzzy extractors can be constructed from  computational fuzzy conductors.
%The size of the circuits becomes important, we would like the \rec procedure to be computable by a circuit significantly smaller than the \hill entropy circuit size.  We simply include both of these parameters in the definition.  We review other definitions of computational entropy in Appendix~\ref{sec:notionsEntropy}.


Suppose we redefine secure sketches using HILL entropy: that is, we say that $(\sketch, \rec)$ is a  \emph{HILL-entropy~$(\mathcal{M}, m, \tilde{m}, t)$ secure sketch} that is $(\epsilon,s_{sec})$-hard with error $\delta$ if it satisfies \defref{def:secure sketch}, with the security requirement replaced by $H^{\hill}_{\epsilon, s_{sec}}(W|\sketch(W))\geq \tilde{m}$. 
Unfortunately, we will show below that such a secure sketch implies an error correcting code with approximately $2^{\tilde{m}}$ points that can correct $t$ random errors (see  \cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08} for a similar bound on information-theoretic secure sketches). For the Hamming metric, our result essentially matches the bound on information-theoretic secure sketches of \cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}.  In fact, we show that, for the Hamming metric, HILL-entropy secure sketches imply information-theoretic ones with similar parameters, and, therefore, the HILL relaxation gives no advantage. 


The intuition for building error-correcting codes from HILL-entropy secure sketches is as follows.  In order to have  $H^{\hill}_{\epsilon, s_{sec}}(W|\sketch(W))\ge \tilde{m}$, there must be a distribution $Y$ such that $\Hav(Y | \sketch(W))\geq \tilde{m}$ and $(Y, \sketch(W))$ is computationally indistinguishable from $(W, \sketch(W))$.  Sample a sketch $s\leftarrow \sketch(W)$. We know that $\sketch$ followed by $\rec$ likely succeeds on $W|s$  (i.e., $\rec (w', s) = w$ with high probability for $w\leftarrow W|s$ and $w'\leftarrow B_t(w)$).  So, by indistinguishability, it must also succeed on $Y$: specifically, $\rec (y',s) = y$ with high probability, for $y\leftarrow Y|s$ and $y'\leftarrow B_t(y)$).
 This means we can construct a large set $C$ from the support of $Y|s$.  $C$ will be an error correcting code and $\rec$ an efficient decoder.  We can then use standard arguments to turn this code into an information theoretic sketch.  


To make this intuition precise, we need an additional technical condition:  sampling a random neighbor of a point is efficient.
\begin{definition}
We say a metric space $(\mathcal{M}, \dis)$ is $(s_{sam}, t)$-\emph{neighborhood efficently-samplable} if there exists a randomized circuit $\sample$ of size $s_{sam}$ that for all $t'\leq t$, $\sample (x, t')$ outputs a random point at distance $t'$ of $x$.  
%That is $\exists \sample$ of size at most $s$ such that $\forall x\in \mathcal{M},$
%\[
%\Delta(\sample(x, t') , \{x'| \dis(x,x') = t'\})=0
%\]
\end{definition}
We require that \sample returns a point different from $x$ to ensure that each $B_t(x)$ contains at least one other point.  While we could consider metric spaces where balls only contain their center, secure sketches are not interesting in this context. \lnote{ do we need this? if distance is exactly $t'$, then the point is different as long as $t'>0$.  So why not just say "exactly $t'$ in the definition?}

%We will concentrate on metrics where the size of each ball is fixed.  We say a metric space that obeys this property is regular~(using terminology from graph theory).  This will allow us to show that the neighbor of a uniformly chosen point is also uniformly random.  
%\begin{definition}
%A metric space $(\mathcal{M},d)$ is \emph{$(t, c)-$distance regular} if for all $x\in \mathcal{M}, |B_t(x)| = c$. 
%\end{definition}
%For a distribution $X$ we define the distribution $N_t(X)$ as the process of sampling a random neighbor of a random point in $X$.  That is $\Pr[N_t(X) = z] = \sum_{x\in X}\Pr[X=x \wedge y \leftarrow B_t(x) \wedge z=y]$.
%\begin{lemma}\label{lem:uniformNeighbor}
%Let $(\mathcal{M}, d)$ be a $(t, c)-$distance regular and let $U$ be uniform over $\mathcal{M}$.  Then $U \overset{d}= N_t(U)$.
%
%\end{lemma}
%\begin{proof}
%Note that $\forall z, \Pr[U = z] =1/|\mathcal{M}|$.  Thus, it suffices to show that $\forall z , \Pr[N_t(U) =z] =1/|\mathcal{M}|$.
%
%Fix an arbitrary $z\in \mathcal{M}$.  Then, 
%\begin{align*}
%\sum_{x\in U} \Pr[U=x \wedge y\leftarrow B_t(x) \wedge y=z] &=
%\sum_{x\in U} \Pr[U = x \wedge x\in B_t(z)] \Pr[y\leftarrow B_t(x) \wedge y=z | x\in B_t(z)]\\
%&=\frac{c}{|\mathcal{M}|} \frac{1}{c} = \frac{1}{\mathcal{M}}.
%\end{align*}
%%The uniform distribution over $\mathcal{M}$ is a random bit string of length $n$.  Let $x\leftarrow \mathcal{M}$, then $\sample (x) = z = x\oplus y$ where $y$ has at weight at least $1$ and at most $t$.  Since $x$ was uniformly random $x\oplus y$ is uniformly random.
%\end{proof}

\ignore{
Finally, we will 
dealing with an arbitrary secure sketch that only needs to satisfy the correctness property of a secure sketch~(see \defref{def:secure sketch}).  \lnote{why do we need this definition?}

\begin{definition}
A pair of functions $(\sketch, \rec) $ is a $(s_{rec}, t)$-\emph{recover functionality for a metric space $\mathcal{M}, d$} if $\sketch : \mathcal{M}\rightarrow \{0,1 \}^*$ is a randomized function and $\rec: \mathcal{M}\times \{0, 1\}^*\rightarrow \mathcal{M}$ is a function computable by a circuit of size at most $s_{rec}$, such that $\rec (w', \sketch (w)) = w$ if $\dis(w, w')\leq t$ for all $w, w' \in \mathcal{M}$.
\end{definition}
}

\ignore{For any recover functionality, there is an inherent tradeoff between the ability to keep points stationary and to perform error correction.

\begin{theorem}\label{thm:compSketchImpUniform}
Let $\mathcal{M}, d$ be a metric space that is $(s_{neigh}, t)$-neighborhood samplable and $(t, c)$-distance regular.
Furthermore, let $(\sketch, \rec)$ be any $(s_{rec}, t)$-recover functionality for $\mathcal{M}, d$.  Then $H^\hill_{\epsilon, s}( U |\sketch(U))\not \geq \Hoo(U)$ for $s \approx (s_{neigh} + 2s_{rec})$ and $\epsilon=1/2$~(if $s_{neigh}, s_{rec}$ are polynomial so is $s$).
\end{theorem}
%\emph{Intuition:  }The goal that a recover functionality cannot be both stable~(maps points to themselves) and correcting~(maps nearby points back to themselves).  If a recovery functionality is correcting then we can check if the input point maps to itself.  If a recovery functionality is stable then we can see if recovery succeeds with nearby points.  

\begin{proof}
Let $U_\mathcal{M}$ be the uniform distribution over $\mathcal{M}$ and let $d$ a  metric.  Furthermore assume $\mathcal{M}$ is $(s_{neigh}, t)$-neighborhood samplable.  Let $(\sketch, \rec)$ be a $(s_{rec}, t)$-recover functionality for $\mathcal{M}, d$.  Let $X\overset{d}= Y\overset{d}= U_\mathcal{M}$ be i.i.d..  It suffices to construct a distinguisher $D$~(whose size is $s\approx s_{neigh}+2s_{rec}$) such that $\expe[D(X, SS(X))]- \expe[D(Y, SS(X))]\geq \epsilon = 1/2$.  
%Let $q$ be a polynomial.  
$D$ is defined as follows:
\begin{enumerate}
\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*$. 
\item Sample $b\leftarrow \{0,1\}, z'\leftarrow \sample(z)$.
\item If $b=1$:
\subitem If $\rec(z', s) = z\wedge \rec(z, s) = z$ output $1$.
\item If $b=0$:
\subitem If $\rec(z, s) = z \wedge \rec(z', s)=z$ output $1$.
\item Output $0$.
\end{enumerate}
First note that $D$ is of size $O(s_{neigh}+2s_{rec})$ and thus in polynomial in $s_{sam}, s_{neigh}$.  It remains to show that $D$ separates $(X,\sketch(X))$ and $(Y,\sketch(X))$.  We being by noting that on input $(X, \sketch(X)), D$ outputs $1$.
By \lemref{lem:uniformNeighbor} we have:
\begin{align*}
0&=\delta([Y, \sample(Y), \sketch(X)],& &[\sample(Y), Y, \sketch(X)])\\
&=\delta([Y, \sketch(X), \sample(Y), \sketch(X)],& &[\sample(Y), \sketch(X), Y, \sketch(X)])\\
&=\delta([\rec(Y,\sketch(X)), \rec(\sample(Y),\sketch(X))],& &[\rec(\sample(Y), \sketch(X)), \rec(Y, \sketch(X))])
\end{align*}
Thus, the output of $\rec$ is the same when given $(\sample(Y), Y)$ or $(Y, \sample(Y))$\footnote{This is also true for a stronger class of recover functionalities that is able to keep state between queries.}.  This means it can properly recover $Y$ with probability at most $1/2$ over the ordering of inputs.  Thus we have, 
\begin{align*}
\Pr[D'(X, \sketch(X))=1] - \Pr[D'(Y, \sketch(X))=1] &=\\
1 - \frac{1}{2}(\Pr[\rec(Y, \sketch(X)) = Y \wedge \rec(\sample(Y),\sketch(X))=Y]&+\\\Pr[\rec(\sample(Y), \sketch(X)) = Y \wedge \rec(Y, \sketch(X)) = Y])&\geq 
1-\frac{1}{2}\left(1\right) =\frac{1}{2}
\end{align*}

%\begin{enumerate}
%\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*$.
%\item Initialize $correct, stable = 0$.
%\item For $i=1...q$:
%\subitem $x_i \leftarrow U$, if $\rec(x_i, s) = x_i, stable +=1/q$.
%\subitem $x_i' \leftarrow \sample (x_i)$, if $\rec(x_i', s) = x_i, correct+=1/q$.
%\item If $\rec(z, s)\neq z$ output $0$.
%\item If $stable<1/2$ output $1$
%\item Else $z'\leftarrow \sample(z)$.
%\subitem If $\rec(z', s) = z$ output $1$.
%\subitem Else output $0$.
%\end{enumerate}
%
%We begin by noting that $D$ is of size $O(q(s_{sam}+s_{neigh}+2s_{rec})+2s_{rec}+s_{neigh})$ and thus is polynomial in $s_{sam}, s_{neigh}, s_{rec}$.  It remains to show that $D$ separates $(X, SS(X))$ and $(Y, SS(X))$ by a noticeable $\epsilon$.  Let $STABLE = \Pr[\rec(U, \sketch(X)) = U]$.  We begin by noting that $\Pr[|stable-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).  That is, after step (3) $D$ has a good approximation
%of $STABLE$.  Similarly, let $CORRECT = \Pr[\rec(\sample(U), \sketch(X)) = U]$.  As before $\Pr[|stable-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).
%Thus, after step (3), $\Pr[|CORRECT-correct|>\ngl \wedge |STABLE-stable|>\ngl|]<\ngl$.  We proceed assuming, that $CORRECT$ and $STABLE$ are known exactly at step (4), and note the output of $D$ is negligibly close to the output of a distinguisher that knows $CORRECT$ and $STABLE$ exactly~(we label this distinguisher $D'$).
%\begin{claim}
%$CORRECT+STABLE\leq 1$.
%\end{claim}
%\begin{proof}
%\begin{align*}
%\Pr[\rec(U, \sketch(X) = U] + \Pr[\rec(\sample(U), \sketch(X)) = U]&\leq\\
%\Pr[\rec(U, \sketch(X) = U]  + \Pr[\rec(Y), \sketch(X)\neq Y] &=1
%\end{align*}
%Where the first inequality proceeds because of \lemref{lem:uniformNeighbor} and the fact that \sample never returns itself.  The equality proceeds as these events as these events are complementary events after renaming. 
%\end{proof}
%We now show that when $(\sketch, \rec)$ is stable or correcting the distinguisher has nonnegligible advantage:
%\begin{itemize}
%\item Case 1: $STABLE<1/2$.
%\begin{align*}
%\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1] &=\\
%\Pr[\rec(X, \sketch(X) = X] - \Pr[\rec(Y, \sketch(X) = Y] &\geq
% 1- 1/2  = 1/2
%\end{align*} 
%\item Case 2: $STABLE\geq 1/2, CORRECT \leq 1/2$.
%\begin{align*}
%\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1]&=\\
%\Pr[\rec(X, \sketch(X)) = X \wedge \rec(\sample(X), \sketch(X)) = X] &-\\\Pr[\rec(Y, \sketch(X) = Y) \wedge \rec(\sample(Y), \sketch(X)) = Y)]&\geq\\
%1 - \Pr[\rec(\sample(Y), \sketch(X)) = Y)]= 1-1/2 = 1/2
%\end{align*}
%\end{itemize}
%Thus, in both cases we have $\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1]\geq 1/2$.  As noted above, $CORRECT, STABLE$ are only known approximately and thus:
%\[
%\Pr[D(X, \sketch(X) )= 1] - \Pr[D(Y, \sketch(X) )= 1]\geq 
%\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1] - \ngl \geq 1/2 -\ngl
%\]
This completes the proof.
\end{proof}

This proof relied on two crucial facts: 1) that the neighbor of a uniform point was uniformly distributed 2) for an point uncorrelated with the value of the secure sketch it was impossible to distinguish between a point and its neighbor.  We can extend this result to distributions other than the uniform distribution but by drawing on more general results from coding theory.  

\subsection{Indistinguishable sketches imply information theoretic sketches}
In this section we show that the results of \thref{thm:compSketchImpUniform} can be extended to whenever $X$ is indistinguishable from a high min-entropy distribution.  }

We review definitions of Shannon codes~\cite{shannon1949mathematical}:
\begin{definition}
\label{def:shannon-code}
Let $\mathcal{C}$ be a set over space $\mathcal{M}$.  We say that $\mathcal{C}$ is an $(t,\epsilon)$-\emph{Shannon code} if there exists an efficient procedure $\rec$ such that for all $t'\le t$ and for all $c\in \mathcal{C}$, $\Pr[\rec(\sample(c, t')) \neq c]<\epsilon$. To distinguish it from the average-error Shannon code defined below, we will sometimes call it \emph{maximal-error} Shannon code.
\end{definition}
This is a slightly stronger formulation than usual, in that we require the code to correct random errors of every size $t'<t$\footnote{In the standard formulation, the code must correct a random error of size up to $t$, which may not imply that it can correct a random error of a much smaller size $t'$, because the volume of the ball of size $t'$ may be negligible compare to the volume of the ball of size $t$.  For codes that are monotone~(if decoding succeeds on a set of errors, it succeeds on all subsets), these formulations are equivalent.  However, we work with an arbitrary recover functionality that is not necessarily monotone.}.  

Shannon codes work for all codewords. We can also consider a formulation that works for an ``average'' codeword. \lnote{is there a source we can reference for the terms average-error and maximal-error?}

 \begin{definition}
Let $C$ be a distribution over space $\mathcal{M}$.  We say that $C$ is an $(t,\epsilon)$-\emph{average error Shannon code} if there exists an efficient procedure $\rec$ such that for all $t'\le t$
$\Pr_{c\leftarrow C}[\rec(\sample(c, t')) \neq c]<\epsilon$.
\end{definition}
An average-error Shannon code is convertible to a maximal-error Shannon code with a small loss.  We use the following pruning argument from Cover and Thomas~\cite[Pages 202-204]{cover2006elements} (we provide a proof in \secref{sec:proof of average to maximal error}):
\begin{lemma}
\label{lem:averageToMaximalError}
Let $C$ be a $(t, \epsilon)$-average error Shannon code with recovery procedure $\rec$ such that $\Hoo(C)\geq k$.  There is a set $C'$ with $|C'|\ge2^{k-1}$ that  is a $(t, 2\epsilon)$-(maximal error) Shannon code with recovery procedure $\rec$.
\end{lemma}

We can now formalize the intuition above.  A sketch that retains $\tilde{m}$-bits of HILL entropy implies a good error correcting code with nearly $2^{\tilde{m}}$ points~(proof in \secref{sec:proof of thm sketch implies code}).
\begin{theorem}\label{thm:impSketchArbitraryW}
Let $(\mathcal{M}, \dis)$ be a metric space that is $(s_{neigh}, t)$-neighborhood \lnote{why $s_{neigh}$ and not $s_{sam}$ as above?} samplable.  Let $(\sketch, \rec)$ be an HILL-entropy $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$.  Assume $s_{sec}= \Omega(t(s_{neigh}+s_{rec}))$, where $s_{rec}$ is the circuit size of $\rec$ \lnote{I don't think the asymptotics work here -- we are using concrete parameters, nothing is going to $\infty$, so what does $\Omega$ mean?} .  Then there exists a value $s$ and a set $C$ with $|C|\geq 2^{\tilde{m}-2}$  that is a $(t, 4(\epsilon+t\delta))$-Shannon code with recovery procedure $\rec(\cdot, s)$.
%Let $W$ be a flat distribution~(all points in the support of $W$ have the same probability) over $\mathcal{M}$ where $\Hoo(W)\geq k$ and   Let $X$ be arbitrarily but independently distributed over $\mathcal{M}$.  If $\delta^D((X, \sketch(X)), (W, \sketch(X)))<\epsilon$ for negligible $\epsilon$ and $D$ of size at least $O(s_{neigh}+s_{rec})$, then $\exists x, W'$ where $|W'|\geq |W|/2 \geq 2^{k-1}$ such that $W'$ is a $(t, 2\epsilon)$ Shannon code with recovery procedure $\rec(\cdot, \sketch(x))$.
\end{theorem}
%The code provided in \thref{thm:impSketchArbitraryW} is weaker than a Shannon error correcting code~\cite{shannon1948}.  In a Shannon code, for all codewords $c, \Pr[c'\leftarrow \sample(c) \wedge \rec(c') \neq c]<\epsilon$.  Bounds for this type of code are well known~\cite{shannon1948} and efficient constructions exist~\cite{forney1966}.  Requiring proper decoding for only a random codeword is a significant restriction particularly in the case where the probability of outcomes in $W$ differs significantly~(e.g. where $\exists w_1,w_2\in W$ such that $\Pr[W=w_1]/\Pr[W=w_2] = \ngl$).  However, this is still a meaningful set of error correcting codes.  A similar question is considered in Appendix C of \cite{DBLP:journals/siamcomp/DodisORS08}.  They are able to achieve results for Hamming codes~(as they are not restricted to sampling a polynomial portion of the space)\bnote{expand on this explanation}.


%It is known (see \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08} and references therein) that, 
For the Hamming metric, any Shannon code (as defined in Definition~\ref{def:shannon-code}) can be converted into an information-theoretic secure sketch~(as described in \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08} and references therein).  The idea is to use the code offset construction, and convert worst-case errors to random errors by randomizing the order of the bits of $w$ first, via a randomly chosen  permutation $\pi$  (which  becomes part of the sketch and is applied to $w'$ during $\rec$). The formal statement is in the following Lemma.
\begin{lemma}
\label{lem:shannon to sketch}
For an alphabet $Z$, let $\mathcal{C}$ over $Z^n$ be a $(t, \delta)$ Shannon code.  % with recovery procedure $\rec'$.
%  Let $W$ be a distribution with $\Hoo(W)\geq k$. 
%
% Define the following procedures
%\lnote{I think this description is buggy}
%\begin{center}
%\begin{tabular}{c|c}
%\begin{minipage}{3in}
%\textbf{\sketch}
%\begin{enumerate}
%\item Input $w\leftarrow W$.
%\item Sample $\pi$ from the space of all permutations from $\pi: [n]\rightarrow [n]$.
%\item Sample $c\leftarrow \mathcal{C}$.
%\item Apply $\pi$ bitwise to get $x_i = w_{\pi(i)}$.
%\item Output $p = x \oplus c, \pi$.
%\end{enumerate}
% \end{minipage} &
%\begin{minipage}{3in}
%\textbf{\rec}
%\begin{enumerate}
%\item Input $(w', p, \pi)$
%\item Apply $\pi^{-1}$ bitwise to get $x_i' = w_{\pi(i)}$.
%\item Compute $c' = p \oplus x'$.
%\item Compute $c = \rec'(c)$.
%\item Output $p\oplus c$.
%\\
%\end{enumerate}
%\end{minipage} 
%\end{tabular}
%\end{center}
Then there exists a $(Z^n, m, m-(n\log|Z|-\log |\mathcal{C}|), t)$ secure sketch with error $\delta$ for the Hamming metric over $Z^n$. 
%satisfies the following properties~(probability over the choice of $\pi$ in $\sketch$):
%\begin{itemize}
%\item Correctness: $\forall w, w'$ where $dis(w, w')\leq t$, $\Pr[\rec(w', \sketch(w))\neq w]<\epsilon$.
%\item Security: $\Hav(W |\sketch(W)) \geq \Hoo(W) - (n-\log |C|)$.
%\item Efficiency: $\sketch$ is efficient if sampling from $C$ is efficient and $\rec$ is efficient if $\rec'$ is efficient.
%\end{itemize}
\end{lemma}
%\begin{construction}
%Let $\delta >0$ be a parameter.
%Let $C$ be a $(t, \epsilon)$ Shannon code over $\{0,1\}^n$.  Choose a permutation $\pi:\mathcal{M}rightarrow \mathcal{M}$ that is $\beta$-almost independent $\ell = \delta n/\log(n)$-wise permutation.  Thefn for $\beta = 2^{-\ell}
%\end{construction}
Putting together \thref{thm:impSketchArbitraryW} and \lemref{lem:shannon to sketch} gives us the negative result for the Hamming metric: a HILL-entropy secure sketch (for the uniform distribution) implies an information-theoretic one with similar parameters:
\begin{corollary}
\label{cor:rec yields sketch}
Let $Z$ be an alphabet. Let $(\sketch', \rec')$ be an $(\epsilon,s_{sec})$-HILL-entropy $(Z^n, n\log |Z|, \tilde{m}, t)$-secure sketch with error $\delta$ for the Hamming metric over $Z^n$, with $\rec'$ of circuit size $s_{rec}$.
If $s_{sec}=\Omega(t(s_{rec} + n\log |Z|))$ \lnote{same comment about $\Omega$}, then there exists a   $(Z^n, n\log |Z|, \tilde{m}-2,t)$ (information-theoretic) secure sketch with error
$4(\epsilon+t\delta)$. 
%Let $(\mathcal{M}, d)$ be a metric space that is $(s_{neigh}, t)$-neighborhood samplable.  If no $(t, \epsilon)$-average error Shannon code over $\mathcal{M}$ exists of size $2^k$ then, $\forall (s_{rec}, t)$ recovery procedures $(\sketch, \rec)$ $H^{\hill}_{\epsilon, s}(X|\sketch(X))\not\geq k$ for $s=O(s_{rec}+s_{neigh})$.
\end{corollary}
\textbf{Notes:} In \corref{cor:rec yields sketch} we make no claim about the efficiency of the resulting  $(\sketch, \rec)$, because the proof of \thref{thm:impSketchArbitraryW} is not constructive.  

\corref{cor:rec yields sketch} extends to non-uniform distributions: if there exists a distribution whose HILL sketch retains $\tilde{m}$ bits of entropy, then for all distributions $W$, there is an information theoretic sketch that retains $\Hoo(W) - (n\log |Z|-\tilde{m})-2$ bits of entropy.

%\corref{cor:rec yields sketch} shows that sketches that retain HILL entropy can be converted into an information-theoretic sketch.  Unfortunately, we have lower bounds for the entropy drop for information-theoretic sketches:

%\begin{proposition}\protect{\cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}}
%\label{prop:lower bound entropy drop}
%For any error rate $1\leq t\leq n$, any secure sketch which corrects $t$ random errors with probability at least $2/3$ has entropy loss at least $n(h(t/n) -o(1))$; that is, $\Hav(W|\sketch(W))\leq n (1-h(t/n) - o(1))$ when $W$ is drawn uniformly from $\{0,1\}^n$.  Here $h(\cdot)$ is the binary entropy function.
%\end{proposition}

%Together \corref{cor:rec yields sketch} and \propref{prop:lower bound entropy drop} say the entropy drop for a ``HILL sketch'' must be the same as the entropy drop of a sketch that corrects random errors.  This entropy drop is tied to the Shannon capacity.  Furthermore, we have information theoretic constructions that meet these bounds.  Thus, defining a computational secure sketch using HILL entropy is unlikely to be useful.


%\begin{theorem}
%Let $\mathcal{M}, d$ be a finite metric space that is $s_{sam}$ efficiently samplable and $s_{ball}, t$ neighborhood efficiently samplable.  Furthermore, let $(\sketch, \rec)$ be any $(s_{rec}, t)$-recover functionality for $\mathcal{M}, d$.  Let $U$ be the uniform distribution over $\mathcal{M}$.  If $s_{ball}, s_{rec}, s_{sam}$ are of polynomial size, then $H^\hill_{\epsilon, s}( U |\sketch(U))\not \geq \Hoo(U)$ for polynomial $s$ and noticeable $\epsilon$.
%\end{theorem}
%\emph{Intuition:  }We will show that a recover functionality cannot be both stable~(maps points to themselves) and correcting~(maps nearby points back to themselves).  If a recovery functionality is correcting then we can check if the input point maps to itself.  If a recovery functionality is stable then we can see if recovery succeeds with nearby points.  
%
%\begin{proof}
%Let $\mathcal{M}, d$ be a finite metric space that is $s_{sam}$ efficiently samplable and $s_{ball}, t$ neighborhood efficiently samplable~(by $\sample$).  Let $(\sketch, \rec)$ be a $s_{rec}, t$-recover functionality for $\mathcal{M}, d$.  Let $X, Y, U$ be i.i.d. and be the uniform distribution over $\mathcal{M}$.  By assumption $s_{ball}, s_{sam}, s_{rec}$ be of polynomial size.  It suffices to construct a polynomial size distinguisher $D$ such that $\expe[D(X, SS(X))]- \expe[D(Y, SS(X)]> \epsilon$ for noticeable $\epsilon$.  Let $q$ be a polynomial.  $D$ is defined as follows:
%
%\begin{enumerate}
%\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*$.
%\item Initialize $correct, stable = 0$.
%\item For $i=1...q$:
%\subitem $x_i \leftarrow U$, if $\rec(x_i, s) = x_i, stable +=1/q$.
%\subitem $x_i' \leftarrow \sample (x_i)$, if $\rec(x_i', s) = x_i, correct+=1/q$.
%\item If $\rec(z, s)\neq z$ output $0$.
%\item If $stable<2/3$ output $1$
%\item Else $z'\leftarrow \sample(z)$.
%\subitem If $\rec(z', s) = z$ output $1$.
%\subitem Else output $0$.
%\end{enumerate}
%
%We begin by noting that $D$ is of size at most $q(s_{sam}+s_{ball}+2s_{rec})+2s_{rec}+s_{ball}$ and thus is polynomial assuming that $q, s_{sam}, s_{ball}, s_{rec}$ are of polynomial size.  It remains to show that $D$ separates $X, SS(X)$ and $Y, SS(X)$ by a noticeable $\epsilon$.  Let $STABLE = \Pr[\rec(U, \sketch(X)) = U]$.  We begin by noting that $\Pr[|stable-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).  That is, after step (3) $D$ has a good approximation
%of $STABLE$.  Similarly, let $CORRECT = \Pr[\rec(\sample(U), \sketch(X)) = U]$.  As before $\Pr[|correct-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).
%
%\begin{claim}
%$CORRECT+STABLE\leq 1$.
%\end{claim}
%\bnote{finish proof}
%
%\end{proof}

\subsection{Bounds on Secure Sketches using Unpredictability Entropy}
\label{sec:imp unp sketch}
In the previous section, we showed that any sketch that retained HILL entropy could be transformed into an information theoretic sketch.  A reasonable question to ask is whether a sketch could retain unpredictability of the source.  We begin by defining conditional unpredictability entropy:

\begin{definition}~\cite[Definition 7]{DBLP:conf/eurocrypt/HsiaoLR07}
\label{def:unp entropy}
For a distribution $(X, Z)$, we say that $X$ has \emph{unpredictability entropy} at least $k$ conditioned on $Z,$ denoted by $H^{\unp}_{\epsilon, s_{sec}} (X|Z) \geq k$, if there exists a collection of distributions $Y_z$ (giving rise to a joint distribution $(Y, Z)$) such that $\delta^{\mathcal{D}_{s_{sec}}}((X, Z),(Y, Z))\leq \epsilon$, and for all circuits $C$ of size $s_{sec}$,
\[
\Pr[C(Z) = Y ] \leq 2^{-k}
.\]
\end{definition}
We will say a pair of procedures $(\sketch, \rec)$ is a \emph{unpredictability-entropy $(\mathcal{M}, m, \tilde{m}, t)$ secure sketch} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if it satisfies \defref{def:secure sketch} with the security requirement replaced with $H^{\unp}_{\epsilon, s_{sec}}(W| \sketch(W))\geq \tilde{m}$.  
We consider this notion to be a natural notion for a computational sketch as it can be extracted from using a reconstructive extractor~(defined in~\cite{barak-computational}):
\begin{lemma}~\cite[Lemma 6]{DBLP:conf/eurocrypt/HsiaoLR07}
\label{lem:unp extraction} 
Let $X$ be a distribution with $H^{\unp}_{\epsilon, s_{sec}}(X|Z)\geq k$, and let $\rext$ be an extractor with a $(k-\log 1/\epsilon, \epsilon)$-reconstruction $(C,D)$.  Then $\delta^{\mathcal{D}_{s'}}((\rext(X, U_d), Z), U_m\times U_d\times Z)\leq 5\epsilon,$ where $s'=s_{sec}/(|C|+|D|)$.
\end{lemma}
Thus, if a sketch retained unpredictability entropy we could build a computational fuzzy extractor~(using the construction in \lemref{lem:fuzzy ext construction}).  
Unfortunately, the unpredictability entropy conditioned on the sketch must recreate with $t$.  
%An adversaries ability to predict the output of the \rec algorithm increases with as $t$ grows.  %That is, $\forall Y$ such that $\delta^{\mathcal{D}_{s_{sec}}}((W, \sketch(W)) , (Y, \sketch(W))$ our ability to predict $Y$ increases as $|B_t(\cdot)|$ increases.  
The formal statement is below:
\begin{theorem}
\label{thm:imp of unp entropy}
Let $W$ be a distribution over $Z^n$ and let $(\sketch, \rec)$ be an unpredictability-entropy $(Z^n, \Hoo(W), \tilde{m}, t)$ secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$.  If $s_{sec} = \Omega(|\rec|+n\log |Z|)$, then $\tilde{m}\leq n\log |Z| - \log |B_t(\cdot)| + \log(1-\epsilon -\delta)$.
\end{theorem}
This result has a weaker condition than \corref{cor:rec yields sketch} and yields a weaker conclusion.  The main difference with \thref{thm:impSketchArbitraryW} is that it is unknown if the indistinguishable distribution contains many points.  This means we cannot construct a good code out of the points of $Y$.  In \secref{sec:proof of imp unp entropy}, we prove a stronger version of \thref{thm:imp of unp entropy}, based on a relaxed notion of unpredictability entropy.%Our bound is a result of the Hamming bound for the space.  

\textbf{Avoiding lower bounds from \corref{cor:rec yields sketch} and \thref{thm:imp of unp entropy}:}

The lower bounds of \corref{cor:rec yields sketch} and \thref{thm:imp of unp entropy} are strongest for high entropy sources.  The results get weaker as we consider sources with lower starter entropy.  This is necessary, if a source contains only codewords (of an error correcting code), no sketch is necessary.  This means we cannot rule out computational sketches for other arbitrary lower entropy distributions.  This same situation occurs when considering lower bounds for information-theoretic sketches.

Both of the lower bound results arise because the reduction can ensure that \rec functions as an error-correcting code for many points of the indistinguishable distribution.  It may be possible to avoid this problem if \rec outputs a fresh random variable\footnote{If some efficient algorithm can take the output of $\rec$ and efficiently transform it back to the source $W$, the bounds of \corref{cor:rec yields sketch} and \thref{thm:imp of unp entropy} both apply.  This means that we need to consider constructions that are hard to invert~(either information-theoretically or computationally).}.  An algorithm that outputs a fresh random variable would be a computational fuzzy conductor~(see~\cite{KanukurthiR09} for the definition of a fuzzy conductor).  Our construction~(in \secref{sec:fuzzyCompExt}) has pseudorandom output and immediately satisfies definition of a computational fuzzy extractor~(\defref{def:comp fuzzy extractor}).  It may be possible to achieve significantly better parameters with a construction that is a computational fuzzy conductor~(but not a computational fuzzy extractor) and then applying an extractor.  We leave this as an open problem.


%\subsection{Defining  Computational Fuzzy Conductors}
%\label{sec:unp secure sketch}
%In Sections \ref{sec:imp HILL sketch} and \ref{sec:imp unp sketch}, we showed lower bounds on an entropy drop of computational sketches.  If, the error correcting algorithm outputs a new random variable, bounds do not apply.  Instead of building a computational sketch, we will build a computational conductor.  We adapt the definition of a fuzzy conductor from~\cite{KanukurthiR09} to the computational setting:
%%\begin{definition}\protect{\cite[Definition]{KanukurthiR09}}
%%The procedures $(\sketch, \rec)$ are an $(\mathcal{M}, m, \tilde{m},t, \epsilon)$-\emph{weakly robust fuzzy conductor} if:
%%\begin{enumerate}
%%\item Error Tolerance: If $\dis(w, w')\leq t$ and $s\leftarrow \sketch(w)$, then $\Pr[\rec(w', \sketch(w)) \neq  \rec(w, \sketch(w))] \leq \epsilon$ where the probability is over the coins of $\sketch, \rec$.
%%\item Security of $\sketch$: If $H_\infty(W) \geq m$, then $\Hav(\rec(W, \sketch(W)) | \sketch(W))\geq \tilde{m}$.
%%\end{enumerate}
%%\end{definition}
%%We will essentially build %if we define the secure sketch using unpredictability entropy~\cite[Section 5]{DBLP:conf/eurocrypt/HsiaoLR07}, this oracle is not available to the adversary.  We begin by defining unpredictability entropy:
%
%
%%We now provide a definition of a robust computational sketch using unpredictability entropy:
%\begin{definition}\label{def:comp secure sketch}
%A pair of randomized procedures $(\sketch, \rec)$ is a $(\mathcal{M},m, \tilde{m}, \epsilon,  t)$-\emph{computational fuzzy conductor} if the following properties hold:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$ and runs in expected time $s_{sketch}$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $d(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))\neq \rec(w, \sketch(w))] = \delta$ and \rec runs in expected time $s_{rec}$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.  
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the output $\rec$ has high HILL entropy given $\sketch(W)$.  That is, $H^{\hill}_{\epsilon, s_{sec}}(\rec(W, \sketch(W))|\sketch(W))\geq \tilde{m}$.
%\end{enumerate}
%We say that $(\sketch, \rec)$ is $(\epsilon, s_{sec})$-secure with error probability $\delta$.
%If $m=\tilde{m}$ we say a conductor is \emph{lossless} and we omit the parameter $\tilde{m}$.
%\end{definition}
%
%%We make $s_{sketch}, s_{rec}$ explicit in the definition as we are considering computationally bounded adversaries and the definition is only meaningful if both $s_{rec}$ and $s_{sketch}$ are significantly smaller than $s_{sec}$.  
%In \secref{sec:comp fuzzy extractors}, we show a natural construction of a computational fuzzy extractor from a computational fuzzy conductor that meets \defref{def:comp secure sketch}.  We show in the next section that a lossless construction is achievable for the uniform distribution using the $\LWE$ problem.

\section{Computational Fuzzy Extractor based on \class{LWE}}
\label{sec:fuzzyCompExt}

In this section we describe our main construction.  In this section, we consider only a uniform source $W$, we consider other distributions in \secref{sec:LWE block fixing sources}.  Our construction uses the code-offset construction~\cite{JW99},\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} instantiated with a random linear code over a finite field $\Fq$.   Let $\mathcal{I}_t$ be an algorithm that decodes a random linear code with at most $t$ errors.  We provide an informal description below.
%Please refer to \consref{cons:informal construction} for the description of the construction.  

\begin{construction}
Let $n$ be a security parameter and let $m> n$.  %Let $W = W_1,..., W_m$ and let $b$ be a constant and let $W_i \in \zo^b$ for all $i$.  
Define $\gen, \rep$ as follows:%The following construction is a computational secure sketch:
%Code-offset construction of a secure sketch using a random linear code over a finite field $\Fq$. $\mathcal{I}_t (\vA, \vect{D})$ denotes an algorithm that decodes up to $t$ errors, i.e., an algorithm that finds $X'\in \Fq^n$ such that $\vect{A}X'$ is of Hamming distance at most $t$ from $\vect{D}$. $m > n$ are parameters to be figured out later.
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w\leftarrow W$ (where $W$ is some distribution over $\Fq^m$).
\item Sample $\vA\in\Fq^{m\times n}, X\in\Fq^n$ uniformly.
\item Compute $p = (\vA, \vA X+w)$, \\\ $r = X_{1,...,n/2}$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rec}
\begin{enumerate}
\item \underline{Input}: $(w', p)$ (where the Hamming distance between $w'$ and $w$ is at most $t$).
\item Parse $p$ as $(\vA, \vect{C})$; let $\vect{D}=\vect{C}-w'$.
\item Let $X = \mathcal{I}_t(\vA, \vect{D})$\\
\item Output $r = X_{1,...,n/2}$.
\end{enumerate}
\end{minipage} 
\end{tabular}
\end{center}
\label{cons:informal construction}
\end{construction}


Intuitively, security comes from the computational hardness of decoding random linear codes from a high number of errors (introduced by $w$).  
In fact, we know that decoding a random linear code is NP-hard~\cite{berlekamp1978}; however, this statement is not sufficient for our security goal, which is to show  $\delta^{\mathcal{D}_{s_{sec}}}((X_{1,..., n/2},P), (U_{n/2 \log q}, P))\leq \epsilon$.  Furthermore, this construction is only useful if $\mathcal{I}_t$ can be efficiently implemented.  We demonstrate an efficient $\mathcal{I}_t$ that corrects a super-polynomial number of error patterns in polynomial time.

The rest of this section is devoted to making these intuitive statements precise.
 We describe the \class{LWE} problem and the security of our construction in \secref{subsec:LWE}.
 %a recent variant due to D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} in \secref{subsec:LWE}.  
We describe conditions for efficient decoding in \secref{sec:time main construction}.  In \secref{sec:lossless extractor}, we describe parameter settings where we extract as many bits as the size of our input distribution and thus our construction is lossless.  In \secref{sec:prg based comparison}, we compare \consref{cons:informal construction} to using a information-theoretic fuzzy extractor and running a pseudorandom generator on the output.



\subsection{Security of \consref{cons:informal construction}}
\label{subsec:LWE}
The $\LWE$ problem was introduced by Regev \cite{regev2005LWE, regevLWEsurvey} as a generalization of ``learning parity with noise." For positive integers $n$ and $q \ge 2$, a vector $\vx \in \Fq^n$, and a probability distribution $\chi$ on $\Fq$, let $A_{\vect{x}, \chi}$ be the distribution obtained by choosing $\vA \overset{\$}\leftarrow \Fq^{m\times n}$ uniformly at random and a noise term $\ve \overset{\$}\leftarrow \chi^m$, and outputting $(\vA, \vA\vx+\ve)$. 
%The Learning With Errors ($\lwe$) is defined as follows: given access to the oracle that outputs a sample from the distribution $A_{\vx, \chi}$ for a uniform $\vx$, and output $\vx$ with high probability. 

\begin{definition}[Decisional $\lwe$]\label{def:dist-LWE}
Let $m = m(n) = \poly(n)$ be an integer, $q = q(n) = \poly(n)$. The decisional version of the $\LWE$ problem, denoted \class{dist}-$\LWE_{n, m, q, \chi}$, is to distinguish between $m$ samples chosen according to $A_{\vx, \chi}$ for a uniformly random $\vx$ from samples chosen according to the uniform distribution over $(\Fq^{m\times n}, \Fq^m)$. 

We say that $\distLWE_{n, m, q, \chi}$ is $(\epsilon, t)$-secure if no (probabilistic) distinguisher running in time $t$ can distinguish the $\lwe$ instances from uniform except with probability $\epsilon$.  If $\distLWE_{n, m, q, \chi}$ is secure for all $t = \poly(n), \epsilon  = 1/\poly(n)$ we say it is \emph{secure}.
\end{definition}

Denote by $\bar{\Psi}_\alpha$ the discretized Gaussian distribution over $\Fq$ with variance $(\alpha q)^2/(2\pi)$, Regev\cite{regev2005LWE} and Peikert \cite{peikert2009latticereduction} show that, when $\chi = \bar{\Psi}_{\alpha}$, $\class{dist}$-$\lwe_{n, m, q, \chi}$ is secure if the worst-case lattice problems are hard to approximate using quantum algorithms.
%The matrix $\vA$ and secret $\vx$ are drawn from the uniform distribution, while the error term is drawn from a discretized Gaussian distribution $\bar{\Psi}_{\alpha}$.  
Furthermore, Regev \cite{regev2005LWE} shows a decision to search equivalence for $q=\poly(n)$.
%We state the decisional version in terms of unpredictability entropy~(see \defref{def:unp entropy} for the definition of unpredictability entropy and \secref{sec:LWEoverview} for an equivalence to the standard formulation of decisional LWE):


\begin{proposition} 
\label{assume:entropy LWE}
Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be a prime and let $\bar{\Psi}_\alpha$ be the discretized Gaussian distribution with variance $(\alpha q)^2/2\pi$.  If GAPSVP and SIVP are hard to approximate~(on lattices of dimension $n$) within polynomial factors for quantum algorithms, then $\distLWE_{n, m, q, \chi}$ is $(\epsilon, s_{sec})$-secure for $\epsilon = \ngl(n)$ and $s_{sec} = \poly(n)$.  That is, $\delta^{\mathcal{D}_{s_{sec}}}(\vA, \vA X+E ), (\vA, U_q^m))\leq \epsilon$.

%H^{\unp}_{\epsilon, s_{sec}}(X | \vA, \vA X+E) =H^{\unp}_{\epsilon, s'_{sec}}(E| \vA, \vA X+E) \geq n\log q $ for $s'_{sec}\approx s_{sec}$.  
\end{proposition}

We cannot immediately adapt the construction in \consref{cons:informal construction} to the $\LWE$ problem.  Standard formulations of $\LWE$ require the error term to come from the discretized Gaussian distribution.  
%\subsection{LWE with uniform errors}
%\label{subsec:LWE uniform error}
Recent work D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} shows the security of $\LWE$ with the uniform distribution over an interval reduces to the security of $\LWE$ where the noise comes from the discretized Gaussian.  This allows us to directly encode $w$ as the error term in an $\LWE$ problem by splitting it properly into $m$ blocks.
%As stated above this has two advantages for Construction~\ref{cons:LWESecureSketch}: 1) the bits of $W$ can be used directly and \rep can output $w$ instead of $sam_w$ 2) there is a fixed number of bits required to sample each dimension.  
We present the formulation of D\"{o}ttling and M\"{u}ller-Quade: 
%\begin{lemma}~\protect{~\cite[Theorem 5]{dottling2012}}
%\label{lem:uniform LWE}
%Let $n$ be a security parameter.  Let $q = q(n))$ and $m = m(n) = \poly(n)$ be integers.  Let $\rho = \rho(n) \in (0,1)$ be such that $\rho q\geq 2n^2m$.  Assume there exists a PPT-algorithm that solves the LWE decision problem where $\vA,X$ are drawn uniformly at random and $E$ is drawn from the interval $[-\rho q, \rho q]$ with non-negligible probability.  Then there exists an efficient quantum-algorithm that approximates the decision-version of the shortest vector problem (GAPSVP) and the shortest independent vectors problem (SIVP) to within $\tilde{O}(n^{5/2}m/\rho)$ in the worst case.
%\end{lemma}

%This problem can be stated in the language of indistinguishability using the Decision-to-search reduction of Regev or Peikert:
\begin{lemma}~\protect{~\cite[Corollary 10]{dottling2012}}
\label{lem:uniform LWE decision}
Let $n$ be a security parameter.  Let $q = q(n) = \poly(n)$ be a prime integer and $m = m(n) = \poly(n)$ be an integer. Let $\gamma\in (0, 1)$ be a constant and let $\rho\in (0,1)$ such that $\rho q \geq 2n^{1/2+3\gamma}m$. Assume decision-version of the shortest vector problem (GAPSVP) and the shortest independent vectors problem (SIVP) are hard to approximate~(on lattices of dimension $n/2$) with the factor $\tilde{O}(n^{1/2+3\gamma}m/\rho)$ for quantum algorithms in the worst case, then, for $\chi = [-\rho q, \rho q]$, then $\distLWE_{n, m, q, \chi}$ is $(\epsilon, s_{sec})\mbox{-}$secure, where $\epsilon = \ngl(n)$ for $s_{sec}=\poly(n)$.
\end{lemma}

To extract pseudoranom bits, we use a result of Akavia, Goldwasser, and Vaikuntanathan~\cite{akavia2009} to show that $X$ has many hardcore bits.  The result says that if $\distLWE_{(n-k, m, q, \chi)}$ is secure then any $k$ variables of $X$ in a $\distLWE_{(n, m, q, \chi)}$ instance are hardcore.  We state their result for a general error distribution~(noting their proof does not consider the error distribution anywhere):
\begin{lemma}\protect{\cite[Lemma 2]{akavia2009}}
\label{lem:many hardcore bits}
Let $n>0$ be an integer, $q\geq 2$, let $\chi$ be an error distribution over $\zq$ and $k\leq n$ be an integer.  Let $\vA\in \zq^{m\times n} , X\in\zq^n$ and $E$ be distributed according to $\chi^m$.  If $\distLWE_{(n-k, m, q, \chi)}$ is $\epsilon, s_{sec}$ hard then $H^{\hill}_{\epsilon, s_{sec}'}(X_{1,..., k}| \vA, \vA X+E )\geq k\log q$ where $s_{sec}' \approx s_{sec} - n^3$.
\end{lemma}

%\begin{lemma}
%\label{lem:conversion to unpredictability}
%Assume that $\distLWE_{n-k, m, q, \chi}$ is $(\epsilon, s_{sec})$-hard then  $H^{\unp}_{\epsilon', s_{sec}'}(X_{1,..., k}|\vA, \vA X+E) \geq k\log q$
%for $\epsilon' \approx \epsilon, s_{sec}'\approx s_{sec}$.
%\end{lemma}
%\begin{lemma}
%\label{lem:dmq with unp entropy}
%Let $n$ be a security parameter.  Let $q = q(n) = \poly(n)$ be a prime integer and $m = m(n) = \poly(n)$ be an integer. Let $\gamma\in (0, 1)$ be a constant and let $\rho\in (0,1)$ such that $\rho q \geq 2n^{1/2+3\gamma}m$.  Assume that approximating GAPSVP and SIVP using a polynomial time quantum algorithm is hard within a factor of $\tilde{O}(n^{1/2+3\gamma}m/\rho)$.  Let $\vA\in \Fq^{m\times n}, X\in\Fq^{n}, E\in [-\rho q, \rho q]^m$ be drawn uniformly.  Then for $s_{sec} = \poly(n)$ and $\epsilon = \ngl(n)$
%\[
%H^{\unp}_{\epsilon, s_{sec}}(E| AX+E) = \min\{|X|, |E|\} = \min \{ n\log q, m\log \rho q\}
%\]
%\end{lemma}
With Lemmas \ref{lem:uniform LWE decision} and~\ref{lem:many hardcore bits}, \consref{cons:informal construction} has many hardcore bits and we use these bits as our key.  
%\lemref{lem:uniform LWE decision} has two advantages: 1) We can use the code-offset construction from \consref{cons:informal construction}. 2) The error term is distributed over a smaller interval $[-\rho q, \rho q]$ instead of $\Fq$.

%\subsection{Computational Secure Sketch based on \class{LWE}}
%\label{subsec:fuzzyExtLWE}
%We now give a formal description of our sketch based on LWE.  We now consider a source $W$~(this fulfills the role of $E$ in standard LWE notation):
%
%\begin{construction}[Computation Secure Sketch based on LWE]
%\label{cons:LWESecureSketch} Let $n$ be a security parameter and let $m = m(n) = \poly(n), q = q(n)\geq 2$ be integers.  Let $\gamma\in (0,1)$ be a constant and let $\rho \in (0,1)$ such that $\rho q\geq 2n^{1/2+3\gamma}m$.  Let $\mathcal{I}_t$ be an algorithm~(not necessarily efficient) that inverts an LWE instance when no more than $t$ of $m$ dimensions have non-zero error.  Let $W$ be a distribution over $\{0,1\}^{(\log \rho q)\times m}$\,.  Then the following is a computational secure sketch:%We present the construction in 
%%\begin{figure}
%%\label{fig:construction figure}
%%\caption{Computational Secure Sketch based on the Learning with Errors problem}
%\begin{center}
%\begin{tabular}{c|c}
%\begin{minipage}{3in}
%\textbf{\sketch}
%\begin{enumerate}
%\item Input $w\leftarrow W$.
%\item Sample $\vA\in\Fq^{m\times n}, x\in\Fq^n$ uniformly.
%\item Output $p = (\vA, \vA x+w)$.\\
%\end{enumerate}
% \end{minipage} &
%\begin{minipage}{3in}
%\textbf{\rec}
%\begin{enumerate}
%\item Input $(w', p)$
%\item Parse $p$ as $(\vA, \vect{C})$
%\item Set $x' = \mathcal{I}_t(\vA, \vect{C}-w') $. 
%\item Output $w = \vect{C}-\vA x'$.
%\end{enumerate}
%\end{minipage} 
%\end{tabular}
%\end{center}
%%\end{figure}
%%\textbf{\gen}
%%\begin{enumerate}
%%\item Input $w\leftarrow W$.
%%\item Sample $A\in\Fq^{m\times n}, x\in\Fq^n$ uniformly at random.
%%\item Use $w$ as the randomness for the sampling algorithm, \\$\sample$, for $\chi$.  Set $E\leftarrow  \sample(w)$.
%%\item Output $p = (A, AX+E)$.
%%\end{enumerate}
%%
%%
%%\textbf{\rep}
%%\begin{enumerate}
%%\item Input $(w', p)$
%%\item Parse $p$ as $(A, C)$
%%\item Compute $E' \leftarrow \sample (w')$.
%%\item Set $X' = \mathcal{I}_t(A, C-E') $. 
%%\item Output $sam_w = C-AX'$.
%%\end{enumerate}
%\end{construction}

\subsection{Efficiency of \consref{cons:informal construction}}
\label{sec:time main construction}
%\subsection{Analysis of Construction~\ref{cons:LWESecureSketch}}
\consref{cons:informal construction} is only useful if $\mathcal{I}_t$ can be efficiently implemented.  We now present a particular $\mathcal{I}_t$ and consider its efficiency:

\begin{construction}
\label{cons:decoding algorithm} Suppose that for a $(n, m, q, \chi)$ where $m\geq 2n$.  We describe inverter $\mathcal{I}_t$:
\begin{enumerate}
\item Input $\vA , \vect{C} = \vA X + W - W'$
\item Randomly select rows without replacement $i_1,..., i_n\leftarrow [1,m]$.  
\item Restrict $\vA, \vect{C}$ to rows $i_1,...,i_n$, denote this is $A_{i_1,...,i_n}, C_{i_1,...,i_n}$.
\item Compute $X' = \vA^{-1}_{i_1,...,i_n}\vect{C}_{i_1,...,i_n}$.  If $C- AX'$ has more than $t$ nonzero coordinates, go to step (2).
\item Output $X'$.
\end{enumerate}
\end{construction}

$\mathcal{I}_t$ is an efficient inverter/recovery procedure when $t$ is small enough.  We need $t$ to be small enough for step (4) to succeed with probability at least $\frac{1}{\poly(n)}$, furthermore, we need the code generated by $\vA$ to have distance at least $t$ with high probability.  The second condition is to ensure that there are few false positives in step (4).  This condition is satisfied for $t = O(\frac{m}{n}\log n)$, since random codes have high distance with probability $1-e^{-\Omega(n)}$.  The exact statement is in \corref{cor:code high distance}.  Furthermore step (4) is computable in time $O(n^3)$.
This is also a sufficient condition for step (4) to succeed with probability at least $1/\poly(n)$.  We consider the setting where the running time of $\mathcal{I}_t$ grows with $t$.
%  We provide the results in \lemref{lem:i t constant time} and \lemref{lem:i t poly time} respectively~(proofs in Sections~\ref{sec:proof lem i t constant time},~\ref{sec:proof lem i t poly time}).  \lnote{can we combine these lemmas into one?  It's hard for the reader to compare line-by-line to see where the differences are.  More errors seems better; we could simply point out that constant number of erros leads to better running time, right in the statement of the lemma or right after it}
%\begin{lemma}[Efficiency of $\mathcal{I}_t$ when $t\leq m/n-1$]
%\label{lem:i t constant time}
%Let $\vA, \vA X+E$ be a  $(n,m, q, \chi)$-LWE instance that is a unique witness relation~(except with negligible probability).  Assume $d(E, E')\leq t$ where $t \leq (m-n)/n$.  Then $\mathcal{I}_t$ runs in expected time $O(n^3\log q+ s_{ver})$ on input distribution $\vA, \vA X+E - E'$ and outputs $X' =X$ for all but a negligible fraction of inputs over $\vA, \vA X+E - E'$.
%\end{lemma}
\begin{lemma}[Efficiency of $\mathcal{I}_t$ when $t\leq d\log n (m/n-1)$]
\label{lem:i t poly time}
Let $\vA, \vA X+W$ be a  $\distLWE_{n,m, q, \chi}$ instance where $m\geq 2n$.  Assume that $W$ is split into $m$ blocks of length $b$ and let $\dis$ be the Hamming distance over alphabet $2^b$.  Let $d$ be a constant and assume that $\dis(W, W')\leq t$ where $t\leq d(\frac{m}{n}-1)\log n$.  Then $\mathcal{I}_t$ runs in expected time $O(n^{2d}(n^3\log q))$ on input distribution $\vA, \vA X+W - W'$ and outputs $W$ with probability $1-e^{-\Omega(n)}$ when $\vA$ is a random matrix.
\end{lemma}
\textbf{Note:} If we consider the setting where $t\leq m/n-1$, then $\mathcal{I}_t$ will run in expected time $O(n^3\log n)$.  Our main theorems apply when $t\leq m/n$ with slightly better parameters.  We concentrate on the setting $t\leq d\log n(m/n-1)$.  Also even though we are correcting $O(\log n)$ errors, this a super polynomial number of error patterns and the running time of $\mathcal{I}_t$ is significantly better than exhaustive search over these error patterns.

%\textbf{Problems:} Construction~\ref{cons:LWESecureSketch} does not meet Definition~\ref{def:comp secure sketch}.  This is because the sampling algorithm may not be invertible.  Furthermore, in standard $\LWE$ the sampling algorithm is Gaussian and there is no fixed number of bits used to sample error in any dimension.  Approximating the Gaussian distribution using a fixed number of bits may be possible but there are still two error patterns which require a significantly different number of bits.  We will address both of these problems by sampling the error from a uniform distribution.

\subsection{Lossless Computational Fuzzy Extractor}
\label{sec:lossless extractor}
We now state a setting of parameters that yields a lossless construction.  

%\begin{theorem}
%\label{thm:lossless secure sketch}
%Let $n$ be a security parameter and let $t$ be a constant.  Consider the Hamming metric with block length $b = \log 2n^3(t+1)$.  Let $W$ be uniform over $\zo^{n(t+1)b}$.  If GAPSVP and SIVP are hard to approximate within polynomial factors, there is a 
%\[
%(\zo^{n(t+1)b}, |W|,|W|, \epsilon, s, s_{sketch}, s_{rec}, t)\text{-computational secure sketch}
%\]
% for $\epsilon = \ngl(n), s = \poly(n), s_{sketch} = O(n^3\log n )$ and $s_{rec}= O(n^3 \log n)$.
%\end{theorem}

\begin{theorem}
\label{thm:lossless secure conductor log}
Let $n$ be a security parameter and let $t = c\log n$ for some constant $c$.  Let $d\in \mathbb{Z}^+$ be a constant and consider the Hamming metric with block length $b = \log (2n^3 (c+d)/d)$.  Let $W$ be uniform over $\zo^{(c+d)nb/d}$.  If GAPSVP and SIVP are hard to approximate within polynomial factors~(on lattices of dimension $n$) using quantum algorithms, then there is a setting of $q = \poly(n)$ such that \consref{cons:informal construction} is a lossless fuzzy extractor.  That is, $H^{\hill}_{\epsilon, s_{sec}}(X_{1,..., n/2}|\vA, \vA X+W) = H_\infty(W)$
% for following parameters:
%\[
%(\zo^{(c+d)/dn\log b}, |W|, \epsilon, s_{sec}, s_{sketch}, s_{rec}, t)
%\] 
with 
$\epsilon = \ngl(n), s_{sec} = \poly(n)$,  sketch runs in time $O(n^3\log n)$, and recover takes expected time $O(n^{2d+3} \log n)$ with error probability $\delta = e^{-\Omega(n)}$ over the choice of $\vA$.
\end{theorem}
\begin{proof}
Security follows by combining Lemmas~\ref{lem:uniform LWE decision} and~\ref{lem:many hardcore bits}, efficiency follows by \lemref{lem:i t poly time}.
\end{proof}

For a more detailed explanation of the various parameters and constraints see \secref{sec:parameter settings}.  In particular, we describe how to set $q = \poly(n)$.  
\thref{thm:lossless secure conductor log} shows that a computational fuzzy conductor can be built without incurring any entropy loss.  This is essentially due to $\vA X+W$ looking like a uniformly random point and thus providing no information about either $X$ or $W$.  
%A natural question to ask is whether security degrades gracefully when $W$ is not the uniform distribution.  We begin to provide an answer in \secref{sec:LWE block fixing sources}.  

\subsection{Comparison with \prg based construction}
\label{sec:prg based comparison}
In this section we to constructed a computational fuzzy extractor by making the whole process ``computational.''  Alternatively, a computational fuzzy extractor could be constructed by using an information theoretic fuzzy extractor and then applying a pseudorandom generator to the output of the fuzzy extractor.  In particular, using the same hardness of approximating GAPSVP and SIVP, Applebaum et. al. use LWE to achieve a pseudorandom generator with linear stretch~\cite{applebaum2006pseudorandom}.  Namely, consider $\vA$ as a public value, they show a setting of parameters where $f_\vA(x, e) = \vA x + e$ is a pseudorandom generator with linear stretch.  Likewise, by increasing the field size in \consref{cons:informal construction}, it is possible to output a linear factor of the number of input bits~(see \secref{sec:parameter settings}).  Both techniques are able to produce a linear stretch on the entropy of the source.  The advantage of \consref{cons:informal construction} is that losses due to error correction $O(\log t)$ and randomness extraction $O(\log 1/\epsilon)$ are not necessary.  In the case where $W$ is small, these losses may leave too few bits for a secure pseudorandom generator.  The disadvantages of \consref{cons:informal construction} are that it supports a limited number of errors and only a uniformly distributed source.  We begin to address this second problem in the next section.

\section{\consref{cons:informal construction} with Nonuniform Sources}
\subsection{\consref{cons:informal construction} with Symbol Fixing Sources}
\label{sec:LWE block fixing sources}
In this section, we show the security of~\consref{cons:informal construction} is preserved for certain high entropy distributions.  Showing security of a computational fuzzy extractor for arbitrary high min-entropy distributions is an open problem.  First we recall the notion of a symbol fixing source~(from~\cite{KZ07}): 
\begin{definition}
Let $W = (W_1,..., W_{m+\alpha})$ be a distribution where $W_i$ takes values over $\{0,1\}^b$, we say that a distribution is a $m$-\emph{symbol fixing source} if $H_\infty(W)\geq mb$ and for all $W_i$ one of the following conditions is met:
\begin{itemize}
\item $W_i \overset{d}= U_b$
\item There exists a value $c_i$ such that $\Pr[W_i = c_i] =1$.
\end{itemize}
\end{definition}
%A block fixing source is an extension of a entropy source where each bit is independently random or fixed to larger alphabets.  

We now study \consref{cons:informal construction} with a symbol fixing source.  Obviously, if $\alpha \geq n$ then the adversary knows $n$ equations with no error and $W$ can be recovered with Gaussian elimination.  We show that security degrades exponentially as symbols are fixed.
\ignore{
\begin{assumption}[LWE w/ entropic errors]
Let $n$ be a security parameter and define $m = \poly(n)$ and $q\geq 2$ be integers.  Let $W = W_1||...||W_{m+\alpha}$ be a $m$-block fixing distribution over $\{0,1\}^{(m+\alpha)\ell}$. For a vector 
Let $A\in\Fq^{(m+\alpha)\times (n+\alpha)}$ and let $x\in\Fq^{n+\alpha}$.  No PPT algorithm given $Ax+W$ can recover $x$ with probability greater than nonnegligible.
\end{assumption}
}
We now define the decisional $\LWE$ problem with symbol fixing errors is defined as follows: 
\begin{problem}[$\distLWE$ w/ symbol fixing errors]
Let $n$ be a security parameter and define $m = \poly(n)$ and $q\geq 2$ be integers.  Let $W = W_1||...||W_{m+\alpha}$ be a $m$-symbol fixing distribution over $\{0,1\}^{(m+\alpha)b}$. For a vector $X \leftarrow \Fq^n$, we denote $A_{X, W}$ be the distribution obtained by choosing a uniform $\vA\in\Fq^{(m+\alpha)\times (n+\alpha)}$ and output $\vA X + W$. The decisional learning with errors problem $\distLWE_{n+\alpha, m+\alpha, q, W}$ with symbol fixing errors is to distinguish samples chosen according $A_{X, W}$ for a uniformly random $X \in \Fq^{n+\alpha}$ from samples chosen according to the uniform distribution over $\Fq^{n+\alpha} \times \Fq$.
\end{problem}

The $\distLWE$ with symbol fixing sources is implied by standard $\distLWE$.
%\vspace{.1in}
\begin{theorem}
\label{thm:blockLWE}
Let $n$ be a security parameter and define $q, m , \alpha = \poly(n)$.  Let $\beta  = \omega(1)$, and $W$ be a $m$-symbol fixing source over $\{0,1\}^{(m+\alpha)b}$ where $\Hoo(W) \geq mb$.  The $\distLWE_{n, m,q, W}$ assumption implies the $\distLWE_{n+\alpha+\beta, m+\alpha, q, W}$ w/ symbol fixing sources problem.
\end{theorem}
We believe this statement is of interest outside our setting.  \thref{thm:blockLWE} says that $\LWE$ is still secure when a small number of $\LWE$ samples have known error.  Roughly, we need the number of variables to be security parameter larger than the number of equations with known error.  Providing a single sample with no error ``fixes'' at most a single variable.  Thus, if there are significantly more variables than samples with no error, search $\LWE$ should still be hard.  We are able to show~(a stronger result) that $\distLWE$ is still hard.  In the reduction, we argue that the additional $\alpha+ \beta$ variables allow us to explain a random value for the last $\alpha$ samples without knowledge of the other variables.  The $\beta$ parameter is the slack needed to ensure that the ``free'' variables have influence on the last $\alpha$ samples.  A formal proof is in \secref{sec:proof of block theorem}.


\thref{thm:blockLWE} allows us to construct a computational fuzzy extractor from block-fixing sources: 

%\begin{theorem}
%\label{thm:lossless block sketch}
%Let $n$ be a security parameter and let $t$ be a constant.  Consider the Hamming metric with block length $b = \log 2n^3(t+1)$.  Let $W$ be an $\alpha$-symbol fixing source over $\zo^{((t+1)n+\alpha)b}$.  There is a \lnote{instead of ``there is'' can we actually point to the to construction?}
%\[
%(\zo^{((t+1)n+\alpha)b}, \Hoo(W),\Hoo(W), \epsilon, s, s_{sketch}, s_{rec}, t)\text{-computational secure sketch}
%\]
% for $s = \poly(n), s_{sketch} = O(n^3\log n )$ and $s_{rec}= O(n^3 \log n)$.
%\end{theorem}

\begin{theorem}
\label{thm:lossless block sketch log}
Let $n$ be a security parameter and let $t = c\log n$ for some constant $c$.  Let $d\in\mathbb{Z}^+$ be a constant and consider the Hamming metric with block length $b = \log (2n^3 (c+d)/d)$.  Let $\alpha \leq n/3$, and let $W$ be $m$-symbol fixing source over $\zo^{\left((c+d)n/d+\alpha \right)b})$.  Assume GAPSVP and SIVP are hard to approximate within polynomial factors~(on lattices of dimension $n/2$) using quantum algorithms.   There is a setting of $q = \poly(n)$ such that \consref{cons:informal construction} is a lossless fuzzy extractor.  That is, $H^{\hill}_{\epsilon, s_{sec}}(X_{1,..., n/3} |\vA, \vA X+W) = H_\infty(W)$ for %the following parameters:
%\[
%(\zo^{((c+d)n/d+\alpha)b}, \Hoo(W),\Hoo(W), \epsilon, s_{sec}, s_{sketch}, s_{rec}, t)\text{-computational secure sketch}
%\]
$\epsilon = \ngl(n), s_{sec} = \poly(n)$, sketch time $O(n^3\log n)$, and expected recover time $O(n^{2d+3} \log n)$ with error probability $\delta = e^{-\Omega(n)}$ over the choice of $\vA$.
\end{theorem}

\thref{thm:lossless block sketch log} is a direct consequence of \thref{thm:blockLWE} and parameter settings~(discussed in  \secref{ssec:block params}).  The weaker entropy source means we are able to extract fewer variables~($n/3$ instead of $n/2$) and that we must work over a slightly larger field.  We could set $\alpha$ larger than $n/3$ and extract fewer hardcore dimensions.  To extract enough bits and keep the field size polynomial in $n$, $n-\alpha$ must be at least a constant in $n$.  The parameter $\beta$ does not significantly impact the asymptotic behavior but is important for smal values of $n$.

\subsection{\consref{cons:informal construction} with slightly deficient sources}
\label{sec:DP general error LWE}
In this section, we discuss how to generalize \consref{cons:informal construction} to an arbitrary high min-entropy distribution.  We will show security using a result of Micciancio and Peikert~\cite{micciancio2013hardness}.  This result allows us to argue security for an arbitrary distribution that is nearly high entropy.  This result should be seen as a complement to the proceeding sections.  It allows for a more general error distribution, but with worse parameters than \thref{thm:lossless secure conductor log}.  Also \thref{thm:lossless block sketch log} allows for more deficient sources but of a more restricted form~(symbol fixing sources).
\begin{theorem}
Let $n$ be a security parameter and let $m=cn, \ell = m-n/2$.  Let $C$ be a constant and define the parameter $\rho \geq (Cm)^{\frac{m-n/2}{n/2}} = (Cm)^{2c-1}$ and let $q$ be a prime where $\poly(n) = q\geq (4\rho)^{\frac{c}{c-1}}$.  Consider a distribution $\chi$ over the space $\{-\rho, ..., \rho \}^m$.  If $\Hoo(\chi)\geq m\log \rho$ and SIVP is hard to approximate in the worst case for quantum algorithms within an approximation factor of $\gamma=\tilde{O}(\sqrt{n} q)$~(on lattices of $n/2$ dimensions), then $\distLWE_{n, m, q, \chi}$ is $(\epsilon, s_{sec})\mbox{-}$secure, where $\epsilon = \ngl(n)$ for $s_{sec}=\poly(n)$.
\end{theorem}
\textbf{Note:}  The above theorem is a generalized version of~\cite[Theorem 4.6]{micciancio2013hardness}.  In particular, we note the requirement of sampling from the uniform distribution over some set $\chi \subset \{-\rho,..., \rho\}^m$ is not necessary.  The only requirement in the proof is that $|\chi|\geq m\log \rho$.  Requiring that $\Hoo(\chi) = m\log \rho$ implies that the support of $\chi$ contains at least $2^{m\log \rho}$ elements.  This theorem allows all distributions over a space of size $(2\rho)^m$ that have entropy at least $m\log \rho$.  This means we support all distributions whose entropy gap is no more than $m$ bits.

\bnote{Need to continue and go through their Lemma 4.1 and 4.4 to be sure I believe this.  Everything else should go through fine.}

\bnote{We need to turn this hardness result into a statement about computational fuzzy extractors.  Use the Akavia result.  Then we need to suggest parameters.  How much detail should we go into?}

The main item I think needs to change is their Lemma 2.4
\begin{lemma}~\cite[Min-Entropy Version of Lemma 2.4]{micciancio2013hardness}
Let $\mathcal{L}$ be a family of functions on a common domain $X$ and let $Z$ be a distribution over $X$.  Then $(\mathcal{L}, Z)$ is $\epsilon$-uninvertible~(even statistically, with respect to computationally unbounded adversaries) for $\epsilon = \expe_{f\leftarrow \mathcal{L}} [|f(X)|]/ 2^{-H_\infty(Z)}$.
\end{lemma}
\begin{proof}
Fix a function $f$.  The best way to predict $Z$ given $f(Z)$ is the average predictability of $Z | f(Z)$.  This predictability is $2^{-\Hav(Z | f(Z))}$.  By Lemma 2.2 of Dodis et.al.~\cite{DBLP:journals/siamcomp/DodisORS08}, 
\[
\tilde{H}_\infty(Z | f(Z))\geq H_\infty(Z) - \log |f(Z)| \geq \Hoo(Z) - \log |f(X)|.
\]
  The statement of the lemma follows by converting to predictability.
\end{proof}
%\section{Computational Fuzzy Extractors}
%In this section, we show that any computational secure sketch implies a computationally fuzzy extractor using the same paradigm as \lemref{lem:fuzzy ext construction}.  We begin with a formal definition of a computational fuzzy extractor:
%\label{sec:comp fuzzy extractors}
%
%
%Average-case extractors produce pseudorandom bits if the input distribution has high HILL entropy:
%\begin{lemma}
%Let $\ext: \chi \times \zo^d \rightarrow \zo^\ell \times \zo^d$ be a $(k, \epsilon_{ext})$-extractor computable by a circuit of size $s_{ext}$.  Let $X$ be a distribution over $\chi$ with $H^{\hill}_{\epsilon, s_{sec}}(X| Z)\geq k$.  Let, $s' = s_{sec}-s_{ext}$, then 
%$H^{\hill}_{\epsilon+\epsilon_{ext}, s_{sec}}(\ext (X, U_d) | Z, U_d)\geq \ell$.
%\end{lemma}
%%  the same paradigm from 
%%Recall, we defined computational secure sketches using unpredictability entropy~(\defref{def:comp secure sketch}).  It is not known if unpredictability entropy can be extracted from, so we cannot use the construction of \lemref{lem:fuzzy ext construction}.  However, unpredictability entropy implies Yao entropy~(\defref{def:yao entropy}) and this can be extracted from using a reconstructive extractor~(defined in~\cite{barak-computational}).  
%%\begin{definition}
%%An $(\ell, \epsilon)$-reconstruction for a function $\rext:\{0,1\}^n\times \{0,1\}^d\rightarrow \{0,1\}^m\times\{0,1\}^d$~(where the last $d$ outputs are equal to the last $d$ input bits) is a pair of machines $C$ and $D$, where $C:\{0,1\}^n\rightarrow \{0,1\}^\ell$ is a randomized Turing machine and $D^{(\cdot)}:\{0,1\}^\ell\rightarrow \{0,1\}^n$ is a randomized oracle Turing machine which runs in time polynomial in $n$.  Furthermore, for every $x$ and $T$, if $|\Pr[T(\rext(x, U_d)) =1] -\Pr[T(U_m\times U_d)=1]|>\epsilon$, then $\Pr[D^T(C^T(x))=x]>1/2$~(the probability is over the random choices of $C$ and $D$).
%%\end{definition}
%%\begin{lemma}~\cite[Lemma 6]{DBLP:conf/eurocrypt/HsiaoLR07}
%%\label{lem:unp extraction} 
%%Let $X$ be a distribution with $H^{\unp}_{\epsilon, s_{sec}}(X|Z)\geq k$, and let $\rext$ be an extractor with a $(k-\log 1/\epsilon, \epsilon)$-reconstruction $(C,D)$.  Then $\delta^{\mathcal{D}_{s'}}((\rext(X, U_d), Z), U_m\times U_d\times Z)\leq 5\epsilon,$ where $s'=s_{sec}/(|C|+|D|)$.
%%\end{lemma}
%We are now ready to provide a computational analogue of \lemref{lem:fuzzy ext construction}:
%\begin{lemma}
%Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon, s_{sec})$ hard with error $\delta$ and denote the range $\rec$ by $\chi$.  Let $\ext:\chi \times \{0,1\}^d\rightarrow \{0,1\}^\ell \times \{0, 1\}^d$ be an $(\tilde{m}, \epsilon_{ext})$ extractor.  Then the following $(\gen , \rep)$ is an $(\mathcal{M}, m, \ell, t)$-computational fuzzy extractor that is $(\epsilon + \epsilon_{ext}, s_{sec}- s_{ext})$-hard with error $\delta$.
%\end{lemma}
%%reconstructive extractor with $(\tilde{m}-\log 1/\epsilon, \epsilon)$-reconstruction $(C, D)$.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, s_{sec}/(|C|+|D|), s_{rec}, 5\epsilon)$-computational fuzzy extractor:
%%\begin{itemize}
%%\item $\gen(w;r, x)$: set $P= (\sketch(w; r), x), R = \rext(w, x)$, and output $(R,P)$.
%%\item $\rep(w', (s, x)):$ recover $w = \rec(w', s)$ and output $R = \rext(w;x)$.
%%\end{itemize}
%%\end{lemma}
%%\begin{proof}
%%Consequence of \defref{def:comp secure sketch} and \lemref{lem:unp extraction}.
%%\end{proof}
%%
%%For completeness, we describe a computational fuzzy extractor based on Construction~\ref{cons:LWESecureSketch} in \secref{sec:fuzzy extractor phrasing}.

\ignore{
Additionally, it is simple to use a pseudorandom generator to produce a computational fuzzy extractor from a standard fuzzy extractor:
\begin{proposition}\label{prop:prg comp fuzzy extractor}
Let \gen, \rep be a $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor~(where \rep is computable by a  $s_{rec}$ size circuit) and let $prg: \{0,1\}^\ell\rightarrow \{0, 1\}^{\ell'}$ be a $(\epsilon_{prg}, s)$ pseudorandom generator (computable by a circuit of size $s_{peg}$) from $\ell$ bits to $\ell'$ bits.  Let \gen' and \rep' be defined as:
\begin{itemize}
\item \gen': $(R, P) \leftarrow \gen(w)$, output $(prg(R), P)$
\item \rep': Output  $prg(\rep (w', P))$.
\end{itemize}
Then $(\rep', \gen')$ is a $(\mathcal{M}, m, \ell', t, s_{rec}+s_{prg}, s, \epsilon+\epsilon_{prg})$-computational fuzzy extractor.
\end{proposition}
Recall, our goal is to construct fuzzy extractors where the entropy drop due to an information theoretic construction were too high.  However, it seems difficult to definitionally exclude this type of construction.  For example, one could require that $R$ have information theoretic entropy in the absence of $P$.  This does not solve the problem:
\begin{proposition}\label{prop:xor comp fuzzy extractor}
Let \gen, \rep be a $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor~(where \rep is computable by a  $s_{rec}$ size circuit) and let $prg: \{0,1\}^\ell\rightarrow \{0, 1\}^{\ell'}$ be a $\epsilon_{prg}, s$ pseudorandom generator (computable by a circuit of size $s_{peg}$) from $\ell$ bits to $\ell'$ bits.  Let \gen' and \rep' be defined as:
\begin{itemize}
\item $\gen'(w)$
\subitem Sample $K\leftarrow \{0,1 \}^{\ell'}$
\subitem $(R, P) \leftarrow \gen(w)$
\subitem $H= prg(R)\oplus K$
\subitem Output $(K, (H, P))$
\item $\rep'(w', (H, P))$
\subitem Output $K' = H\oplus prg(\rep (w', P))$.
\end{itemize}
Then (\rep', \gen') is a $(\mathcal{M}, m, \ell', t, s_{rec}+s_{prg}, s, \epsilon+\epsilon_{prg})$-computational fuzzy extractor.  Furthermore $H_\infty(K) = \ell'$.
\end{proposition}

Both of these constructions rely on a crucial property $\ell$ was long enough to provide a seed to a pseudorandom generator.  We are interested in the case where the remaining entropy is small and may not sufficient for pseudorandom generator.  We leave Definition~\ref{def:comp fuzzy extractor} as stated and note constructing a computational fuzzy extractor is desirable when a standard fuzzy extractor cannot be used. 

In the information theoretic setting, Dodis et. al.~\cite{DBLP:journals/siamcomp/DodisORS08} construct fuzzy extractors by using a secure sketch to perform (secure)~error correction and a randomness extractor to convert the distribution to close to uniform.  A similar paradigm seems desirable in the computational case: can a computational fuzzy extractor can be produced by a ``computational'' secure sketch and a randomness extractor~(which convert conditional HILL entropy to pseudorandom bits~\cite[Lemma 5]{DBLP:conf/eurocrypt/HsiaoLR07}).
}
\bibliographystyle{alpha}
\bibliography{crypto}
\appendix
\section{Random Linear Codes Have High Distance}
%\subsection{Notions of Computational Entropy}
%\label{sec:notionsEntropy}
%In this sections we provide some further definitions of computational entropy.  HILL entropy is defined in \secref{sec:defCompFuzzyExtractors}.  HILL entropy generalizes the notion of indistinguishability.  Yao entropy~\cite{DBLP:conf/focs/Yao82a, barak-computational} generalizes the notion of compressibility.  Here we present the computational definition due to~\cite{DBLP:conf/eurocrypt/HsiaoLR07}:
%\begin{definition}
%\label{def:yao entropy}
%For a distribution $X, Z$ we say that $X$ has Yao entropy at least $k$ conditioned on $Z$, denoted $H^{\yao}_{\epsilon, s}(X|Z)\geq k$, if for every pair of circuits $c,d$ of total size at most $s$ with the outputs of $c$ having length $\ell$,
%\[
%\Pr_{(x,z)\leftarrow (X,Z)}[d(c(x,z),z) = x]\leq 2^{\ell-k}+\epsilon
%\]
%\end{definition}
%It should be clear that HILL entropy implies Yao entropy in both the unconditional and conditional case.  In the conditional case it is possible for Yao entropy to be significantly larger than HILL entropy~\cite{DBLP:conf/eurocrypt/HsiaoLR07}.  We also use a less common notion called unpredictability entropy.  This measures an adversaries ability to recover a secret given some side information, see \defref{def:unp entropy}.
%Unpredictability entropy is a notion between HILL and Yao entropy:
%\begin{lemma}~\cite[Lemmas 8, 9]{DBLP:conf/eurocrypt/HsiaoLR07}\label{lem:hillimplyunpimplyyao}
%\[ H^{\hill}_{\epsilon, s}(X|Z)\geq k \Rightarrow H^{\unp}_{\epsilon, s}(X|Z)\geq k\Rightarrow H^{\yao}_{\epsilon, s}(X|Z)\geq k.
%\]
%\end{lemma} 
%\subsection{Unique Witness Relations}

%We will need the ability to verify if we have a correct solution in~\consref{cons:informal construction}.  
%
%\begin{definition}
%Let $R$ be a relation.  We say that $R$ is $s_{ver}$-\emph{unique witness verifiable} if $\forall y$ there exists a unique $x$ such that $R(x, y)=1$ and $R(x, y) =1$ is computable in time $s_{ver}$.
%\end{definition}
%We can relax this definition slightly to allow a relation that is unique witness verifiable with all but negligible probability~(when $x, y$ grow with the security parameter).
%
%\begin{definition}
%\label{def:unique witness relation}
%Let $R:\mathcal{M}_1\times \mathcal{M}_2\rightarrow \zo$.  We say that $R$ is $(\epsilon, s_{ver})$-\emph{unique witness verifiable} if over $y\in \mathcal{M}_2$, drawn uniformly, the probability that $\exists x_1, x_2, x_1\neq x_2$ such that $R(x_1, y) =1 = R(x_2, y)$ is at most $\epsilon$ and $R$ is computable in time $s_{ver}$.
%\end{definition}
%
For efficient decoding of \consref{cons:informal construction}, we need the $\LWE$ instance to have high distance with overwhelming probability.  We will use the $q$-ary entropy function, denoted $H_q(x)$ and defined as $H_q(x) = x\log _q(q-1) - x\log_q x - (1-x)\log_q (1-x)$.  Note that $H_2(x) = -x\log x - (1-x)\log (1-x)$.  In the region $[0, \frac{1}{2}]$ for any value $q'\geq q$, $H_{q'}(x)\leq H_{q}(x)$.  The following theorem is standard in coding theory:

\begin{theorem}~\cite[Theorem 8]{venkatLecture}
\label{thm:random code good distance}
For prime $q, \delta\in [0, 1-1/q), 0<\epsilon< 1-H_q(\delta)$ and sufficiently large $m$, the following holds for $n = \lceil (1-H_q(\delta) - \epsilon)m\rceil$ .  If $\vA \in \Fq^{m\times n}$ is drawn uniformly at random, then the linear code with $\vA$ as a generator matrix has rate at least $(1-H_q(\delta) -\epsilon)$ and relative distance at least $\delta$ with probability at least $1-e^{-\Omega(m)}$.
\end{theorem}
Of particular, concern will be the case where $m = poly(n)\geq 2n$ and $\delta = O (\log n /n)$.  This setting of parameters satisfies \thref{thm:random code good distance}:
\begin{corollary}
\label{cor:code high distance}
Let $n$ be a parameter and let $m = \poly(n)\geq 2n$.  
Let $q$ be a prime and $t = O(\frac{m}{n}\log n )$.  For large enough values of $n$, when $\vA\in \Fq^{m\times n}$ is drawn uniformly, the code generated by $\vA$ has distance at least $t$ with probability at least $1-e^{-\Omega(m)}\geq 1-e^{-\Omega(n)}$.
\end{corollary}
\begin{proof}
Let $c$ be some constant.  Let $\delta = t/m = \frac{c\log n}{n}$.  We show the corollary for the case when $m = 2n$~(increasing the size of $m$ only increases the relative distance).  It suffices to show that for sufficiently large $n$, $1- H_q(\frac{c\log n}{n}) - \epsilon = 1/2$ or equivalently that $H_q(\frac{c\log n}{m})< 1/2$ as then setting $\epsilon = 1/2-H_q(\frac{c\log n}{n})$ satisfies  \thref{thm:random code good distance}.  For sufficiently large $n$:
\begin{itemize}
\item $\frac{c\log n}{n}< 1/2$, so we can work with the binary entropy function $H_2$.  
\item $\frac{c\log n}{n}< .1 < 1/2$ and thus $H_q(\frac{c\log n}{n})< H_q(.1)$. 
\end{itemize}  Putting these statements together, for large enough $n$, $H_q(\frac{c\log n}{n})< H_q(.1) < H_2(.1)< 1/2$ as desired.  This completes the proof.

% and note that there $\exists \epsilon$ where $0<\epsilon< 1-H_q(\frac{c\log n}{2n})< 1-H_q(\frac{c\log n}{m})$ and $1-H_q(c\log n/n) -\epsilon =1/2$ or equivalently $H_q(c\log n/n)+\epsilon = 1/2$.  Let $d$ be some constant such that $q< n^d$.  Consider the value of $H_q(\frac{c\log n}{n})$:
%\begin{align*}
%H_q(\frac{c\log n}{n})&\leq  H_{n^d}(\frac{c\log n}{n})\\
%&=\frac{c\log n}{n} \log_{n^d}(n^d-1) - \frac{c\log n}{n} \log_{n^d} (\frac{c\log n}{n}) - (1-\frac{c\log n}{n})\log_{n^d} (1-\frac{c\log n}{n})\\
%&\leq \frac{c\log n}{n}  - \frac{c\log n}{n} \log_{n^d} (\frac{c\log n}{n}) - (1-\frac{c\log n}{n})\log_{n^d} \left(1-\frac{c\log n}{n}\right)\\
%&=\frac{c\log n}{n}  - \frac{c\log n}{n} \left(\log_{n^d} (\frac{c\log n}{n})-\log_{n^d} (\frac{n-c\log n}{n}) \right)- \log_{n^d} \left(1-\frac{c\log n}{n}\right)\\
%&=\frac{c\log n}{n}  - \frac{c\log n}{n} \left(\log_{n^d} (\frac{c\log n}{n-c\log n}) \right)- \log_{n^d} \left(1-\frac{c\log n}{n}\right)\\
%&=\frac{c\log n}{n}  + \frac{c\log n}{n} \left(\log_{n^d} (\frac{n}{c\log n}-1) \right)- \log_{n^d} \left(1-\frac{c\log n}{n}\right)\\
%&=\frac{c\log n}{n}  + \frac{c\log n}{n} \left(\log_{n^d} n \right)- \log_{n^d} \left(\frac{1}{n}\right)\\
%&\leq \frac{c\log n}{n}  + \frac{c\log n}{dn} - \log_{n^d} \left(\frac{1}{n}\right)\\
%&\leq \frac{c\log n}{n}  + \frac{c\log n}{dn} + \frac{1}{d}
%\end{align*}
%Thus for large enough values of $n$, $H_{n^d}(\frac{c\log n}{n})\leq 1/2 + O(\frac{c\log n}{dn}) \leq 2/3$, thus $1 - H_q(\frac{c\log n}{n})> 1-H_{n^d}(\frac{c\log n}{n} )> 1/3$.  Taking $\epsilon  = 1/21/6$ the conditions of the corollary are satisfied.
\end{proof}

\ignore{
\section{Possibility of Constructing Computational Fuzzy Extractors}\label{ssec:constructing comp fuzzy extractors}
In the \secref{sec:imp HILL sketch}, we showed that defining a computational secure sketch using HILL entropy connects the parameters to the best constructible codes.  Thus, some entropy drop seems necessary when considering $H^\hill_{\epsilon, s}(X | SS(X))$.  However, in \propref{prop:prg comp fuzzy extractor}, we showed that a computational fuzzy extractor exists using a pseudorandom generator.  A natural question is whether this is necessary.  Do there exist clever constructions of computational fuzzy extractors that rely on the hardness of some cryptographic primitive that is easy within some distance $t$.  We begin by recalling the important parameters of a computational fuzzy extractor:
\begin{itemize}
\item The input entropy $m$.
\item The length of the output string $\ell$.  This output string is required to be $(\epsilon, s_{sec})$-pseudorandom.
\item The accepting distance $t$.
\end{itemize}
We consider the possibility of constructing a computational fuzzy extractor based on the remaining entropy of $W$ conditioned on the string $P$~(generated by $(R, P)\leftarrow \gen(W)$).  We first define a parameter $k_{prg}$.  Let $k_{prg}$ be the smallest integer such that there exists a pseudorandom generator from $k_{prg}$ to at least $k_{prg}+1$ bits.  More precisely, let $k_{prg}$ be the smallest integer such that there exists a deterministic function $prg:\{0,1\}^{k_{prg}}\rightarrow \{0,1 \}^{k_{prg}+1}$ such that $\forall D\in \mathcal{D}_s$: $\delta^D(prg(U_{k_{prg}}), U_{k_{prg}+1})<\epsilon$ for some negligible $\epsilon$ and super polynomial $s$.  We now classify the existence of computational fuzzy extractors based on this parameter.  Let $CODE$ be some error-correcting code with parameters, $(n, k, 2t+1)$.  We define $|CODE| = n-k$.

\begin{tabular}{c | c}
Remaining Entropy & Constructible?\\\hline
$CODE$ is computationally computable. & Yes, use \propref{prop:prg comp fuzzy extractor} \\
$H_\infty (W) - |CODE|\geq k_{prg}$\\\hline
$CODE$ exists, not necessarily efficiently computable. & Seems impossible to disprove, efficient\\
$H_\infty(W) - |CODE|\geq k_{prg}$ & construction of $CODE$ admits \propref{prop:prg comp fuzzy extractor}.\\\hline
Let $CODE$ be the best code. & Unknown, main area of interest\\
$H_\infty(W) - |CODE|< k_{prg}\leq H_\infty(W)$\\\hline 
$H_\infty(W)< k_{prg}$ & Seems to imply existence \\
&of improved $prg$ by using $\gen$.
\end{tabular}

We begin by focusing on the last case where $H_\infty(W) < k_{prg}$.  We first introduce two concepts, efficiently-generatable sources and sources that are $prg$ qualified.

\begin{definition}
A distribution $W$ over $\mathcal{M}$ is \emph{$(s_{gen}, \ell)$-efficiently-generatable} if there exists a deterministic circuit $G$ of size at most $s_{gen}$ such that $W \overset{d}= G(U_{\ell})$.
\end{definition}
\begin{definition}
A distribution $W$ over $\mathcal{M}$ is \emph{$(\epsilon, s, s_{f})$ - prg-qualified} if there exists a deterministic function $f:\mathcal{M}\rightarrow \{0,1\}^d$~(computable in space $s_{f}$) such that for all $D\in\mathcal{D}_s$, $\delta^D(f(W), U)<\epsilon$ and $d\geq k_{prg}$\footnote{We can extend this definition to allow $f$ to have public-randomness.  The definition is not meaningful if $f$ has private randomness as $f$ can simply generate a fresh seed of length $d$.}.
\end{definition}
We use the term prg-qualified as the composition of $f$ and a $prg$ gives pseudorandom bits:
\begin{proposition}
Let $W$ over $\mathcal{M}$ be a $(\epsilon_{W}, s_W, s_{f})$-prg-qualified distribution with function $f$ to $d$ bits.  Furthermore let $prg:\{0,1\}^k\rightarrow \{0,1\}^{k+1}$ be a $(\epsilon_{prg}, s_{sec})$- pseudorandom-generator~(computable by a circuit of size $s_{prg}$) where $k\leq d$.  Then $\forall D\in\mathcal{D}_{\min\{s_W-s_{prg}, s_{sec}\}}$
\[
\delta^D(prg(f(W)_{1,..., k}), U_{k+1})< \epsilon_W + \epsilon_{prg}.
\]
\end{proposition}
\begin{proof}
Let $W$ be a $(\epsilon_{W}, s_W, s_{f})$-prg-qualified distribution to $d$ bits.  Furthermore let $prg:\{0,1\}^k\rightarrow \{0,1\}^{k+1}$ be a $(\epsilon_{prg}, s_{sec})$- pseudorandom-generator where $k\leq d$.  We proceed by contradiction.  Suppose that $\exists D$ of size at most $s= \min\{s_W -s_{prg}, s_{sec}\}$ such that $\delta^D(prg(f(W)), U_{k+1})\geq \epsilon_W+\epsilon_{prg}$.  By the triangle inequality, one has:
\begin{align*}
\epsilon_W+\epsilon_{prg}&\leq \delta^D(prg(f(W)_{1,...,k}), U_{k+1})\\
&\leq \delta^D(prg(f(W)_{1,...,k}, prg(U_k)) + \delta^D(prg(U_k), U_{k+1})
\end{align*}
Thus, either $\delta^D(prg(f(W)_{1,...,k}, prg(U_k))\geq \epsilon_{W}$ or $\delta^D(prg(U_k), U_{k+1})\geq \epsilon_{prg}$.

In the first case, we construct a distinguisher $D'$ that contradicts the $(\epsilon_W, s_W, s_{f})$-qualification of $W$.  $D'$ upon receiving input $x\in \{0,1\}^d$ computes $D(prg(x))$ and outputs $D$'s answer.  $D'$ is of size at most $s + s_{prg} \leq s_W-s_{prg}+s_{prg} = s_W$ and has advantage at least $\epsilon_W$.  This is a contradiction.

In the second case, $D$ immediately contradicts the $(\epsilon_{prg}, s_{sec})$ security of $prg$.  Thus, we have a contradiction in both cases.  This completes the proof.
\end{proof}

We now return to the question of whether we can build a computational fuzzy extractor when $H_\infty(W)<k_{prg}$.
\begin{theorem}

\end{theorem}
\begin{proof}
\bnote{Do the proof!}
\end{proof}
}
%We now provide a candidate construction of fuzzy extractor based directly on a computational problem~(the LWE problem introduced by Regev~\cite{regevLWEsurvey}).
%\begin{definition}
%An $(\mathcal{M}, m, \tilde{m}, t, s_{dist}, \epsilon)$-\emph{computational secure sketch} is a pair of randomized algorithms, ``sketch'' (\sketch) and ``recover'' (\rec) with the following properties:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns two strings, $(x,s)\in\{0,1\}^*$ where $x$ is drawn from a distribution $X$ and $\Hoo(X)\geq m$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of a computational secure sketches guarantees that if $d(w,w')\leq t$, then $Rec(w', SS(w)_2) = SS(w)_1$.  
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, observing $SS(W)_2$ does not significantly help the adversary.  That is, $H^\hill_{\epsilon, s_{dist}}(X | SS(W)_2)\geq \tilde{m}$.
%\end{enumerate}
%A secure sketch is \emph{efficient} if \sketch runs in expected polynomial time and \rec runs in expected polynomial time if $d(w', w) < t$.
%\end{definition}
%This definition has two significant changes from the definition of a secure sketch.  First, we do not require that the distribution $X$ is equal to the input distribution $W$.  This is because constructions will utilize computationally hard problems that may be difficult to invert.  However, we can consider the special case where $X=W$.  None of the power of a secure sketch is lost when $X\neq W$.

%\textbf{Note: }  The entropy of $X$ is somehow limited by the entropy of $X$ we cannot completely hide some very high entropy string, using a low entropy string.  This would have to be reflected in the parameters of \hill entropy but I am not yet sure how to formalize this.
%
% Furthermore, note there are two conditions on the distribution $X$, $\Hoo(X)\geq m$ and $H^\hill_{\epsilon, s}(X|SS(W)_2)\geq \tilde{m}$.  This means that if an adversary does not see the helper value $s$ it has true entropy and its computational entropy is decrease by seeing $s$.  

%Second, we now require that the value $X$ only has conditional \hill entropy.  Considering the special case where $m=\tilde{m}$ is interesting but constructions that improve on the $\tilde{m}$ achieved in secure sketch are interesting.  Furthermore, it seems natural to consider the case where $|\rec|<s$ however, \rec has a significantly more difficult task, \rec must recover all of $x$ while the distinguisher in the conditional \hill entropy definition only needs to distinguish $x$ from a high average min-entropy distribution.
%
%Thus, we can define a stronger definition where all of these desiderata are achieved.  
%
%\begin{definition}
%A $(\mathcal{M}, m, \tilde{m}, t, s, \epsilon)$-computational secure sketch is strong if the following properties hold:
%\begin{enumerate}
%\item Seeing $s$ does not decrease the conditional $\hill$ entropy of $X$.  That is, $m=\tilde{m}$.
%\item There exists a circuit of \rec' where the size of \rec' is polynomial, $|\rec'|<s_{dist}$ and \rec' satisfies the correctness property.
%\end{enumerate}
%\end{definition}
\ignore{
\subsection{Learning With Errors (\class{LWE})}
\label{subsec:LWE in detail}
The LWE problem was introduced by Regev \cite{regev2005LWE, regevLWEsurvey} as a generalization of ``learning parity with noise". For positive integers $n$ and $q \ge 2$, a vector $\vect{s} \in \Fq$, and a probability distribution $\chi$ on $\Fq$, let $A_{\vect{s}, \chi}$ be the distribution obtained by choosing a vector $\vect{a} \overset{\$}\leftarrow \Fq^n$ uniformly at random and a noise term $e \overset{\$}\leftarrow \chi$, and outputting $(\vect{a}, \langle \vect{a}, \vect{s}\rangle + e) \in (\Fq^m, \Fq)$. A formal definition follows.


\label{sec:LWEoverview}
\begin{problem}[\class{LWE}]\label{prob:LWE}
Let $n$ be the security parameter. For a integer $q = q(n)$ and the error distribution $\chi = \chi(n)$ over $\Fq$, the Learning With Error problem $\class{LWE}_{n, m, q,\chi}$ is defined as follows: given access to an oracle that outputs $m$ samples from $A_{\vect{s}, \chi}$ for a uniformly random $\vect{s}$, output $\vect{s}$ with high probability.
\end{problem}

We will use the fixed form of the LWE assumption where a matrix is prepared ahead of time.  The matrix form is as follows: $\class{LWE}$ find $\vect{x}$ given $(\vect{A},\vect{Ax+e})$, where $\vect{A}\leftarrow \Fq^{m \times n}$ is chosen uniformly and the error vector $\vect{e} \leftarrow \chi$.
Results of Regev~\cite{regev2005LWE} and Peikert~\cite{peikert2009latticereduction} connect the hardness of the LWE problem to solving worst-case lattice problems.  The matrix $\vA$ and secret $\vx$ are drawn from the uniform distribution, while the error term is drawn from a discretized Gaussian distribution $\bar{\Psi}_\alpha$~(following the notation of~\cite{regev2005LWE}).  We denote by $\bar{\Psi}_\alpha$ the discretized Gaussian distribution over $\Fq$ with variance $(\alpha q)^2/(2\pi)$.  Here we present the theorem of~\cite{regev2005LWE}.

\begin{theorem}~\cite[Theorem 3.1]{regev2005LWE}
Let $n$ be a security parameter and $q = q(n) \in\Fq^+$, let $\alpha = \alpha(n)\in (0,1)$ be such that $\alpha q > 2\sqrt{n}$.  Let $\vA\in\Fq^{m\times n}, X\in \Fq^n$ be chosen uniformly at random and $E$ be chosen according to $\bar{\Psi}_\alpha$.  If there exists a PPT-algorithm $\mathcal{I}$ such that $\Pr_{\vA, X, E}[X\leftarrow  \mathcal{I}(\vA, \vA X+E) ]>\epsilon$ for some noticeable $\epsilon$, then there exists an efficient quantum-algorithm that approximates the decision-version of the shortest vector problem~(GAPSVP) and the shortest independent vectors problem~(SIVP) to within $\Omega(n/\alpha)$ in the worst case.
\end{theorem}

We will consider a generalized assumption of \probref{prob:LWE}:
\begin{assumption}
\label{assume:general LWE}
Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ and $q=q(n)\geq 2$ be integers and let $\chi$ be a distribution on $\Fq$.  If $X\in\Fq^n, \vA\in\Fq^{m\times n}$ are chosen uniformly at random and $E$ is chosen according to $\chi$, then for all $\mathcal{I}$ of size at most $s$
\[
\Pr_{\vA, X, E}[X\leftarrow \mathcal{I}(\vA, \vA X+E)]<\epsilon
\]
\end{assumption}
Furthermore, Regev shows a decision to search equivalence for $q=\poly(n)$, so we can consider an equivalent indistinguishability formulation:
\begin{assumption}
\label{assume:indist LWE}
Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be prime and let $\chi$ be a distribution on $\Fq$.  If $X\in\Fq^n, A\in\Fq^{m\times n}, B\in\Fq^m$ are chosen uniformly at random and $E$ is chosen according to $\chi$, then for all $\mathcal{D}$ of size at most $s$:
\[
\Pr[D(\vA, \vA X+E)=1]-\Pr[D(\vA, B)=1]<\epsilon
\]
\end{assumption}
We can restate Assumption~\ref{assume:indist LWE} in terms of unpredictability entropy~(see Definition~\ref{def:unp entropy}).  
}

%\section{\lemref{lem:conversion to unpredictability}}
%\label{sec:proof of unpred conversion}
%In this section we show that for an indistinguishable LWE instance, the unpredictability entropy of $E$ conditioned on $\vA X+E$ remains high.  %We first note that any inverter that can recover $X$ can recover $E$ and vice versa by using simple a single addition and multiplication~(and vice versa).  Thus, we have the following claim:
%%\begin{claim}
%%\label{lem:unp of x and e}
%%Let $A, X, E$ be as above, then:
%%\begin{itemize}
%%\item $H^{\unp}_{\epsilon, s}(X| \vA, \vA X+E)\geq k \Rightarrow H^{\unp}_{\epsilon, s'}(E| \vA, \vA X+E) \geq k$ for $s'\approx s$.
%%\item $H^{\unp}_{\epsilon, s}(E | \vA, \vA X+E) \geq k \Rightarrow H^{\unp}_{\epsilon', s'}(X | \vA, \vA X+E) \geq k'$ for $k'\approx k, \epsilon' \approx \epsilon, s'\approx s$. 
%%\end{itemize}
%%\end{claim}
%%In the body of the text we use \lemref{lem:unp of x and e} to present  \assref{assume:indist LWE} in terms of unpredictability entropy~(\assref{assume:entropy LWE}).
%%We now restate Assumption~\ref{assume:indist LWE} in terms of unpredictability entropy:
%%\begin{assumption}
%%\label{assume:entropy LWE}
%%Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be prime and let $\chi$ be a distribution on $\Fq$.  If $X\in\Fq^n, A\in\Fq^{m\times n}$ are chosen uniformly at random and $E$ is chosen according to $\chi$, $H^{\unp}_{\epsilon, s}(X | A, AX+E) =H^{\unp}_{\epsilon, s'}(E| A, AX+E) \geq m\log q $ for $s'\approx s$.  
%%\end{assumption}
%%For clarity, we delay the proof of \clref{lem:unp of x and e} until the end of this section~(the proof is quite straightforward).
%We first recall a result of Akavia, Goldwasser, and Vaikuntanathan~\cite{akavia2009}.  This says that if $\distLWE$ is hard then $X$ has many hardcore bits.  We state their result for a general error distribution~(noting their proof does not consider the error distribution anywhere):
%\begin{lemma}\protect{\cite[Lemma 2]{akavia2009}}
%\label{lem:many hardcore bits}
%Let $n>0$ be an integer, $q\geq 2$, let $\chi$ be an error distribution over $\zq$ and $k\leq n$ be an integer.  Let $\vA\in \zq^{m\times n} , X\in\zq^n$ and $E$ be distributed according to $\chi^m$.  Then $H^{\hill}_{\epsilon, s_{sec}}(X_{1,..., k}| \vA, \vA X+E )= k\log q$ assuming $\distLWE_{(n-k, m, q, \chi)}$.
%\end{lemma}
%
%\begin{proof}[Proof of \lemref{lem:conversion to unpredictability}]
%Assume that $\distLWE_{(n-k, m, q, \chi)}$ is $(\epsilon, s_{sec})$ hard and that $m\geq n$.  Let $\vA\in \zq^{m\times n}, X\in \zq^n$ and $E$ be distributed according to $\chi^m$.  Then according to \lemref{lem:many hardcore bits} we know that $H^{\hill}_{\epsilon, s_{sec}}(X_{1,..., k} | \vA , \vA X+E)\geq k\log q$.  By \lemref{lem:hillimplyunpimplyyao}, we know that $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k} | \vA, \vA X+E)\geq k\log q$.  To expand to $E$ we use the following claim:
%
%\begin{claim}
%Let $\vA, X, E$ be distributed as in \lemref{lem:conversion to unpredictability} and $m\geq n$ and let $k\leq n$.  Then 
%\[
%H^{\unp}_{\epsilon', s_{sec}'} ( E | \vA X+ E) \geq H^{\unp}_{\epsilon, s_{sec}} ( X_{1,..., k} | \vA X+E)
%\]
%\end{claim}
%%\begin{claim}
%%If $H^{\unp}_{0, s}(X_{1,..., k} | Z) \geq k$ then $H^{\unp}_{0, s}(X|Z)\geq k$.
%%\end{claim}
%%\begin{proof}
%%Suppose that $H^{\unp}_{0, s}(X|Z)< k$ that is there exists a circuit $C$ of size at most $s$ such that $\Pr[C(Z) = X]> 2^{-k}$.  Then let $C'$ be a circuit that simply takes the output of $C$ and returns the first $k$ bits.  Then $\Pr[C'(Z) = X_{1,...,k}]\geq \Pr[C(Z) = X] > 2^{-k}$.  This completes the claim.
%%\end{proof}
%
%%\begin{claim}
%%If $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k} | Z) \geq k$ then $H^{\unp}_{\epsilon, s_{sec}'}(X|Z)\geq k$ for $s_{sec} \approx s_{sec}'$.
%%\end{claim}
%%\begin{proof}
%%Assume that $H^{\unp}_{\epsilon, s_{sec}}(X_{1,...,k}|Z)\geq k$.
%%Suppose that $H^{\unp}_{\epsilon, s_{sec}'}(X|Z)< k$.  Let $Y$ be a distribution such that $\delta^{\mathcal{D}_{s_{sec}'}}((X, Z), (Y, Z))\leq \epsilon$.  Furthermore, let $C$ be a circuit of size at most $s_{sec}'$ where $\Pr[C(Z) = Y] > 2^{-k}$.  Define the circuit $C'$ that outputs the first $k$ bits of $C$'s output.  Thus, $\Pr[C'(Z) = Y_{1,..., k}] \geq \Pr[C(Z) = Y] > 2^{-k}$.  Note that $\delta^{\mathcal{D}_{s_{sec}}}((X_{1,...,k}, Z), (Y_{1,..., k}, Z))<\delta^{\mathcal{D}_{s_{sec}'}}((X, Z), (Y, Z))\leq \epsilon$.  Together, these two facts contradict $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k}|Z)\geq k$.
%%This completes the claim.
%%\bnote{This claim isn't accurate because we can't show for all $Y_{1,..., k}$.  In order to complete the proof we need to show that $\forall Y_{1,..., k}$ such that $(X_{1,..., k}, Z), (Y_{1,...k}, Z)<\epsilon$ there exists an extension which is also indistinguishable.}
%% that is there exists a circuit $C$ of size at most $s$ such that $\Pr[C(Z) = X]> 2^{-k}$.  Then let $C'$ be a circuit that simply takes the output of $C$ and returns the first $k$ bits.  Then $\Pr[C'(Z) = X_{1,...,k}]\geq \Pr[C(Z) = X] > 2^{-k}$.  This completes the claim.
%%\end{proof}
%
%%\begin{claim}
%%If $H^{\yao}_{\epsilon, s_{sec}}(X_{1,..., k} | Z)\geq k$ then $H^{\yao}_{\epsilon , s_{sec}'}(X|Z)\geq k$ for $s_{sec}\approx s_{sec}'$.
%%\end{claim}
%%Suppose that $H^{\yao}_{\epsilon, s_{sec}}(X_{1,..., k} |Z ) \geq k$.  This implies $\exists c, d$ of total size $s_{sec}$ where $c$ has outputs of length $\ell$ such that $\Pr[d(c(X_{1,..., k}, Z), Z) = X_{1,..., k}] \leq 2^{\ell -k} + \epsilon$.  Furthermore, suppose $\forall c', d'$ of total size $s_{sec}$ where $c$ has output length $\ell$ and $\Pr[d(c(X), Z), Z) = X] > 2^{\ell-k} +\epsilon$.
%%
%%
%%Thus, we know that $H^{\unp}_{\epsilon, s_{sec}'}(X | \vA, \vA X+E)\geq k\log q$.  By \lemref{lem:unp of x and e} we know that $H^{\unp}_{\epsilon, s_{sec}'} (E | \vA, \vA X+E)\geq k\log q$.  This completes the proof of \lemref{lem:conversion to unpredictability}.
%%\end{proof}
%\begin{proof}
%Suppose that $H^{\unp}_{\epsilon', s_{sec}}( E | \vA X+E) < \ell$.  We will show that $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k} | \vA X+E) < \ell$.  That is, $\forall Y$ such that $\delta^{\mathcal{D}_{s_{sec}'}}((E, \vA X+E), (Y, \vA+E))\leq \epsilon$, there exists $C$ of size $s_{sec}'$ such that $\Pr[C(\vA, \vA X+E) = Y]> 2^{-\ell}$.  Fix one such $Y$.  Let $\mathcal{E}$ be the event that $A_{1,..., n}^{-1}$ exists.  Note that $\mathcal{E}$ occurs with probability $1-\gamma$.
%
%Define the distribution $Y' =A^{-1}_{1,...,n}(\vA X+E -Y)$.  Then one has that 
%\begin{align*}
%\delta^{\mathcal{D}_{s_{sec}}}((Y', \vA, \vA X+E ), (X, \vA, \vA X+E))&\leq \delta^{\mathcal{D}_{s_{sec}}}((Y', \vA, \vA X+E ), (X, \vA, \vA X+E) |\mathcal{E} )+ \gamma\\
%&\leq \delta^{\mathcal{D}_{s_{sec}}}(\vA^{-1}_{1,..., n}(\vA X+E - Y), \vA, \vA X+E), (X, \vA, \vA X+E))+\gamma\\
%&\leq \delta^{\mathcal{D}_{s_{sec}'}}((Y, \vA, \vA X+E), (E, \vA, \vA X+E))+\gamma\leq \epsilon' + \gamma
%\end{align*}
%Where the second to last inequality follows by applying the transform $\vA X+E - \vA(\cdot)$ to the first argument.  Note  that $\delta^{\mathcal{D}_{s_{sec}}}((Y'_{1,...k}, \vA, \vA X+E ), (X_{1,..,k}, \vA, \vA X+E))\leq \epsilon' + \gamma$. Otherwise there would be a distinguisher for the entire distribution by just ignoring the last $n-k$ dimensions.
%
%We now show that $\exists C'$ such that $\Pr[C'(\vA, \vA X+E) = Y']> 2^{-k}$.  Define $C'$ as follows:
%\begin{enumerate}
%\item Input $(\vA, \vA X+E)$.
%\item Run $y\leftarrow C(\vA, \vA X+E)$.
%\item Try to compute $\vA^{-1}_{1,.., n}$, if $\vA^{-1}_{1,..., n}$ does not exist output $\perp$.
%\item Output $z = \vA^{-1}_{1,..., n}(\vA X+E -y)$
%\end{enumerate}
%Observer that $\Pr[C'(\vA, \vA X+E) = Y']> 2^{-k} - \gamma$.  Finally note that $\exists C''$ that simply outputs the first $k$ dimensions of the output of $C'$.  Thus, $H^{\unp}_{\epsilon, s_{sec}}( X_{1,..., n}| \vA , \vA X+E)< 2^{-k} - \gamma$.
%\end{proof}
%
%The statement of \lemref{lem:conversion to unpredictability} follows by the statement of the claim.
%%\begin{proof}[Proof of \clref{lem:unp of x and e}]
%%First suppose that $H^{\unp}_{\epsilon, s}(X| \vA, \vA X+E) < k$.  That is, $\exists I \in\mathcal{D}_s$ such that for exists $Y$ where $\delta^{\mathcal{D}_s}((X, \vA, \vA X+E), (Y, \ A, \vA X+E)\leq \epsilon$, $\Pr[I(\vA, \vA X+E) = Y] > 2^{-k}$.  Define $I'$ by the following:
%%\begin{enumerate}
%%\item Input $(\vA, \vA X+E)$.
%%\item Run $y\leftarrow I(\vA, \vA X+E)$.
%%\item Output $z = \vA y - \vA X+E$.
%%\end{enumerate}
%%Note that $|I'|\approx |I|$.
%%It suffices to show that $\delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (Z, A, \vA X+E))\leq \epsilon$.  One has the following:
%%\begin{align*}
%%\delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (Z, \vA, \vA X+E))&=
%%\delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (\vA Y-\vA X+E, \vA, \vA X+E))\\
%%&\leq \delta^{\mathcal{D}_s'}((\vA X, \vA, \vA X+E), (\vA Y, \vA, \vA X+E))\\
%%&\leq \delta^{\mathcal{D}_s'}((X, \vA, \vA X+E), (Y, \vA, \vA X+E)) \leq \epsilon.
%%\end{align*}
%%Now suppose that $H^\unp_{\epsilon, s}(E | A, \vA X+E)<k$.  That is, $\exists I \in\mathcal{D}_s$ such that for exists $Y$ where $\delta^{\mathcal{D}_s}((E, \vA, \vA X+E), (Y, \vA, \vA X+E)\leq \epsilon$, $\Pr[I(\vA, \vA X+E) = Y] > 2^{-k}$.  Define $I'$ by the following:
%%\begin{enumerate}
%%\item Input $(A, \vA X+E)$.
%%\item Run $y\leftarrow I(\vA, \vA X+E)$.
%%\item Try to compute $\vA^{-1}$, if $\vA^{-1}$ does not exist output $\perp$.
%%\item Output $z = \vA^{-1}(\vA X+E-y)$.
%%\end{enumerate}
%%Note that $|I'|\approx |I|$.  For sufficient parameter sizes the probability of $A^{-1}$ not existing is negligible.  Denote this probability by $\gamma$.  Then note that $\Pr[I'(\vA, \vA X+E) = Z] > 2^{-k}-\gamma = 2^{-k'}$.  Then one has the following~(conditioned on the event that $\vA^{-1}$ exists):
%%\begin{align*}
%%\delta^{\mathcal{D}_s'} ((X, \vA, \vA X+E), (Z, \vA, \vA X+E))&=
%%\delta^{\mathcal{D}_s'} ((X, \vA, \vA X+E), (\vA^{-1}(\vA X+E-Y), \vA, \vA X+E)) \\
%%&\leq \delta^{\mathcal{D}_s'} ((\vA^{-1}E, \vA, \vA X+E), (\vA^{-1}Y, \vA, \vA X+E))\\
%%&\leq \delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (Y, \vA, \vA X+E))\leq \epsilon
%%\end{align*}
%%Since $A^{-1}$ exists with probability $1-\gamma$ we obtain total distance at most $\epsilon(1-\gamma)=\epsilon'$.
%%\end{proof}
%
%\end{proof}

\ignore{
\section{Proof of Theorem \ref{thm:blockLWE}}
\label{sec:proof of block theorem}

%\bnote{We need to be clear about notation here.  Why are $x, e$ sometimes bold and sometimes capital?} 
\begin{proof}
We assume that all of the fixed blocks are located at the end and their fixed value is $0$, that is, $W_{m+1}, ..., W_{m+\alpha} =0$.  If the blocks are fixed to some other value the reduction is essentially the same.

%It should be clear to the reader where the reduction differs for another block fixing source.\lnote{I don't like the phrasing; maybe we should say that if the blocks are fixed to some non-0 value, the reduction is essentially the same}

Let $\D$ be a distinguisher that can distinguish $(m+\alpha)$ $\LWE$ instances from uniform where $\alpha$ of the equations have no error added.  That is, let $\vA$ be the uniform distribution over $\Fq^{(m+\alpha)\times(n+\alpha)}$, let $\vx$ be uniform over $\Fq^{(n+\alpha)}$, $\ve= (e_1,..., e_{m+\alpha})$ where $e_1,..., e_m$ are generated from error distribution $\chi$ and $e_{m+i} =0$ for $i=1,..,\alpha$. Furthermore, let $\vu$ be the uniform distribution over $\Fq^{m+\alpha}$.  We assume that, 
\[
|\Pr[\D(\vA, \vA\vx+\ve) = 1] - \Pr[\D(\vA, \vu) =1]|> \epsilon.
\]

We build a distinguish $\D'$ that distinguishes standard LWE instances from uniform.  Let $\vA'$ be uniform over $\Fq^{m\times n}$, $\vx'$ be uniform over $\Fq^n$ and $\ve'$ be drawn from $\chi^m$.  Our goal is to prepare a random block fixing instance from the random instance we are given.  The code for $\D'$ is given in \figref{fig:perfectLWEreduction}.

It suffices to show that $\D'$ properly prepares a random instance for the distinguisher.  We show this in three claims: 
\begin{enumerate}
\item If $\vA'$ is a random matrix then $\vA$ is a random matrix.
\item If $\vb'$ is uniformly distributed conditioned on $\vA'$ then $\vb$ is uniformly distributed conditioned on $\vA$.
%\bnote{This should be if $\vb' | A$ is uniformly distributed, then $\vb |A $ is uniformly distributed.}
\item If $\vb' = \vA'\vx'+\ve'$ then $\exists \vx$~(uniformly distributed) such that $\vb = \vA \vx + \ve'$ where the last $\alpha$ coordinates of $\ve'$ are $0$ and $\ve_i' = \ve_i$ for $1\leq i\leq m$.
\end{enumerate}
\begin{figure}
\begin{enumerate}
\item Input $\vA', \vb'$, where $\vA' \overset{\$} \leftarrow \Fq^{m\times n}$ and $\vb'$ is either uniform over $\Fq^m$ or $\vb' = \vA'\vx' +\ve$.
\item Randomly generate $\vect{R} \overset{\$}\leftarrow \Fq^{\alpha \times n}$.
\item Let $\vb^* = (\vb', b^*_{m+1}, \ldots,b^*_{m+\alpha})$, where $(b^*_{m+1}, \ldots, b^*_{m+\alpha} )\overset{\$} \leftarrow \Fq^\alpha$.

\item Initialize $\vA^*  = \left(\begin{array}{c | c}\vA' & \mathbf{0}\\\hline \vect{R} & \vect{I}\end{array}\right)$.
\item \label{step:randomization}
For {$i=1\ldots m+\alpha$}:
\subitem Randomly generate $(\gamma_{i,1},\ldots, \gamma_{i,\alpha}) \leftarrow \Fq^\alpha$.
 \subitem For {$j = 1 \ldots n+\alpha$}:
\subsubitem Set $A_{ij} \leftarrow A^*_{ij}+\sum_{k=1}^\alpha \gamma_{i, k} A^*_{(m+k)j}$
\subsubitem Set $b_i \leftarrow b^*_i +  \sum_{k=1}^\alpha\gamma_{i, k} b^*_{m+k}$
\item Run $\D$ on input $\vA, \vb$.
\item Output $\D$'s output.
\end{enumerate}
\caption{A PPT $\mathcal{D'}$ that distinguishes LWE using distinguisher for LWE w/ block fixing source}
\label{fig:perfectLWEreduction}
\end{figure}

\begin{claim}
\label{cl:randomMatrixDist}
The matrix $\vA$ generated after Step \ref{step:randomization} is a random matrix.
\end{claim}
\begin{proof}
In the randomized matrix $\vect{A}$, for $1\le i \le m$ and $n+1\le j \le n+\alpha$, $A_{ij}$ is assigned the value $\gamma_{i,(j - n)}$, which is a random value.  Similarly, for $m+1\le i \le m+\alpha$ and $n+1\le j \le n+\alpha$, each entry $A_{ij}$ is random due to the original construction of the matrix $\vA^*$.  Then before step \ref{step:randomization} the matrices $\vA', \vect{R}$ are random submatrices.  Thus, it remains to show that $\vA', \vect{R}$ remain random after step 5. For $1\le i \le m+\alpha$ and $1\le j \le n$, by construction $A_{ij} = A_{ij}'+\sum_{k=1}^\alpha \gamma_{i, k} R_{kj}$, which is a truly random number. Overall, each row vector $\va_{i}$ in $\vA$ is the sum of a random vector and a random linear combination independent vectors. Therefore, the entire matrix $\vA$ is a truly random matrix.  This completes the claim.
\end{proof}
\begin{claim}
\label{cl:random ax+e}
If $\D'$ is provided with input distributed as $\vA', \vA'\vx'+\ve'$ then $\vb = \vA \vx+\ve$ where $x_i = x_i'$ for $1\leq i \leq n$ and $x_{n+i} = b_{m+i}^*$ is uniformly generated otherwise and  $e_i = e_i'$ for $1\leq i\leq m$ and $e_i = 0$ for $m<i\leq m+\alpha$.
\end{claim}
\begin{proof}
Before step \ref{step:randomization} the claim is satisfied.  Denote by $\vx'$ the value such that $\vA'\vx'+\ve = \vb$.  
Then define $\vx^*$ as:
\[
x^*_i = \begin{cases}
x_i', & \text{if }1\leq i\leq n \\
b_{m-n+i}^*-\sum_{j=1}^n R_{i, j} x_j', & n+1\leq i\leq n+\alpha\,.
\end{cases}
\]
Then $\vx^*$ is uniformly random and $\vb^* = \vA^*\vx^* + \ve^*$~(where $\ve^*$ is $\ve'$ with $\alpha$ 0s appended).  Thus, it remains to show that step~\ref{step:randomization} preserves a solution.
%For convenience, denote by $\vx^*$ the vector where $x_i^* = x_i'$ for $1\leq i \leq n$ and $x_{n+i}^* = b_{m+i}^*$ otherwise.  
For a matrix $\vA$ we will denote the $i$-th row by $\va_i$~(similarly for the matrix formed by $\gamma_{i,j}$).  Thus it suffices to show for all rows $i$, if $b_i^* = \va^*_i \vx^*+e^*_i$  then $b_i = \va_i \vx^* + e^*_i$.
%, we have added $\alpha$ equations as well as $\alpha$ unknowns, furthermore, those $\alpha$ equations have no error.  The number of unknowns we added are $x_{n+i} = b^*_{m+i} - \sum^{n}_{j=1}R_{ij}x_j$ for $i=n+1,..., n+\alpha$.
\bnote{We need to justify these steps better.}
\bnote{Is it really standard to go to lower case for the rows?}
\xnote{lower case for vectors, which is row vector in the matrix}


\xnote{Needs to be clean up the $\gamma$, it's not correct, at least not consistent with the randomized algorithm.}
We have the following for all $i=1,..., m+\alpha$:
\begin{align*}
\va_i \vx + e_i &= \sum_{j=1}^{n+\alpha} \vA_{ij} x_j + e_i\\
& = \sum_{j=1}^{n+\alpha}\left(\left(\vA^*_{ij}+\sum_{k=1}^\alpha \gamma_{i, k} \vA^*_{(m+k)j}\right)x_j\right) + e_i\\
& = \sum_{j=1}^{n+\alpha}\vA^*_{ij}x_j + e_i + \sum_{j=1}^{n+\alpha}\sum_{k=1}^\alpha \gamma_{i, k} \vA^*_{(m+k)j}x_j \\
& = \sum_{j=1}^{n+\alpha}\vA^*_{ij}x_j + e_i + \sum_{k=1}^\alpha  \gamma_{i, k} \sum_{j=1}^{n+\alpha} \vA^*_{(m+k)j}x_j\\
& = \sum_{j=1}^{n+\alpha}\vA^*_{ij}x_j + e_i + \sum_{k=1}^\alpha \gamma_{i, k} \left(\left( \sum_{j=1}^n \vR_{i, j}x_j^* \right) + x_{n+k}^*\right)\\
& = \sum_{j=1}^{n+\alpha}\vA^*_{ij}x_j + e_i + \sum_{k=1}^\alpha\gamma_{i, k} b_{m+k}^*\\
&= b_i^* +\sum_{k=1}^\alpha\gamma_{i,k} b_{m+k}^*= b_i
\end{align*}
%\begin{align*}
%\vA_i \vx + e_i &= \sum_{j=1}^{n+\alpha} A_{ij} x_j +e_i = \sum_{j=1}^{n+\alpha}(A^*_{ij}x_j+\sum_{k=1}^\alpha \gamma_{i, k} A^*_{(m+k)j}x_j)+e_i\\
%& = \sum_{j=1}^{n+\alpha}(A^*_{ij}x_j)+e_i +\sum_{k=1}^\alpha \gamma_{i, k} \sum_{j=1}^{n+\alpha} (A^*_{(m+k)j}x_j)\\
%& = b^*_i +  \sum_{k=1}^\alpha\gamma_{i, k} b^*_{m+k} = (\va_i^* \vx + e_i) + \sum_{k=1}^\alpha\gamma_{i, k} (\va_{m+k}^* \vx + e_i)\\
%& = b_i \label{reductionEq}
%\end{align*}
\end{proof}
\begin{claim} 
\label{clm:random b}
If $\D'$ is provided with the input distributed as $\vA', \vb'$ where $\vb'$ is uniformly random, then $\vb$ is uniformly random conditioned on $\vA$.
\end{claim}
\begin{proof}
Before step~\ref{step:randomization}, each $b_i^*$ is uniformly and independently random.  After step~\ref{step:randomization}, since the $b_i*$ and $\gamma_i$ are random, then each $b_i$ is a fresh random linear combination of random values:
\[
b_i = b_i^* + \sum_{k=1}^\alpha \gamma_i b_{m+k}^*.
\]
%We also need to show that $\vb$ is uniformly distributed even conditioned on $\vA$. Step~\ref{step:randomization} randomized each 

$\vb$ doesn't dependent on $\vA$ simply because $\vb'$ doesn't dependent on $\vA'$ \xnote{It need show more work on this, maybe?}
Thus, each of these values is uniformly and independently random.
\end{proof}

%Let $\tilde{\vA} = (\sum_{k=1}^\alpha \gamma_{1, k} \va^*_{m+1}, \ldots, \sum_{k=1}^\alpha \gamma_{1, k} \va^*_{m+1})$ and $\tilde{\vb} = (\sum_{k=1}^\alpha\gamma_{1, k}b^*_{m+1}, \ldots, \sum_{k=1}^\alpha\gamma_{1, k} b^*_{m+\alpha})$. 
%
%The equation above gives us $\vA = \vA' + \tilde{\vA}$ and $\vb = \vA'\vx + \ve + \tilde{\vb}$. Given a uniform $\vu \overset{\$}\leftarrow\Fq^m$ and a $\LWE$ instance $\vA'\cdot \vx + \ve$, the probability that the distinguisher $\D'$ can distinguish a uniform instance from a $\LWE$ instance, when given access to the distinguisher $\D$ is given below:
%\begin{align*}
%&\left|\Pr\left[\D'(\vA', \vu') = 1\right]- \Pr\left[\D'(\vA', \vA'\vx + \ve)=1\right]  \right|\\
%& =\left|\Pr\left[\D(\vA'+ \tilde{\vA}, \vu' + \tilde{\vb})=1\right]- \Pr\left[\D(\vA' + \tilde{\vA}, \vA'\vx + \ve + \tilde{\vb})=1\right]  \right|\\
%&= \left| \Pr\left[\D(\vA, \vu)=1\right]- \Pr\left[\D(\vA, \vA\vx + \ve)=1\right]  \right| \ge \epsilon
%\end{align*}

\begin{claim}
If the pair $(\vA', \vb')$ is uniformly distributed, then the pair $(\vA, \vb)$ is uniformly distributed.
\end{claim}
\begin{proof}
Assume that $(\vA', \vb')$ is uniformly distributed.  By \clref{cl:randomMatrixDist}, $\vA$ is uniformly distributed.  Thus, it suffices to show that $\vb | \vA$ is uniformly distributed.  Let $\vb^*$ be the vector generated in step~\ref{step:b generation} and note that $\vb^*$ is uniformly random.  
We show  a stronger claim that $\vb |( \vA, \vA', \vR)$ is uniformly distributed.  Let $\vgamma$ represent the matrix formed by $\gamma_{i,j}$.  
 
 The matrix $\vgamma$ is a deterministic function of $(\vA, \vA', \vR)$.  Thus, $\vb | (\vA, \vA', \vR) \overset{d}= \vb | (\vA, \vA', \vR, \vgamma)$.  The only dependence of $\vb$ on $(\vA, \vA', \vR, \vgamma)$ is in the matrix $\vgamma$.  Thus, $ \vb | (\vA, \vA', \vR, \vgamma) \overset{d}= \vb | \vgamma$. Recall that $\vb = \vb^* +  \left(\begin{array}{c | c}\mathbf{0} &\vgamma\end{array}\right)\vb^*$.  Let $\vT = \vect{I}+  \left(\begin{array}{c | c}\mathbf{0} & \vgamma \end{array}\right)$.  Since $\vT$ is a deterministic function of $\vgamma$, $\vb | \vgamma \overset{d} = \vb | \vT$.
 
 \bnote{Now we want to say that $\vb$ is uniformly distributed since $\vb'$ is.  If $\vT$ exists we can just that $\vb | \vT = (\vT^{-1} \vb )| \vT = \vb' | \vT = U | \vT$.  It does with very high probability, but I don't want to add this restriction.  Is there a way to get around this problem?}
%\[
%\vA=  \left(\begin{array}{c | c}\vA' & \mathbf{0}\\\hline \vect{R} & \vect{I}\end{array}\right) + \left(\begin{array}{c | c}\mathbf{0} & \mathbf{\Gamma}\end{array}\right) \left(\begin{array}{c | c}\vA' & \mathbf{0}\\\hline \vect{R} & \vect{I}\end{array}\right) 
%\]
%\begin{align*}
%(\vA, \vb) =  \left(\left(\begin{array}{c | c}\vA' & \mathbf{0}\\\hline \vect{R} & \vect{I}\end{array}\right) + \left(\begin{array}{c | c}\mathbf{0} & \mathbf{\Gamma}\end{array}\right) \left(\begin{array}{c | c}\vA' & \mathbf{0}\\\hline \vect{R} & \vect{I}\end{array}\right) \right) , \left(\vb' + \left(\begin{array}{c | c}\mathbf{0} & \mathbf{\Gamma}\end{array}\right) \vb'\right)
%\end{align*}
\end{proof}
\bnote{Remember this isn't good enough.  We need the enhanced claim above.}
Together Claims~\ref{cl:randomMatrixDist},~\ref{cl:random ax+e}, and~\ref{clm:random b} show that $\D'$ properly prepares the instance thus, 
\begin{align*}
&\left|\Pr\left[\D'(\vA', \vu') = 1\right]- \Pr\left[\D'(\vA', \vb'=\vA'\vx + \ve)=1\right]  \right|\\
& =\left|\Pr\left[\D(\vA, \vu)=1\right]- \Pr\left[\D(\vA, \vb)=1\right]  \right|\geq \epsilon
\end{align*}
%==========
\ignore{
Before randomized:
$$\vA'\vx' + \ve = \vb'$$
For $i = 1\ldots m$,
$$\sum^{n+\alpha}_{j=1}A_{ij}x_j + e_i = b_i$$
For $i = 1\ldots \alpha$,
$$\sum^{n+\alpha}_{j=1}A_{(m+i)j}x_j = \sum^{n}_{j=1}F_{ij}x_j + x_{n+i} = b_{m+i}$$
}
\end{proof}
}





\section{Proof of Theorem \ref{thm:blockLWE}}
\label{sec:proof of block theorem}

%\bnote{We need to be clear about notation here.  Why are $x, e$ sometimes bold and sometimes capital?} 
\begin{proof}
We assume that all of the fixed blocks are located at the end and their fixed value is $0$, that is, $W_{m+1}, ..., W_{m+\alpha} =0$.  If the blocks are fixed to some other value the reduction is essentially the same.  For a matrix $\vA$ we will denote the $i$-th row by $\va_i$.  For a set $T$ of column indices, we denote by $\vA_T$ the restriction of the matrix $\vA$ to the columns contained in $T$.  Similarly, for a vector $\vx$ we denote by $\vx_T$ the restriction of $\vx$ to the variables contained in $T$.  We use similar notation for the complement of $T$, denoted $T^c$.

%It should be clear to the reader where the reduction differs for another block fixing source.\lnote{I don't like the phrasing; maybe we should say that if the blocks are fixed to some non-0 value, the reduction is essentially the same}

Let $n$ be a security parameter, $m ,q , \alpha= \poly(n)$.  Let $\beta = \omega(1)$.  All operations are computed $(\mod q)$ and we omit this notation.  
Let $\D$ be a distinguisher that breaks $\distLWE_{(m+\alpha), (n+\alpha+\beta), q, \chi'}$ where $\chi'_i = \chi_i$ for $i\leq m$ and $\chi_i \overset{d}= 0$ otherwise.  That is, $\D$ can distinguish $(m+\alpha)$ $\LWE$ instances from uniform over $n+\alpha+\beta$ variables where the last $\alpha$ equations have no error added.  We denote by $\vA$ be the uniform distribution over $\Fq^{(m+\alpha)\times(n+\alpha+\beta)}$, $\vx$ be uniform over $\Fq^{(n+\alpha+\beta)}$, and $\ve= (e_1,..., e_{m+\alpha})$ where $e_1,..., e_m$ are generated from error distribution $\chi$ and $e_{m+i} =0$ for $i=1,..,\alpha$. Let $\epsilon \geq 1/\poly(n)$, then, 
\[
|\Pr[\D(\vA, \vA\vx+\ve) = 1] - \Pr[\D(\vA, \vu) =1]|> \epsilon.
\]
We build a distinguisher that breaks $\distLWE_{m, n, q, \chi}$.  Let $\vA'$ be uniform over $\Fq^{m\times n}$, $\vx'$ be uniform over $\Fq^n$ and $\ve'$ be drawn from $\chi^m$.  It suffices to build a ppt distinguisher $\D'$  where 
\begin{align}
\label{eq:block LWE dist}
|\Pr[\D'(\vA', \vA'\vx'+\ve') = 1] - \Pr[\D'(\vA', \vu) =1]|> (\epsilon - \ngl(n))(1-\ngl(n)) \approx \epsilon.
\end{align}
We will make a single call to $\D$ so we focus on how to prepare a random block fixing instance from the random instance we are given.  The code for $\D'$ is given in \figref{fig:perfectLWEreduction}.

\begin{figure}[t]
\begin{framed}
\begin{enumerate}
\item Input $\vA', \vb'$, where $\vA' \overset{\$} \leftarrow \Fq^{m\times n}$ and $\vb'$ is either uniform over $\Fq^m$ or $\vb' = \vA'\vx' +\ve$.
\item Randomly generate $\vect{R} \overset{\$}\leftarrow \Fq^{\alpha \times n}$. Initialize $\vW \in \Fq^{n\times (\alpha+\beta)}$ to be the zero matrix.
\item Let $\vb^* = (\vb', b^*_{m+1}, \ldots,b^*_{m+\alpha})$, where $(b^*_{m+1}, \ldots, b^*_{m+\alpha} )\overset{\$} \leftarrow \Fq^\alpha$.\label{step:b generation}
\item Randomly generate $\vect{S} \overset{\$}\leftarrow \Fq^{\alpha \times (\alpha+\beta)}$.
		\subitem If $\rank(\vect{S})<\alpha$, stop and output a random bit.
\item Find a set of $\alpha$ linearly independent columns in $\vS$.  Let $T$ be the set of these indices.
\item For all $x_i$ where $n\leq i$ and $i\not \in T$:\label{step:fill in matrix}
\subitem Generate $x_i\overset{\$}\leftarrow \Fq$.  
\subitem For $j=1,..., m$:
\subsubitem Generate $\vW_{j, i}\overset{\$}\leftarrow \Fq$
\subsubitem Set $b_j^* = b_j^* + \vW_{j, i} x_i$.
\item Initialize $\vA^*  = \left(\begin{array}{c | c}\vA' & \vW\\\hline \vR & \vS\end{array}\right)$.
\item \label{step:randomization}
For {$i=1,..., m$}:
\subitem Randomly generate $\gamma_i \leftarrow \Fq^{1 \times \alpha}$.
\subitem Set $\va_{i} \leftarrow \va^*_{i}+\gamma_i (\vR||\vS)$
\subitem Set $b_i \leftarrow b^*_i + \gamma_i (b^*_{m+1},..., b^*_{m+k})$
\item Run $\D$ on input $\vA, \vb$.
\item Output $\D$'s output.
\end{enumerate}
\end{framed}
\caption{A PPT $\mathcal{D'}$ that distinguishes LWE using distinguisher for LWE w/ block fixing source}
\label{fig:perfectLWEreduction}
\end{figure}
The distinguisher $\mathcal{D'}$ has an advantage when $S$ is of rank $\alpha$.  This occurs with overwhelming probability~(techniques from Cooper~\cite{cooper2000rank}):
\begin{claim}
Let $\vS \overset{\$}\leftarrow \Fq^{\alpha \times (\alpha+\beta)}$ be randomly generated.  Then $\Pr[\rank(\vS)=\alpha]\geq  1- \ngl(n)$.
\end{claim}
\begin{proof}
The probability that all $\alpha$ rows are linearly independent is the successive probability that each row is linearly independent of the previous rows, for the $i$th row this probability is $q^{i-1}/q^{\alpha+\beta}$.  By union bound and $\beta = \omega(1)$, the probability that $\vS$ is not full rank is:
\begin{align*}
\Pr[\rank(\vS) < \alpha] &= \sum_{i=1}^{\alpha} \frac{q^{i-1}}{q^{\alpha+\beta}}
\leq \alpha \frac{q^{\alpha}}{q^{\alpha+\beta} }=\ngl(n)\\
\end{align*}
Thus, $\Pr[\rank(\vS) = \alpha] \geq 1-\ngl(n)$.
\end{proof}
\begin{corollary}
$
|\Pr[\D(\vA, \vA\vx+\ve) = 1  | \rank(\vS) = \alpha] - \Pr[\D(\vA, \vu) =1 | \rank(\vS) = \alpha]|> \epsilon - \ngl(n).
$
\end{corollary}

We are interested in the case where $\rank(\vS) = \alpha$, and it suffices to show that $\D'$ prepares a random instance in this case. We show this in the following three claims: 
\begin{enumerate}
\item If $\vA'$ is a random matrix then $\vA$ is a random matrix with $\rank(\vS) = \alpha$.
%\bnote{This should be if $\vb' | A$ is uniformly distributed, then $\vb |A $ is uniformly distributed.}
\item If $\vb' = \vA'\vx'+\ve'$ then $\exists \vx$~(uniformly distributed) where $\vb = \vA \vx + \ve'$ and where $\ve_i' = \ve_i$ for $1\leq i\leq m$ and $\ve_i' = 0$ otherwise.
\item If $\vb' | \vA'$ is uniformly distributed then $\vb | \vA$ is uniformly distributed.
\end{enumerate}

\begin{claim}
\label{cl:randomMatrixDist}
The matrix $\vA$ generated after Step \ref{step:randomization} is a random matrix with $\rank(\vS) = \alpha$.
\end{claim}
\begin{proof}
It should be clear that $\vR, \vS$ are randomly generated~(conditioned on $\rank(\vS) =\alpha$).  The quadrant represented by $\vA'$ remains random after step~\ref{step:randomization} as we are adding uncorrelated random values.  
The matrix $\vW_{T^c}$~(recall this is the restriction of $\vW$ to the columns not in $T$) is random as it is initialized with random values and only uncorrelated random values are added.

Thus, we restrict attention to $\vW_T$~(the columns of $W$ included in $T$).  Fix a particular row $i$.  Then $\vW_{i, T} = \gamma_i \vS_{T}$.  Since $\vS_T$ is full rank and $\gamma_i$ is uniformly generated, $\vW_{i, T}$ is uniformly generated.

Thus, the entire matrix $\vA$ after step~\ref{step:randomization} is uniformly generated conditioned on $\rank(\vS) = \alpha$.
% In the randomized matrix $\vect{A}$, for $1\le i \le m$ and $n+1\le j \le n+\alpha$, $A_{ij}$ is assigned the value $\gamma_{i,(j - n)}$, which is a random value.  Similarly, for $m+1\le i \le m+\alpha$ and $n+1\le j \le n+\alpha$, each entry $A_{ij}$ is random due to the original construction of the matrix $\vA^*$.  Then before step \ref{step:randomization} the matrices $\vA', \vect{R}$ are random submatrices.  Thus, it remains to show that $\vA', \vect{R}$ remain random after step 5. For $1\le i \le m+\alpha$ and $1\le j \le n$, by construction $A_{ij} = A_{ij}'+\sum_{k=1}^\alpha \gamma_{i, k} R_{kj}$, which is a truly random number. Overall, each row vector $\va_{i}$ in $\vA$ is the sum of a random vector and a random linear combination independent vectors. Therefore, the entire matrix $\vA$ is a truly random matrix.  This completes the claim.
\end{proof}
\begin{claim}
\label{cl:random ax+e}
If $\D'$ is provided with input distributed as $\vA', \vb' = \vA'\vx'+\ve'$ then $\vb = \vA \vx+\ve$ where $x_i = x_i'$ for $1\leq i \leq n$ and $x_{n+i}$ are uniformly generated otherwise,  $e_i = e_i'$ for $1\leq i\leq m$ and $e_i = 0$ for $m<i\leq m+\alpha$.
\end{claim}
\begin{proof}
Denote by $\vx'$ the value such that $\vA'\vx'+\ve = \vb$.  
Then partially define $\vx$ as $x_i = x_i'$ if $1\leq i \leq n$ and $x_i$ as the value generated in step~\ref{step:fill in matrix} for $i>n $ and $i\not\in T$.  We define the remaining variables $\vx_T$ as the solution to the following system of equations.
\[
\vS_T  \vx_T = \begin{pmatrix} b_{m+1}  \\ \vdots \\b_{m+\alpha} \end{pmatrix}  - \vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}- \vS_{T^c} \vx_{T^c}  
\]
A solution $\vx_T$ exists as $\vS_T$ is full rank and furthermore, $\vx_T$ is uniformly distributed.
Denote by $\vA^*$ the matrix before step~\ref{step:randomization} and denote by $\ve$ the vector $\ve'$ with $\alpha$ 0s appended.  The vector $\vx$ is uniformly random and $\vb^* = \vA^*\vx + \ve$.  Thus, it remains to show that step~\ref{step:randomization} preserves this solution.
%For convenience, denote by $\vx^*$ the vector where $x_i^* = x_i'$ for $1\leq i \leq n$ and $x_{n+i}^* = b_{m+i}^*$ otherwise.  
We now show that for all rows $1\leq i\leq m$, if $b_i^* = \va^*_i \vx+e_i$  then $b_i = \va_i \vx + e_i$.
%, we have added $\alpha$ equations as well as $\alpha$ unknowns, furthermore, those $\alpha$ equations have no error.  The number of unknowns we added are $x_{n+i} = b^*_{m+i} - \sum^{n}_{j=1}R_{ij}x_j$ for $i=n+1,..., n+\alpha$.
We have the following for $1\leq i\leq m$:
\begin{align*}
\va_i \vx + e_i &= \left(\va_{i}^*+ \gamma_i(\vR || \vS)\right) \vx + e_i\\
&=\va_i^* \vx + e_i +\gamma_i(\vR||\vS) \vx\\&= b_i^* + \gamma_i (\vR||\vS)\vx
\end{align*}
Recall that $b_i =b_i^* + \gamma_i(b_{m+1}^*,..., b_{m+k}^*)$.  We consider the product $(\vR|| \vS) \vx$.  It suffices to show that $(\vR|| \vS) \vx = (b_{m+1}^*,..., b_{m+\alpha}^*)$,
\begin{align*}
(\vR|| \vS) \vx &= \vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}+  \vS_{T^c} \vx_{T^c}  + \vS_T \vx_T \\
&=\vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}+  \vS_{T^c} \vx_{T^c}   + \begin{pmatrix} b_{m+1}  \\ \vdots \\b_{m+\alpha} \end{pmatrix}  - \vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}- \vS_{T^c} \vx_{T^c}  
\\&=  \begin{pmatrix} b_{m+1}  \\ \vdots \\b_{m+\alpha} \end{pmatrix}
\end{align*}
This completes the proof of the claim.
\end{proof}
\begin{claim}\label{clm:random b}
If the pair $\vb' | \vA'$ is uniformly distributed, then $\vb |\vA$ is uniformly distributed.
\end{claim}
\begin{proof}
Let $\vb^*$ be the vector generated after step~\ref{step:fill in matrix}.  The only correlation between $\vb^*$ and $\vA^*$ is in the matrix $\vW$, i.e. for $1\leq i\leq n$, $b_i^* = b_i + \sum_{n<j \wedge j\not\in T} \vW_{i, j} x_j$, thus conditioning on $\vW$, $b_i^*$ is uniform assuming $b_i$ is uniform.  Thus $\vb^* | \vA^*$ is uniformly distributed. We show that $\vb |( \vA, \vA^*)$ is uniformly distributed, therefore, showing that $\vb | \vA$ is uniformly distributed after step~\ref{step:randomization}.  Let $\vgamma$ represent the matrix formed by $\gamma_{i}$.  
 
 The matrix $\vgamma$ is a deterministic function of $(\vA, \vA^*)$.  Thus, $\vb | (\vA, \vA^*) \overset{d}= \vb | (\vA, \vA^*, \vgamma)$.  The only dependence of $\vb$ on $(\vA, \vA^*, \vgamma)$ is in the matrix $\vgamma$.  Thus, $ \vb | (\vA, \vA^*, \vgamma) \overset{d}= \vb | \vgamma$. Recall that 
 \[
 \begin{pmatrix} b_{1}  \\ \vdots \\b_{m} \end{pmatrix}  = \begin{pmatrix} b_{1}^*  \\ \vdots \\b_{m}^* \end{pmatrix}  + \vgamma \begin{pmatrix} b_{m+1}^*  \\ \vdots \\b_{m+\alpha}^* \end{pmatrix}
\]
Thus, $b_1,..., b_m $ are uniformly distributed conditioned on $\vgamma, b_{m+1}^*,..., b_{m+\alpha}^*$.  This completes the claim.
\end{proof}

Finally, the reduction runs in polynomial time and together Claims~\ref{cl:randomMatrixDist},~\ref{cl:random ax+e}, and~\ref{clm:random b} show that when $\rank(\vS) = \alpha$ the distinguisher $\D'$ properly prepares the instance thus, 
\begin{align*}
&\left|\Pr\left[\D'(\vA', \vu') = 1\right]- \Pr\left[\D'(\vA', \vb'=\vA'\vx + \ve)=1\right]  \right|\\
&\, = \left| \Pr\left[\D'(\vA', \vu') = 1 | \rank(\vS) = \alpha \right]- \Pr\left[\D'(\vA', \vb'=\vA'\vx + \ve)=1 | \rank(\vS) = \alpha\right]\right| \Pr[\rank(\vS) = \alpha] \\
&\, =\left|\Pr\left[\D(\vA, \vu)=1 | \rank(S) =\alpha \right]- \Pr\left[\D(\vA, \vb)=1 | \rank(S) =\alpha \right]  \right| \Pr[\rank(\vS) = \alpha] \\
&\, \geq (\epsilon - \ngl(n))(1-\ngl(n)) \approx \epsilon
\end{align*}
Thus, Equation~(\ref{eq:block LWE dist}) is satisfied, this completes the proof.
\end{proof}
\section{Additional Proofs}

\subsection{Proof of \lemref{lem:averageToMaximalError}}
\label{sec:proof of average to maximal error}
\begin{proof}
Let $C$ be the  $(t,\epsilon)$-average error Shannon code with recovery procedure $\rec$ such that  $\Hoo(C)\geq k$.  Then for all $t'\le t$
\[
\sum_{c\in C} \Pr[C=c]\Pr[ c'\leftarrow \sample (c, t') \wedge \rec(c') \neq c]\leq \epsilon.
\]
For $c$ denote by $\epsilon_c = \Pr[c'\leftarrow \sample(c, t') \wedge \rec(c') \neq c]$.  
Then by Markov's inequality:
\[
\Pr_{c\in C}[ \epsilon_c \leq 2\expe_{c\leftarrow C} [\epsilon_c ] ] = \Pr_{c\in C} [\epsilon_c \le 2\epsilon ] \geq \frac{1}{2}
\]
Let $C'$ denote the of  set all $c\in C$ where $\epsilon_c\leq 2\epsilon$.  Note that $\Pr_{c\leftarrow C}[c\in C']\geq 1/2$.  Since $H_\infty(C)\geq k$, we know $|C'|\geq 2^{k-1}$~(otherwise $\Pr_{c\leftarrow C}[c\in C']=\sum_{c\in C'}\Pr[C=c]$ would be less than $2^{k-1}\frac{1}{2^k} = 1/2$).  This completes the proof of the~\lemref{lem:averageToMaximalError}.

%Suppose that for all $W'\subset W$ where $|W'|\geq 2^{k-1}$ there exists a $w\in W'$ such that $\Pr[ w'\leftarrow \sample (w,t') \wedge \rec(w') \neq w]> 2\epsilon$.  
%This implies that there exists a set $V$of size $V\geq |W|-2^{k-1}+1$ where $\forall v\in V, \epsilon_v >2\epsilon$.  Then note that $\Pr[W\in V]> 1/2$ as $W\setminus V$ contains at most $2^{k-1}-1$ points each of which has probability at most $1/2^k$ so $\Pr[W\in (W\setminus V)]\leq (2^{k-1}-1)/2^k < 1/2$.  Thus,
%\begin{align*}
%\Pr_{w\in W}[ w'\leftarrow \sample (w,t') \wedge \rec(w') \neq w]&\geq \sum_{w\in V}  \Pr[W=w]\Pr[ w'\leftarrow \sample (w,t') \wedge \rec(w') \neq w]\\
%&\geq \sum_{w\in V} \Pr[W=w] 2\epsilon   = 2\epsilon \Pr[W\in V]> 2\epsilon\left(\frac{1}{2}\right)= \epsilon
%\end{align*}
%% where $\forall d\in D, \Pr[d'\leftarrow \sample(d) \wedge \rec(d') > 2\epsilon$.  In turn this implies that  
%%\begin{align*}
%%\Pr_{c\in C}[ c'\leftarrow \sample (c) \wedge \rec(c') \neq c]&=\frac{1}{|C|} \sum_{c\in C}  \Pr[ c'\leftarrow \sample (c) \wedge \rec(c') \neq c]\\
%%&\geq \frac{1}{|C|} \sum_{d\in D}  \Pr[ d'\leftarrow \sample (d) \wedge \rec(d') \neq d]\\
%%&\geq \frac{1}{|C|}\sum_{d\in D} 2\epsilon = \frac{1}{|C|}\left(\frac{|C|}{2}+1\right)2\epsilon = \epsilon + 2\epsilon/|C|> \epsilon
%%\end{align*}
%This is a contradiction.  The statement of~\lemref{lem:averageToMaximalError} follows directly.
\end{proof}

\subsection{Proof of \thref{thm:impSketchArbitraryW}}
\label{sec:proof of thm sketch implies code}
\begin{proof}
  Let $W$ be an arbitrary distribution of min-entropy $m$.  Let $Y_s$ be a collection of distributions over $\mathcal{M}$ giving rise to $Y$ such that $\Hav(Y | \sketch(W))\geq k$ and
\[ 
\delta^{\mathcal{D}_{s_{sec}}}((W, \sketch(W)), (Y, \sketch(W)))<\epsilon.
\]  
where  $s_{sec} = O(t(s_{neigh}+s_{rec}))$\lnote{again, why asysmptotics?}.  One such $Y$ must exist by the definition of conditional HILL entropy.
%Let $X$ be arbitrarily but independently distributed over $\mathcal{M}$. Let $\delta^D((X, \sketch(X)), (W, \sketch(X)))<\epsilon$ for negligible $\epsilon$ and $D$ of size at least $s=O(s_{neigh}+s_{rec})$.  This implies that $H^{\hill}_{\epsilon, s}(X|\sketch(X))\geq k$.  
Define $D$ as:
\begin{enumerate}
\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*, t$.
\item For all $1\leq t'\leq t$: 
\subitem  $z'\leftarrow \sample(z, t')$.
\subitem If $\rec(z', s) \neq  z$ output $0$.
\item Output $1$.
\end{enumerate}
 By correctness of the sketch $ \Pr[D(W, \sketch(W)) =1]\ge 1-t\delta$.  Since 
$\delta^D((Y, \sketch(W)), (Y, \sketch(W)))<\epsilon$, we know $\Pr[D(Y, \sketch(W)) = 1]>1-\epsilon-t\delta$.  By Markov's inequality,  there exists a set $S_W$ such that $\Pr[\sketch(W)\in S_W]\ge 1/2$ and for all $s\in \sketch(W)$, $\Pr[ D(Y_s, s) =1]> 1- 2(\epsilon + t\delta)$.  Moreover, by \cite[Lemma 2.2a]{DBLP:journals/siamcomp/DodisORS08} (which is derived simply via Markov's inequality applied to the positive random variable $\max_y \Pr[Y_s=y]$), there exists a set $S'_W$ such that $\Pr[\sketch(W)\in S'_W]> 1/2$ and for all $s\in \sketch(W)$, $\Hoo(Y_s)\ge k-1$. \lnote{simplified this derivation}  Fix one value $s \in S_W\cap S'_W$ (which exists because the sum of probabilities of $S_W$ and $S'_W$ is greater than 1).  
Thus, for all such that $t', 1\leq t'\leq t$, 
\[ \Pr_{y\leftarrow Y_s}[y'\leftarrow \sample(y, t') \wedge \rec(y',s) = y]> 1-2(\epsilon+t\delta).\]  
%Thus, $\rec(\cdot, \sketch(x'))$ is a decoding procedure that succeeds on a random neighbor of a random codeword with probability all but $\epsilon$.  
Thus,  $Y$ is a $(t, 2(\epsilon+t\delta))$-average error Shannon code with recovery $\rec(\cdot,s)$.  The statement of the theorem follows by application of \lemref{lem:averageToMaximalError}.  
\end{proof}

\subsection{Proof of \thref{thm:imp of unp entropy}}
\label{sec:proof of imp unp entropy}
Instead of proving \thref{thm:imp of unp entropy}, we will prove a stronger result based on a weaker form of entropy.  This relaxed definition allows both the variable and the condition to be replaced.  Gentry and Wichs utilize an analogous relaxation for HILL entropy in~\cite[Lemma 3.1]{gentry2011separating}.

\begin{definition}
\label{def:relaxed unp entropy}
For a distribution $(X, Z)$, we say that $X$ has \emph{relaxed unpredictability entropy} at least $k$ conditioned on $Z,$ denoted by $H^{\unp}_{\epsilon, s_{sec}} (X|Z) \geq k$, if there exists a joint distribution $Y, Z'$ such that $\delta^{\mathcal{D}_{s_{sec}}}((X, Z),(Y, Z'))\leq \epsilon$, and for all circuits $C$ of size at most $s_{sec}$,
\[
\Pr[C(Z) = Y ] \leq 2^{-k}
.\]
\end{definition}

\begin{theorem}
Let $W$ be a distribution over $Z^n$ and let $(\sketch, \rec)$ be a relaxed unpredictability-entropy $(Z^n, \Hoo(W), \tilde{m}, t)$ secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$.  If $s_{sec} = \Omega(|\rec|+n\log |Z|)$, then $\tilde{m}\leq n\log |Z| - \log |B_t(\cdot)| + \log(1-\epsilon -\delta)$.

\end{theorem}
\begin{proof}
Let $W$ be a distribution over $Z^n$ and let $(\sketch, \rec)$ be an relaxed unpredictability-entropy $(Z^n, \Hoo(W), \tilde{m}, t)$ secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$ where $s_{sec} = \Omega(|\rec|+b)$.  Let $Y, Z'$ be a joint distribution such that $\delta^{\mathcal{D}_{s_{sec}}}((W, \sketch(W)), (Y, Z'))\leq \epsilon$.  It suffices to show that $\exists \mathcal{I}$ of size $s_{sec}$ such that $\Pr[\mathcal{I}(Z') = Y]\geq 2^{\tilde{m}}$ where $\tilde{m} = n\log |Z| - \log |B_t(\cdot)| + \log(1-\epsilon -\delta)$.  We begin by showing that \rec must recover points of $Y$.
\begin{claim}
\label{clm:y is recoverable}
\begin{align*}
\Pr[\rec(B_t(Y), Z') = Y]&=\\
\Pr[(y, z)\leftarrow (Y, Z) \wedge y'\leftarrow B_t(y) \wedge \rec(y', z) = y] &\geq 1-\epsilon -\delta.
\end{align*}
\end{claim}
\begin{proof}
Suppose that $\Pr[\rec(B_t(Y), Z') = Y]<1-\epsilon -\delta$.  We construct the following $D\in\mathcal{D}_{s_{sec}}$:
\begin{itemize}
\item Input $x\in Z^n, s\in\zo^*$.
\item Sample $x'\leftarrow B_t(x)$.
\item Output $1$ if $\rec(x', s) = x$ otherwise output $0$.
\end{itemize}
Since $(\rec, \sketch)$ has error $\delta$ we know that $\forall w, w'\in Z^n$ where $\dis(w, w')\leq t$ \[ \Pr[s\leftarrow \sketch(w) \wedge \rec(w', s) =  w] \geq 1-\delta.\]  This immediately implies that $\Pr[\rec(B_t(W), \sketch(W) )= W)  ]\geq 1-\delta$.  The statement of the claim follows as 
\begin{align*}
\Pr[D(W, \sketch(W)) = 1]  - \Pr[D(Y, Z')] &=\\
\Pr[\rec(B_t(W), \sketch(W) )= W] - \Pr[\rec(B_t(Y), Z') = Y] &> (1-\delta)-(1-\delta - \epsilon)>\epsilon.
\end{align*}
\end{proof}
We now consider the experiment $(y, z)\leftarrow (Y, Z'), y'\leftarrow B_t(y)$ and define the quantity $P_{y' , z, y} = \Pr[\rec(y', z) =y]$.  \clref{clm:y is recoverable} says that 
\[
 \Pr[(y, z)\leftarrow (Y, Z')] \sum_{y'\in Z^n} \Pr[B_t(y) =y' | Y = y] P_{y',  z, y} \geq 1-\delta -\epsilon.
\]
We note that for any particular $y, y'$ such that $\dis(y, y')\leq t, \Pr[B_t(y) = y' | Y=y] = 1/|B_t(\cdot)|$ and if $\dis(y, y')>t, \Pr[B_t(y) = y' | Y=y] = 0$.  Thus, for all $y, y', \Pr[B_t(y) = y' | Y=y] \leq 1/|B_t(\cdot)|$
We now define $\mathcal{I}\in\mathcal{D}_{s_{sec}}$ as follows:
\begin{itemize}
\item Input $z\in\zo^*$.
\item Sample $x\leftarrow Z^n$.
\item Output $\rec(x, z)$.
\end{itemize}
Let $X$ the random variable corresponding to the sample $x$ taken by $\mathcal{I}_t$.
We now show that $\mathcal{I}$ predicts $Y$:
\begin{align*}
2^{\tilde{m}}& = \Pr[\mathcal{I}(Z') = Y] \\
&= \Pr[(y, z)\leftarrow (Y, Z')] \Pr[\mathcal{I}(z) = y]\\
&\geq \Pr[(y, z)\leftarrow (Y, Z')] \sum_{y'\in Z^n} \Pr[X = y' ]\Pr[\mathcal{I}(z) = y | X = y']\\
&\geq \Pr[(y, z)\leftarrow (Y, Z')] \sum_{y'\in Z^n} \Pr[X = y' ] P_{y', z, y}\\
&\geq \Pr[(y, z)\leftarrow (Y, Z')] \sum_{y'\in Z^n} \frac{\Pr[X = y']}{\Pr[B_t(y) = y' | Y = y]}\Pr[B_t(y) = y' | Y=y ] P_{y', z, y}\\
&\geq  \Pr[(y, z)\leftarrow (Y, Z')] \sum_{y'\in Z^n} \frac{|Z|^{-n}}{1/|B_t(\cdot)|} \Pr[B_t(y) = y' | Y=y ] P_{y', z, y}\\
&\geq \frac{|B_t(\cdot)|}{|Z|^n} \Pr[(y, z)\leftarrow (Y, Z')] \sum_{y'\in Z^n} \Pr[B_t(y) = y' | Y=y ] P_{y', z, y}\\
&\geq \frac{|B_t(\cdot)|}{|Z|^n}(1-\epsilon - \delta)
\end{align*}
Finally converting to entropy, one has $\tilde{m}\leq n\log |Z|-\log |B_t(\cdot)| - \log (1-\epsilon - \delta)$.
\end{proof}

%\bnote{This is the proof for not relaxed unpredictability entropy}
%\begin{proof}
%Let $W$ be a distribution over $\zo^b$ and let $(\sketch, \rec)$ be an Unpredictability-entropy $(\zo^b, \Hoo(W), \tilde{m}, t)$-secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$ where $s_{sec} = \Omega(|\rec|+b)$.  Let $Y_{W=w \wedge \sketch(w) = s} $ be a collection of distributions giving rise to a joint distribution $Y$ such that $\delta^{\mathcal{D}_{s_{sec}}}((W, \sketch(W)), (Y, \sketch(W)))\leq \epsilon$.  It suffices to show that $\exists \mathcal{I}$ of size $s_{sec}$ such that $\Pr[\mathcal{I}(\sketch(W)) = Y]\geq 2^{\tilde{m}}$ where $\tilde{m} = n - \log |B_t(\cdot)| + \log(1-\epsilon -\delta)$.  We begin by showing that $Y$ must correctly recover most of the time.
%\begin{claim}
%\label{clm:y is recoverable}
%\begin{align*}
%\Pr[\rec(B_t(Y), \sketch(W)) = Y]&=\\
%\Pr[y\leftarrow Y\wedge y'\leftarrow B_t(y) \wedge \rec(y', s) = y] &=\\ 
%\Pr[w\leftarrow W, s\leftarrow \sketch(w), \wedge y \leftarrow Y_{s} \wedge y' \leftarrow B_t(y) \wedge \rec(y', s) =y ] &\geq 1-\epsilon -\delta.
%\end{align*}
%\end{claim}
%\begin{proof}
%Suppose that $\Pr[\rec(B_t(Y), \sketch(W)) = Y]<1-\epsilon -\delta$.  We construct the following $D\in\mathcal{D}_{s_{sec}}$:
%\begin{itemize}
%\item Input $x\in \zo^b, s\in\zo^*$.
%\item Sample $x'\leftarrow B_t(x)$.
%\item Output $1$ if $\rec(x', s) = x$ otherwise output $0$.
%\end{itemize}
%Since $(\rec, \sketch)$ has error $\delta$ we know that $\forall w, w'\in\zo^b$ such that \[\dis(w, w')\leq t, \Pr[s\leftarrow \sketch(w) \wedge \rec(w', s) =  w] \geq 1-\delta.\]  This immediately implies that $\Pr[\rec(B_t(W), \sketch(W) = W)  ]\geq 1-\delta$.  The statement of the claim follows as 
%\begin{align*}
%\Pr[D(W, \sketch(W)) = 1]  - \Pr[D(Y, \sketch(W))] &=\\
%\Pr[\rec(B_t(W), \sketch(W) )= W] - \Pr[\rec(B_t(Y), \sketch(W)) = Y] &> (1-\delta)-(1-\delta - \epsilon)>\epsilon.
%\end{align*}
%\end{proof}
%We now consider the experiment $w\leftarrow W, s\leftarrow \sketch(w), y\leftarrow Y_{s}, y'\leftarrow B_t(y)$ and define the quantity $P_{y' , s, y} = \Pr[\rec(y', s) =y]$.  \clref{clm:y is recoverable} says that 
%\[
% \Pr[w\leftarrow W, s\leftarrow \sketch(w), y\leftarrow Y_s] \sum_{y'\in \zo^b} \Pr[B_t(y) =y' | Y = y] P_{y',  s, y} \geq 1-\delta -\epsilon.
%\]
%We note that for any particular $y, y'$ such that $\dis(y, y')\leq t, \Pr[B_t(y) = y' | Y=y] = 1/|B_t(\cdot)|$ and if $\dis(y, y')>t, Pr[B_t(y) = y' | Y=y] = 0$.  Thus, for all $y, y', \Pr[B_t(y) = y' | Y=y] \leq 1/|B_t(\cdot)|$
%We now define $\mathcal{I}\in\mathcal{D}_{s_{sec}}$ as follows:
%\begin{itemize}
%\item Input $s\in\zo^*$.
%\item Sample $x'\leftarrow \zo^b$.
%\item Output $\rec(x', s)$.
%\end{itemize}
%We define by $X$ the random variable corresponding to the sample $x'$ performed by $\mathcal{I}_t$.
%We now show that $\mathcal{I}$ predicts $Y$:
%\begin{align*}
%2^{\tilde{m}}& = \Pr[\mathcal{I}(\sketch(W)) = Y] \\
%&= \Pr[w\leftarrow W, s\leftarrow \sketch(W), y\leftarrow Y_s] \Pr[\mathcal{I}(s) = y]\\
%&\geq \Pr[w\leftarrow W, s\leftarrow \sketch(W), y\leftarrow Y_s] \sum_{y'\in \zo^b} \Pr[X = y' ]\Pr[\mathcal{I}(s) = y | X = y']\\
%&\geq \Pr[w\leftarrow W, s\leftarrow \sketch(W), y\leftarrow Y_s] \sum_{y'\in \zo^b} \Pr[X = y' ] P_{y', s, y}\\
%&\geq \Pr[w\leftarrow W, s\leftarrow \sketch(W), y\leftarrow Y_s] \sum_{y'\in \zo^b} \frac{\Pr[X = y']}{\Pr[B_t(y) = y' | Y = y]}\Pr[B_t(y) = y' | Y=y ] P_{y', s, y}\\
%&\geq  \Pr[w\leftarrow W, s\leftarrow \sketch(W), y\leftarrow Y_s] \sum_{y'\in \zo^b} \frac{2^{-b}}{1/|B_t(\cdot)|} \Pr[B_t(y) = y' | Y=y ] P_{y', s, y}\\
%&\geq \frac{|B_t(\cdot)|}{2^b} \Pr[w\leftarrow W, s\leftarrow \sketch(W), y\leftarrow Y_s] \sum_{y'\in \zo^b} \Pr[B_t(y) = y' | Y=y ] P_{y', s, y}\\
%&\geq \frac{|B_t(\cdot)|}{2^b}(1-\epsilon - \delta)
%\end{align*}
%Finally converting to entropy, one has $\tilde{m}\leq b-\log |B_t(\cdot)| - \log (1-\epsilon - \delta)$.
%\end{proof}

%\subsection{Proof of \lemref{lem:i t constant time}}
%\label{sec:proof lem i t constant time}
%\begin{proof} We consider the expected number of iterations in step 2 when $t+1\leq m/n$.  Steps 3 and 4 can be accomplished in time $n^3\log q +s_{ver}$.  An iteration of $\mathcal{I}_t$ succeeds when all selected rows of $\vA, \vect{C}$ have no error.  The probability of each selected row having an error is at most $\frac{t}{m - i}$ where $i$ is the number of rows already selected.  That is,
%\begin{align*}
%\Pr[i_1,..., i_n\text{ have no errors}]&\geq \prod_{i=0}^{n-1}\left(1 - \frac{t}{m-i}\right)\geq  \prod_{i=0}^{n-1}\left( 1-\frac{m-n}{n(m-i)}\right)\\
%&\geq \prod_{i=0}^{n-1}\left( 1-\frac{1}{n}\right)\geq \left(1-\frac{1}{n}\right)^n \geq 1/4.
%\end{align*}
%Note in the last step we assume that $n\geq 2$.  The minimum of $(1-1/n)^n$ is achieved at $n=2$ and it quickly converges to $1/e$.  Thus, the expected number of iterations is at most $4$ giving the stated running time for $\mathcal{I}_t$.
%\end{proof}

\subsection{Proof of \lemref{lem:i t poly time}}
\label{sec:proof lem i t poly time}
\begin{proof}
We first note that $R(X, (\vA, \vA X+W))$ is computable in time $O(n^3\log q)$ by computing $AX+(W-W')- AX$ and checking if all dimensions have magnitude at most $|W|_\infty$ and at most $t$ dimensions have nonzero magnitude. We consider the expected number of iterations in step 2 when $t\leq d\log n\left(\frac{m}{n}-1\right)$.  Steps 3 and 4 can be accomplished in time $n^3\log q $.  An iteration of $\mathcal{I}_t$ succeeds when all selected rows of $\vA, \vect{C}$ have no error.  The probability of each selected row having an error is at most $\frac{t}{m - i}$ where $i$ is the number of rows already selected.  That is,
\begin{align*}
\Pr[i_1,..., i_n\text{ have no errors}]&\geq \prod_{i=0}^{n-1}\left(1 - \frac{t}{m-i}\right)\geq \prod_{i=0}^{n-1}\left( 1-\frac{d\log n\left(\frac{m}{n}-1\right)}{m-i}\right)\\
&\geq  \prod_{i=0}^{n-1}\left( 1-\frac{d\log n}{n}\left(\frac{m-n}{m-i}\right)\right)\geq \prod_{i=0}^{n-1}\left( 1-\frac{d\log n}{n}\right) \\
&= \left(1-\frac{d\log n}{n}\right)^n  = \left(\left(1-\frac{d\log n}{n}\right)^{\frac{n}{d\log n}}\right)^{d\log n}\geq \frac{1}{4^{d\log n}} = \frac{1}{n^{2d}}
\end{align*}
Thus, the expected number of iterations in step 2 is $n^{2d}$.  We assume all matrix operations can be completed in time $n^3\log q$, giving expected running approximately $n^{2d}(n^3 \log q)$.
\end{proof}

\section{Parameter Settings for \consref{cons:informal construction}}
\label{sec:parameter settings}
In this section, we explain the different parameters that go into our construction.  In \thref{thm:lossless secure conductor log} we give lossless conductors from a security parameter $n$ and an error $t$.  In this section, we discuss constraints imposed by 1) efficient decoding 2) maintaining security of the LWE instance and 3) ensuring no entropy loss of the construction.  We begin by reviewing the parameters that make up our construction:

\begin{itemize}
\item $|W|$: The length of the source.  
\item $t$: Number of errors that can be supported.  
\item $n$: LWE security parameter (i.e., number of field elements in $X$), which must be greater than some minimum value $n_0$ for security.
\item $q$: The size of the field.  
\item $\rho$: The fraction of the field needed for error sampling.  
\item $m$: The size of each number of samples in the LWE instance.  
\item $k$: The number of hardcore bits in $X$~(from \lemref{lem:many hardcore bits}).
\end{itemize}
We will split the source $|W|$ into $m$ blocks each of size $\rho q$~(that is, $|W| = m\log \rho q$).  We will ignore the parameter $|W|$ and focus on $t, n, q, \rho,$ and $m$.  As stated above we have three constraints:
\begin{itemize}
\item Maintain security of LWE.  If we assume GAPSVP and SIVP are hard to approximate within polynomial factors then \lemref{lem:uniform LWE decision} says that we get security for all $n$ greater than some minimum $n_0$ and $q = \poly(n)$ and $\rho q \geq 2 n^{1/2 + 3\gamma} m = \poly(n)$.  The only reason to increase $\rho q$ over this minimum amount (other than security) is if the number of errors in $W$ decreases with a slightly larger block size.  We ignore this effect and assume that $\rho q = 2n^{1/2+3\gamma}m$.
\item Maintain efficient decoding of Construction~\ref{cons:decoding algorithm}.  Using \lemref{lem:i t poly time}, this means that $t\leq d\log n(m/n-1)$.
\item Minimize entropy loss of the construction.  We will output $X_{1,...,k}$ so the entropy loss of the construction is $|W|-|X_{1,..., k}|$.  We want the entropy loss to be zero, that is, $|W| = |X_{1,..., k}|$.  Substituting, one has $m\log \rho q = k \log q$.
\end{itemize}
Collecting constraints we can support any setting where $t, n, q, \rho, m, k$ satisfy the following constraints~(for constants $d, f$):
\begin{align*}
n_0&< n -k \\
t&\leq d \log n\left(\frac{m}{n}-1\right)\\
q &= n^f\\
\rho q  &= 2n^{1/2+3\gamma}m\\
m\log \rho q &= k \log q
\end{align*}
Substituting $q = n^f$ and $\rho q = 2n^{1/2+3\gamma}m$ yields the following system of equations:
\begin{align*}
n_0&< n - k\\
t&\leq d\log n\left(\frac{m}{n}-1\right)\\
m \log 2n^{1/2+3\gamma}m &= k \log n^f
\end{align*}
%\xnote{$ m \log 2n^{1/2+3\gamma}m = n \log n^f$}
This is the most general form of our construction, we can support any $n, t, m$ that satisfy these equations for constants $d, f$.  However, the last equation may have no solution for $f$ constant.  Putting the last equation in terms of $f$ one has:
\begin{align*}
n_0&< n -k \\
t&\leq d\log n\left(\frac{ m }{n} -1\right)\\
%f \log n &= \frac{m}{n}\log 2n^2m\\
f &= \frac{m}{k}\frac{\log 2n^{1/2+3\gamma} m}{\log n}
\end{align*}
To ensure $f$ is a constant, we set $t = c \log n$ for some constant $c$ and that $k = n /g$ for some constant $g> 1$.  Finally we assume that $m$ is the minimum value such that $t \leq d \log n(m/n-1)$~(that is, there are only as many dimensions as necessary for decoding using \lemref{lem:i t poly time}):
\begin{align*}
n_0&< n -k \\
m &= \frac{(c+d)n \log n}{d \log n} = \frac{(c+d)n}{d}\\
f &= \frac{m}{k}\frac{\log 2n^{1/2+3\gamma}m}{\log n} = \frac{g(c+d)}{d}\frac{\log \frac{2(c+d)}{d} n^{3/2+3\gamma}}{\log n}
\end{align*}

%\xnote{$f = \frac{m}{n}\frac{\log 2n^{1/2+3\gamma}m }{\log n} = \frac{c+d}{d}\frac{\log 2(c+d)/d n^{3/2+3\gamma}}{\log n}$}
Note that $f$ is at most a constant in $n$.

Assuming $n-k = n(1-1/g) > n_0$ and letting $t= c\log n$ we get the following setting:
\begin{align*}
m &= \frac{(c+d)n}{d}\\
q & = n^f = n^{\frac{m}{n}\frac{\log 2n^{1/2+3\gamma}m}{\log n}} = \poly(n)\\
\rho q &= 2n^{1/2+3\gamma}m = \frac{2(c+d)n^{3/2+3\gamma}}{d}
\end{align*}
Furthermore, we get decoding in time $O(n^{2d}(n^3\log n))$.  We can output a $k$ fraction of $X$ and the bits will be pseudorandom~(conditioned on $\vA, \vA X+W$).  The parameter $g$ allows is a tradeoff between the number of dimensions needed for security and the size of the field $q$.  In \thref{thm:lossless secure conductor log}, we set $g=2$ and output the first half of $X$.  This analysis proceeds similarly when we desire a linear stretch instead of $m\log \rho q = k\log q$.  Essentially, this corresponds to multiplying $g$ by the inverse of the desired stretch factor.

\subsection{Parameter Settings for \thref{thm:lossless block sketch log}}
\label{ssec:block params}
We repeat parameter settings for block fixing sources.  We now have $m+\alpha$ as the number of samples, while $n + \alpha+\omega(1)$ is the number of variables.  We can support any setting where $t, n, q, \rho, m, k, \alpha$ satisfy the following constraints~(for $\beta = \omega(1)$ and constants $d, f$):
\begin{align*}
n_0&< n -k  -\alpha -\beta\\
t&\leq d \log n\left(\frac{m}{n}-1\right)\\
q &= n^f\\
\rho q  &= 2n^{1/2+3\gamma}m\\
m\log \rho q &= k \log q
\end{align*}
Substituting $q = n^f$ and $\rho q = 2n^{1/2+3\gamma}m$ yields the following system of equations:
\begin{align*}
n_0&< n - k - \alpha -\beta\\
t&\leq d\log n\left(\frac{m}{n}-1\right)\\
m \log 2n^{1/2+3\gamma}m &= k \log n^f
\end{align*}
As before we can support any setting any $n, t, m, \alpha$ that satisfy these equations for $\beta = \omega(1)$ and constants $d, f$.  However, the last equation may have no solution for $f$ constant.  Putting the last equation in terms of $f$ one has:
\begin{align*}
n_0&< n -k  - \alpha - \omega(1) \\
t&\leq d\log n\left(\frac{ m }{n} -1\right)\\
%f \log n &= \frac{m}{n}\log 2n^2m\\
f &= \frac{m}{k}\frac{\log 2n^{1/2+3\gamma} m}{\log n}
\end{align*}
To ensure $f$ is a constant, we set $t = c \log n$ for some constant $c$ and that $k, \alpha = n/3$ and $\beta = \log n$.  Finally we assume that $m$ is the minimum value such that $t \leq  d \log n(m/n-1)$~(that is, there are only as many dimensions as necessary for decoding using \lemref{lem:i t poly time}):
\begin{align*}
n_0&< n/3 -  \log n\\
m &= \frac{(c+d)n \log n}{d \log n} = \frac{(c+d)n}{d}\\
f &= \frac{m}{k}\frac{\log 2n^{1/2+3\gamma}m}{\log n} = \left(\frac{3(c+d)}{d}\right)\frac{\log \frac{2(c+d)}{d} n^{3/2+3\gamma}}{\log n}
\end{align*}

Assuming $n/3-\log(n)> n_0$ and letting $t= c\log n$ we get the following setting:
\begin{align*}
m &= \frac{(c+d)n}{d}\\
q & = n^f = n^{\frac{m}{n}\frac{\log 2n^{1/2+3\gamma}m}{\log n}} = \poly(n)\\
\rho q &= 2n^{1/2+3\gamma}m = \frac{2(c+d)n^{3/2+3\gamma}}{d}
\end{align*}


\ignore{
\subsection{Old parameter setting}
The question then becomes whether there is a setting 
Our construction has two goals: 1) maximizing correcting capability~(while retaining polynomial time decoding, and 2) maximizing the unpredictability entropy of $W$ conditioned on the construction~($\tilde{m}$ in \defref{def:comp secure sketch}).  Our parameters will differ significantly depending on what setting of $t$ is used.  We will consider both settings where $t\leq m/n-1$ and $t\leq c\log n (m/n-1)$.  Recall these corresponding to Construction~\ref{cons:decoding algorithm} running either in fixed polynomial time or time approximately $n^{2c}$.  We consider each of these settings:

$\mathbf{t \leq m/n-1}$

We first find the appropriate range of $n$.  First, we need $n>n_0$ for LWE security. Increasing $n$, up to the point when $H_\infty(X)=H_\infty(W)$, i.e., $n\log q = m\log\rho q$, increases the unpredictability entropy of the construction~(see \assref{assume:entropy LWE}). However, this decreases the number of errors $t$ we can correct. There is no advantage to increasing $n$ further\footnote{We primarily consider the construction with no entropy drop, if a drop is unpredictability entropy is acceptable, more errors can be corrected.  However, if the remaining unpredictability entropy is worse than a known information theoretic construction~(see~\cite{DBLP:journals/siamcomp/DodisORS08}), an information theoretic construction should be used.
}.

We first analyze the lossless construction, when  $H_\infty(X)=H_\infty(W)$ and thus $n\log q = m\log\rho q$.
Substituting $m \ge (t+1)n$ above one has:
\[
n \log q  = m \log \rho q \geq (t+1)n \log \rho q
\]
Thus, the lossless construction can support settings where $t+1\leq \frac{\log q}{\log \rho q}$.  In order to apply \lemref{lem:uniform LWE}, we need that $\rho q\geq 2n^2m \geq 2(t+1)n^3$.  We consider the setting $q = n^d$ for some constant $d$ (larger $q$ is unlikely to improve parameters unless 
%In Appendix~\ref{sec:parameters q expo}, we review parameters where $q=2^n$, these parameters are worse unless 
lattice problems are hard to approximate within exponential factors.)  
Dividing the above equations above we have:
\[
t+1\leq \frac{\log q}{\log \rho q}\leq \frac{\log q}{\log 2(t+1) n^3}\leq \frac{\log n^d}{\log 2t n^3} \approx d/3
\]
(the last step follows because the second-to-last step already implies $t\le d\log n$).
Thus, we can support a constant $d/3-1$ number of errors where $q = n^d$ is the size of our field.  Because we need $\rho q\geq 2n^2m \geq 2(t+1)n^3$ (to apply \lemref{lem:uniform LWE}), we require $\rho > n^{3-d}$.  Thus, our construction is secure if SIVP and GAPSVP are hard within approximation factors of 
\[
\tilde{O}(n^{5/2}m/\rho) = \tilde{O}\left(\frac{n^{5/2} m }{\rho}\right)
= \tilde{O}\left(\frac{n^{5/2}t}{n^{3-d}}\right)
= \tilde{O}\left(n^{d-1/2}m\right)
\]

%As an example if $\gamma = n^{c}$, this yields:
%\[
%t\leq \frac{n}{n + \log n + \log t - \log \gamma} = \frac{n}{n+\log n +\log t  -\log n} = \frac{n}{n + \log t}
%\]
%Alternatively, if $\gamma = n^{\log n}$, this yields:
%\[
%t\leq \frac{n}{n + \log n + \log t - \log \gamma} = \frac{n}{n+\log n +\log t  -\log^2 n} = \frac{n}{n+\log n - \log^2 n + \log t}
%\]
%Thus, as $q$ grows we can support a larger number of errors.  However, the growth of $q$ with a fixed size $\rho q$ leads to the approximation factor in \lemref{lem:uniform LWE} no longer being a harder problem.  Thus, $q$ can only be increased while the lattice problems in \lemref{lem:uniform LWE} is still believed to be hard.  Furthermore, in the decision formulation of \lemref{lem:uniform LWE decision} requires that $q(n) = poly(n)$.  This means that $t\leq O(\log n)$.

$\mathbf{t\leq c\log n(m/n -1)}$

We repeat the above analysis.  Recall we now consider the case where $m\geq (t+1)n/(c\log n)$.  We seek to satisfy:
\[
n\log q = m\log \rho q \geq (t+1)n \log \rho q /(c\log n)
\]
Again, we assume that $q = n^d$ for some constant $d$.  Then 
\[
t+1\leq \frac{c \log n \log q }{\log \rho q}\leq \frac{cd\log^2 n}{ \log \frac{2(t+1) n^3}{c\log n}}\leq \frac{cd \log^2 n}{3 \log n} \approx \frac{cd \log n}{3}.
\]
This gives $\rho > n^{3-d}/\log n$ and security if SVIP and GAPSVP are hard within approximation factors of $\tilde{O}(n^{d-1/2}m/\log n)$.

\textbf{Formulation from $W, t$:}

In a standard application, a source $W$ and number of errors $t$ will be given\footnote{Parameter settings will change the alphabet size and thus might change $t$, we ignore these effects and assume $t$ is given.}.  Thus, the goal is to maximize security subject to correcting $t$ errors.  This means the following parameters must be set:
\begin{itemize}
\item How large should $X$ be?  The size of $X$ is the limiting factor for the resulting unpredictability entropy.  Thus, $n$ should be as large as possible while allowing decoding.  We must also ensure that $n>n_0$ for security.
\item How large a field should we operate over?  This is the parameter $q$.  Increasing this parameter allows correcting more errors~(subject to the underlying lattice problems still being hard to approximate).
\item How length block should $W$ be split into?  This is the parameter $\rho q$.  Setting this parameter also sets $m = |W|/\log \rho q$.
\end{itemize}

Unfortunately, the parameters $m, n$ and depend on each other and cannot be set independently.  As before, we will consider the fixed poly time decoder and variable poly time decoder in turn.

$\mathbf{t\leq (m/n-1)}$

We set parameters in the following order:
\begin{itemize}
\item Let $m, n$ be integer solutions to the following equations that maximizes $n$~(if the maximum solution is less than $n_0$, we cannot support a secure construction):
\begin{align*}
|W| &= m\log 2n^2m\\
n&\leq m/(t+1)
\end{align*}
We assume that the solution for $m,n$ exists and is tight~(in the sense that $n = m/(t+1)$).  If not, there is an entropy loss in the construction due to size mismatches~(in the entropy analysis below) and padding~(to ensure that $|W|$ is an integer number of blocks).
\item Set $\rho q = 2n^2m $.
\item Set $q = n^{(t+1)\log( 2(t+1)n)/\log n}$.  Note that although $n$ appears in the exponent if $t$ is a fixed constant~(does not depend on $|W|$), this value is $q =\poly(n)$.  
\end{itemize}  Given these parameters, we can calculate the sizes of $X$ and $W$~(as we show in the \secref{sec:security of LWE cons} $\tilde{m} =|W|$):
\begin{align*}
H^{\unp}_{\epsilon, s} ( W | AX+W) &= \min\{ H_\infty(W), H_\infty(E)\}\\
H_\infty(X) = |X| &= n \log q= \frac{m}{t+1}\log n^{(t+1)\log 2(t+1)n/\log n} \\
&= \frac{m}{t+1} \frac{(t+1)\log 2(t+1)n}{\log n} \log n = m \log 2(t+1)n^3 \approx m\log 2n^2m\\
H_\infty(W) = |W| & = m \log \rho q = m \log 2n^2 m\,.
\end{align*}

$\mathbf{t \leq c\log n(m/n-1)}$
We set parameters in the following order:
\begin{itemize}
\item Let $m, n$ be integer solutions to the following equations that maximizes $n$~(if the maximum solution is less than $n_0$, we cannot support a secure construction):
\begin{align*}
|W| & = m\log 2n^2m\\
 \frac{nt}{c\log n} + n&\leq m
\end{align*}
We assume that the solution for $m,n$ exists and is tight~(in the sense that $ \frac{nt}{c\log n} + n=m$).  If not, there is an entropy loss in the construction due to size mismatches~(in the entropy analysis below) and padding~(to ensure that $|W|$ is an integer number of blocks).
\item Set $\rho q = 2n^2m $.
\item Set $q = (2n^2m)^{m/n}$.  Note if $t = O(\log n)$, then $m/n = \frac{t}{c\log n} +1 = O(1)$ and thus $q$ is polynomial in $n$.
\end{itemize}  Given these parameters, we can calculate the sizes of $X$ and $W$~(as we show in the \secref{sec:security of LWE cons} $\tilde{m} =|W|$):
\begin{align*}
H^{\unp}_{\epsilon, s} ( W | \vA X+W) &= \min\{ H_\infty(W), H_\infty(E)\}\\
H_\infty(X) = |X| &= n \log q= n \log (2n^2m)^{m/n} = m \log 2n^2m\\
H_\infty(W) = |W| & = m \log \rho q = m \log 2n^2 m\,.
\end{align*}

We'll now consider a formulation for a fixed length biometric and a number of block errors.  We assume that $n$ is given as a security parameter and thus we ask how many errors can be corrected for a noisy uniform distribution of length $w>>n$.  We denote the number of errors by $t$~(we'll be able to correct block errors over the alphabet $[-\rho q, \rho q]$).  
Let $n$ be a security parameter and let $W$ be the uniform distribution of length $w>>n$.  Let $m$ as the minimum integer such that $m\log 4m^2 n\geq w$.  Set $q = 2^{m/n}4m^2n$.  Then split $W=( W_1,..., W_m)$ where each $W_i$ is of length $\log 4m^2 n$ bits.  Then, for $W' =( W_1',.., W_m')$ if  $t = |\{i |  W_i \neq W_i'\}| \leq m/n$ block errors, Construction~\ref{cons:decoding algorithm} allows for decoding in expected polynomial time.  Furthermore, $H^{\unp}_{\epsilon, s}(W | \vA X+W)\geq |W|$~(stating \lemref{lem:uniform LWE decision} in the language of Assumption~\ref{assume:entropy LWE}).  

\begin{theorem}
\label{thm:security of secure sketch}
Let $n_0$ be a security parameter and let $W$ be uniform over $\mathcal{M}$ and let $t$ be a constant.  Let $m,n $ be the solution to the following equations that maximizes $n$.
\begin{align*}
|W| &= m\log 2n^2m\\
n&\leq m/t
\end{align*}
If $n>n_0$ and $n = m/t$ and \assref{assume:entropy LWE} holds, then for 
\begin{align*}
q &= n^{3t\log( \sqrt[3]{2t}n)/\log n}\\
\rho q &= 2n^2m,
\end{align*}  Construction~\ref{cons:LWESecureSketch} with uniform error over the interval $[-2n^2m, 2n^2m]$ is a $(\mathcal{M}, |W|, |W|, \epsilon, s, s_{sketch}, s_{rec}, t)$-computational secure sketch for $s = \poly(n), s_{sketch} = O(m\times n )$ and $s_{rec}$ is expected polynomial time.
\end{theorem}

\bnote{Not sure what to do with this}
\begin{theorem}
\label{thm:security of block sketch}
Let $n_0$ be a security parameter and let $t$ be a constant.  Let $W\in \{0, 1\}^{(m+\alpha)\times \log 2n^2m}$ be an $\alpha$-symbol fixing source where $m,n $ are the solution to the following equations that maximizes $n$.
\begin{align*}
|W| &= (m+\alpha)\log 2n^2m\\
(n+\alpha)&\leq (m+\alpha)/t
\end{align*}
If $n>n_0$ and $n+\alpha = (m+\alpha)/t$ and \assref{assume:entropy LWE} holds, then for 
\begin{align*}
q &= n^{3t\log( \sqrt[3]{2t}n)/\log n}\\
\rho q &= 2n^2m,
\end{align*}  Construction~\ref{cons:LWESecureSketch} with uniform error over the interval $[-2n^2m, 2n^2m]$ is a $(\mathcal{M}, \Hoo(W), \Hoo(W), \epsilon, s, s_{sketch}, s_{rec}, t)$-computational secure sketch for $s= \poly(n)$,  $s_{sketch} = O(m\times n )$ and $s_{rec}$ is expected polynomial time under .
\end{theorem}

}

\ignore{
\section{Fuzzy Extractor Statement of Construction~\ref{cons:LWESecureSketch}}
\label{sec:fuzzy extractor phrasing}
In this section we restate Construction~\ref{cons:LWESecureSketch} as a computational fuzzy extractor instead of a computational secure sketch:
\begin{construction}[Computational Fuzzy Extractor based on LWE] 
\label{cons:LWEFuzzyExtractor} Let $n$ be a security parameter and let $m = m(n) = \poly(n), q = q(n)\geq 2$ be integers, furthermore let $\chi$ be a distribution $\Fq$ that can be sampled with a fixed number of bits $s_{err}$.
Let $\mathcal{I}_t$ be an algorithm~(not necessarily efficient) that inverts an LWE instance when no more than $t$ of $m$ dimensions have non-zero error.  Furthermore, let $\rext$ be a reconstructive extractor.  Let $W$ be a distribution over $\{0,1\}^{s_{err}\times m}$.

\textbf{\gen}
\begin{enumerate}
\item Input $w\leftarrow W$.
\item Sample $A\in\Fq^{m\times n}, x\in\Fq^n$ uniformly at random.
\item Use $w$ as the randomness for the sampling algorithm, $\sample$, for $\chi$.  Set $E\leftarrow  \sample(w)$.
\item Sample $seed\leftarrow U$ as required for \rext.  Compute $r\leftarrow \rext(E, seed)$
\item Set $p = (A, AX+E, seed)$.
\item Output $(r, p)$.
\end{enumerate}

\textbf{\rep}
\begin{enumerate}
\item Input $(w', p)$
\item Parse $p$ as $(A, C, seed)$
\item Compute $E' \leftarrow \sample (w')$.
\item Set $X' = \mathcal{I}_t(A, C-E') $. 
\item Compute $r\leftarrow \rext (C-AX', seed)$.
\item Output $r$.
\end{enumerate}
\end{construction}

\begin{theorem}[Security of Construction~\ref{cons:LWEFuzzyExtractor}]\label{thm:LWEFuzzyExtractor}
Fix $(n, m, q, \chi)$ such that Assumption~\ref{assume:general LWE} holds for $\mathcal{I}$ of size at most $s$ with success $\epsilon$.  Furthermore assume that a fixed number of bits $s_{err}$ are necessary to sample from $\chi$.  Let $\rext: \chi^m \times\{0,1\}^\ell\rightarrow \{0,1\}^{k_{len}}\times \{0,1\}^\ell$ be an extractor with $(\log 1/\epsilon - \log 1/\delta, \delta)$-reconstruction $(\cons,\decons)$.  Let $\mathcal{I}_t$ be an inverter as described in Construction~\ref{cons:LWEFuzzyExtractor} of size $s_{\mathcal{I}_t}$.  Then Construction~\ref{cons:LWEFuzzyExtractor} is a $(\{0,1\}^{m\times s_{err}}, m\times s_{err}, k_{len}, t, s_{rec}, s/(|\cons|+|\decons|), 5\delta)$ computational fuzzy extractor where $s_{rec, t} = s_{\mathcal{I}_t}+ |\sample| + O(m\times n\times \log q) + |\rext|$.
\end{theorem}
}

\ignore{
\section{Min-entropy error LWE}
Throughout this section we will assume that \sample uses $\ell$ bits of randomness to produce the required distribution in a single dimension.  Usually, this is the normal (Gaussian) distribution with some mean.  Note that for the uniform distribution sampling requires $\log q$ bits where $q$ is the size of the range.  By information theory we know that in expectation, the number of random bits is smaller for an distribution that is not uniform, but this provides us no guarantee about the worst case number of bits.  We will begin with noting a few cases that seem significantly easier to show.  Recall that the distribution $W$ is drawn from is specified and known by the adversary.
\begin{itemize}
\item Case 1: $\Hoo(W)\leq \ell (m- n)$.  This case is not secure.  Let $W\in\{0,1\}^{\ell m}$ and we further specify $W=W_1,...,W_m$ where each $W_i\in\{0,1\}^\ell$.  We then set $W_1,...,W_n$ equal to some fixed value, $0$ without loss of generality, and $W_{n+1},...,W_m$ to the uniform distribution.  Then it is clear for the first $n$ equations, the adversary is solving a system $Ax=b$ without any error and this can be done in $P$.
\item Case 2: Full or no entropy.  As before, let $W\in\{0,1\}^{\ell m}$ and we further specify $W=W_1,...,W_m$ where each $W_i\in\{0,1\}^\ell$.  We further restrict to the case where each $W_i$ either $W_i\overset{d}=U_\ell$ or $\Hoo(W_i) = 0$.  Thus, for each dimension either, one of the two occurs: the error is properly generated or the adversary knows the error exactly. So, for each dimension, given the $W_i\overset{d}=U_\ell$, the adversary can only guess the correct random variable not better than $2^{\ell}$. Similarly, if given the $\Hoo(W_i) = 0$, the adversary can always guess the correct variable.(\xnote{This is trivial, since if we give $\Hoo(W_i) = 0$, we are actually giving the a random variable has probably of 1.})

We are considering the case that the adversary will be given the ensemble $\{W_1,...,W_n\}$, where each $W_i$ has some entropy.

\item Case 3: General case.
\item \textbf{Hypothetical worry case}  Suppose for convenience we are working with the normal distribution that has variance $\frac{2}{\pi}$ (centered around 0).  That is 
\[
\Pr[X=x] = \frac{1}{\sqrt{\frac{2}{\pi}}\sqrt{2\pi}}e^{-\frac{1}{2}{\frac{2}{\pi}x^2}}=\frac{1}{2}e^{-\pi x^2}
\]
Thus, $\Pr[X=0] =1/2$.  One can easily design a procedure $\sample'$ that generates this distribution using a maximum of some number $\ell$ bits.  Let $w_1,...,w_\ell$ be the bits that $\sample'$ takes as input.  Then when $w_1=0$ the sampler outputs $0$.  The generation for other values does not matter (only that generating some value takes $\ell$ bits).  Thus, to generate $n$ samples we need at most $\ell n$ bits.  However, there is a distribution with min-entropy $\Hoo(W)=(\ell -1) m$, that produces always produces the all zero error.  This distribution has the first bit of each $W_i$ 0 and the remaining bits truly random.  Thus, it seems impossible for our scheme to be secure for an arbitrary procedure \sample, but we will need to specify a specific \sample.  Another possibility is to use a different distribution that looks like the normai distribution but requires a ``flatter'' number of coins to be used for each distribution.  Is there a definition of an algorithm that uses the same number of random bits for each invocation?  Would this be enough to get rid of this problem?
\end{itemize}
Questions: 
\begin{enumerate}
\item How we sample this error vector $\vect{e}$?  Can we do something other than Gaussian to remove some worries?
\item Given the two biometric data which have small distance between each other, will we get the small distance between the error vector after sampling?
\textbf{Solved, each dimension is sampled independently.  Thus, our distance blowup is only as large as the number of bits needed to sample each dimension.}
\item When is LWE easy and when is LWE hard?  Are we going to be able to create a sufficient gap between these two cases?
\item Does the parallel sampler of Peikert allow for the distance in two randomness being small, creating small output distance?
\textbf{Solved, was trying to do the wrong thing.  Just Gaussian in each dimension.}
\item Does the sampler of Peikert allow for nonuniform input distribution?
\textbf{Solved, was trying to do the wrong thing (was trying to sample from lattice points, instead of adding noise around a lattice point).}
\subitem If not how does applying an extractor change the distance of two distributions?
\item How if it all can this be used concurrently?  Seems very one time right now.
\item How efficient will our decoding procedure be?  Efficient enough for practical usage?
\end{enumerate}

\textbf{What security do we need: }
}

\ignore{
\section{Robustness of LWE to error with min-entropy}
\label{sec: lwe min-entropy}

In this section we explore the robustness of the LWE assumption when the errors are drawn from a deficient distribution.  We provide a short introduction to the Learning with Errors problem in \secref{sec:fuzzyCompLWE}.  For a more complete introduction see the survey by Regev~\cite{regevLWEsurvey}.  The tools presented in this section are used to show the security of the computational secure sketch presented \subsecref{subsec:fuzzyExtLWE}.  This work is also of independent interest and can be viewed in a similar way to the work of Goldwasser et. al.~\cite{goldwasserRobustLWE}.  In their work, they consider the robustness of an LWE instance $A, Ax+e$ where $x$ is sampled from a distribution that has min-entropy but is not uniform.  They then use this result to show LWE-based cryptosystems are leakage-resilient.  Our theorem statement will follow a similar structure to the theorem statement of Goldwasser et. al. so we first present their informal theorem:
\begin{theorem}~(\cite[Theorem 1 (Informal)]{goldwasserRobustLWE})
For any super-polynomial modulus $q=q(n)$, any $k\geq \log q$, and any distribution $\mathcal{D}=\{D\}_{n\in\mathbb{N}}$ over $\{0,1\}^n$ with min-entropy $k$ , the (non-standard) LWE assumption, where the secret is drawn from distribution $\mathcal{D},$ follows from the (standard) LWE assumption with secret size $\ell\overset{\Delta}=\frac{k-\omega(\log n)}{\log q}$ (where the ``error rate'' is super-polynomially small and the adversaries run in time $\poly(n)$).
\end{theorem}

Before considering high entropy error, we must be clear about what this means.  The $x$ in an LWE instance is sampled uniformly from $\{0,q\}^n$, thus the sampling procedure requires $\lceil \log q\rceil\times n$ bits.  However, $e$ is sampled from some error distribution $\chi$.  $\chi$ is normally the normal distribution centered around $0$ with some variance $\sigma^2$ and then rounded to the nearest integer between $[-q/2, q/2]$.  Sampling from a rounded normal distribution requires a variable number of random bits for each dimension.    Indeed, using the normal distribution there are events with nonzero but negligible probability.  These events necessarily take a large number of bits to sample.  

\textbf{This is internal discussion and needs to be taken care of before presentation:}  There are two ways to provide these bits: 1) break the random string into chunks where each chunk is as long as the maximum number of random bits needed 2) provide the random bits needed to each dimensions.  One might hope that this problem is avoided by rounding to the nearest integer, but there is still some weight assigned to integers far from $0$.  We could consider a distribution whose statistical distance to the normal distance is exponentially small and gives weight 0 to all integers outside some radius.

For the moment we will ignore these problems and assume that a fixed number of bits is used to sample each dimension.  This would be the case if we substituted the uniform distribution on a significantly smaller domain.  \bnote{We need to be sure this produces a secure LWE instance}.

Thus, we assume that there exists a $\sample$ that uses $\ell$ bits of randomness to produce an error term in one dimension.  We are now ready to present the LWE assumption:
\begin{assumption}[Learning with Errors]
Let $k$ be a security parameter and define $q=\poly(k)$\bnote{Do we want the poly or exponential case?}and $n = \poly( k)$.
Let $x\overset{\$}\leftarrow \{0,q\}^n$.   An LWE sample is of the form $(a,b) = (a, a_1x_1+...+a_nx_n+\sample(e) \mod q)$ where $a_1,...,a_n\overset{\$}\leftarrow\{0,q\}^n$ and $e\overset{\$}\leftarrow\{0,1\}^\ell$.  The $(m,n,q,k)$-LWE assumption is that no algorithm given $m = \poly(k)$ LWE samples running in time $\poly(k)$ can recover $x$ with probability greater than negligible in $k$.
\end{assumption}

We then achieve our non-standard LWE assumption by sampling $e$ from a high-entropy distribution instead of the uniform distribution.

\subsection{Affine min-entropy distributions}
In the previous section, we investigated the security of the LWE assumption with deficient distributions.  Namely, we saw that the LWE assumption for block-fixing sources was implied by the LWE assumption with fewer dimensions and samples.  We now ask: what other types of min-entropy distributions can be shown to reduce to the LWE assumption?  Using a reduction to standard LWE seems to require that the errors be considered homomorphically.  This is because recovering the error terms would break LWE.  Furthermore, these errors are first put through the \sample procedure, so this needs homomorphic properties as well.  Thus, the class we can hope for in this reduction is a linear set of errors.

Recall, a function, $f:D^n \mapsto R$, over field $K$ is a \textbf{linear transformation} if $\forall x,y\in D$ and $\forall a, b\in K, f(ax+by) = af(x) + bf(y)$.

We will consider the \textbf{LWE assumption w/ linear sources}.  That is, we consider an error distribution $E = E_1||...||E_k$ where $\Hoo(E) = qn$ and $E = \mathbf{T}(U_1||...||U_n)$ for rank $n$ matrix $\mathbf{T}: \Fq^n\mapsto \Fq^k$.
\vspace{.1in}
\begin{theorem}\label{thm:linearLWE}
Let $k$ be a security parameter and define $q, n, m = \poly(k)$.  Let $E$ be a linear source over $\{0,1\}^{\ell(m+\alpha)}$ where $\Hoo(E) \geq m\ell$.  Then the $(m,n,q,k)$-LWE assumption where \sample is a linear transformation implies the $(m+\alpha, n+\alpha, q, k)$-LWE w/ linear sources assumption.
\end{theorem}
\begin{proof}
The main difference between \thref{thm:blockLWE} and \thref{thm:linearLWE} is how we produce the errors terms.  We will use the same basic strategy of introducing equations and randomizing, however, the equations will errors will now be involved in the randomization.  As before, let $\mathcal{A}$ be an algorithm that accepts $m+\alpha$ LWE samples each of length $n+\alpha$ where the errors are generated $(E_1,..., E_{m+\alpha})^T = \mathbf{T} (U_1,..., U_m)^T$ for some matrix $\mathbf{T}$.  Further, assume that $\mathcal{A}$ returns $x$ with noticeable probability.  Our algorithm $\mathcal{A'}$ will make a single call to $\mathcal{A}$ and we will focus on properly preparing the LWE w/ linear sources instance.  We use $A',A,C,D,F,G,b,b'$ with the same meaning as before.
\end{proof}
}



\ignore{
\subsection{LWE search reduction}
\begin{proof}
Let $\mathcal{A}$ be an algorithm that accepts $m+\alpha$ LWE samples each of length $n+\alpha$ where $\alpha$ of the equations have a known error (and the remainder of the errors are uniformly generated) and returns $x$ with noticeable probability.  Denote this probability as $\epsilon$.  We will show how to convert $\mathcal{A}$ into an algorithm $\mathcal{A'}$ that accepts $m$ LWE samples of length $n$ and returns $x'$ with noticeable probability.  We begin by making the following assumptions for clarity:
\begin{itemize}
\item $\mathcal{A}$ asks for all $m+\alpha$ samples and asks for them simultaneously.  Since $\mathcal{A}$ has no effect on the samples, this does not limit $\mathcal{A}$'s power.
\item For the known error samples, the added error is $0$.  It is clear in our reduction where the added error would be added, but exposition is clearer without these errors.
\item The last $\alpha$ samples are the known error samples.  For other positioning of the known error samples, we can provide a permutation from our input, but this form is clearer.
\end{itemize}
The algorithm $\mathcal{A'}$ will make a single call to the algorithm $\mathcal{A}$ and thus the entire analysis will be showing that $\mathcal{A'}$ is able to properly prepare the instance that $\mathcal{A}$ is expecting and convert the result of $\mathcal{A}$ into a solution for the input of $\mathcal{A'}$.  We will use the matrix notation for the remainder of the proof.  Thus, the job of $\mathcal{A'}$ is to perform the following transformation:
\begin{align*}
\begin{array}{c | c}
\mathcal{A'}\text{ receives:        } & \mathcal{A}\text{ expects: }\\\hline
\left(A'\right) , b' = A' x' + e' & A = \left( \begin{array}{c | c}C & D \\ \hline F & G\end{array} \right), b=Ax+e
\end{array}
\end{align*}
Where the matrices have the dimensions as shown in \figref{fig:matrixDimension}: 
\begin{figure}[b]
\[
\begin{array}{c | c | c | c | c | c}
A & A' & C & D & F & G\\\hline
m\times n & (m+\alpha)\times( n+\alpha) & m\times n & m \times \alpha & \alpha \times n & \alpha \times \alpha
\end{array}
\]
\caption{Dimensions of matrices in reduction.}
\label{fig:matrixDimension}
\end{figure}
Thus, we have two major jobs, produce an augmented matrix $A$ that is a truly random matrix and to produce an augmented $b$ that given a solution for $x$ will yield a solution for $x'$.  The code of $\mathcal{A'}$ is presented in \figref{fig:imperfectLWEreduction}.  The main idea is to generate $\alpha$ new LWE samples and use $\alpha$ new variables to exactly solve these equations.  These last $\alpha$ equations then have no errors and can be used to randomize the instance to produce a random matrix without augmenting the error in the $b$ vector.
\begin{figure}
\begin{enumerate}
\item Input $A', b' = A'x' +e'$.
\item Randomly generate $F$.
\item Set $b = (b'_1,..., b'_n, \$, ..., \$)$ that is set the first $n$ values of $b$ equal to $b'$ and randomly generate the remainder.
\item Initialize $A = \left(\begin{array}{c | c}C = A' & D = \mathbf{0}\\\hline F & G = I\end{array}\right)$.
\item For $i=1...m+\alpha$, perform the following randomization:\label{step:randomizationSearch}
\subitem Randomly generate $\gamma_{i,1},.., \gamma_{i,\alpha}$.
\subitem Set $A_{i, \cdot} = A_{i, \cdot} +\sum_{j=1}^\alpha \gamma_{i,j}A_{m+j, \cdot}$.
\subitem Set $b_i = b_i +\sum_{j=1}^\alpha \gamma_{i,j} b_j$.
\item Run $\mathcal{A}$ on input $A, b$.
\item Receive output $x$ from $\mathcal{A'}$
\item Output $x' = x_1,..., x_n$.
\end{enumerate}
\caption{Code for $\mathcal{A'}$ to generate LWE w/ symbol fixing source from standard LWE instance}
\label{fig:imperfectLWEreduction}
\end{figure}
\begin{claim}\label{cl:randomMatrix}
The matrix $A$ generated after Step \ref{step:randomizationSearch} is a random matrix.
\end{claim}
\begin{proof}
First, the submatrices $D, G$ are clearly truly random after step \ref{step:randomizationSearch}.  For the matrix $D, D_{i, j}$ is assigned the value $\gamma_{(i,j-n)}$.  Similarly, for the matrix $G$, each entry is assigned as $\gamma_{(i-n, j-n)}$.  This is due to the original construction of the matrix $G$.  Then before step \ref{step:randomizationSearch} the matrices $C, F$ are random submatrices ($C=A'$ by assumption and $F$ by construction).  Thus, it remains to show that $C, F$ remain random after step 5.  We consider a single row vector of $C$.  The new row vector $C_{(i,\cdot)} = C_{(i,\cdot)} +\sum_{j=1}^\alpha \gamma_{i,j}F_{(j, \cdot)}$.  That is, $C_{(i,\cdot)}$ is the sum of a random vector and a random linear combination independent vectors.  That is, $C_{(i,\cdot)}$ is a random vector.  

Similarly, consider a single row vector $F$. The new row vector $F_{(i,\cdot)} = \sum_{j=1}^\alpha \gamma_{i+n,j}F_{(i,\cdot)}$ is a random linear combination of independent vectors and thus truly random (since all $\gamma$ are chosen independently and randomly).  \bnote{I feel these claims are obvious but I would like to have a citation.}

Thus, the entire matrix $A=\left(\begin{array}{c | c}C & D \\\hline F & G\end{array}\right)$ is a truly random matrix.  This completes the claim.
\end{proof}
\begin{claim}\label{cl:goodLWEinstance}
The inputs $A, b$ provided to $\mathcal{A}$ is a random LWE w/ symbol fixing sources instance (provided that $A', b'$ was a random instance).
\end{claim}
\begin{proof}
By \clref{cl:randomMatrix} $A$ provided to $\mathcal{A}$ is a truly random matrix.  Thus, we must show that $b$ is of the proper form.  That is, we must show there exists an $x$ such that $b_i = A_{(i , \cdot)} x + e_i$ for $i=1,...,m$ and $b_i = A_{(i, \cdot)} x$ for $i=m+1,...,m+\alpha$.

By assumption there exists an $x'$ such that $A'x'+e' = b'$.  Before step \ref{step:randomizationSearch}, we have added $\alpha$ equations as well as $\alpha$ unknowns.  This means there no solutions have been eliminated.  Namely, $x^*_i = x'_i$ for $i=1,...,n$ and $x^*_i = b_i - \sum_{j=1}^m F_{i, j} x_j'$ for $i=n+1,..., n+\alpha$.  The case where $F$ does not have full row rank is also handled because the $G$ is the identity matrix and thus $F|G$ always has full row rank.  Furthermore, note that the last $\alpha$ equations have no error.  Thus, before step \ref{step:randomizationSearch} there exists a solution $x^*$ of the proper form.  It remains to show that the randomization in step \ref{step:randomizationSearch} retains the solution $x^*$.

The newly randomized system $A,b$ retains the same solution $x^*$  because we are not adding rows that have any error terms.  More precisely, let $A^*, b^*$ be the system before randomization (where $A^*x^*+e^* = b^*$).  Then for each row vector 
\begin{align*}
A_{(i, \cdot)} &= A^*_{(i, \cdot)}+\sum_{j=1}^\alpha \gamma_{i, j} A^*_{(m+j, \cdot)}, \\b_i &= b^*_i + \sum_{j=1}^\alpha \gamma_{i,j}b_j^*.
\end{align*}
Thus, we have the following for all $i=1,..., m$:
\begin{align*}
A_{(i, \cdot)} x^* +e_i^*&= (A^*_{(i, \cdot)}+\sum_{j=1}^\alpha \gamma_{i, j} A^*_{(m+j, \cdot)})x^*+e_i^*\\
&= A^*_{(i, \cdot)}x^* + \sum_{j=1}^\alpha \gamma_{i,j} A^*_{(m+j, \cdot)}x^*+e_i^*\\
&=\left( A^*_{(i, \cdot)}x^* + e_i^*\right)+ \sum_{j=1}^\alpha \gamma_{i,j} A^*_{(m+j, \cdot)}x^* \\
&= b^*_i + \sum_{j=1}^\alpha \gamma_{i,j}b_j^* = b_i
\end{align*}

The same equations follow without the term $e_i^*$ for $i=m+1,..., m+\alpha$.  
Thus, $x^*$ is a solution to the randomized LWE instance as well.  It remains to show that $x^*$ is a random vector.  This is clear for $x^*_i, i=1,...,n$.  For the remaining $x^*_i, i=m+1, ..., m+\alpha$ this follows since $x^*_i = b_i - \sum_{j=1}^m F_{i, j} x_j'$ where $b_i, F_{i, j}$ are randomly and independently chosen.  This completes the claim.
\end{proof}
\begin{claim}
If $x$ is a ``good'' solution output by $\mathcal{A}$ then $x'_i = x_i$ for $i=1,...,n$.  
\end{claim}
This claim follows by the analysis in \clref{cl:goodLWEinstance}.  The added equations and randomization do not affect the solution to the original $A', A'x'+e'$ instance provided.
\begin{claim}
$\mathcal{A'}$ outputs a ``good'' solution with probability only negligibly different  than the probability $\mathcal{A}$ outputs a good solution.
\end{claim}
The only time that $\mathcal{A'}$ does not produce the correct distribution is when the rows of $F$ are not linearly independent.  However, this occurs with negligible probability~\cite{something}.

Thus, we have constructed an algorithm $\mathcal{A'}$ that is able to recover a secret $x'$ of a standard LWE instance by adding samples  and equations that have no error.  Notice it was critical to add a variable for each new sample being provided to $\mathcal{A}$.  Furthermore, adding a degree of freedom for each new sample also allowed for construction of an equation with no error.  This completes the proof.

\end{proof}

\ignore{
\section{Parameters for Construction~\ref{cons:LWESecureSketch} when $q=2^n$}
\label{sec:parameters q expo}
The results of~\lemref{lem:uniform LWE decision} only support $q = \poly(n)$.  However, the reductions of Peikert may extend~\lemref{lem:uniform LWE decision} to exponential $q$.  

$q = O(2^n)$. In this setting, we are no longer constrained by field size.  We need $\rho$ be large enough that the underlying lattice problems are still hard to approximate within a $\tilde{O}(n^{5/2}m/\rho)$ factor.  Unfortunately, even correcting a constant $c$ number of errors is difficult in this domain.  Let $\gamma$ be the maximum approximation factor where the underlying lattices problems are still hard.  Then, $\rho \leq \frac{n^{5/2}m}{\gamma}$.  To correct $c$ errors we need that $\log q/\log \rho q \geq c$.  That is,
\begin{align*}
c&\leq \frac{\log q}{\log \rho q}\\
&\leq \frac{n}{\log \frac{n^{5/2}mq}{\gamma}}\\
&\leq \frac{n}{\log \frac{n^{5/2}m2^n}{\gamma}}\\
&\leq \frac{n}{n + \log n + \log m - \log \gamma}
\end{align*}
Ignoring the $\log n$ and $\log m$ terms this yields that:
\begin{align*}
c&\leq \frac{n}{n-\log \gamma}\\
\frac{n(c-1)}{c}&\leq \log \gamma\\
2^{n(c-1)/c} &\leq \gamma
\end{align*}
Thus, supporting even a constant number of errors for $q=2^n$ requires lattice problems are exponentially hard to approximate, which would be quite surprising.  
}
}
\end{document}











