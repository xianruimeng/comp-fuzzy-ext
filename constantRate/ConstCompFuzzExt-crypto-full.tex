\def\shownotes{0}
\def\blinded{1}
\def\lncs{1}

\ifnum\lncs=0
	\documentclass[11pt]{article}
	\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
	\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
	%\geometry{landscape}                % Activate for for rotated page geometry
	%\usepackage[parfill]{parskip}
	\usepackage{amsthm}
	\newtheorem{theorem}{Theorem}[section]
	\newtheorem{lemma}[theorem]{Lemma}
	\newtheorem{proposition}[theorem]{Proposition}
	\newtheorem{corollary}[theorem]{Corollary}
	\newtheorem{definition}[theorem]{Definition}
	\newtheorem{assumption}[theorem]{Assumption}
	\newtheorem{claim}[theorem]{Claim}
	\newtheorem{problem}[theorem]{Problem}
	\newtheorem{construction}[theorem]{Construction}

\else
	\documentclass[runningheads]{../Asiacrypt/llncs}
	\newtheorem{construction}{Construction}
	\renewcommand{\paragraph}[1]{\subsubsection{#1}}
\fi


\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\error}{\ensuremath{\class{Err}}\xspace}
\newcommand{\weight}{\ensuremath{\class{Wgt}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\cond}{\ensuremath{\mathtt{cond}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}


\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\blinded=0
\newcommand{\blind}[1]{{#1}}
\else
\newcommand{\blind}[1]{}
\def\shownotes{0}
\fi


\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi


\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Vol}{\mathsf{Vol}}
\newcommand{\subsetEntropy}{\alpha}

\title{Fuzzy Extractors for Noisy Sources \\with More Errors Than Entropy}
%\author{Ran Canetti \and Benjamin Fuller \and Omer Paneth \and Leonid Reyzin \and Adam Smith}
\blind{
\author{Ran Canetti\footnote{Email: {\tt canetti@cs.bu.edu}. Boston University and Tel Aviv University.} \and Benjamin Fuller\footnote{Email: {\tt bfuller@cs.bu.edu}.  Boston University and MIT Lincoln Laboratory.} \and Omer Paneth\footnote{Email: {\tt paneth@cs.bu.edu}. Boston University.} \and Leonid Reyzin\footnote{Email: {\tt reyzin@cs.bu.edu}.  Boston University.} \and Adam Smith\footnote{Email: {\tt asmith@cse.psu.edu}.  Pennsylvania State University; work performed while at Boston University's Hariri Institute for Computing and RISCS Center, and Harvard University's
``Privacy Tools'' project.} }
}
\begin{document}
\maketitle


\begin{abstract}
Fuzzy extractors (Dodis et al., Eurocrypt 2004) convert repeated noisy readings of a high-entropy secret into the same uniformly distributed key. To eliminate noise, they require an initial enrollment phase that takes the first noisy reading of the secret and produces a nonsecret helper string to be used in subsequent readings. This helper string reduces  the entropy of the original secret---in the worst case, by as much as the logarithm of the number of tolerated error patterns. For many practical sources of secrets, reliability demands that the number of tolerated error patterns is large, making this loss greater than the original entropy of the secret. We say that such sources have \emph{more errors than entropy}.  Most known approaches for building fuzzy extractors cannot be used for such sources.

\ifnum\lncs=1\medskip\fi

%\emph{Reusable} fuzzy extractors (Boyen, CCS 2004)
% remain secure even when the initial enrollment phase is repeated multiple times on correlated readings, producing multiple helper strings. Prior approaches to building reusable fuzzy extractors severely restricted allowed correlations among initial enrollment readings.
%   
%We provide constructions of fuzzy extractors for large classes of sources with more errors than entropy.  We also construct the first reusable fuzzy extractors that has no restrictions on correlations among multiple readings of the source. Our constructions exploit the structural properties of a source in addition to its entropy guarantees. One is information-theoretic; the other two rely computational assumptions. They differ in the types of sources they support.

We provide constructions of fuzzy extractors for large classes of sources with more errors than entropy.  Our constructions exploit the structural properties of a source in addition to its entropy guarantees. Some are made possible by relaxing the security requirement from information-theoretic to computational.

%One is information-theoretic; the other two rely computational assumptions. They differ in the types of sources they support.
\ifnum\lncs=1\medskip\fi

\emph{Reusable} fuzzy extractors (Boyen, CCS 2004)
 remain secure even when the initial enrollment phase is repeated multiple times with the same or correlated secrets, producing multiple helper strings. By relying on computational security, we construct the first reusable fuzzy extractors that make no assumption about how multiple readings of the source are correlated.


\medskip

\textbf{Keywords} Fuzzy extractors, reusability, key derivation, error-correcting codes, computational entropy, point obfuscation.
\end{abstract}


\section{Introduction}\label{sec:introduction}

\paragraph{Fuzzy Extractors}
Cryptography relies on long-term secrets for key derivation and authentication. However, many sources with sufficient randomness to form long-term secrets provide similar but not identical values of the secret at repeated readings. Prominent examples include biometrics and other human-generated data~\cite{daugman2004,zviran1993comparison,brostoff2000passfaces,ellison2000protecting,mayrhofer2009shake,monrose2002password},
physically unclonable functions (PUFs)~\cite{pappu2002physical,tuyls2006puf,gassend2002silicon,suh2007physical},
and quantum information \cite{bennett1988privacy}. Turning similar readings into identical values is known as \emph{information reconciliation}; further converting those values into uniformly random secret strings is known as \emph{privacy amplification}~\cite{bennett1988privacy}.
Both of these problems have interactive and non-interactive versions.  In this paper, we are interested in the non-interactive case, which is useful for a single user trying to produce the same key from multiple noisy readings of a secret at different times.
 A \emph{fuzzy extractor} is the primitive that accomplishes both information reconciliation and privacy amplification non-interactively; fuzzy extractors are defined in~\cite{DBLP:journals/siamcomp/DodisORS08}.


Fuzzy extractors consist of a pair of algorithms: \gen (used once, at ``enrollment'') takes a source value $w$, and produces a key $r$ and a public helper value $p$.  The second algorithm \rep (used subsequently) takes this helper value $p$ and a close $w'$ to reproduce the original key $r$.
The correctness guarantee is that $r$ will be correctly reproduced by \rep as long as $w'$ is no farther than $t$ from $w$ in some metric space. In this work, we consider the Hamming metric.
 The security guarantee is that $r$ produced by \gen is close to uniform (information-theoretically \cite{DBLP:journals/siamcomp/DodisORS08} or computationally \cite{fuller2013computational}), even given $p$. This guarantee holds as long as $w$ comes from a high-quality distribution, which
traditionally has been defined as \emph{any} distribution with sufficient min-entropy $m$. Note that in this paper we consider only passive attacks on $p$; the problem of protecting against modification of $p$ has been addressed in, e.g., \cite{Boyen05secureremote,DKKRS12}.  In particular, the random-oracle-based transform of \cite[Theorem 1]{Boyen05secureremote} can be applied to our constructions.

\paragraph{Limitations of Known Approaches}

Constructions of fuzzy extractors are limited by the tension between security and correctness guarantees: if we allow for higher error tolerance $t$, then we also need higher starting entropy $m$. The reason for this tension is simple: if an adversary who knows $p$ can guess some $w'$ within distance $t$ of $w$, then it will be able to easily obtain the true $r$ by running $\rep$.
%If $t$ is larger, then a single guess for $w'$ may be within distance $t$ of more $w$ values, thus increasing adversarial probability of winning.
In fact, if $t$ is high enough that there are $2^m$ points in a ball of radius $t$, then there exists a distribution of $w$ of min-entropy $m$  \emph{contained entirely in a single ball}.  For this distribution, an adversary can run $\rep$ on the center of this ball and always learn the key $r$.
Thus, if the security guarantee of a given fuzzy extractor holds for \emph{any} source of a given min-entropy $m$ and the correctness guarantees holds for any $t$ errors, then $m$ must   be greater than $\log |B_t|$, where $|B_t|$ denotes the number of points in a ball of radius $t$.  This condition on the source holds regardless of whether the fuzzy extractor is information-theoretic or computational, and extends even to the interactive setting.
If a source fails this condition, we will says that it has \emph{more errors than entropy}.



%More generally, for any $m$ and $t$, there is a distribution of min-entropy $m$ such that the adversary can guess a correct $w'$ with probability $1/\lceil( 2^m/B_t) \rceil\approx B_t 2^{-m}$: the distribution consists of the uniform distribution over all points in several non-overlapping balls of radius $t$ (the metric space must be large enough for these balls not to intersect). We thus call $m-\log B_t$ the \emph{minimum usable} entropy, denoted by $\Huse$. The previous paragraph shows that  no fuzzy extractor can handle all distributions of a given min-entropy $m$ if  $\Huse\le 0$.

%Prime candidate sources for authentication have $\Huse\le 0$.
Unfortunately, sources that have been proposed as prime candidates for authentication have more errors than entropy.
For example, the IrisCode~\cite{daugman2004}, which is the state of the art approach to handling what is believed to be the best biometric \cite{prabhakar2003biometric}, produces a source that more errors than entropy~\cite[Section 5]{blanton2009biometric}. PUFs with slightly nonuniform outputs suffer from similar problems~\cite{koeberl2014entropy}.

%As an example, the iris is believed to be the best biometric for high security applications~%\cite{prabhakar2003biometric}.  Daugman~\cite{daugman2004} designs a transform called IrisCode (using specialized wavelets) to derive a $2048$ bit string from an iris.  Let the outcome of this transform (on different irises) define a distribution $w$.  The precise number of errors that must be tolerated depends on the desired correctness.  For correctness of around $20\%$, a $t$ of approximately $205$ is required. Thus,
%\[
%\log |B_t|
%= \log \sum_{i=0}^{205} {2048 \choose i} \approx 956\,.
%\]
%In contrast, $w$ is estimated to have about $249$ bits of entropy~\cite{daugman2004}.
%There is considerable subsequent research~\cite{gentile2009slic,gentile2009efficient,rathgeb2011combining}, but it does not affect the above comparison dramatically.%\footnote{The work of Hao et al.~\cite[Section 4.3]{hao2006combining} provides a similar analysis but their calculation underestimates the number of possible error patterns; their calculation, which we cannot confirm, is $\Huse \approx 44$.}




\begin{table}
\begin{center}
\ifnum\lncs=0
\begin{tabular}{l l l l l}
  & Source Structure & Error Rate  & Techniques\\
\hline
Cons. \ref{cons:sampling} (reusable)  & high-entropy samples & any subconstant &  Sample-then-obfuscate \\
Cons. \ref{cons:first construction}  & sparse high-entropy  marginals & constant &  Error-correct-and-obfuscate\\
Cons. \ref{cons:info theoretic} (info-theoretic)   & sparse block source  & constant & Condense-then-fuzzy-extract\\
\end{tabular}
\else
\begin{tabular}{l l l l l}
  & Source Structure & Error Rate  & Techniques\\
\hline \vspace{-3mm} \\
Construction \ref{cons:sampling} & high-entropy & any subconstant \ \ &  Sample-then-obfuscate \\
\ \ (reusable)& \ \ samples\\ \vspace{-3mm} \\
Construction \ref{cons:first construction}  & sparse high-entropy \ \  & constant &  Error-correct-and-obfuscate\\
& \ \ marginals\\ \vspace{-3mm} \\
Construction \ref{cons:info theoretic}   & sparse block source  & constant & Condense-then-fuzzy-extract\\
\ \ (info-theoretic)\\
\hline\end{tabular}
\fi
\end{center}
%end centering
\caption{Summary of new constructions. \consref{cons:sampling} supports the widest class of sources, including the sources supported by the other two constructions, but tolerates fewer errors than the other two constructions.  \consref{cons:first construction} and \consref{cons:info theoretic} support incomparable classes of sources.}
\label{tab:upper bounds}
\end{table}

\paragraph{Our Contributions: Handling More Errors than Entropy}
We provide the first constructions of fuzzy extractors that can be used for large classes of sources that have more errors than entropy.  Our constructions work for Hamming errors for strings $w$ of length $\gamma$ over some alphabet $\mathcal{Z}$. Naturally, as argued above, these constructions cannot work for all sources of a given entropy and for any $t$ errors. In this work, each construction takes advantage of the specific properties of a source, but still works for any $t$ errors. (It is an open problem to achieve our goal by restricting the error patterns instead of restricting the source distribution.)

Our first construction works for \emph{sources with high-entropy samples}: these are sources for which a randomly sampled substring of some superlogarithmic length is likely to have superlogarithmic  entropy. For example, a source can satisfy this condition if a constant fraction of symbols are almost $k$-wise independent for some superlogarithmic $k$ (note there is no total entropy requirement:  the total entropy of such a source may be not  much greater than the entropy of the sample). This construction uses digital lockers~\cite{canetti2008obfuscating}, which lock up the output $r$ using the sampled substring as the key to the locker. Such lockers can be constructed very efficiently via a single application of a cryptographic hash function (under appropriate assumptions) or, more generally, from point obfuscation, as we discuss below.

We augment the first construction with error-correcting codes to obtain our second construction, which works for \emph{sources with sparse high-entropy marginals}: sources over large alphabets for which sufficiently many symbols have high entropy individually, but no independence among symbols is assumed. In this construction, we use single symbols of $w$ to lock up bits of a secret that we then transform into $r$. This construction tolerates a higher error rate than the first construction, but over a larger alphabet, where errors may be more likely. 

Our third construction provides information-theoretic, rather than computational, security. It works for \emph{sparse block sources}. These are sources in which a sufficient fraction of the symbols have entropy conditioned on previous symbols. It uses symbol-by-symbol condensers to reduce the alphabet size while preserving most of the entropy, and then applies a standard fuzzy extractor to the resulting string.

Our approach in all three constructions is different from most known constructions of fuzzy extractors, which put sufficient information in $p$ to recover the original $w$ from a nearby $w'$ during $\rep$ (this procedure is called a \emph{secure sketch}). We deliberately do not recover $w$, because known techniques for building secure sketches do not work for sources with more errors than entropy, since they lose at least $\log |B_t|$ bits of entropy regardless of the source. (This loss is necessary when the source is uniform~\cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08} or when reusability against a sufficiently rich class of correlations is desired~\cite[Theorem 11]{Boyen2004}; computational definitions of secure sketches suffer from similar problems~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational}.)

\paragraph{Our Contributions: Reusability}
An additional desirable security property of fuzzy extractors, introduced by Boyen~\cite{Boyen2004}, is called reusability. This property is necessary if a user enrolls the same or correlated values multiple times. For example, if the source is a biometric reading, the user may enroll the same biometric with different organizations.  Each of them will get a slightly different enrollment reading $w_i$, and will run $\gen(w_i)$ to get a key $r_i$ and a helper value $p_i$. Security for each $r_i$ should hold even when an adversary is given all the values $p_1, \dots, p_q$ (and, in case some organizations turn out to compromised or adversarial, a stronger security notion requires security for $r_i$ even in the presence of $r_j$ for $j\neq i$).  Many traditional fuzzy extractors are not reusable~\cite{Boyen2004,simoens2009privacy,blanton2012non,blanton2013analysis}. In fact, the only known construction of reusable fuzzy extractors \cite{Boyen2004} requires very particular relationships between $w_i$ values, which are unlikely to hold in any practical source.

Our first construction provides reusability (against computationally bounded adversaries).  
The reusability we obtain is very strong:  security holds even if the multiple readings $w_i$ used in $\gen$ are \emph{arbitrarily correlated}, as long as each $w_i$ \emph{individually} comes from a source with high-entropy samples. This construction is the first to provide reusability for a realistic class of correlated readings.
 





%\begin{table}
%\begin{tabular}{l | l | l | l }
%Construction & Security & Main Feature  & Main Limitations \\
%\hline
%\consref{cons:info theoretic} & Info-theoretic & Exploit distribution & Symbols must add\\
%&&structure & fresh entropy \\\hline
%\consref{cons:sampling} & Computational  & Reusability & Sub-constant fraction \\
%&& & error tolerance\\\hline
%\consref{cons:first construction} & Computational & Allows correlated & Super-polynomial \\
%& &  symbols & size symbols
%\end{tabular}
%\caption{Summary of new constructions.  All constructions support families of distributions with more errors than entropy.}
%\label{tab:upper bounds}
%\end{table}


\paragraph{Relation to Obfuscation} 
Our constructions use simulation-secure obfuscation of digital lockers. The precise definition we require is not full-fledged virtual black-box obfuscation~\cite{barak2001possibility}. Instead, we rely on the relaxed notion of \emph{virtual grey-box} obfuscation, as long as it remains secure even when several digital lockers with correlated keys are composed~\cite{bitansky2010strong}. 

The following simple and efficient construction of digital lockers, proposed by Lynn, Prabhakaran, and Sahai~\cite[Section 4]{lynn2004positive}, provides the desired security in the random oracle model of~\cite{DBLP:conf/ccs/BellareR93}. Let $H$ be a cryptographic hash function, modeled as a random oracle. To lock up a value $y$ using a key $x$, output the pair $r, H(r, x)\oplus (y||0^k)$, where $r$ is a nonce, $||$ denotes concatenation, and $k$ is some security parameter. As long as the entropy of $x$ is superlogarithmic, the adversary has negligible probability of finding the correct $x$; and if the adversary doesn't find the correct $x$, then the adversarial knowledge about $x$ and $y$ is not significantly affected by this locker.  Concatenation with $0^k$ is used to make sure that it is possible to tell (with certainty $1-2^{-k}$) when the correct value is unlocked. 

It is seems plausible that in the standard model (without random oracles), specific cryptographic hash functions, if used in this construction, will provide the necessary security~\cite[Section 3.2]{canetti2008obfuscating}, \cite[Section 8.2.3]{dakdoukThesis}. 
Moreover, Bitansky and Canetti~\cite{bitansky2010strong}, building on the work of~\cite{canetti2008obfuscating,CKVW10}, show how to obtain composable digital lockers based on a strong version of the Decisional Diffie-Hellman assumption without random oracles.


%Any procedure that converts a high-entropy input to a high-entropy output is known as a \emph{conductor} \cite{CRVW02}; if it's error-tolerant, then it's a \emph{fuzzy conductor}~\cite{KanukurthiR09}. Our first construction is a \emph{computational fuzzy conductors}.
%This may be converted to a computational fuzzy extractors using information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors~(\lemref{lem:cond and cext}).

%Both constructions are based on  obfuscation of point programs~\cite{canetti1997towards}.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.
%We need a strong version of point obfuscation that remains secure even when several obfuscations of correlated points are composed. While the standard definition of obfuscation \cite{barak2001possibility} does not imply security under composition, we can base our construction on the relaxed notion of \emph{virtual grey-box} obfuscation introduced in~\cite{bitansky2010strong}. For this notion, \cite{bitansky2010strong} construct composable obfuscation of point programs under particular number-theoretic assumptions. Additionally, such obfuscation can be made very efficient under a strong assumption on cryptographic hash functions~\cite{canetti1997towards}.

%Both of our constructions use techniques from Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  Let $w=w_1 \dots w_\ell$, for $w_j\in \mathcal{Z}$. In the first construction (\consref{cons:first construction}), for each $j$, $\gen$ flips a coin $c_j$ and either obfuscates $I_{w_j}$ or picks a random point $r_j$ and obfuscates $I_{r_j}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rep$ then runs the obfuscated program $P_j$ on $w_j'$ and checks whether $P_j(w_j')=1$.  For most locations $j$, \rep can determine whether $w_j$ or a random value was obfuscated.  Thus, most bits of $c_j$ are recoverable. To tolerate errors,  the set of coins $c_1\dots c_\ell$  is chosen at random from the codewords of an error correcting code. This construction conducts entropy from $w$ to $c$.

%Obfuscation of point functions provides no security if a point can be guessed; thus, in order for the first construction to be secure, sufficiently many coordinates of $w$ have to be unguessable (even to an adversary who can make equality queries for the values of other coordinates). We relax this requirement in our second construction (\consref{cons:sampling}), called \emph{sample-then-obfuscate}: it transforms $w$ into a string of blocks and then obfuscates each string of blocks. $\gen$ randomly samples several coordinates of $w$ and concatenates them to form a block. This reduces the entropy requirement on the individual symbols, but lowers the error-tolerance. We limit the degradation of error-tolerance by using digital lockers in place of point obfuscation~(sufficiently composable point obfuscation implies digital lockers).  This approach is similar to the sample-then-extract paradigm for building locally computable extractors~\cite{lu2002hyper,vadhan2003constructing}.  Unlike in locally computable extractors, we can form multiple blocks sampling from the same value $w$ and only argue about their individual entropy, because correlations among blocks are allowed in the first construction. Computational, rather than information-theoretic, analysis seems crucial for achieving this property.


We also note that fuzzy extractors for sources with more errors than entropy can be trivially constructed from virtual grey-box obfuscation for the class of {\em proximity point programs}. A proximity point program $I_w(x)$ tests if $x$ is within distance $t$ of $w$. 
Recently, Bitansky et al.~\cite{BitanskyCKP14} constructed such obfuscation for large classes of programs, including proximity point programs, based on the strong assumption of {\em semantically secure graded encodings} \cite{PassTS13}.
However, in contrast to obfuscated digital lockers, the  more general construction of Bitansky et al. is based on stronger assumptions and is highly impractical in terms of efficiency.  
It is not known how to construct \emph{reusable} fuzzy extractors from general obfuscation, because known obfuscation of proximity point programs is not known to be composable.


%\paragraph{Open Problems}
%\lnote{can we kill this paragraph? I don't see how it helps our cause. And in any case the grouping construction supports constant size alphabets}
%All of our constructions support more errors than entropy by using two metrics spaces.  Errors are tolerated in one metric space and corrected in a second.  To handle more errors than entropy,  we map to a metric space where multiple error patterns are grouped together.  All our constructions require the first metric space to have a super-constant size alphabet.  An alternative approach to the problem may support constant size alphabets.

%Our first construction achieves a constant fraction error tolerance but at the cost of a large alphabet size.  Our second construction reduces the alphabet size significantly but only achieves a sub-constant error tolerance.  It remains an open question to support a constant fraction of errors on a small alphabet.  More generally, the problem of constructing a secure fuzzy extractor whenever a negligible fraction lies in any ``ball'' remains an open problem.  %Both constructions require a large alphabet $\mathcal{Z}$---one whose size is more than polynomial in the security parameter.\footnote{Codes over large alphabets are often used to correct burst errors~\cite{gilbert1960capacity}.}  It is possible to tweak the sample-then-obfuscate construction for use with a small alphabet.  However, distributions with $\Huse \le 0$ are supported only for large alphabets.  For small alphabets, $\Huse>0$ and there are good information-theoretic constructions known~\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08}.
%Constructing a computational fuzzy extractor when $\Huse\le 0$ and a small alphabet is an open problem.  \bnote{Should this just be removed?}

%Using information-theoretic fuzzy extractors with additional privacy properties, Dodis and Smith~\cite[Section 5]{DBLP:conf/stoc/DodisS05} construct program obfuscators for the program $I_w(x)$ that tests if $x$ is within Hamming distance $t$ of $w$. The obfuscation is secure as long as $w$ comes from a distribution of sufficient min-entropy; in particular, the entropy must be high enough so that $\Huse>0$. Our constructions do not provide obfuscators for proximity queries, because they leak more information than whether $x$ is within distance $t$ of $w$ (for example, they may provide some information about the actual distance or about which coordinates agree). Constructing an efficient obfuscator for proximity queries when $\Huse\le 0$ is an open problem.

%In this work we restrict the distribution of the original reading $w$ and allow $w'$ to be an arbitrary point within distance $t$.  An alternative approach is to restrict the set of $w'$ where $\gen$ produces the correct key. \lnote{move this somewhere that makes more sense, probably above where we talk about a ball around a $w'$ and the obvious impossibility}%  A meaningful restriction of correctable errors is an open problem. 
%\fi

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote by $X = X_1,..., X_\gamma$  the tuple $(X_1,\dots, X_\gamma)$.  For a set of indices $J$, $X_{J}$ is the restriction of $X$ to the indices in $J$.  The set $J^c$ is the complement of $J$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$,
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   For a random variable $W$, let $H_0(W)$ be the logarithm of the size of the support of $W$,  that is $H_0(W) = \log |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$ we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$ (we extend it to a class of distinguishers $\mathcal{D}$ by taking the maximum over all distinguishers $D\in\mathcal{D}$).  We denote by $\mathcal{D}_{s}$ the class of randomized circuits which output a single bit and have size at most $s$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric over vectors in $\mathcal{Z}^\gamma$, defined via $\dis(x,y) = |\{i | x_i \neq y_i\}|$.  For this metric, $|B_t| = \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
Usually, we use capitalized letters for random variables and corresponding lowercase letters for their samples.

\subsection{Fuzzy Extractors}
\label{sec:fuzzy extractors}

In this section we define computational fuzzy extractors.  Definitions for information-theoretic fuzzy extractors can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  %Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' ($\gen$) and ``reproduce'' ($\rep$) is an $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\kappa$ and a helper string $p\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(r, p)\leftarrow \gen(w)$, then $\Pr[\rep( w', p) = r] \geq 1-\delta$, where the probability is over the randomness of $(\gen, \rep)$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $r$ is pseudorandom conditioned on $p$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon_{sec}$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness. In Constructions~\ref{cons:sampling} and~\ref{cons:first construction} it is crucial that $w'$ is chosen independently of the outcome of \gen.

Information-theoretic fuzzy extractors are obtained by replacing computational distance by statistical distance.  We do make a second definitional modification.  The standard definition of information-theoretic fuzzy extractors considers $\mathcal{W}$  consisting of all distributions of a given entropy.  As described in the introduction, it is impossible to provide security for all distributions with more errors than entropy.  In both the computational and information-theoretic settings we consider a family of distributions $\mathcal{W}$.

\subsection{Reusable Fuzzy Extractors}
\label{sec:reusable}

An additional desirable feature of fuzzy extractors is reusability~\cite{Boyen2004}. Inuitively, it is the ability to support multiple independent enrollments of the same value, allowing users to reuse the same biometric or PUF, for example, with multiple noncommunicating providers. More precisely, the algorithm $\gen$ may be run multiple times on correlated readings $w_1,..., w_q$ of a given source. Each time, $\gen$ will produce a different pair of values $(r_1, p_1),..., (r_q, p_q)$. Security for each extracted string $r_i$ should hold even in the presence of all the helper strings $p_1, \dots, p_q$ (the reproduction procedure $\rep$ at the $i$th provider still obtains only a single $w'_i$ close to $w_i$ and uses a single helper string $p_i$). Because the multiple providers may not trust each other, a stronger security feature (which we satisfy) ensures that each $r_i$ is secure even when all $r_j$ for $j\neq i$ are also given to the adversary.

 Our ability to construct reusable fuzzy extractors depends on the types of correlations allowed among $w_1, \dots, w_q$. Boyen \cite{Boyen2004} showed how to do so when each $w_i$ is a shift of $w_1$ by a value that is oblivious to the value of $w_1$ itself (formally, $w_i$ is a result of a transitive isometry applied to $w_1$). Boyen also showed that even for this weak class of correlations, any secure sketch must lose at least $\log |B_t|$ entropy~\cite[Theorem 11]{Boyen2004}.
  

We modify the definition of Boyen~\cite[Definition 6]{Boyen2004} for the computational setting.  We first present our definition and then compare to the definitions of Boyen.

\begin{definition}[Reusable Fuzzy Extractors]
\label{def:outsider fuzz ext}
Let $\mathcal{W}$ be a family of distributions over $\mathcal{M}$.  Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$.
Fix some $W_1 \in \mathcal{W}$.  Let $f_2,.., f_q , D$ be a split adversary.  Define the following game for all $j=1,..., q$:
\begin{itemize}%[leftmargin=-.6in]
\item \textbf{Sampling} The challenger samples $w_1\leftarrow W_1, u\leftarrow \zo^\kappa$.
\item \textbf{Perturbation} For $i=2,..., q$: the challenger computes $(r_i, p_i)\leftarrow \gen(w_i)$.  Set $w_{i+1} = f_i(w_1, p_1,..., p_i)$.\bnote{should we allow keys to the perturbation adversary?}
\item \textbf{Distinguishing} The advantage of $D$ is
\begin{align*}
\text{Adv}(D)&\overset{def}= \Pr[D(r_1,..., r_{j-1}, r_j, r_{j+1},..., r_q, p_1,..., p_q)=1]\\ &- \Pr[D(r_1,..., r_{j-1}, u, r_{j+1},..., r_q, p_1,..., p_q)=1].
\end{align*}
\end{itemize}
$(\gen, \rep)$ is $(q, \epsilon_{sec}, s_{sec}, f_2,..., f_q)$-reusable if for all $D\in\mathcal{D}_{s_{sec}}$ the advantage is at most $\epsilon_{sec}$.
\end{definition}

The definition is parameterized by $f_2,..., f_q$.  This adversary implicitly defines distributions $W_2,..., W_q$ (which depend on $W_1$ and the public values $P_1,... P_i$).  Security seems hopeless if fuzzy extractor is not secure on each of these distributions on their own.  This is the only requirement we make on these functions.  We call these types of functions admissible:

\begin{definition}
Let $(\gen, \rep)$ be a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$.  In the reusability game above, we say a set of functions $f_2,..., f_q$ are \emph{admissible} if for all $W_1\in \mathcal{W}$ for all $w_1\in W_1$ and $\forall p_1,..., p_q$ that are the public outputs of $\gen$ the distribution $W_{i,w_1,p_1,..., p_{i-1}} = f_i(w_1,p_1,..., p_{i-1})$ is a member of $\mathcal{W}$.
\end{definition}

\paragraph{Comparison with the definition of Boyen}
Boyen considers two versions of reusable fuzzy extractors, first where the adversary sees $p_1,..., p_q$~(outsider security~\cite[Definition 6]{Boyen2004}) and tries to learn about the values $w_1,..., w_q$ or the keys $r_1,..., r_q$.  Second, where the adversary controls some subset of the servers and can key generation on arbitrary $p_i'$~(insider security~\cite[Definition 7]{Boyen2004}).   This allows the adversary to learn a subset of keys $r_i$~(by performing key generation on the valid $p_i$).  This definition makes sense when servers are compromised~(after enrollment) and act maliciously.  In both definitions, the adversary creates a perturbation function $f_i$ after seeing $p_1,..., p_{i-1}$~(and generated keys for outsider security) and the challenger generates $w_i = f_i(w_1)$.  The definition is parameterized by the class of allowed perturbation functions.

Boyen constructs a outsider reusable cryptographic fuzzy extractor for unbounded $q$ when the perturbation family is a transitive isometric permutation groups. Boyen transforms this construction to insider security using random oracles. %In both constructions, the correlation between  $w_1,..., w_q$ is limited to a transitive isometric permutation group.  

Insider security strengthens outsider security in two ways.  First, it allows the adversary to see some subset of keys, second it allows the adversary to perform key generation on arbitrary $p_i$.  This mixes two properties of a fuzzy extractor:  reusability and robustness~\cite{dkrs2006}.  Robust fuzzy extractors provide security against modified $p$.  
In this work, we show reusability when $r_i$ are observed but do not handle the issue of robustness.  That is, we assume keys may be exposed but servers keep honest state.  Our definition lies between outsider and insider security.

The definition of Boyen considers a single adversary.  \defref{def:outsider fuzz ext} splits the adversary into two parts, one of which is information-theoretic and another that is computationally bounded.  The functions $f_2,..., f_q$ can be thought of as a single adversary that sees all prior state.  However, to provide meaningful security in the computational setting, we cannot have communication between $f_2,..., f_q$ and $D$.  This is because $f_2,..., f_q$ are computationally unbounded and thus could break any security that only holds against computationally bounded adversaries.
Because these two adversaries do not communicate we strengthen the definition by allowing the perturbation functions,  $f_i$, to see the original sample $w_1$.\footnote{An alternative would be to have a single computationally bounded adversary that provides perturbation functions but does not see the original reading $w_1$ as in the definition of Boyen.  \consref{cons:sampling} also satisfies this alternative adaption of Boyen's definition.}  This was not allowed in the definition of Boyen, because the adversary was not split-state, and so giving $w_1$ to the adversary would make security impossible.

%Finally, we allow the distinguishing adversary to see other keys \lnote{we do? then why do we call our definition ``outsider''?}.  This is strictly stronger than the definition of Boyen \lnote{you mean than the outsider security definition of Boyen?}.  However, it is weaker than the notion of insider reusable fuzzy extractors where the adversary sees key generation on arbitrary $p$.  This \lnote{what is ``this''?} is a mixture of reusability and robustness~\cite{dkrs2006} of the fuzzy extractor.  Our reusable construction~(\consref{cons:sampling}) is not insider secure \lnote{according to Boyen's notion, right? Why not say ``not robust'' and/or clarify that $r_i$ can leak but $p_i$ cannot be modified.}.
\subsection{Obfuscation}
Our constructions will use obfuscation for two types of circuits: point functions and digital lockers. The family of point functions $\mathtt{I}_n = \{I_w\}_{w \in \zo^n}$ defined as follows:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
and the class of digital lockers is $\mathtt{I}_n = \{I_{w, r}\}_{w \in \zo^n, r\in\zo^\kappa}$ defined as follows:
\[
I_{w, r}(x):\begin{cases} r & x=w\\\perp & \text{otherwise}\end{cases}.
\]
The required notion of obfuscation is virtual grey-box (VGB) introduced in \cite{bitansky2010strong}. This notion is weaker then the standard notion of virtual black-box (\cite{barak2001possibility}), as it allows the simulator to run in unbounded time while making at a polynomial number of oracle queries to the function. In addition, we require that the obfuscation is composable and secure with respect to auxiliary input. 

Composable auxiliary-input VGB obfuscators for point functions and digital lockers are constructed in \cite[Theorem 6.1]{bitansky2010strong} (following \cite{canetti2008obfuscating,CKVW10}) from the Strong Vector Decision Diffie-Hellman assumption, which is a generalization of the strong DDH assumption of \cite{canetti1997towards} for tuples of points. As described in the Introduction, they can also be constructed by assuming strong properties of cryptographic hash functions~\cite{canetti1997towards}, \cite[Section 4]{lynn2004positive}, \cite[Section 3.2]{canetti2008obfuscating}, \cite[Section 8.2.3]{dakdoukThesis} (such a construction results in an obfuscated function that, at a few hard-to-find-inputs, differs from the original function; this change does not materially affect our results).

\begin{definition}[composable obfuscation VGB obfuscation with auxiliary input \cite{bitansky2010strong}]
\label{def:obf} A PPT algorithm $\mathcal{O}$ is an $\ell$-composable VGB obfuscator for $\mathtt{I}_{n}$~(resp. $\mathtt{I}_{n+\kappa}$) with auxiliary-input if the following conditions are met:
\begin{enumerate}
\item \emph{Functionality:} for every $n$ and $I \in \mathtt{I}_n$, $\mathcal{O}(I)$ is a circuit that computes the same function as $I$.\lnote{imperfect correctness allowed? It's need for the RO construction.}
\item \emph{Virtual grey-box:}  For every PPT adversary $A$ and polynomial $p$, there exists a (possibly inefficient) simulator $S$ and a polynomial $q$ such that for all sufficiently large $n$, any  sequence of circuits $I^1,\dots,I^\ell \in \mathtt{I}_n$, (where $\ell=\poly(n)$) and for all auxiliary inputs $z\in \zo^*$:
\ifnum\lncs=0
\[
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^\ell)) = 1] - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | < \frac{1}{p(n)} \enspace,
\]
\else
\begin{align*}
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^\ell)) = 1]   - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | \\ < \frac{1}{p(n)} \enspace,
\end{align*}
\fi
where $(I^1,\dots,I^\ell)[q(n)]$ is an oracle that answers at most $q(n)$ queries, and where every query of the form $(i,x)$ is answered by $I^i(x)$.
\end{enumerate}
\end{definition}
For notational convenience, when we use point function obfuscation, we denote the oracle $(I^1,\dots,I^\ell)$ provided to the simulator in the above definition as $I_w(\cdot, \cdot)$ where $w = w_1,..., w_\gamma$ is the vector of obfuscated points.  When we use digital lockers, we denote the oracle  $(I^1,\dots,I^\ell)$ provided to the simulator as $I_{v, r}(\cdot, \cdot)$ where $v = v_1,..., v_\ell$ is the vector of obfuscated points and $r$ is the hidden value~(we will hide the same value in each obfuscation). 


\section{Reusable Construction for Sources with High-Entropy Samples}
\label{sec:sampling}

\paragraph{Sources with High-Entropy Samples}
Let the source $W=W_1, \dots, W_\gamma$ consist of strings of length $\gamma$ over some arbitrary alphabet $\mathcal{Z}$. For some parameters $\eta, \subsetEntropy$, we say that the source $W$ is a source with  \emph{$\subsetEntropy$-entropy $\eta$-samples} if 
\ifnum\lncs=0
$\Hav(W_{j_1}, \dots, W_{j_\eta}\, |\, j_1, \dots, j_\eta)>\subsetEntropy$ 
\else
\[
\Hav(W_{j_1}, \dots, W_{j_\eta}\, |\, j_1, \dots, j_\eta)>\subsetEntropy
\]
\fi
for uniformly random distinct $1< j_1, \dots, j_\eta \le \gamma$. This property can hold, for example, if the symbols $W_i$ are $\eta$-wise independent, and each has entropy at least $\subsetEntropy/\eta$. However, that's not the only way for the property to hold: for example, we can allow sources in which some symbols have little entropy or depend on other symbols, by simply choosing a larger $\eta$ to compensate (since the property has to hold only on  average for random $j_1, \dots, j_\eta$, a sufficiently small fraction of low-entropy symbols in $W$ can be tolerated).

Such sources $W$ can arise naturally as a result of signal processing and discretization. For example, suppose we start with a high-entropy reading $x$ of some physical source, and obtain each symbol $w_i$ by applying a short-output condenser (e.g., a random projection followed by discretization) to $x$. 
Each subsequent element in the sequence $w_{j_1}, \dots, w_{j_\eta}$ will extract fresh entropy as long as there remains entropy in $x$ conditioned on the previous elements (by a simple conditional entropy argument).


\paragraph{The Sample-then-Obfuscate Construction}
The construction first chooses a random $r$ to be used as the output of the fuzzy extractor. It then samples a random subset of symbols $v_1=w_{j_1},..., w_{j_\eta}$ and creates a digital locker that hides $r$ using $v_1$ (We present and analyze the construction with uniformly random subsets; however, if necessary, it is possible to substantially decrease the required public randomness and the length of $p$ by using more sophisticated samplers. See~\cite{goldreich2011sample} for an introduction to samplers.) This process is repeated to produce some number $\ell$ of digital lockers all containing $r$,  each unlockable with $v_1,..., v_\ell$, respectively. The use of the composable digital lockers allows us to sample multiple times, because we need to argue only about individual entropy of $V_i$.  Composability also allows reusability.

\begin{construction}[Sample-then-Obfuscate]
\label{cons:sampling}
Let $\mathcal{Z}$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source with $\subsetEntropy$-entropy $\eta$-samples, where each $W_j$ is over $\mathcal{Z}$.  Let $\ell$ be a parameter, to be determined later. 
Let $\mathcal{O}$ be an $\ell$-composable VGB obfuscator for the family of digital lockers with $\kappa$-bit outputs. Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\ifnum\lncs=0
\begin{minipage}{3in}
\else
\begin{minipage}{2.3in}
\fi
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $r \overset{\$}\leftarrow \zo^\kappa$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Choose uniformly random distinct $1\le j_{i, 1},..., j_{i, \eta}\le \gamma$
\item Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\item Set $\rho_i = \mathcal{O}(I_{v_i, r})$.
\item Set $p_i = \rho_i, (j_{i, 1},..., j_{i, \eta})$.
\end{enumerate}
\item Output $(r, p)$, where $p=p_1\dots p_\ell$.
\end{enumerate}
 \end{minipage}\ \  &
\ifnum\lncs=0
\begin{minipage}{3in}
\else
\begin{minipage}{2.2in}
\fi
\ \ \textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w'=w_1',..., w_\gamma', p = p_1\dots p_\ell)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Parse $p_i$ as $\rho_i, (j_{i, 1},..., j_{i, \eta})$.
\item Set $v_i' = w_{j_{i, 1}}',..., w_{j_{i, \eta}}'$.
\item Set $\rho_i(v_i') = r_i$.  If $r_i\neq \perp$ output $r_i$.
\end{enumerate}
\item Output $\perp$.
\end{enumerate}
\ifnum\lncs=0
\vspace{0.6in}
\fi
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
%There are three main differences between this construction and \consref{cons:first construction}.  They are as follows:
%\begin{itemize}
%\item Multiple blocks are concatenated together.  This comes at a cost to error tolerance but allows us to significant decrease the required entropy in each block.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.
%\item Instead of encoding a bit of the key with each obfuscated value we encode the entire key with each obfuscated value.  This requires the use of digital lockers in place of point obfuscations.  Instead of having to open ``most'' of the obfuscations, it is only necessary for us to open a single obfuscation.  \consref{cons:first construction} only hid part of the key in each obfuscation.  This allowed some blocks in the distribution to be ``weak.''  However, sampling smoothes out $W$ so that all $V_i$ are ``good'' simultaneously.
%\item \consref{cons:first construction} required a large alphabet as blocks were individually obfuscated.  This construction works for an arbitrary size alphabet.  We show it supports $\Huse (W)\le 0$ when the alphabet size is super-constant in the security parameter.
%\end{itemize}


\begin{theorem}
\label{thm:sampling}
Let $n$ be a security parameter, $\mathcal{Z}$ be a constant-size alphabet, and $\gamma, \subsetEntropy$, and $t$ be functions of $n$, as follows.  Let $\mathcal{W}$ be a family of sources over $\mathcal{Z}^\gamma$ with $\alpha$-entropy $\eta$-samples for $\subsetEntropy = \omega(\log n)$. Let $t$ be such that $\left(1-\frac{t}{\gamma-\eta}\right)^\eta>1/p(n)$ for some positive polynomial $p$. Then there exists $\ell = \poly(n)$ such that for any
$s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that \consref{cons:sampling} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $\left (\epsilon_{sec}, s_{sec}\right)$-hard with negligible error.
\end{theorem}
 
\begin{proof} It suffices to argue correctness and security.

\textbf{Correctness:} Correctness follows immediately: for any given $i$, the probability that $v'_i=v_i$ is at least $1/p(n)$. By choosing $\ell$ sufficiently bigger than $1/p(n)$, we guarantee that for at least one $i$,  $v'_i=v_i$ with all but negligible probability
(indeed, because the $\ell$ samples are independent, the probability this does not happen is at most $(1-1/p(n))^\ell \le e^{-\ell/p(n)}$, using $1+x \le e^x$).

\textbf{Security:}
We now argue security.
 Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U, P))\le \epsilon_{sec}$. Let $D$ be a distinguisher of size at most $s_{sec}$. We want to bound 
\[
| \expe[D(R, P)] - \expe[D(U, P)]|
\]
by a negligible function.

Define the oracle $I_{v_1, ..., v_\ell, r}(\cdot, \cdot)$ as follows:
\[I_{v_1,..., v_\ell, r}(x, i) =
\begin{cases}
r & v_i = x\\
\perp & \text{otherwise.}
\end{cases}\]
By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q\le s_{sec}$ queries) such that
\begin{align}
\label{eq:dist before sampling}
\left|\expe [D(R, P_1,..., P_\ell)] - \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)} \left (R, \{j_{ik}\}, 1^{\ell (\kappa+\eta\log |\mathcal Z|)} \right) \right|  \leq\ngl(n).
\end{align}

The same is true if we replaced $R$ above by and independent uniform random variable $U$ over $\zo^\kappa$. 
We now prove the following lemma, which shows that $S$ cannot distinguish between $R$ and $U$.

\begin{lemma}
\label{lem:sim cannot distinguish samp}
Let $U$ denote the uniform distribution over $\zo^\kappa$. Then
\ifnum\lncs=0
\begin{align}
\left| \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)} \left (R, \{j_{ij}\}, 1^{\ell (\kappa+\eta\log |\mathcal Z|)} \right) 
- \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}  
\left (U, \{j_{ij}\}, 1^{\ell (\kappa+\eta\log |\mathcal Z|)} \right)\right|
\le \frac{q(q+1)}{2^\subsetEntropy} \le \ngl(n)\,.
\label{eq:sim cannot distinguish samp}
\end{align}
\else
\begin{eqnarray}
\lefteqn{
\left| \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)} \left (R, \{j_{ij}\}, 1^{\ell (\kappa+\eta\log |\mathcal Z|)} \right) 
\right.} \nonumber \\
& & \left.
- \expe [S^{I_{v_1, ..., v_\ell, r}(\cdot, \cdot)}  
\left (U, \{j_{ij}\}, 1^{\ell (\kappa+\eta\log |\mathcal Z|)} \right)\right|
\le \frac{q(q+1)}{2^\subsetEntropy} \le \ngl(n)\,.
\label{eq:sim cannot distinguish samp}
\end{eqnarray}
\fi


\end{lemma}

\begin{proof}
Fix any $u\in \zo^\kappa$ (the lemma will follow by averaging over all $u$). Let $r$ be the correct value of $R$.
The only information about whether the value is $r$ or $u$ can obtained by $S$ through the query responses. First, modify $S$ slightly to quit immediately if it gets a response not equal to $\perp$ (such $S$ is equally successful at distinguishing between $r$ and $u$, because the first non-$\perp$ response suffices). There are $q+1$ possible values for the view of $S$ on a given input ($q$ of those views consist of some number of $\perp$ responses followed by the first non-$\perp$ response, and one view has all $q$ responses equal to $\perp$) . By \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}, $\Hav(V_i | View(S), \{j_{ik}\}) \ge \Hav(V_j | \{j_{ik}\}) - \log (q+1) \ge \subsetEntropy - \log(q+1)$. Therefore, at each query, the probability that $S$ gets a non-$\perp$ answer (equivalently, guesses $V_i$) is at most $(q+1)2^{-\subsetEntropy}$. Since there are $q$ queries of $S$, the result of \lemref{lem:sim cannot distinguish samp} follows by the union bound. Note that  $q(q+1)/2^\subsetEntropy$ is negligible, because $q$ is polynomial and $\subsetEntropy=\omega(\log n)$.
\end{proof}

Adding together Equation~\ref{eq:dist before sampling}, Equation~\ref{eq:sim cannot distinguish samp}, and Equation~\ref{eq:dist before sampling} in which $R$ is replaced with $U$, we obtain the desired result by the triangle inequality.  This completes the proof of \thref{thm:sampling}.
\end{proof}
 
\paragraph{Any sublinear error-rate can be supported if most symbols in a sample contribute entropy.}
The requirement on $t$ can be expressed (by taking the negative logarithm of both sides) as $\eta \log  \left(1-\frac{t }{\gamma-\eta}\right) = O (\log n)$. Using $\ln (1+x)\approx x$ and relying on $O$ to absorb any 
constant factor involved in this approximation and the switch from $\ln$ to $\log$, we get $\eta \frac{t }{\gamma-\eta} = O(\log n)$. If $t$ is sublinear in $\gamma$
 (and $\eta$ is no more than a constant fraction of $\gamma$), then $t/(\gamma-\eta)$ is subconstant, and therefore it is possible to satisfy this requirement on $t$ while simultaneously maintaining $\eta = \omega (\log n)$, which implies $\subsetEntropy = \omega(\log n)$ as long as most symbols in a sample of size $\eta$ contribute a constant amount of entropy each.
 
\paragraph{More Errors than Entropy?}
The amount of entropy in $W$ need not exceed $\subsetEntropy$---thus, it can be as low as $\omega(\log n)$. Assuming $\gamma = n$, the number $t$ of errors may be any function in $o(n)$, and thus $\log |B_t| \ge t \log |Z|$ may far exceed $\Hoo(W)$.


\subsection{Reusability of \consref{cons:sampling}}
The reusability of \consref{cons:sampling} follows from the security of the VGB obfuscator with auxiliary input. Consider any $q = \poly(n)$ number of reuses.  For each fixed $i\in \{1,..., q\}$, we can treat the keys $r_1,\dots, r_{i-1}, r_{i+1}, \dots, r_q$ and the strings $p_1, \dots, p_q$ as auxiliary input to the obfuscation adversary. The result follows by simulatability of this adversary, using the same argument as the proof of Theorem~\ref{thm:sampling} above. Note that the simulator in the definition of composable obfuscation is required to function for arbitrary circuits in the family even if the choice of these circuits depends on the previous obfuscations, which allows the $i$th reading of $w$ to be chosen depending on public values $p_1,..., p_{i-1}$, as required by definition of reusability.


  Thus we achieve the following result:

\begin{theorem}
\label{thm:reusability}
Let $q = \poly(n)$, and let all the variables be as in \thref{thm:sampling}, except that $\mathcal{O}$ is an $\ell\times q$-composable VGB obfuscator for digital lockers~(with $\kappa$ bit outputs) with auxiliary inputs.  For any admissible $f_2,..., f_q$, for all $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $(\gen, \rep)$ is $(q, \epsilon_{sec}, s_{sec}, f_2,..., f_q)$-reusable fuzzy extractor.
\end{theorem}


\section{Construction for Sources with Sparse High-Entropy Marginals}
\label{sec:cor construction}
In this section, we consider an alternative construction that is particularly well suited to sources over large alphabets. It requires enough symbols individually to contain sufficient entropy, but does not require independence of symbols, or even ``fresh'' entropy from them. Unlike the previous construction, it tolerates a linear fraction of errors. However, it cannot work for small alphabets, and is not reusable.

\paragraph{Sources with Sparse High-Entropy Marginals} This construction works for distributions $W=W_1, ... ,W_\gamma$ over $\mathcal{Z}^\gamma $ in which enough symbols $W_j$ are unpredictable even after adaptive queries to equality oracles for other symbols. This quality of a distribution is captured in the following definition.

\begin{definition}
\label{def:block guessable}
Let $I_w (\cdot, \cdot)$ be an oracle that returns \[I_w(j, w_j')=
\begin{cases}
1 & w_j = w_j'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1, ... ,W_\gamma$ has $\beta$-sparse $\alpha$-entropy $q$-marginals if there exists a set $J\subset\{1,..., \gamma\}$ of size at least $\gamma -\beta$ such that for any unbounded adversary $S$ with oracle access to $I_w$ making at most $q$ queries
\[
\forall j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}

We show some examples of such sources in \apref{sec:characterize}.  In particular, any source $W$ where for all $j$, $\Hoo(W_j) \geq \omega(\log n)$~(but all symbols may arbitrarily correlated) is a source with sparse high-entropy marginals~(\clref{cl:all blocks entropy}).


\paragraph{The Error-Correct-and-Obfuscate Construction}
This construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}.  Instead of having large parts of the string $w$ unlock $r$, we have individual symbols unlock bits of the output.  

Before presenting the construction we provide some definitions from error correcting codes.
We use error-correct codes over $\{0,1\}^\gamma$ which correct up to $t$ bit flips from $0$ to $1$ but no bit flips from $1$ to $0$ (this is the Hamming analog of the $Z$-channel~\cite{tallini2002capacity}).\footnote{Any code that corrects $t$ Hamming errors also corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
Codes with $2^{\Theta(\gamma)}$ codewords and $t = \Theta(\gamma)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.
The class of errors we support in our source~($t$ Hamming errors over a large alphabet) and the class of errors for which we need codes~($t$ $0\rightarrow 1$ errors) are different.
}
\begin{definition}
\label{def:hamming z channel}
Let $e, c\in \zo^\gamma$ be vectors.  Let $x = \error(c, e)$ be defined as follows
\[x_i = \begin{cases} 1 & c_i=1 \vee e_i=1\\
0& \text{ otherwise}.\end{cases}\]
\end{definition}

\begin{definition}
A set $C$~(over $\zo^\gamma$) is a $(t, \delta_{code})$-Z code if there exists an efficient procedure $\decode$ such that \[\forall e \in \zo^\gamma | \weight(e)\le t, \Pr_{c\in C}[\decode(\error(c,e)) \neq c] \leq \delta_{code}.\]
\end{definition}


\begin{construction}[Error-Correct-and-Obfuscate]
\label{cons:first construction}
Let $\mathcal{Z}$ be an alphabet and let $W = W_1,..., W_\gamma$ be a distribution over $\mathcal{Z}^\gamma$.  Let $\mathcal{O}$ be an obfuscator for point functions with points from $\mathcal{Z}$.  Let  $C\subset \zo^\gamma$ be $(t, \delta_{code})$-Z code.
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\ifnum\lncs=0
\begin{minipage}{3in}
\else
\begin{minipage}{2.25in}
\fi
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Sample $c\leftarrow C$.
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item If $c_j = 0$: $p_j = \mathcal{O}(I_{w_j})$.
\item Else: $r_j \overset{\$}\leftarrow \mathcal{Z}$.
\subitem Let $p_j = \mathcal{O}(I_{r_j})$.
\end{enumerate}
\item Output $(c, p)$, where $p=p_1\dots p_\gamma$.
\end{enumerate}
 \end{minipage} \ \ &
\ifnum\lncs=0
\begin{minipage}{3in}
\else
\begin{minipage}{2.25in}
\fi
\ \ \textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item If $p_j(w_j') = 1$: set $c_j' = 0$.
\item Else: set $c_j' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

%\bnote{shorten this discussion?}
%The input $w$ is hidden in two different ways.  In locations where $c_j=1$, the block $w_j$ is information-theoretically unknown.
%In locations where $c_j=0$, it is hard to find $w_j$ given access to the point obfuscation.
%There are two possible reasons for a bit $c_j'$ to be $1$: because the true value was $1$ and because $w_j \neq w_j'$.  However, if a bit $c_j'$ is $0$, this likely means that $w_j=w_j'$ because collisions when $c_j=0$ are unlikely~(occurring with probability $1/|\mathcal{Z}|$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips.

\noindent
As presented, \consref{cons:first construction} is not yet a computational fuzzy extractor.  The codewords $c$ are not uniformly distributed and it is possible to learn some bits of $c$~(for the symbols of $W$ without much entropy).  However, we can show that $c$ looks like it has entropy to a computationally bounded adversary who knows $p$. Applying a randomness extractor with outputs over $\zo^\kappa$ (technically, an average-case computational randomness extractor) to $c$, and adding the extractor seed to $p$, will give us the desired fuzzy extractor. See \apref{sec:further defs} for the formal details.

\consref{cons:first construction} is secure if no distinguisher can tell whether it is working with random obfuscations or obfuscations of $W_j$.  By the security of point obfuscation, anything learnable from the obfuscation is learnable from oracle access to the function. Therefore, our construction is secure as long as enough symbols are unpredictable even after adaptive queries to equality oracles for individual symbols, which is exactly the property satisfied by sources with sparse high-entropy marginals.

The following theorem formalizes this intuition~(proof in \apref{sec:construction analysis}).
\begin{theorem}
\label{thm:main thm first cons}
Let $n$ be a security parameter. Let $\mathcal{Z}$ be an alphabet where $|\mathcal{Z}| \ge 2^{ \omega(\log(n))}$.
Let $\mathcal{W}$ be a family of sources with $\beta$-sparse $\alpha=\omega(\log n)$-entropy $q$-marginals over $\mathcal{Z}^\gamma$, for any $q = \poly(n)$.  Furthermore, let $C$ be a $(t, \delta_{code})$-Z code over $\mathcal{Z}^\gamma$.  Let $\mathcal{O}$ be an $\gamma$-composable VGB obfuscator for point functions with auxiliary inputs. Then for any $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec}=\ngl(n)$ such that \consref{cons:first construction}, followed by a $\kappa$-bit randomness extractor, is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta_{code} + \gamma/|\mathcal{Z}|$.
\end{theorem}

\paragraph{More Errors than Entropy?}
\label{sec:discussion}
In this section, we show that \consref{cons:first construction} can support distributions with more errors than entropy.
We first calculate the size of the Hamming ball.
\[
\log |B_t| = \log \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i> \log {\gamma \choose t} (|\mathcal{Z}|-1)^t =\Theta(t\log |\mathcal{Z}|) + \log {\gamma\choose t}\,.
\]
The simplest type of a source with high-entropy marginals is where each symbol is independent and has super-logarithmic entropy~(\clref{cl:independent high ent}).  For this type of source the entropy is $\Hoo(W) = \gamma\omega(\log n)$.  This yields:
\[
\text{\# errors} - \text{entropy} = \log |B_t| -  \Hoo(W)  >\left( \Theta(t\log |\mathcal{Z}|) + \log {\gamma \choose t}\right) -  \gamma \omega(\log n) .
\]
When $t =\Theta(\gamma)$ and the entropy of each block is $o(\log |\mathcal{Z}|)$, then the construction supports more errors than entropy. Furthermore, the output entropy is $H_0(C) -\beta$~(if $C$ is a constant rate code, this is $\Theta(\gamma)$).

\paragraph{Improvements}  If most codewords have Hamming weight close to $1/2$, we can decrease the error tolerance needed from the code from $t$ to  about $t/2$, because roughly half of the mismatches between $w$ and $w'$ occur where $c_j =1$.

If $\gamma$ is not long enough to get a sufficiently long output, the construction can be run multiple times with the same input and independent randomness.


\section{Information-Theoretic Construction for Sparse Block Sources}
\label{sec:info theory cons}
The construction in this section has information-theoretic security, in contrast to only computational security of the previous two constructions. It requires less entropy from each symbol than the previous construction; however, it places more stringent independence requirements on the symbols. It tolerates a linear number of errors.

\paragraph{Sparse Block Sources}
This construction works for sources $W=W_1, ... ,W_\gamma$ over $\mathcal{Z}^\gamma $ in which enough symbols $W_j$ contribute fresh entropy conditioned on previous symbols.  We call this such sources \emph{sparse block sources}, weakening the notion of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}), which require every symbol to contribute fresh entropy.

\begin{definition}
\label{def:partial source}
A distribution $W = W_1,..., W_\gamma$ is an $(\alpha, \beta)$-sparse block source if there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_{j-1} \in W_1,..., W_{j-1}, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
\end{definition}
The choice of conditioning on the past is arbitrary: a more general sufficient condition is that there exists some ordering of indices where most items have entropy conditioned on all previous items in this ordering~(for example, is possible to consider a sparse reverse block source~\cite{vadhan2003constructing}).  

\paragraph{The Condense-then-Fuzzy-Extract Construction}
The construction first condenses entropy from each symbol of the source and then applies a fuzzy extractor to the condensed symbols. We'll denote the fuzzy extractor on the smaller alphabet as $(\gen', \rep')$.  A condenser is like a randomness extractor but the output is allowed to be slightly entropy deficient.  Condensers are known with smaller entropy loss than possible for randomness extractors~(e.g.~\cite{dodis2014key}).
\begin{definition}
\label{def:conductor}
A function $\cond : \mathcal{Z}\times \zo^d\rightarrow \mathcal{Y}$ is a $(m, \tilde{m}, \epsilon)$-randomness condenser if whenever $\Hoo(W)\ge m$, then there exists a distribution $Y$ with $\Hoo(Y)\ge \tilde{m}$ and $(\cond(W, seed), seed) \approx_\epsilon (Y, seed)$.
\end{definition}

The main idea of the construction is that errors are ``corrected'' on the large alphabet~(before condensing) while the entropy loss for the error correction is incurred on a smaller alphabet~(after condensing).

\begin{construction}
\label{cons:info theoretic}
Let $\mathcal{Z}$ be an alphabet and let $W=W_1,..., W_\gamma$ be a distribution over $\mathcal{Z}^\gamma$.  We describe $\gen, \rep$ as follows:
\begin{center}
\begin{tabular}{c|c}
\ifnum\lncs=0
\begin{minipage}{3in}
\else
\begin{minipage}{2.25in}
\fi
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item Sample $seed_i\leftarrow \zo^d$.
\item Set $v_i = \cond(w_i, seed_i)$.
\end{enumerate}
\item Set $(r, p') \leftarrow \gen'(v_1,..., v_\gamma)$.
\item Set $p = (p', seed_1,..., seed_\gamma)$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} & \ \ 
\ifnum\lncs=0
\begin{minipage}{3in}
\else
\begin{minipage}{2.25in}
\fi
\ \ \textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p = (p', seed_1,..., seed_\gamma))$
\item For $j=1,..., \gamma$:
\begin{enumerate}[(i)]
\item Set $v_i' = \cond(w_i', seed_i)$.
\end{enumerate}
\item Output $r = \rep'(v', p')$.
\end{enumerate}
\ifnum\lncs=0
	\vspace{0.7in}
\else
	\vspace{0.15in}
\fi
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
The following theorem shows the security of this construction.

\begin{theorem}
\label{thm:info theory sec}
%Let $n$ be a security parameter and let $\gamma = \omega(\log n)$.
Let $\mathcal{W}$ be a family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-sparse block sources over $\mathcal{Z}^\gamma$ and let $\cond: \mathcal{Z} \times \zo^d\rightarrow \mathcal{Y}$ be a $(\alpha, \tilde{\alpha}, \epsilon_{cond})$-randomness conductor.  Define $\mathcal{V}$ as the family of all distributions with min-entropy at least $\tilde{\alpha}(\gamma-\beta)$ and let $(\gen', \rep')$ be $(\mathcal{Y}^\gamma, \mathcal{V}, \kappa, t, \epsilon_{fext})$-fuzzy extractor with error $\delta$.\footnote{We actually need $(\gen', \rep')$ to be an average case fuzzy extractor~(see \cite[Definition 4]{DBLP:journals/siamcomp/DodisORS08} and the accompanying discussion).  Most known constructions of fuzzy extractors are average-case fuzzy extractors.  For simplicity we refer to $\gen', \rep'$ as simply a fuzzy extractor.}  Then $(\gen, \rep)$ is a $(\mathcal{Z}^\gamma, \mathcal{W}, \kappa, t, \gamma\epsilon_{cond}+\epsilon_{fext})$-fuzzy extractor with error $\delta$.
\end{theorem}
\begin{proof}
Let $W\in \mathcal{W}$.
It suffices to argue correctness and security.  We first argue correctness.

\textbf{Correctness:}
When $w_i = w_i'$, then $\cond(w_i , seed_i) = \cond(w_i', seed_i)$ and thus $v_i = v_i'$.  Thus, for all $w, w'$ where $\dis(w, w')\le t$, then $\dis (v, v')\le t$.  Then by correctness of $(\gen', \rep')$, $\Pr[(r, p)\leftarrow \gen'(v) \wedge r'\leftarrow \rep(v',p) \wedge r' = r]\ge 1-\delta$.

\textbf{Security:}
We now argue security.  Denote by $seed$ the random variable consisting of all $\gamma$ seeds and $V$ the entire string of generated $V_1,..., V_\gamma$.  To show that \[R | P, seed \approx_{\gamma \epsilon_{cond} + \epsilon_{fext}} U | P, seed,\] it suffices to show that $\Hav(V | seed)$ is $\gamma \epsilon_{cond}$ close to a distribution with average min-entropy $\tilde{\alpha}(\gamma - \beta)$.  The lemma then follows by the security of $(\gen', \rep')$.\footnote{Note, again, that $(\gen', \rep')$ must be an average-case fuzzy extractor.  Most known constructions are average-case and we omit this notation.}

We now argue that there exists a distribution $Y$ where $\Hav(Y | seed)\ge \tilde{\alpha}(\gamma - \beta)$ and $(V, seed_1,..., seed_\gamma)\approx (Y, seed_1,.., seed_\gamma)$.  First note since $W$ is $(\alpha, \beta)$ sparse block source that
there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_{j-1} \in W_1,..., W_{j-1}, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
Then consider the first element of $j_1\in J$, $\forall w_1,..., w_{j_1-1}\in W_1,..., W_{j_1-1}$,
\[\Hoo(W_{j_1} | W_1 = w_1,..., W_{j_1-1} = w_{j_1-1})\ge \alpha.\]

%\[(\ext (W_1, seed_1), seed_1) \approx_{\epsilon} (U_{\mathcal{Y}}, seed_1)\]
%Then since $W$ is a block-source,
%\[ \forall w_1\in W_1, \Hoo(W_2 | w_1)\ge k.\]
\noindent
Thus, there exists a distribution $Y_{j_1}$ with $\Hav(Y_{j_1} | seed_{j_1}) \ge \tilde{\alpha}$ such that
\[(\cond (W_{j_1}, seed_{j_1}), seed_{j_1}, W_1,..., W_{j_1-1}) \approx_{\epsilon_{cond}} (Y_{j_1}, seed_{j_1}, W_1,..., W_{j_1-1})\]
and since $(seed_1,..., seed_{j_1})$ are independent of these values
\ifnum\lncs=0
\[(\cond (W_{j_1},seed_{j_1}), W_{j_1-1},..., W_1, seed_{j_1}, ..., seed_{1}) \approx_{\epsilon_{cond}} (Y_{j_1}, W_{j_1-1},..., W_1, seed_{j_1}, , ...,  seed_{1})\,.\]
\else
\begin{align*}
(\cond (W_{j_1},seed_{j_1}), W_{j_1-1},..., W_1, seed_{j_1}, ..., seed_{1}) \approx_{\epsilon_{cond}}\\ (Y_{j_1}, W_{j_1-1},..., W_1, seed_{j_1}, , ...,  seed_{1})\,.
\end{align*}
\fi
Consider the random variable $Z_{j_1} =( Y_{j_1}, \cond(W_{j_1-1},seed_{j_1-1}),..., \cond(W_{1}, seed_{1}))$ and note that \[\Hav(Z_{j_1} | seed_1,...,seed_{j_1})\ge \alpha'.\]
Applying a deterministic function does not increase statistical distance and thus,
\begin{align*}
(\cond (W_{j_1}, seed_{j_1}), \cond(W_{j_1-1}, seed_{j_1-1}),..., \cond(W_1, seed_1), seed_{j_1},..., seed_{1}) \\\approx_{\gamma \epsilon_{cond}} (Z_{j_1}, seed_{j_1},..., seed_1)
\end{align*}

\noindent
By a hybrid argument there exists a distribution $Z$ with $\Hav(Z | seed) \ge \tilde{\alpha}(\gamma -\beta)$ where
\[
(\cond(W_\gamma, seed_\gamma), ..., \cond(W_1, seed_1), seed_\gamma,..., seed_1) \approx_{\gamma \epsilon_{cond}} (Z, seed_\gamma,...,  seed_1).\]
%By the security of $(\sketch, \rec)$ we know that $\Hav(Z | seed, ss) \ge \tilde{m}$.  %Note that $U_{\mathcal{Y}}^\gamma$ is independent of $seed_1,..., seed_\gamma$ and so $\Hav(U_{\mathcal{Y}}^\gamma, seed_1,..., seed_\gamma, p)\ge \tilde{m}$.
%That is,
%\[
%(V, seed_1,..., seed_\gamma, p) \approx_{\epsilon\gamma} (U_{\mathcal{Y}}^\gamma, seed_1,..., seed_\gamma, p).\]

This completes the proof of \thref{thm:info theory sec}
\end{proof}

\paragraph{More errors than entropy}

We now show that \consref{cons:info theoretic} supports sparse block sources with more errors than entropy.  The structure of a sparse block source implies that  $\Hoo(W) \ge \alpha (\gamma-\beta ) = \Theta(\gamma)$.  We assume that $\Hoo(W) = \Theta(\gamma)$. The condenser of Dodis et al~\cite{dodis2014key} has a constant entropy loss, so $\alpha-\tilde{\alpha} = \Theta(1)$. This means that the input entropy to $(\gen', \rep')$ is $\Theta(\gamma)$.   We assume that the new alphabet $\mathcal{Y}$ is of constant size.  Standard fuzzy extractors on constant size alphabets correct a constant fraction of errors at a entropy loss of $\Theta(\gamma)$, yielding $\kappa = \Theta(\gamma)$.  Thus, our construction is secure for distributions with more errors than entropy whenever $|\mathcal{Z}| = \omega(1)$.
More formally:
\[
\text{\# Errors} - \text{Entropy} = \log |B_t| - \Hoo(W) \ge  t \log |\mathcal{Z}| - \Theta(\gamma)-= \Theta(\gamma) \log |\mathcal{Z}| - \Theta(\gamma)  > 0
\]
That is, there exists a super-constant alphabet size for which \consref{cons:info theoretic} is secure with more errors than entropy.

\blind{
\section*{Acknowledgements}
The authors are grateful to Nishanth Chandran, Sharon Goldberg, Gene Itkis, Bhavana Kanukurthi, and Mayank Varia for helpful discussions, creative ideas, and important references.

Ran Canetti is supported by the NSF MACS project, an NSF Algorithmic foundations grant 1218461, the Check Point Institute for Information Security, and  ISF grant 1523/14.
Omer Paneth is additionally supported by the Simons award for graduate students in theoretical computer science.
The work of Benjamin Fuller is sponsored in part by US NSF grants 1012910 and 1012798 and  the United States Air Force under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government. 
Leonid Reyzin is supported in part by US NSF grants 0831281, 1012910, 1012798, and 1422965.  Adam Smith is supported in part by NSF awards 0747294 and 0941553. 
}
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

\section{Definitions}
\subsection{Computational Fuzzy Conductors and Computational Extractors}
\label{sec:further defs}

We first define a slightly weaker object than a computational fuzzy extractor: it outputs a key with computational entropy~(instead of a pseudorandom key).  
We call this object a computational fuzzy conductor.  It is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
Before defining this object, we define conditional computational ``HILL'' (\cite{DBLP:journals/siamcomp/HastadILL99}) entropy.

\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon_{sec}, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S), \allowbreak (X,S))\leq \epsilon_{sec}$.
\end{definition}

\begin{definition}
\label{def:comp fuzzy cond}
A pair of randomized procedures ``generate'' ($\gen$) and ``reproduce'' ($\rep$) is an $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ if $\gen$ and $\rep$ satisfy \defref{def:comp fuzzy extractor}, except the last condition is replaced with the following weaker condition:
\begin{itemize}
\item for any distribution $W\in \mathcal{W}$, the string $r$ has high HILL entropy conditioned on $P$.  That is $H^{\hill}_{\epsilon_{sec}, s_{sec}}(R |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}

Computational fuzzy conductors can be converted to computational fuzzy extractors (\defref{def:comp fuzzy extractor}) using standard techniques, as follows.
The transformation uses a computational extractor.
A computational extractor is the adaption of a randomness extractor to the computational setting.  Any information-theoretic randomness extractor is also a computational extractor; however, unlike information-theoretic extractors, computational extractors can expand their output arbitrarily via pseudorandom generators once a long-enough output is obtained. We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average case:
\begin{definition}
A function $\cext: \zo^\gamma \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon_{sec}, s_{sec})$-average-case computational extractor} if for all pairs
of random variables $X, Y$ (with $X$ over $\zo^\gamma$) such that
$\tilde{H}_\infty(X|Y) \ge m$, we have 
\ifnum\lncs=0
$
\else
\[
\fi
\delta^{\mathcal{D}_{s_{sec}}}((\cext(X; U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon_{sec}
\ifnum\lncs=0
$.
\else
\,.\]
\fi
\end{definition}


Combining a computational fuzzy conductor and a computational extractor yields a computational fuzzy extractor:

\begin{lemma}
\label{lem:cond and cext}
Let $(\gen'$, $\rep')$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\gamma$.  Let $\cext:\zo^\gamma\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon_{ext}, s_{ext})$-average case computational extractor.  Define $(\gen, \rep)$ as:
\begin{itemize}
\item $\gen(w; seed)$ (where $seed\in \zo^d$): run $(r', p')= \gen'(w)$ and output $r = \cext(r'; seed)$, $p = (p', seed)$.
\item $\rep(w', (p', seed)):$ run $r' = \rep'(w'; p')$ and output $r = \cext(r'; seed)$.
\end{itemize}
Then $(\gen, \rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \min\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}

\begin{proof}
It suffices to show if there is some distinguisher $D'$ of size $s'$ where
\[\delta^{D'}((\cext(X; U_d), U_d, P'), (U_\kappa, U_d, P'))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D$ of size $s_{cond}$ such that for all $Y$ with $\Hav(Y|P') \geq \tilde{m}$,
 \[
 \delta^{D}((X,P'), (Y, P'))\geq \epsilon_{cond}.
 \]
Let $D'$ be such a distinguisher.  That is,
\[
\delta^{D'}(\cext(X, U_d)\times U_d \times P', U_\kappa\times U_d\times P')> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D$ as follows.  On input $(y, p')$ sample $seed\leftarrow U_d$, compute $r\leftarrow \cext(y; seed)$ and output $D(r, seed, p')$.  Note that $|D| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D}((X, P'), (Y, P'))&= \delta^{D'}((\cext(X, U_d), U_d, P'), \cext(Y, U_d), U_d, P')\\
&\geq \delta^{D'}((\cext(X, U_d), U_d, P'), (U_\kappa\times U_d \times P')) \\
&- \delta^{D'}((U_\kappa\times U_d \times P'), (\cext(Y, U_d), U_d, P'))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Where the last line follows by noting that $D'$ is of size at most $s_{ext}$.  Thus $D$ distinguishes $X$ from all $Y$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

%\subsection{Reusability of Fuzzy Extractors}
%\label{sec:comparing reusability}



%\subsection{Obfuscation}
%\label{sec:obfuscation def}


%\consref{cons:sampling} is the fuzzy extractor that allows for arbitrary correlation between $w_1,..., w_q$ as long as each $w_i$ comes from an admissible distribution.  %Our construction can be made secure information-theoretically for fixed values of $q$ or computationally for an arbitrary value $q = \poly(n)$.  Our construction is based on a modification of the computational fuzzy extractor of Canetti et al.~\cite[Construction 5.2]{canetti2014key}.  Their construction is secure for a restricted class of distributions.\footnote{This is necessary as they secure distributions with more errors than entropy.  In order to provide any meaningful security guarantee, the source distribution must be restricted.  See the introduction of~\cite{canetti2014key} for discussion.}  In addition to showing the reusability of their construction, we provide two significant improvements:

%\section{Proof of Security of \consref{cons:sampling}}
%\label{sec:analysis sampling}

\section{Analysis of \consref{cons:first construction}}
\label{sec:construction analysis}

\subsection{Characterizing sources with sparse high-entropy marginals}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and a little unwieldy.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.
The majority of the difficulty in characterizing \defref{def:block guessable} is that different symbols may be dependent, so an equality query on symbol $i$ may reshape the distribution of symbol $j$.  In the examples that follow we denote the adversary by $S$ as we consider security against computationally unbounded adversaries defined in VGB obfuscation~(\defref{def:obf}).  We first show some sources that have sparse high-entropy marginals
\ifnum\lncs=0
(\secref{sec:positive ex})
\fi
and then show sources with high overall entropy that do not have sparse high-entropy marginals
\ifnum\lncs=0
(\secref{sec:negative ex}).
\fi

\subsubsection{Positive Examples}
\label{sec:positive ex}
We begin with the case of independent symbols.

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,  ... , W_\gamma$ be a source in which all symbols $W_j$  are mutually independent.  Let $\alpha$ be a parameter.  Let $J\subset \{1,..., \gamma\}$ be a set of indices such that for all $j\in J$, $\Hoo(W_j ) \ge \alpha$.  Then for any $q$, $W$ is a source with $(\gamma - |J|)$-sparse $(\alpha - \log (q+1))$-entropy $q$-marginals.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then  $W$ is a source with $(\gamma - |J|)$-sparse $\omega(\log n)$-entropy $q$-marginals.
\end{claim}
\begin{proof}
It suffices to show that for all $j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}) = \alpha -\log (q+1)$.
We can ignore queries for all symbols but the $j$th, as the symbols are independent. Furthermore, without loss of generality, we can assume that no duplicate queries are asked, and that the adversary is deterministic ($S$ can calculate the best coins). Let $A_1, A_2, \dots A_q$ be the random variables representing the oracle answers for an  adversary $S$ making $q$  queries about the $i$th symbol. Each $A_k$ is just a bit, and at most one of them  is equal to 1 (because duplicate queries are disallowed). Thus, the total number of possible responses is $q+1$. Thus, we have the following,
\begin{align*}
\Hav(W_j | View(S^{\mathcal{O}_{W}(\cdot, \cdot)}) &= \Hav(W_j| A_1, \dots, A_q)\\
&=\Hoo(W_j) - |A_1, \dots, A_q|\\
&=\alpha - \log (q+1)\,,
\end{align*}
where the second line follows from the first by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
\end{proof}
\noindent In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} show a construction for symbol-fixing sources, where each symbol is either uniform or a fixed symbol~(symbol-fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} shows that \defref{def:block guessable} captures, in particular, this class of distributions.
However, \defref{def:block guessable} captures more distributions.  We now consider more complicated distributions where symbols are not independent.

\begin{claim}
\label{cl:each block from single seed}
Let $f:\zo^e \rightarrow \mathcal{Z}^\gamma$ be a function.  Furthermore, let $f_j$ denote the restriction of $f$'s output to its $j$th coordinate.  If for all $j$, $f_j$ is injective then $W = f(U_e)$ is a source with  $0$-sparse $(e - \log (q+1))$-entropy $q$-marginals.
\end{claim}
\begin{proof}
Since $f$ is injective on each symbol, 
\ifnum\lncs=0
$
\else
\[
\fi
\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) = \Hav(U_e | View(S^{I_{W}(\cdot, \cdot)}))
\ifnum\lncs=0
$.  
\else
\,.\]
\fi
Consider a query $q_k$ on symbol $j$.  There are two possibilities: either $q_k$ is not in the image of $f_j$,  or $q_k$ can be considered a query on the preimage $f_j^{-1}(q_k)$. Then (by assuming $S$ knows $f$) we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ (like in the
proof of \clref{cl:independent high ent}),
 and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $j$, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) \geq \Hoo(W_j) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so there is a family of distributions with total entropy $\omega(\log n)$ for which \consref{cons:first construction} is secure.  For these distributions, all the coordinates are as dependent as possible: one determines all others.
We can prove a slightly weaker claim when the correlation between the coordinates $W_j$ is arbitrary:

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\gamma$ be a source.  Suppose that for all $j$, $\Hoo(W_j)\geq \alpha$, and that $q \le 2^{\alpha}/4$ (this holds asymptotically, in particular, if $q$ is polynomial and $\alpha$ is super-logarithmic). Then  $W$ is a source with $0$-sparse $(\alpha-1-\log(q+1))$-entropy $q$-marginals.
\end{claim}

\begin{proof}
Intuitively, the claim is true because the oracle is not likely to return 1 on any query. Formally, we proceed by induction on oracle queries,
using the same notation as in the proof of   \clref{cl:independent high ent}. Our inductive hypothesis is
that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.  If the inductive hypothesis holds, then, for each $j$,
\begin{equation}
\label{eq:cond-entropy}
\Hoo(W_j | A_1= \dots= A_{k-1}=0) \ge \alpha-1\,.
\end{equation}
This is true for $k=1$ by the condition of the theorem. It is true for $k>1$ because, as a consequence of the definition of $\Hoo$,
for any random variable $X$ and event $E$, $\Hoo(X|E)\ge \Hoo(X)+\log\Pr[E]$; and $(k-1) 2^{1-\alpha}\leq 2 q 2^{-\alpha} \leq 1/2$.

We now show that $\Pr[A_1\neq 0 \vee \dots \vee A_{k}\neq 0] \leq k 2^{1-\alpha}$, assuming that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.
\begin{align*}
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0 \vee A_k\neq 0] & = 
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \\
&\ \ \ \ +\Pr[A_1=\dots = A_{k-1}=0 \wedge A_k=1]\\
& \le  (k-1)2^{1-\alpha}\\
& \ \ \ \ +\Pr[A_k=1\,|\,A_1=\dots = A_{k-1}=0]\\
& \le  (k-1)2^{1-\alpha}+\max_j 2^{-\Hoo(W_j | A_1=\dots =A_{k-1}=0)}\\
& \le   (k-1)2^{1-\alpha}+ 2^{1-\alpha}\\
& =  k 2^{1-\alpha}
\end{align*}
(where the third line follows by considering that to get $A_k=1$, the adversary needs to guess some $W_j$, and the fourth line follows by~\eqref{eq:cond-entropy}).
Thus, using $k=q+1$ in~\eqref{eq:cond-entropy},
 we know $\Hoo(W_j | A_1= \dots= A_q=0) \ge \alpha-1$.  Finally this means that

\ifnum\lncs=0
\begin{align*}
\Hav(W_j | A_1,\dots, A_q) &\ge -\log \left( 2^{-\Hoo(W_j | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]+1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] \right)\\
& \ge -\log \left(  2^{-\Hoo(W_j | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\else
\begin{align*}
\Hav(W_j | A_1,\dots, A_q) &\ge -\log ( 2^{-\Hoo(W_j | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]\\ &\ \ \ \ \ \ \ \ \ \ \ \  +1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] )\\
& \ge -\log \left(  2^{-\Hoo(W_j | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\fi

\end{proof}

\subsubsection{Negative Examples}
\label{sec:negative ex}
Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some symbols to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$ and let $f_1,..., f_{\gamma}$ be injective functions where $f_j:\zo^{j\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \gamma \times \log p$, that is the source has a small number of symbols.}  Then define the distribution $W_1 = f_1(U_{1,...,\gamma}), W_2 = f_2(U_{1,..., 2\gamma}),...., W_\gamma = f_\gamma(U)$.  There is an adversary making $p\times \gamma = \poly(n)$ queries such that $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
Let $x$ be the true value for $U_{p\times \gamma}$.
We present an adversary $S$ that completely determines $x$.  $S$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $S$ queries on $(1, y_1),..., (1, y_p)$, exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $S$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered~(and thus $w$).  %The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \gamma)$.  Once $x$ has been recovered then $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relies on an adversary's ability to determine a symbol from the previous symbols.  We formalize this notion next.  We define the entropy jump of a source as the remaining entropy of a symbol when previous symbols are known:

\begin{definition}
Let $W = W_1,..., W_\gamma$ be a source under ordering $i_1,..., i_\gamma$.  The \emph{jump} of a symbol $i_j$ is $\mathtt{Jump}(i_j) = \max_{w_{i_1},..., w_{i_{j-1}}} H_0 (W_{i_j} | W_{i_1} = w_{i_1} ,..., W_{i_{j-1}} = w_{i_{j-1}})$.
\end{definition}

An adversary who can learn symbols in succession  can eventually recover the entire secret.  In order for a source to have sparse high-entropy marginals, the adversary must get ``stuck'' early enough in this recovery process.  This translates to having a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\gamma$ such that for all $j\le \gamma-\beta +1$, $\mathtt{Jump}(i_j) = \log q /(\gamma-\beta+1)$, then $W$ is not a  source with $\beta$-sparse high-entropy $q$-marginals.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \gamma$.  We describe an unbounded adversary $S$ that determines $W_1,..., W_{\gamma-\beta+1}$.  As before $S$ queries the $q /\gamma$ possible values for $W_1$ and determines $W_1$.  Then $S$ queries the (at most)~$q/(\gamma-\beta+1)$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\gamma-\beta+1}$ is learned.
\end{proof}

Presenting a sufficient condition for security is more difficult as $S$ may interleave queries to different symbols.  It seems like the optimum strategy for $S$ is to focus on a single symbol at a time, but it is unclear how to formalize this intuition.


\subsection{Security}
It suffices to prove that \consref{cons:first construction} is a  $(\mathcal{Z}^\gamma, \mathcal{W}, \tilde{m}=H_0(C)-\beta, t)$-computational fuzzy conductor, i.e., that $C$ has HILL entropy $H_0(C)-\beta$ conditioned on $P$. The final extraction step will convert it to a computational fuzzy extractor~(see \apref{sec:further defs}).


Security proof of \consref{cons:first construction} is similar to the security proof of \consref{cons:sampling}.  However, it is made more complicated by the fact that the definition of sources with sparse high-entropy marginals~(\defref{def:block guessable}) allows for certain weak symbols that can easily be guessed.  This means we must limit our indistinguishable distribution to symbols that are difficult to guess.  Security is proved via the following lemma:

\begin{lemma}
\label{lem:security of cons}
Let all variables be as in \thref{thm:main thm first cons}.  For every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}( C | P ) \geq H_0(C) - \beta$.
\end{lemma}

We give a brief outline of the proof, followed by the proof.
It is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))\le \ngl(n)$.  Let $J$ be the set of indices that exists according to \defref{def:block guessable}. Define the distribution $C'$ as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $J$.  We first note that $C'$ has sufficient entropy, because $\Hav(C' |P) = \Hav(C' | C_{J^c}) \ge \Hoo(C', C_{J^c}) - H_0(C_{J^c})  = H_0(C) - |J^c|$ (the second step is by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P)) \le \ngl(n)$.
%Define the distribution $X$ as follows:
%\[X_i =
%\begin{cases}
%W_i & C_i = 0\\
%R_i & C_i = 1.
%\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$. Since $P$ is a collection of obfuscated programs, there exists a simulator $S$~(outputting a single bit), such that $\Pr[D(C, P)=1]$ is close to $\Pr[S^{\mathcal{O}}(C)=1]$.
\item Show that even an unbounded $S$ making a polynomial number of queries to the stored points cannot distinguish between $C$ and $C'$.  That is, $\Delta(S^{\mathcal{O}}(C),S^{\mathcal{O}}(C'))$ is small.
\item By the security of obfuscation, $\Pr[S^{\mathcal{O}}(C')=1]$ is close to $\Pr[D(C', P)=1]$.
\end{itemize}
\begin{proof}[Proof of \lemref{lem:security of cons}]
\label{app:security of main cons}

\lnote{I need to check this}
 Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P)\geq H_0(C)- \beta$. % for $\epsilon' = 2\epsilon_{obf} + (\gamma - \beta)2^{-(\alpha-1)}$.
Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P) < H_0(C)-\beta$.
By \defref{def:block guessable} there exists a set of indices $J$ such that all symbols within $J$ are unguessable.  Define by $C'$ the distribution of sampling a uniform codeword where all locations outside $J$ are fixed.  Then
$\Hav(C' | C_{J^c}) \ge \Hoo(C', C_{J^c}) - H_0(C_{J^c})  = H_0(C) - \beta$ (by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).

Let $D$ a distinguisher of size at most $s_{sec}$ such that
\[
| \expe[D(C, P)] - \expe[D(C', P)] > \epsilon_{sec} = 1/\poly(n).
\]
Define the distribution $X$ as follows:
\[X_j =
\begin{cases}
W_j & C_j = 0\\
R_j & C_j = 1.
\end{cases}\]  By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before marginals}
|\expe [D(P_1,..., P_\gamma, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $C$ and $C'$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})) \le (\gamma-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $J^c$, the statistical distance is at most $(\gamma-\beta)2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then,
\[
\Delta( S^{I_X(\cdot, \cdot)}(c^*, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(c', 1^{\gamma \log |Z|})) \le ( \gamma -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $j\in J$, $\Hav(W_j | View(S))\geq \alpha$.  The only information about the correct value of $c_j^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $j \in J$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not. That is, suppose the probability of at least one nonzero response on index $j$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_j$, the probability of this happening when $c^*_j = 1$ is at most $q/\mathcal{Z}$ or  equivalently $2^{-\log |\mathcal{Z}|+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }j]\nonumber \\
 &= \Pr[c_j^* =1]\Pr[\text{non zero response location }j\wedge c_j^*=1]\nonumber \\&+ \Pr[c_j^*=0] \Pr[\text{non zero response location }j \wedge c_j^*=0]\nonumber \\
&\le 1\times 2^{-\log|\mathcal{Z}|+\log q} + 1\times  \Pr[\text{non zero response location }j \wedge c_j^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that for  $\alpha\leq \log |\mathcal{Z}|-\log q $:
\begin{claim}
\label{cl:ent bounded away from n}
If $W$ is a source with $\beta$-sparse $\alpha$-entropy $q$-marginals over $\mathcal{Z}$, then $\alpha \le \log |\mathcal{Z}|-\log q$.
\end{claim}
\begin{proof}
 Let $J\subset\{1,..., \gamma\}$ the set of good indices.
It suffices to show that there exists an $S$ making $q$ queries such that for some $j\in J, \Hav(W_j | S^{I_{W}(\cdot, \cdot)})\le \log |\mathcal{Z}| - \log q$.  Let $j\in J$ be some arbitrary element of $J$ and denote by $w_{j,1}, ..., w_{j,q}$ the $q$ most likely outcomes of $W_j$~(breaking ties arbitrarily).  Then $\sum_{i=1}^q \Pr[W_j = w_{j,i}]\geq q/|\mathcal{Z}|$.  Suppose not. This means that there is some $w_{j,i}$ with probability $\Pr[W_j = w_{j,i}] < 1/|\mathcal{Z}|$.  Since there are $\mathcal{Z} - q $ remaining possible values of $W_j$ for their total probability to be at least $1-q/|\mathcal{Z}|$ at least of these values has probability at least $1/\mathcal{Z}$.  This contradicts the statement $w_{j,1},..., w_{j,q}$ are the most likely values.  Consider $S$ that queries its oracle on $(j, w_{j,1}),.., (j, w_{j,q})$.  Denote by $Bad$ the random variable when $W_j\in \{w_{j,1},.., w_{j,q}\}$  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_j | S^{J_W(\cdot, \cdot)}) &=  -\log \left(\Pr[Bad=1]\times 1+ \Pr[Bad=0]\times \max_{w}\Pr[W_j = w| Bad =0]\right)\\
&\leq  -\log \left(\Pr[Bad=1]\times 1\right)\\
&=-\log\left( \frac{q}{|\mathcal{Z}|} \right) = \log|\mathcal{Z}|-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }j \wedge c_j=0] &>2^{-\alpha+1} - 2^{-(\log |\mathcal{Z}|-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_j=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the condition on $W$ (\defref{def:block guessable}).  By the union bound over the indices $j\in J$ the total probability of a $1$ in $J$ is at most $(\gamma-\beta)2^{-\alpha+1}$. Recall that $c^*, c'$ match on all indices outside of $J$. Thus, for all $c^*, c'$ the statistical distance is at most $(\gamma- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}.
\end{proof}
By averaging over all points in $C'$ we conclude that 
\[
\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})) < (\gamma -\beta)2^{-(\alpha+1)}\,.
\]
This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after marginals}
|\expe [D(P_1,..., P_\gamma, C') ]- \expe [S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before marginals} and~\ref{eq:dist after marginals} and \lemref{lem:sim cannot distinguish}, we have
\begin{align*}
\delta^{D}(( P, C), (P, C'))&\leq |\expe [D(P_1,..., P_\gamma, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})]| \\
&+|\expe[S^{I_X(\cdot, \cdot)}(C, 1^{\gamma \log |Z|})] - \expe[S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] |\\
&+|\expe [S^{I_X(\cdot, \cdot)}(C', 1^{\gamma \log |Z|})] - \expe [D(P_1,..., P_\gamma, C') ]|\\
&\leq \epsilon_{sec}/3+ (\gamma-\beta)2^{-(\alpha-1)}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:security of cons}.
\end{proof}

\subsection{Correctness}
We now argue correctness of \consref{cons:first construction}.
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
Let all variables be as in \thref{thm:main thm first cons}.
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random symbol being interpreted as the obfuscation of the point) is $ \le \gamma/|\mathcal{Z}| = \ngl(n)$.
\end{lemma}
\begin{proof}
Consider a coordinate $j$ for which $c_j=1$. Since $w'$ is chosen independently of the points $r_j$, and $r_j$ is uniform, $\Pr[r_j =w_j']  = 1/|\mathcal{Z}|$. The lemma follows by the union bound, since there are at most $\gamma$ such coordinates.
\end{proof}

Since there are most $t$ locations for which $w_j\neq w_j'$ there are at most $t$ $0\rightarrow 1$ bit flips in $c$, which the code will correct with probability $1-\delta_{code}$, because $c$ was chosen uniformly.
Therefore, \consref{cons:first construction} is correct with error at most $\gamma/|\mathcal{Z}|$.




\end{document} 