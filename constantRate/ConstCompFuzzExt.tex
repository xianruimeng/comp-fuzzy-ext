\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Vol}{\mathsf{Vol}}

\title{Key Derivation From Noisy Sources With More Errors Than Entropy}
%\author{Ran Canetti \and Benjamin Fuller\footnote{The Lincoln Laboratory portion of this
%work was sponsored by the Department of the Air Force under Air Force
%Contract
%\#FA8721-05-C-0002.  Opinions,
%interpretations, conclusions and recommendations are those of the author
%and
%are not necessarily endorsed by the United States Government.} \and Omer Paneth \and Leonid Reyzin}

\begin{document}
\maketitle


\begin{abstract}
Fuzzy extractors convert a noisy source of entropy into a consistent uniformly distributed key.  In the process of eliminating noise, they  lose some of the entropy of the original source---in the worst case, as much as the logarithm of the number of correctable error patterns. We call what is left after this worst-case loss the \emph{minimum usable entropy}.  Unfortunately, this quantity is negative for some sources that are important in practice. Most known approaches for building fuzzy extractors work in the worst case and cannot be used when the minimum usable entropy is negative.

We construct the first fuzzy extractors that work for a large class of distributions in which the minimum usable entropy is negative. Their security is computational.  They correct Hamming errors over a large alphabet. In order to avoid the worst-case loss, they necessarily restrict distributions for which they work.  

Our first construction requires high individual entropy of a constant fraction of symbols, but permits symbols to be dependent.  Our second construction requires  a constant fraction of symbols to have a constant amount of entropy conditioned on prior symbols. The constructions can be implemented efficiently based on number-theoretic assumptions or assumptions on cryptographic hash functions.
\end{abstract}


\section{Introduction}\label{sec:introduction}

\paragraph{Fuzzy Extractors}
Cryptography relies on long-term secrets for key derivation and authentication. However, many sources with sufficient randomness to form long-term secrets provide similar but not identical values at repeated readings~(prominent examples include biometrics and other human-generated data~\cite{daugman2004,zviran1993comparison,brostoff2000passfaces,ellison2000protecting,mayrhofer2009shake,monrose2002password},
physically unclonable functions~\cite{pappu2002physical,tuyls2006puf,gassend2002silicon,suh2007physical},
and quantum information~\cite{bennett1988privacy}). Turning similar readings into identical values is known as \emph{information reconciliation}; further converting those values into uniformly random secret strings is known as \emph{privacy amplification}~\cite{bennett1988privacy}.
Both of these problems have interactive and noninteractive versions.  In this paper, we are interested in the noninteractive case, which is useful for a single user trying to produce the same key from multiple readings of a physical source at different times.
 A \emph{fuzzy extractor} is the primitive that accomplishes both information reconciliation and privacy amplification noninteractively; fuzzy extractors are defined information-theoretically in~\cite{DBLP:journals/siamcomp/DodisORS08}.


Fuzzy extractors consist of a pair of algorithms: \gen takes a source value $w$, and produces a key $r$ and a public helper value $p$.  The second algorithm \rep takes this helper value $p$ and a close $w'$ to reproduce the original key $r$.  The security guarantee is that $r$ produced by \gen is close to uniform (information-theoretically \cite{DBLP:journals/siamcomp/DodisORS08} or computationally \cite{fuller2013computational}), even given $p$, as long as $w$ comes from a high-quality distribution (traditionally, any distribution with sufficient min-entropy $m$). The correctness guarantee is that $r$ will be correctly reproduced by \rep as long as $w'$ is no farther than $t$ from $w$ in some metric space (in this paper, we focus on the Hamming metric on length $\ell$ strings over some alphabet $\mathcal{Z}$).

\paragraph{Limitations of Entropy-Based Approaches}
Constructions of fuzzy extractors are limited by the tension between security and correctness guarantees: if we allow for higher error tolerance $t$, then we also need higher starting entropy $m$. The reason for this tension is simple: if an adversary who knows $p$ can guess some $w'$ within distance $t$ of $w$ they will be able to easily distinguish $r$ from uniform by running $\rep$. If $t$ is larger, then $\rep$ tolerates more $w'$ values, so the adversary's job is easier. In fact, if $t$ is high enough that there are $2^m$ points in a ball of radius $t$, then the \emph{entire distribution} of $w$ may have min-entropy $m$ and be contained in a single ball.  Then the adversary can always produce the key by running $\rep$ with the center of the ball as $w'$.

More generally, let $B_t$ denote the number of points in a ball of radius $t$. For any $m$ and $t$, there is a distribution of min-entropy $m$ such that the adversary can guess a correct $w'$ with probability $1/\lceil 2^m/B_t) \rceil\approx B_t 2^{-m}$: the distribution consists of the uniform distribution over all points in several nonoverlapping balls of radius $t$ (the metric space must be large enough for these balls not to intersect). We thus call $m-\log B_t$ the \emph{minimum usable} entropy, denoted by $\Huse$. The previous paragraph shows that  no fuzzy extractor can handle all distributions of a given min-etropy $m$ if  $\Huse\le 0$.

Prime candidate sources for authentication have $\Huse\le 0$.  As an example, the iris is believed to be the best biometric for high security applications~\cite{prabhakar2003biometric}.  The best estimate for iris entropy is $249$ bits~\cite{daugman2004}.  Daugman uses specialized wavelets to derive a $2048$ bit string called an iris code.  Let the outcome of this transform~(on different irises) define a distribution $w$.  The precise number of errors that must be tolerated depends on the desired false reject rate (how often the correct key is produced).  For a false reject rate of $\le 80\%$, a $t$ of approximately $205$ is required.  We have the following calculation for $\Huse$:
\[
\Huse = \Hoo(W) - \log |B_t|
= 249 - \log \sum_{i=0}^{205} {2048 \choose i} \approx -707.
\]
There is considerable subsequent research~\cite{gentile2009slic,gentile2009efficient,rathgeb2011combining} to Daugman but it does not affect $\Huse$ dramatically.\footnote{The work of Hao et al.~\cite[Section 4.3]{hao2006combining} provides a similar analysis but underestimates the number of possible error patterns; their calculation, which we cannot confirm, is $\Huse \approx 44$.}  

\paragraph{Our Contributions}
We provide the first constructions of computational fuzzy extractors that can be used for a large class of distributions with $\Huse\le 0$ over $\mathcal{Z}^\ell$ for a large alphabet $\mathcal{Z}$.  As explained above, such constructions cannot work without some restriction on the distribution.
Our first construction is secure when symbols in $w$
each have individual superlogarithmic min-entropy, even if they are arbitrarily correlated. Moreover,
a constant fraction of symbols in $w$ may have little entropy, as long as knowledge of their values does not reduce the entropy of the high-entropy symbols too much (see \defref{def:block guessable}).

We improve the entropy requirement in the second construction, which requires only a constant fraction of the symbols $w$ to have constant min-entropy conditioned on the previous symbols.
However, this improvement comes at a price to error-tolerance: whereas the first construction tolerates a constant fraction of errors, the second construction tolerates $\ell/\omega(\log\ell)$ errors.



\paragraph{Our Approach}
Our constructions are computationally secure.  Known techniques for proving security of information-theoretic  fuzzy extractors work for all distributions for a given $m, t$, and thus cannot be used with $\Huse\le 0$ (because they would prove security for distributions contained within a single ball of radius of $t$). 

Most known constructions of fuzzy extractors put sufficient information in $p$ to recover the original $w$ from a nearby $w'$ during $\rep$ (this procedure is called a \emph{secure sketch}), and then apply a randomness extractor to $w$ to get $r$.
Fuller, Meng, and Reyzin~\cite{fuller2013computational} show that replacing secure sketches with a similar computational component is unlikely to be fruitful~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational}.  Instead, they suggest two alternatives: combine the information-reconciliation and privacy amplification components~(their approach) or produce a new consistent secret with computational entropy instead of recovering $w$.  We take the second approach.

Any procedure that converts a high-entropy input to a high-entropy output is known as a \emph{conductor} \cite{CRVW02}; if it's error-tolerant, then it's a \emph{fuzzy conductor}~\cite{KanukurthiR09}. We show two constructions of \emph{computational fuzzy conductors}.
These constructions may be converted to computational fuzzy extractors using information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors~(\lemref{lem:cond and cext}).

Both constructions are based on  obfuscation of point programs~\cite{canetti1997towards}.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.
We need a strong version of point obfuscation that remains secure even when several obfuscations of correlated points are composed. While the standard definition of obfuscation \cite{barak2001possibility} does not imply security under composition, we can base our construction on the relaxed notion of \emph{virtual grey-box} obfuscation introduced in~\cite{bitansky2010strong}. For this notion, \cite{bitansky2010strong} construct composable obfuscation of point programs under particular number-theoretic assumptions. Additionally, such obfuscation can be made very efficient under a strong assumption on cryptographic hash functions~\cite{canetti1997towards}.

Both of our constructions are inspired by Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  Let $w=w_1 \dots w_\ell$, for $w_j\in \mathcal{Z}$. In the first construction (\consref{cons:first construction}), for each $j$, $\gen$ flips a coin $c_j$ and either obfuscates $I_{w_j}$ or picks a random point $r_j$ and obfuscates $I_{r_j}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rep$ then runs the obfuscated program $P_j$ on $w_j'$ and checks whether $P_j(w_j')=1$.  For most locations $j$, \rep can determine whether $w_j$ or a random value was obfuscated.  Thus, most bits of $c_j$ are recoverable. To tolerate errors,  the set of coins $c_1\dots c_\ell$  is chosen at random from the codewords of an error correcting code. This construction conducts entropy from $w$ to $c$.

Obfuscation of point functions provides no security if a point can be guessed; thus, in order for the first construction to be secure, sufficiently many coordinates of $w$ have to be unguessable (even to an adversary who can make equality queries for the values of other coordinates). We relax this requirement in our second construction (\consref{cons:sampling}), called \emph{sample-then-obfuscate}: it transforms $w$ into a string of blocks and then applies the first construction. $\gen$ randomly samples several coordinates of $w$ and concatenates them to form a block. This reduces the entropy requirement on the individual symbols, but lowers the error-tolerance. This approach is similar to the sample-then-extract paradigm for building locally computable extractors~\cite{lu2002hyper,vadhan2003constructing}.  Unlike in locally computable extractors, we can form multiple blocks sampling from the same value $w$ and only argue about their individual entropy, because correlations among blocks are allowed for the first construction. Computational, rather than information-theoretic, analysis seems crucial for achieving this property.

\paragraph{On the Relationship to General Obfuscation}
We note that fuzzy extractors for the setting where $\Huse\le 0$ can be trivially constructed from a strong form of obfuscation, specifically, virtual black-box obfuscation for the class {\em proximity point programs}, $I_w(x)$ that tests if $x$ is within Hamming distance $t$ of $w$. However, currently we do not know if such obfuscation exists, let alone whether it can be made as efficient as our constructions. Recent works constructed candidate indistinguishability obfuscators \cite{GargGH0SW13,PassTS13} and virtual black-box obfuscators in an idealized model \cite{BrakerskiR13,BarakGKPS13} for all programs. However, these notions of obfuscation do not imply virtual-black-box obfuscation for proximity point programs in the plain model. The work of \cite{BarakBCKPS13} suggests an average-case virtual-black-box obfuscator for the family of {\em evasive} functions that test if a low-degree multi-variant arithmetic circuit evaluates to zero. However, we do not know if these functions include proximity point programs.



\paragraph{Open Problems}
Both constructions require a large alphabet $\mathcal{Z}$---one whose size is more than polynomial in the security parameter.\footnote{Alternatively, in case of burst errors~\cite{gilbert1960capacity}, multiple symbols from a small alphabet can be combined into a single symbol from a large alphabet.}  It is possible to tweak the sample-then-obfuscate construction for use with a small alphabet.  However, distributions with $\Huse \le 0$ are supported only for large alphabets.  For small alphabets, $\Huse>0$ and there are good information-theoretic constructions known~\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08}.
Constructing a computational fuzzy extractor when $\Huse\le 0$ and a small alphabet is an open problem.

Using information-theoretic fuzzy extractors with additional privacy properties, Dodis and Smith~\cite[Section 5]{DBLP:conf/stoc/DodisS05} construct program obfuscators for the program $I_w(x)$ that tests if $x$ is within Hamming distance $t$ of $w$. The obfuscation is secure as long as $w$ comes from a distribution of sufficient min-entropy; in particular, the entropy must be high enough so that $\Huse>0$. Our constructions do not provide obfuscators for proximity queries, because they leak more information than whether $x$ is within distance $t$ of $w$ (for example, they may provide some information about the actual distance or about which coordinates agree). Constructing an efficient obfuscator for proximity queries when $\Huse<0$ is an open problem.

\medskip
The remainder of this paper is organized as follows: we cover notation and background on obfuscation and error correcting codes in \secref{sec:preliminaries}, describe computational fuzzy extractors in \secref{sec:fuzzy extractors}, and present our two constructions in Sections \ref{sec:construction} and \ref{sec:sampling} respectively.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote by $X = X_1,..., X_\ell$  the tuple $(X_1,\dots, X_\ell)$.  For a set of indices $J$, $X_{J}$ is the restriction of $X$ to the indices in $J$.  The set $J^c$ is the complement of $J$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$,
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   For a random variable $W$, let $H_0(W)$ be the logarithm of the size of the support of $W$,  that is $H_0(W) = \log |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$ we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$ (we extend it to a class of distinguishers $\mathcal{D}$ by taking the maximum over all distinguishers $D\in\mathcal{D}$).  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric over vectors in $\mathcal{Z}^\ell$, defined via $\dis(x,y) = \{i | x_i \neq y_i\}$.  For this metric, $|B_t| = \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
Usually, we use capitalized letters for random variables and corresponding lowercase letters for their samples.

\subsection{Coding Theory}
\label{sec:coding theory}
We will consider slightly nonstandard error-correct codes over $\{0,1\}^\ell$, which correct up to $t$ bit flips from $0$ to $1$ but no bit flips from $1$ to $0$ (this is the Hamming analog of the $Z$-channel~\cite{tallini2002capacity}).
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$.
\end{definition}

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \delta_{code})$-code if there exists an efficient procedure $\decode$ such that $\Pr_{c\in C}[\exists c'\in \neigh_t(c) \text{ s.t. } \decode(c') \neq c] \leq \delta_{code}$.
\end{definition}

We note that for any code, learning a few bits of a codeword does not inform on the remainder of the bits~(the claim is a direct result of \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).

\begin{claim}
\label{cl:many locations ent}
Let $C$ be a subset of $\{0,1\}^\ell$ with a uniform distribution, and let $J\subset \{1,\dots,\ell\}$.  Then $\Hav(C | C_{J^c}) =  H_0(C) - |J^c|$.
\end{claim}

\textbf{Notes:}
Any code that corrects $t$ Hamming errors also corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
Codes with $2^{\Theta(\ell)}$ codewords and $t = \Theta(\ell)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.
The class of errors we support in our source~($t$ Hamming errors over a large alphabet) and the class of errors for which we need codes~($t$ $0\rightarrow 1$ errors) are different.  See Constructions~\ref{cons:first construction} and~\ref{cons:sampling} for the translation between the error classes.

\subsection{Obfuscation}
Our construction uses obfuscation for a family of point functions $\mathtt{I} = \{I_w\}_{w \in \zo^*}$ defined as follows:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
The required notion of obfuscation is virtual grey-box (VGB) introduced in \cite{bitansky2010strong}. This notion is weaker then the standard notion of virtual black-box (\cite{barak2001possibility}), as it allows the simulator to run in unbounded time while making at most a polynomial number of oracle queries to the function. In the following definition we also require that the obfuscation is composable and secure with respect to auxiliary input. Composable auxiliary-input VGB obfuscators for point functions are constructed in \cite[Theorem 6.1]{bitansky2010strong} from the Strong Vector Decision Diffie-Hellman assumption, which is a generalization of the strong DDH assumption of \cite{canetti1997towards} for tuples of points. They can also be constructed by assuming strong properties of cryptographic hash functions!\cite{canetti1997towards}

\begin{definition}[$\ell$-composable obfuscation VGB obfuscation with auxiliary input \cite{bitansky2010strong}]
\label{def:obf} Let $\mathtt{I}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is an $\ell$-composable VGB obfuscator for $\mathtt{I}$ with auxiliary-input if the following conditions are met:
\begin{enumerate}
\item \emph{Functionality:} for every $ I \in \mathtt{I}$, $\mathcal{O}(I)$ is a circuit that computes the same function as $I$.
\item \emph{Virtual grey-box:}  For every PPT adversary $A$ and polynomial $p$, there exists a (possibly inefficient) simulator $S$ and a polynomial $q$ such that for all sufficiently large $n$, any  sequence of circuits $I^1,\dots,I^\ell \in \mathtt{I}_n$, (where $\ell=\poly(n)$) and for all auxiliary inputs $z\in \zo^*$:
\[
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^\ell)) = 1] - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | < \frac{1}{p(n)} \enspace,
\]
where $(I^1,\dots,I^\ell)[q(n)]$ is an oracle that answers at most $q(n)$ queries, and where every query of the form $(i,x)$ is answered by $I^i(x)$.
\end{enumerate}
\end{definition}
For notational convenience, since we only use point function obfuscation, we denote the oracle provided to the simulator as $I_w(\cdot, \cdot)$ where $w = w_1,..., w_\ell$ is the vector of obfuscated points.
\section{Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

In this section we present our paradigm for constructing computational fuzzy extractors.  Definitions for information-theoretic fuzzy extractors can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness. In both our constructions it is crucial that $w'$ is chosen independently of the outcome of \gen.
The definition of computational fuzzy extractors specifies a family of sources for which the fuzzy extractor works rather than the family of all sources of a given min-entropy $m$.  As discussed in the introduction, when $\Huse\le 0$ some restriction on the source is necessary.  Instead of restricting the class of source distributions, the definition could restrict the correctable errors to errors that are ``likely'' to occur in the source.  We leave this as an open problem.

Fuller, Meng, and Reyzin~\cite{fuller2013computational} present two approaches for constructing a computational fuzzy extractor: analyzing the information-reconciliation and privacy amplifications components together or using a fuzzy conductor and a privacy amplification component.  We follow the second approach.
In \apref{sec:conductors}, we show that information-theoretic fuzzy conductors are subject to the same lower bounds on entropy loss as information-theoretic  fuzzy extractors.  To overcome these bounds, we use a computational version of a fuzzy conductor.
We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition} \lnote{I haven't checked this section below this line}
We now define a computational fuzzy conductor and a (computational)~randomness extractor.  A computational fuzzy conductor is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' ($\gen'$) and ``reproduce'' ($\rep'$) is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $X\in\{0,1\}^\ell$ and a helper string $SS\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $SS\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(X, SS)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', SS) = X] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $X$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon_{cond}, s_{cond}}(X |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}
A computational extractor is the adaption of a randomness extractor to the computational setting.  Any information-theoretic randomness extractor is also a computational extractor; however, unlike information-theoretic extractors, computational extractors can expand their output arbitrarily via pseudorandom generators once a long-enough output is obtained. We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average-case:
\begin{definition}
Let $\chi$ be a finite set.
A function $\cext: \zo^\ell \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon_{ext}, s_{ext})$-average-case computational extractor} if for all pairs
of random variables $X, Y$ over $\zo^\ell, \chi$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{ext}}}((\cext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon_{ext}$.
\end{definition}

Combining a computational fuzzy conductor and an appropriate computational extractor yields a computational fuzzy extractor~(proof in \apref{sec:cond and cext}):

\begin{lemma}
\label{lem:cond and cext}
Let $\gen'$, $\rep'$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\ell$.  Let $\cext:\zo^\ell\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon_{ext}, s_{ext})$-average case computational extractor.  Define $(\gen, \rep)$ as:
\begin{itemize}
\item $\gen(w; seed):$ run $(x, ss)= \gen'(w)$ and set $r = \cext(x; seed)$, $p = (ss, seed)$.  Output $(r, p)$.
\item $\rep(w, (ss, seed)):$ recover $x = \rec'(w'; p')$ and output $r = \cext(x; seed)$.
\end{itemize}
Then $(\gen, \rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \min\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}


\section{Tolerating a Constant Fraction of Errors when $\Huse\le 0$}
\label{sec:construction}
For the remainder of this work, we consider the Hamming metric over some alphabet $\mathcal{Z}$.  Our goal is to derive strong keys for a large class of sources where $0\ge \Huse = \Hoo(W) - \log|B_t|$.
In the computational setting, a ``long enough'' key may be expanded using a computational extractor~(\lemref{lem:cond and cext}).  We therefore focus on building a computational fuzzy conductor with meaningful output entropy.\lnote{where do we talk about running it multiple times if we don't get enough output from one run?} \bnote{nowhere, it was something I realized yesterday.  For second construction it doesn't make much sense, just increase $\ell$ has same effect.} Our first construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}.

\begin{construction}
\label{cons:first construction}
Let $\mathcal{Z}$ be an alphabet and let $W = W_1,..., W_\ell$ be a distribution over $\mathcal{Z}^\ell$.  Let $\mathcal{O}$ be an obfuscator for the family of point functions $\mathtt{I}_{\mathcal{Z}}$.  Let  $C\subset \zo^\ell$ be an error-correcting code.
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C$.
\item For $j=1,..., \ell$:
\begin{enumerate}[(i)]
\item If $c_j = 0$: $p_j = \mathcal{O}(I_{w_j})$.
\item Else: Sample $r_j \overset{\$}\leftarrow \mathcal{Z}$.
\subitem Let $p_j = \mathcal{O}(I_{r_j})$.
\end{enumerate}
\item Output $(c, p)$, where $p=p_1\dots p_\ell$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $j=1,..., \ell$:
\begin{enumerate}[(i)]
\item If $p_j(w_j') = 1$: set $c_j' = 0$.
\item Else: set $c_j' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

The input $w$ is hidden in two different ways.  In locations where $c_j=1$, the block $w_j$ is information-theoretically unknown.
In locations where $c_j=0$, it is hard to find $w_j$ given access to the point obfuscation.
There are two possible reasons for a bit $c_j'$ to be $1$: because the true value was $1$ and because $w_j \neq w_j'$.  However, if a bit $c_j'$ is $0$, this likely means that $w_j=w_j'$ because collisions when $c_j=0$ are unlikely~(occurring with probability $1/|\mathcal{Z}|$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips.

\consref{cons:first construction} is secure if no distinguisher can tell whether it is working with random obfuscations or obfuscations of $W_j$.  By using the security of point obfuscation, anything learnable from the obfuscation is learnable from oracle access to the function.  More formally, our construction is secure as long as enough blocks are unpredictable even after adaptive queries to equality oracles for individual symbols. This restriction on the distribution is captured in the following definition.

\begin{definition}
\label{def:block guessable}
Let $I_w (\cdot, \cdot)$ be an oracle that returns \[I_w(j, w_j')=
\begin{cases}
1 & w_j = w_j'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1||...|W_\ell$ is a $(q, \alpha, \beta)$-\emph{unguessable block distribution} if there exists a set $J\subset\{1,..., \ell\}$ of size at least $\ell -\beta$ such that for any unbounded adversary $S$ with oracle access to $I$ making at most $q$ queries
\[
\forall j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}
\lnote{stopped going through section 4 here}

We show some examples of unguessable block distributions in \apref{sec:characterize}.  In particular, any source $W$ where for all $j$, $\Hoo(W_j) \geq \omega(\log n)$~(but all blocks may arbitrarily correlated) is an unguessable block distribution~(\clref{cl:all blocks entropy}).

\begin{theorem}
\label{thm:main thm first cons}
Let $n$ be a security parameter.
Let $\mathcal{W}$ be a family of $(q,\alpha= \omega(\log n),  \beta)$-unguessable block distributions for any $q = \poly(n)$ where each $W_j$ is over $\mathcal{Z}$ where $|\mathcal{Z}| = \omega(\poly(n))$.  Furthermore, let $C$ be a $(\neigh_t, \delta_{code})$-code.  Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for $\mathtt{I}_{\mathcal{Z}}$. Then for $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\mathcal{Z}^\ell, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta_{code} + \ell/|\mathcal{Z}|$ and resulting entropy $\tilde{m} =H_0(C) - \beta$.
\end{theorem}
\begin{proof}
We argue security in~\lemref{lem:security of cons} and correctness in~\lemref{lem:correct of cons}.
\end{proof}

\begin{lemma}
\label{lem:security of cons}
Let all variables be as in \thref{thm:main thm first cons}.  For every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{obf}}( C | P ) \geq H_0(C) - \beta$.% for $\epsilon' = 2\epsilon_{obf} + (\ell-\beta)2^{-(\alpha - 1)}$.
\end{lemma}

We give a brief outline of the proof here; the proof is in \apref{app:security of main cons}.
\begin{proof}[Outline]
It is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))\le \ngl(n)$.  Let $J$ be the set of indices that exists in \defref{def:block guessable}, the distribution $C'$ is defined as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $J$.  We first note that $C'$ has sufficient entropy.  That is, $\Hav(C' |P) =\Hav(C' | C_{J^c} = C'_{J^c}) = H_0(C) - |J^c|$.  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P)) \le \ngl(n)$.
%Define the distribution $X$ as follows:
%\[X_i =
%\begin{cases}
%W_i & C_i = 0\\
%R_i & C_i = 1.
%\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$, since $P$ is a collection of obfuscated programs there exists a simulator, $S$~(outputting a single bit), such that $\Pr[D(C, P)=1]$ is close to $\Pr[S^{\mathcal{O}}(C)=1]$.
\item Show that even an unbounded $S$ making a polynomial number of queries to the stored points cannot distinguish between $C$ and $C'$.  That is, $\Delta(S^{\mathcal{O}}(C),S^{\mathcal{O}}(C'))$ is small.
\item By the security of obfuscation, $\Pr[S^{\mathcal{O}}(C')=1]$ is close to $\Pr[D(C', P)=1]$.
\end{itemize}
\end{proof}
We now a rgue correctness of \consref{cons:first construction}.
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
Let all variables be as in \thref{thm:main thm first cons}.
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \ell/|\mathcal{Z}| = \ngl(n)$.
\end{lemma}
\begin{proof}
Recall that $c$ contains at most $\ell$ $1$s.  Since $w'$ is chosen independently of the points $r_j$, this probability is the size of each block, that is $\Pr[r_j =w_j']  = 1/|\mathcal{Z}|$. Since the $r_j$ are chosen independently, one has,
\[
\Pr[\text{no $1\rightarrow 0$ flips}] \geq 1-\Pr[\text{some }r_j = w_j']\geq 1-\sum_{j=1}^\ell \Pr[r_j = w_j'] \geq 1-\frac{\ell}{|\mathcal{Z}|}.
\]
\end{proof}

We now consider the number of possible $0\rightarrow 1$ bit flips in $c$.  A $0\rightarrow 1$ flip on index $j$ occurs when two conditions are met $w_j\neq w_j'$ and $c_j = 0$.  Since the first conditioned is only fulfilled at most $t$ times, we have the following lemma:

\begin{lemma}
\label{lem:correct of cons}
Let all variables be as in \thref{thm:main thm first cons}.
For $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$
\[
\Pr[\rep( w', P) = R] \geq 1-\left(\frac{\ell}{|\mathcal{Z}|}+\delta_{code}\right).
\]
That is, \consref{cons:first construction} is correct with error at most $\ell/|\mathcal{Z}|+\delta_{code}$.
\end{lemma}

\textbf{Note: }By using a more structured type of code, the necessary error tolerance may be decreased from $t$ to  $(1/2+\Omega(1))t$ by arguing that roughly half of the mismatches between $w, w'$ occur where $c_j =1$.  This requires a code where most codewords have Hamming weight close to $1/2$.

\subsection{Discussion of \consref{cons:first construction}}
\label{sec:discussion}
To show that $\Huse$ can be negative for \consref{cons:first construction}, we first calculate the size of the Hamming ball.  We allow $t$ errors with an alphabet $\mathcal{Z}$ of size $2^n$.  This means that
\[
\log |B_t| = \log \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i> \log {\ell \choose t} (|\mathcal{Z}|-1)^t =\Theta(t\log |\mathcal{Z}|) + \log {\ell\choose t}
\]

We consider what entropy is necessary for security.  The simplest type of unguessable block distribution is where each block is independent and has super-logarithmic entropy~(\clref{cl:independent high ent}).  For this type of source the required entropy is $\Hoo(W) = \ell\omega(\log n)$.  This yields:
\[
\Huse = \Hoo(W) - \log |B_t| < \ell \omega(\log n) -\left( \Theta(t\log |\mathcal{Z}|) + \log {\ell \choose t}\right).
\]
When $t =\Theta(\ell)$ and the entropy of each block is $o(\log |\mathcal{Z}|)$, then $\Huse\le 0$ and the output entropy is $H_0(C) -\beta$~(if $C$ is a constant rate code, this is $\Theta(\ell)$).

%\textbf{Limitations of \consref{cons:first construction}:}  \lnote{I feel like this repeats a lot of what said above (alphabet size), and what is said below (lots of entropy per block). Do we need it?} There are three major drawbacks to \consref{cons:first construction}.   First, we require blocks to be super-polynomial in size.  Second, we only obtain a single bit from each block.  Third, since each block is individually obfuscated, we leak information about individual blocks if they can be guessed.  To compensate for this leakage, we need most\lnote{just some linear fraction, not most, right? and, in fact, isn't just one block enough given our repeated use argument which we haven't really made yet?} blocks to be unguessable given equality queries.  Our second construction addresses this last weakness.

\section{Trading Errors for Entropy}
\label{sec:sampling}
\consref{cons:first construction} is a computational fuzzy conductor with $\Huse\le 0$.  Unfortunately, it requires many blocks of $W$ to have super-logarithmic min-entropy.  In this section, we reduce the required entropy of blocks by obfuscating several blocks simultaneously, at the price of  decreasing the effective error tolerance.
%In this section, we present a generalization where blocks are not individually obfuscated, instead several blocks are obfuscated together~(these blocks are selected using an oblivious sampler).
The main idea is to sample a random subset of blocks $W_{j_1},..., W_{j_\eta}$ and obfuscate the concatenation of these blocks.  Denote this concatenated value by $V_1$.  This process is repeated to produce $V_1,..., V_\ell$ and the construction proceeds by either obfuscating $V_i$ or a random point as before. For security each value $V_i$ needs to be unguessable.  This will hold as long as enough blocks contribute some entropy:

\begin{definition}
\label{def:partial source}
A distribution $W = W_1,..., W_\gamma$ is an $(\alpha, \beta)$-partial block source if there exists a set of indices $J$ where $|J| \geq \gamma - \beta$ such that the following holds:
\[
\forall j\in J, \forall w_1,..., w_\gamma \in W_1,..., W_\gamma, \Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}) \geq \alpha.
\]
\end{definition}

\defref{def:partial source} is a weakening of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}), as only some blocks are required to have entropy conditioned on the past.  The choice of conditioning on the past is arbitrary: a more general sufficient condition is that there exists some ordering of indices where most items have entropy conditioned on all previous items in this ordering~(for example, a ``partial'' reverse block source~\cite{vadhan2003constructing}).
%Note a partial block source requires worst case entropy while an unguessable block distribution only requires average conditional entropy. \lnote{this comparison is unclear to me}  However, we achieve security for significantly lower entropy levels than for unguessable block distributions.

Let $\sample_{\gamma, \eta}(\cdot)$ be an algorithm that  outputs a random subset of $\{1,..., \gamma\}$ of size $\eta$ given let $r_{sam}$ bits of randomness.

\begin{construction}[Sample-then-Obfuscate]
\label{cons:sampling}
Let $\mathcal{Z}$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source where each $W_j$ is over $\mathcal{Z}$. %and $\gamma = \Omega(n)$.  Let $\ell = \omega(\log(n))$ and $\eta = o(\gamma)$ be integer parameters (we will set them later). Let $C\subset \zo^\ell$ be a $(\neigh_{t'}, \delta_{code})$.
Let $\eta$ be a parameter, $C\subset\zo^\ell$ be an error-correcting code and $\mathcal{O}$ be an obfuscator for the family of point functions over $\mathtt{I}_{\mathcal{Z}^\eta}$.  Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Select $c\overset{\$}\leftarrow C$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Select $\lambda_i\overset{\$}\leftarrow \zo^{r_{sam}}$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\eta,\gamma}( \lambda_i)$
\item If $c_i = 0$:
\subitem Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\subitem Set $\rho_i = \mathcal{O}(I_{v_i})$.
\subitem Set $p_i = \rho_i, \lambda_i$.
\item If $c_i = 1$: Select $r_i \overset{\$}\leftarrow \mathcal{Z}^{\eta}$.
\subitem Let $p_i = \mathcal{O}(I_{r_i}), \lambda_i$.
\end{enumerate}
\item Output $(c, p)$, where $p=p_1\dots p_\ell$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Parse $p_i$ as $\rho_i, \lambda_i$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\gamma, \eta}(\lambda_i)$.
\item Set $v_i' = w_{j_{i, 1}},..., w_{j_{i, \eta}}$.
\item If $\rho_i(v_i') = 1$ set $c_i' = 0$.
\item Else set $c_i' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.37in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

The main change in \consref{cons:sampling} is that the obfuscated values are concatenated symbols of $W$.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.  A crucial difference is that the use of a computational primitive~(obfuscation) allows us to sample multiple times, because we need to argue only about individual entropy of $V_i$, as opposed to the information-theoretic setting, where it would be necessary to argue about the entropy of the joint variable $V$.

This construction uses a na\"{i}ve sampler that takes truly random samples, but the public randomness may be substantially decreased by using more sophisticated samplers. Goldreich provides an introduction to samplers in~\cite{goldreich1997sample}.


\begin{theorem}
\label{thm:sampling}
Let $n$ be a security parameter.
Let $\mathcal{W}$ be the family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-partial block sources over $\mathcal{Z}^\gamma$ where $\gamma = \Omega(n)$.   Let $\eta$ be such that $\eta = \omega(\log n)$ and $\eta = o(\gamma)$,  and $\ell$ be such that $\ell = O(\poly(n))$ and
$\ell = \omega(\log n)$.  Let $C$ be a $(\neigh_{t'}, \delta_{code})$ where $t' = \Theta(\ell)$.  Then for every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that \consref{cons:sampling} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ for
\begin{align*}
t&\le \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta}  = O(\gamma/\eta) = n /\omega(\log n)\\
\tilde{m} &=H_0(C)\\
\delta &= O(\ell/|\mathcal{Z}|+2^{-\ell} +\delta_{code}).
\end{align*}
\bnote{By increasing the size of $\gamma$ we can actually reduce the hit here.  For example if $\gamma = n^c$, then we only get $t \le \gamma / \omega(\log^{1/c} \gamma)$. Don't worry for submission.}
\end{theorem}


The next two sections are dedicated to proving this theorem.
 For security we argue that each of the $v_i$ values is unguessable.  For correctness we show that the induced error rate in $v$ and $v'$ is a small constant~(with overwhelming probability), so that $c'$ will be corrected to $c$ with overwhelming probability.

\subsection{Security of \consref{cons:sampling}}
In order to show security \consref{cons:sampling}, we
show that with overwhelming probability, at each of the $\ell$ iterations, the sampler will choose enough coordinates of $W$ that have high entropy, making $V_i$ have sufficient entropy.   We can then argue that $V_1,..., V_\ell$ forms a block-unguessable distribution.  Then \consref{cons:sampling} is just \consref{cons:first construction} applied to $V_1,.., V_\ell$, and security follows by \lemref{lem:security of cons}.  We begin by showing that each $V_i$ is statistically close to a high entropy distribution~(proof in \apref{sec:proof of sampling lemma}).   Let $\Lambda$ represent the random variable of all the coins used by $\sample$ and $\lambda=\lambda_1 \dots \lambda_\ell$
be some particular outcome.

\bnote{think about how to make this work when $\eta = \Theta(1)$.  Will make probabilities gross.  Not for submission.}
\begin{lemma}
\label{lem:sampling works}
Let all variables be as in \thref{thm:sampling}.
There exists $\epsilon_{sam} = O(e^{-\eta}) = \ngl(n)$ and $\alpha' = \alpha\eta(\gamma-\beta-\eta)/\gamma = \omega(\log n)$ such that for each $i$,
\[
\Pr_{\lambda\leftarrow \Lambda}[\Hoo(V_i | \Lambda= \lambda) \geq \alpha'] \geq 1- \epsilon_{sam}.
\]
\end{lemma}

\noindent
We can then argue that all $V_i$ simultaneously have individual entropy with good probability:
\begin{corollary}
\label{cor:samp sec}
 Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works}, and all the other variables be as in \thref{thm:sampling}.  Then $\Pr_{\lambda\leftarrow \Lambda}[\forall i, \Hoo(V_i | \Lambda = \lambda)  \ge \alpha'] \leq 1-\ell\epsilon_{sam}$.
%[V_i(V, \Lambda)$ is $(\ell\epsilon_{sam})$-close to a distribution $(V', U_{\ell\times r_{sam}})$ where for $u\in U_{\ell\times r_{sam}}$ for all $i$, $\Hoo(V_i' | U_{\ell\times r_{sam}} =u)\geq \alpha'$.
\end{corollary}
\begin{proof}
Union bound over the probability in \lemref{lem:sampling works}.
%Hybrid argument over the statistical distance in \lemref{lem:sampling works}.
\end{proof}

In ~\clref{cl:all blocks entropy} we show that any distribution where each individual block has super-logarithmic min-entropy forms a unguessable block distribution.  This allows us to conclude:
\begin{corollary}
\label{cor:v are unguessable}
Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works},  and all the other variables be as in \thref{thm:sampling}. Take any $q=\poly(n)$.  For $\alpha'' =\alpha'-1-\log (q+1) =  \omega(\log n)$, with  probability $1-\ell \epsilon_{sam}$ over the choice of $\Lambda=\lambda$, the distribution $V| \Lambda=\lambda$ is a $(q, \alpha'', 0)$-unguessable block distribution.
\end{corollary}


Thus, unless the choice of $\lambda$ is very unlucky,
\consref{cons:sampling} is \consref{cons:first construction} applied to an unguessable block distribution $V_1,..., V_\ell$. This allows us to conclude the following.
\begin{corollary}
\label{cor:samp unguess}
Let all the variables be as in \thref{thm:sampling}.
For every $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C | P) \geq H_0(C)$. %for $\epsilon' = (2\epsilon_{obf} + \ell(\epsilon_{sam}+ 2^{-(\alpha''-1)})) = \ngl(n)$.
\end{corollary}
\begin{proof}
Result of \corref{cor:v are unguessable} and \lemref{lem:security of cons} noting that $\ell \epsilon_{sam} = \ngl(n)$.
\end{proof}

\subsection{Correctness of \consref{cons:sampling}}
\label{sec:correct sampling}
The argument that $1\rightarrow 0$ flips from $c$ to $c'$ are unlikely carries over from \consref{cons:first construction}.  This probability is at most $\ell/\mathcal{Z}^\eta$.   Recall that the code $C$ corrects up to $t'$ flips from $0$ to $1$. We now show that $C$ is up to the task with overwhelming probability, i.e., that  $\Pr_{(v, v')\leftarrow (V, V')}[v'\not\in\neigh_{t'}(v)] <\ngl(n)$.  The proof is in \apref{sec:sampling errors}.


\begin{lemma}
\label{lem:sampling errors}
Let all the variables be as in \thref{thm:sampling}.
 Then $\Pr[v'\in\neigh_{t'}(v)]\geq 1-O(2^{-\ell})$, where the probability is over the coins of $\gen$.  %That is, for any $t\leq \mu(\gamma-\eta)/\eta$, \consref{cons:sampling} is correct with overwhelming probability.
\end{lemma}

\begin{corollary}
Let all the variables be as in \thref{thm:sampling}.
When
\[
\dis(w, w')\le \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta} = \Theta(\gamma/\eta)
\]
and $(c, p)\leftarrow \gen(w)$, then
\[
\Pr[\rep(w', p) = c] \geq 1-\frac{\ell}{|\mathcal{Z}|^\eta} - O(2^{-\ell}) -\delta_{code}= 1-\ngl(n).
\]
That is, \consref{cons:sampling} is correct with overwhelming probability.
\end{corollary}

\subsection{Discussion of \consref{cons:sampling}}
\lnote{I need to check this}
We now show \consref{cons:sampling} can work for partial block sources when $\Huse\le 0$.  The required entropy of partial block source is $\alpha (\gamma-\beta ) = \Theta(\gamma)$.  We are able to correct $O(\gamma/\eta)$ errors.
This yields:
\begin{align*}
\Huse &= \Hoo(W) -\log |B_t| \\
&< \Theta(\gamma)- t \log |\mathcal{Z}|\\
&= \Theta(\gamma) - \Theta(\gamma/\eta) \log |\mathcal{Z}|
\end{align*}
That is, \consref{cons:sampling} achieves $\Huse\le 0$ when the starting alphabet is super polynomial~(noting that for super polynomial size $\mathcal{Z}$ we can set $\eta$ to be super logarithmic and $o(\log |\mathcal{Z}|)$).  We note that for polynomial-size alphabets, \consref{cons:sampling} will still work as long as we use a code that corrects Hamming errors in both directions (with a polynomial size alphabet the probability of $1\rightarrow 0$ bits flips is noticeable); however, for a polynomial-size alphabet, $\Huse>0$.
%However, in this setting $\Huse =\Omega(\gamma)$ and known information-theoretic fuzzy extractors provide superior performance.
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

%\section{Use of Fuzzy Extractors for Biometrics}
%Fuzzy extractors derive stable from noisy sources.  Biometrics represent an important noisy physical source.  Unlike physical unclonable functions, biometrics are fixed distributions.  Biometrics cannot be redesigned to  increase their entropy or reduce their error rates.  Many biometric systems have incorporated fuzzy extractors to improve system security~(see the work of Jain, Nadakumar, and Nagar for a survey~\cite{jain2008biometric}).  Unfortunately, most work measures system security using false accept rate vs. false reject rate.  This assumes an adversary that creates inputs to the \rec algorithm without looking at the public helper data.  False accept rate provides no bound on the information leaked by the helper data.  Indeed the helper data may completely reveal the original input.  In high security applications, measuring false accept rate vs. false reject rate is insufficient to argue system security.
%
%\label{sec:iris no key}
%As an example, we focus on the human iris.  The iris is believed to be the best biometric for high security applications~\cite{prabhakar2003biometric}.  The best estimate for iris entropy is $249$ bits~\cite{daugman2004}.  Daugman uses specialized wavelets to derive a $2048$ bit string called an iris code.  There has been considerable research since the work of Daugman~\cite{daugman2004}.  These works do not dramatically alter the entropy estimate or error rate necessary for a reasonable false reject rate.  We use the Daugman's parameters for our calculations.
%
%The precise number of errors that must be tolerated depends on the desired false reject rate (how often the correct key is produced).  For a false reject rate of $\le 80\%$, a $t$ of approximately $205$ is required.%\footnote{Daugman  improves his true accept rate using a masking vector, where bits that are not considered ``trustworthy'' are excluded from the comparison.  The comparison between the two readings is then made using bits not excluded by either image.  It is not clear how to extend into the fuzzy extractor setting.  Even with the masking vector $\Huse$ is negative for irises.}
%We have the following calculation for $\Huse$:
%\begin{align*}
%\Huse &= \Hoo(W) - \log |B_t|\\
%&= 249 - \log \sum_{i=0}^{205} {2048 \choose i} \approx 707.
%\end{align*}
%Thus, it is impossible to output a key using a fuzzy extractor that outputs the same strength key for all distributions with the same entropy and error tolerance~(see \apref{sec:minimal conditions} for minimal conditions for fuzzy extractor security).
%Irises are believed to be the strongest biometric but applying current fuzzy extractors to irises yields no meaningful security.
%
%\section{Minimal Conditions for Fuzzy Extractor Security}
%\label{sec:minimal conditions}
%
%A necessary condition for fuzzy extractor security is that an adversary should not be able to learn the key simply by inputting a point into the \rep algorithm.  This means a negligible portion of the source distribution $W$ lies within any Hamming ball.  We make this intuition formal here:
%
%\begin{definition}
%\label{def:fuzzy min-ent}
%A distribution $W$ in a metric space $(\mathcal{M}, \dis)$ has $(t, k)$-fuzzy min-entropy, denoted $\Hfuzz(W) \ge k$ if the following holds:
%\[
%\forall m\in \mathcal{M}, Pr_{w\in W}[\dis(w, m) \leq t] \leq 2^{-k}.
%\]
%\end{definition}
%For a metric space $\mathcal{M}$, let $\max |\mathcal{M}|$ the maximum length required to describe an element $m\in\mathcal{M}$~(for most natural metric spaces this is $\log |\mathcal{M}|$).
%\begin{lemma}
%\label{lem:fuzz necessary}
%Let $n$ be a security parameter and let $W$ be a distribution over $(\mathcal{M}, \dis)$.
%If $\Hfuzz (W) = \Theta(\log n)$ there is no $(\mathcal{M}, W, \kappa, t)$-computational fuzzy extractor that is $(\max |\mathcal{M}| +  |\rep|, \epsilon)$-hard for $\epsilon = \ngl(n)$ with error $\delta = \ngl(n)$~(and thus no fuzzy extractor) for $\kappa =\omega(\log n)$.
%\end{lemma}
%\begin{proof}
%Let $W$ be a distribution where $\Hfuzz(W) = \Theta(\log n)$.  This means that there exists a point $m\in \mathcal{M}$ such that $\Pr_{w\in W}[\dis (w, m)\leq t] \geq 1/\poly(n)$.  Consider the following distinguisher $D$:
%\begin{itemize}
%\item On input $r, p$.
%\item If $\rep(m, p) = r$, output $1$.
%\item Else output $0$.
%\end{itemize}
%First note that $|D|$ is of size $\max |\mathcal{M}|+ |\rep|$.  Clearly, $\Pr[D(R, P) = 1]\geq 1/\poly(n) - \delta$, while $\Pr[D(U_\kappa, P)=1 ]\leq 1/2^{-\kappa}$.  Thus, when $\kappa = \omega(\log n)$:
%\[
%\delta^D((R, P), (U_\kappa, P))\geq \frac{1}{\poly(n)} -\delta -  \frac{1}{2^{-\kappa}} = 1/\poly(n).
%\]
%\end{proof}
%\lemref{lem:fuzz necessary} generalizes to interactive protocols, $D$ only provides an input to the protocol and looks at the output.  This means that fuzzy min-entropy is also a necessary condition for interactive solution.  %The original fuzzy extractors paper of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08} separated the starting entropy of $W$ and the desired error tolerance.
%If we wish to support parameter regimes where $\Huse\le 0$ there is some distribution with $\Hfuzz(W)=0$~(consider some fixed point $m\in\mathcal{M}$ and let $W$ be the uniform distribution over points within distance $t$).  Thus, if the analysis of a fuzzy extractor is for all input distributions with a particular $\Huse$, no key can be output when $\Huse\le 0$.   This motivates our restriction to meaningful classes of source distributions with $\Huse\le 0$.
%%This means we inherently must talk about the security and errors together.
%
%In this work we consider the Hamming distance over some alphabet $\mathcal{Z}$.  There are two minimal types of distributions where $\Hfuzz(W)\geq \omega(\log n)$, the first is where the $(t+1)$-st least entropic block has super-logarithmic entropy~(the $t$ most entropic blocks are essentially free to the adversary).  The other is that there $t+\omega(\log n)$ blocks that contribute some entropy~(there is a continuum between these two types).  Definitions~\ref{def:block guessable} and~\ref{def:partial source} were developed for these types of sources respectively.  However, we do not achieve security when $\Hfuzz(W)= \omega(\log n)$ for either definition, this is an open problem.

\section{Fuzzy Conductors}
\lnote{I need to check this}
\label{sec:conductors}
Fuzzy extractors  have strong upper bounds on remaining entropy based on the best error correcting codes~(if they provide the same guarantee for all input distributions with the same entropy and error tolerance).  Fuller, Meng, and Reyzin show that computational information-reconciliation techniques are subject to similar bounds~\cite{fuller2013computational}.  They suggest these bounds may be avoided by outputting a fresh random variable.  This is known as a fuzzy conductor~\cite{KanukurthiR09}.
\begin{definition}
A $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-\emph{fuzzy conductor} with error $\delta$ is a pair of randomized procedures, ``generate''~(\gen') and ``reproduce''~(\rep'), with the following properties:
\begin{enumerate}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $x\in\zo^*$ and a helper string $ss\in\zo^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $ss\in\zo^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w, w')\leq t$, if $X, SS$ were generated by $(X, SS)\leftarrow \gen'(w)$, then $\Pr[\rep(w', SS) = X]\geq 1-\delta$~(over the coins of $\gen', \rep'$).  If $\dis(w, w')>t$, then no guarantee is provided about the output of $\rep'$.
\item The security property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $X$ is close to a high entropy distribution $Y$.  That is, $\exists Y$ with $\Hav(Y | SS ) \geq \tilde{m}$ such that $\Delta((X, SS), (Y, SS))\leq \epsilon$.
\end{enumerate}
\end{definition}

\noindent In this section, we show that fuzzy conductors are subject to the same lower bounds as fuzzy extractors.  This motivates our use of a computational fuzzy conductor in \defref{def:comp fuzzy cond}.
We borrow the following notation from the work of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08}:
\begin{itemize}
\item A $(\mathcal{M}, K, t)$ code is a subset of $\mathcal{M}$ of size $K$ where a procedure exists that corrects $t$ errors.
%\item $K(\mathcal{M}, t)$ is the largest $K$ for which there exists an $(\mathcal{M}, K, t)$-code.
\item $K(\mathcal{M}, t, S)$ is the largest $K$ such that there exists an $(\mathcal{M}, K, t)$ code all of whose $K$ points belong to $S$.
\item $L(\mathcal{M}, t, m) = \log (\min_{|S| = 2^m} K(\mathcal{M}, t, S))$.  This is the performance of worst error-correcting code for an arbitrary subset of size $2^m$.
\end{itemize}
Intuitively, $K(\mathcal{M}, t, S)$ is the size of the best code that covers a given subset and $L(\mathcal{M}, t, m)$ is size of the code that covers the hardest subset in the metric space.
\begin{lemma}
The existence of a $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-fuzzy conductor implies that $\tilde{m}\leq L(\mathcal{M}, t, m) +1 -\log (1-2\epsilon)$.
\end{lemma}
\begin{proof}  Assume $(\gen', \rep')$ is a fuzzy conductor with the parameters stated.  Let $A$ be a set of size $2^m$ in $\mathcal{M}$ and let $W$ be the uniform distribution over $A$.  Define $(X,SS) \leftarrow \gen'(W)$.  Then there must exist a distribution $Y$ with $\Hav(Y|SS) \geq \tilde{m}$ such that $\Delta((X,SS), (Y, SS))\leq \epsilon$.  By Markov's inequality, there exists some set $B_{SS}$ such that $\Pr[ss\in B_{SS}] \geq 1/2$ and $\forall s\in B_{SS}$ one has $\Delta(X | SS = s, s ), (Y | SS = s, s)<2\epsilon$.  Now applying Markov's inequality to $\max_{Y} \Pr[Y=y | SS=s]$, there exists a set $B_{SS'}$ such that $\Pr[SS\in B_{SS'}]>1/2$, and for all $s\in B_{SS'}$, $\Hoo(Y| SS =s ) \geq \tilde{m}-1$.  Denote by $s^*$ a value in $B_{SS}\cap B_{SS'}$~(one value must exist).  Then $\Delta((X | SS =s^* , s^*), (Y| SS = s^*, s^*))\leq 2\epsilon$ and $\Hoo(Y|SS=s^*)\geq \tilde{m}-1$.  Denote by the set $A'$ the possible values of $X$ when $SS=s^*$.  For the statistical distance property to hold, $|A'| \geq  (1-2\epsilon)2^{\tilde{m}-1}$.  Associate with every $x\in A'$ some $w\in S$ which could have produced $x$ with nonzero probability given $SS=s^*$, and call this map $C$.  $C$ defines an error correcting code with the required parameters.
\end{proof}

\section{Characterizing unguessable block distributions}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and thus a little unwieldy for a distribution.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.
The majority of the difficulty in characterizing \defref{def:block guessable} is that different blocks may be dependent, so an equality query on block $i$ may shape the distribution of block $j$.  Thus, we begin with the case of independent blocks.  In the examples that follow we denote the adversary by $S$ as we consider security against computationally unbounded adversaries defined in VGB obfuscation~(\defref{def:obf}).

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,  ... , W_\ell$ be a source in which all blocks $W_j$  are mutually independent.  Let $\alpha$ be a parameter.  Let $J\subset \{1,..., \ell\}$ be a set of indices such that for all $j\in J$, $\Hoo(W_j ) =\alpha $.  Then for any $q$, $W$ is a $(q, \alpha - \log (q+1), \ell - |J|)$-unguessable block distribution.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then $W$ is a $(q, \omega(\log n), \ell - |J|)$-unguessable block distribution.
\end{claim}
\begin{proof}
It suffices to show that for all $j\in J, \Hav(W_j |View(S^{I_{W}(\cdot, \cdot)}) = \alpha -\log (q+1)$.
We can ignore queries for all blocks but the $j$th, as the blocks are independent. Furthermore, without loss of generality, we can assume that no duplicate queries are asked, and that the adversary is deterministic (we can hardwire the best coins as nonuniform advice). Let $A_1, A_2, \dots A_q$ be the random variables representing the oracle answers for an  adversary $S$ making $q$  queries about the $i$th block. Each $A_k$ is just a bit, and at most one of them  is equal to 1 (because duplicate queries are disallowed). Thus, the total number of possible responses is $q+1$. Thus, we have the following,
\begin{align*}
\Hav(W_j | View(A^{\mathcal{O}_{W}(\cdot, \cdot)}) &= \Hav(W_j| A_1, \dots, A_q)\\
&=\Hoo(W_j) - |A_1, \dots, A_q|\\
&=\alpha - \log (q+1)\,,
\end{align*}
where the second line follows from the first by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
\end{proof}
\noindent In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} show a construction for block-fixing sources, where each block is either uniform or a fixed symbol~(block fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} shows that \defref{def:block guessable} captures, in particular, this class of distributions.
However, \defref{def:block guessable} captures more distributions.  We now consider more complicated distributions where blocks are not independent.

\begin{claim}
\label{cl:each block from single seed}
Let $f:\zo^e \rightarrow \mathcal{Z}^\ell$ be a function.  Furthermore, let $f_j$ denote the restriction of $f$'s output to its $j$th coordinate.  If for all $j$, $f_j$ is injective then $W = f(U_e)$ is a $( q, e - \log (q+1), 0)$-unguessable block distribution.
\end{claim}
\begin{proof}
Since $f$ is injective on each block, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) = \Hav(U_e | View(S^{I_{W}(\cdot, \cdot)}))$.  Consider a query $q_k$ on block $j$.  There are two possibilities: either $q_k$ is not in the image of $f_j$,  or $q_k$ can be considered a query on the preimage $f_j^{-1}(q_k)$. Then (by assuming $S$ knows $f$) we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ (like in the
proof of \clref{cl:independent high ent}),
 and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $j$, $\Hav(W_j | View(S^{I_{W}(\cdot, \cdot)})) \geq \Hoo(W_j) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so this is a family of distributions with total entropy $\omega(\log n)$ for which \consref{cons:first construction} is secure.  For these distributions, all the coordinates are as dependent as possible: one determines all others.

We can prove a slightly weaker claim when the correlation between the coordinates $W_j$ is arbitrary:

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\ell$ be a source.  Suppose that for all $j$, $\Hoo(W_j)\geq \alpha$, and that $q \le 2^{\alpha}/4$ (this holds asymptotically, in particular, if $q$ is polynomial and $\alpha$ is superlogarithmic). Then  $W$ is a $(q, \alpha-1-\log(q+1), 0)$-unguessable block distribution.
\end{claim}

\begin{proof}
Intuitively, the claim is true because the oracle is not likely to return 1 on any query. Formally, we proceed by induction on oracle queries,
using the same notation as in the proof of   \clref{cl:independent high ent}. Our inductive hypothesis is
that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.  If the inductive hypothesis holds, then, for each $j$,
\begin{equation}
\label{eq:cond-entropy}
\Hoo(W_j | A_1= \dots= A_{k-1}=0) \ge \alpha-1\,.
\end{equation}
This is true for $k=1$ by the condition of the theorem. It is true for $k>1$ because, as a consequence of the definition of $\Hoo$,
for any random variable $X$ and event $E$, $\Hoo(X|E)\ge \Hoo(X)+\log\Pr[E]$; and $(k-1) 2^{1-\alpha}\leq 2 q 2^{-\alpha} \leq 1/2$.

We now show that $\Pr[A_1\neq 0 \vee \dots \vee A_{k}\neq 0] \leq k 2^{1-\alpha}$, assuming that $\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0] \leq (k-1)2^{1-\alpha}$.
\begin{align*}
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0 \vee A_k\neq 0] & =
\Pr[A_1\neq 0 \vee \dots \vee A_{k-1}\neq 0]+\Pr[A_1=\dots = A_{k-1}=0 \wedge A_k=1]\\
& \le  (k-1)2^{1-\alpha}+\Pr[A_k=1\,|\,A_1=\dots = A_{k-1}=0]\\
& \le  (k-1)2^{1-\alpha}+\max_j 2^{-\Hoo(W_j | A_1=\dots =A_{k-1}=0)}\\
& \le  (k-1)2^{1-\alpha}+ 2^{1-\alpha}\\
& = k 2^{1-\alpha}
\end{align*}
(where the third line follows by considering that to get $A_k=1$, the adversary needs to guess some $W_j$, and the fourth line follows by~\eqref{eq:cond-entropy}).
Thus, using $k=q+1$ in~\eqref{eq:cond-entropy},
 we know $\Hoo(W_j | A_1= \dots= A_q=0) \ge \alpha-1$.  Finally this means that
\begin{align*}
\Hav(W_j | A_1,\dots, A_q) &\ge -\log \left( 2^{-\Hoo(W_j | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]+1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] \right)\\
& \ge -\log \left(  2^{-\Hoo(W_j | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\end{proof}

Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some blocks to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$, let $X = U_{\log p\times \ell}$ be a distribution and let $f_1,..., f_{\ell}$ be injective functions where $f_j:\zo^{j\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \ell \times \log p$, that is the source has a small number of blocks.}  Then define the distribution $W_1 = f_1(U_{1,...,\ell}), W_2 = f_2(U_{1,..., 2\ell}),...., W_\ell = f_\ell(U)$.  There is an adversary making $2^e\times \ell = \poly(n)$ queries such that $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
We present an adversary $S$~(running in polynomial time) that completely determines the value $x$.  $S$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $S$ queries on $y_1,..., y_p$, exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $S$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered.  The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \ell)$.  Once $x$ has been recovered then $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relied on an adversaries ability to completely determine a block from the previous blocks.  We formalize this notion next.  We defining the entropy jump of a block source as the remaining entropy when other blocks are known:

\begin{definition}
Let $W = W_1,..., W_\ell$ be a source under ordering $i_1,..., i_\ell$.  The \emph{jump} of a block $i_j$ is $\mathtt{Jump}(i_j) = \max|W_{i_j} |$ conditioned on the values $W_{i_1},..., W_{i_{j-1}}$.
\end{definition}

\noindent
We now show that there must be a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\ell$ such that for all $j\le \ell-\beta +1$, $\mathtt{Jump}(i_j) = \log q /\ell$, then $W$ is not $(q, 0, \beta)$-unguessable.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \ell$.  We describe an unbounded adversary that determines $W_1,..., W_{\ell-\beta+1}$.  As before $A$ queries the $q /\ell$ possible values for $W_1$ and determines $W_1$.  Then $A$ queries the $q/\ell$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\ell-\beta+1}$ is learned.  Note the optimum ordering for the adversary can be encoded nonuniformly but it may take an unbounded amount of time/space to sample from $W_j | W_1,.., W_{j-1}$.
\end{proof}

\section{Proof of \lemref{lem:cond and cext}}
\label{sec:cond and cext}
\lnote{I need to check this}
\begin{proof}
It suffices to show if there is some distinguisher $D'$ of size $s'$ where
\[\delta^{D'}((\cext(X; U_d), U_d, SS), (U_\kappa, U_d, SS))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D$ of size $s_{cond}$ where for all $Y$ where $\Hav(Y|SS) \geq \tilde{m}$ such that
 \[
 \delta^{D}((X, SS), (Y, SS))\geq \epsilon_{cond}.
 \]
Let $D'$ be such a distinguisher.  That is,
\[
\delta^{D'}(\cext(X, U_d)\times U_d \times SS, U_\kappa\times U_d\times SS)> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D$ as follows.  On input $(y, ss)$ sample $seed\leftarrow U_d$, compute $r\leftarrow \cext(y; seed)$ and output $D(r, seed, ss)$.  Note that $|D| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D}((X, SS), (Y, SS))&= \delta^{D'}((\cext(X, U_d), U_d, SS), \cext(Y, U_d), U_d, SS)\\
&\geq \delta^{D'}((\cext(X, U_d), U_d, SS), (U_\kappa\times U_d \times SS)) - \delta^{D'}((\cext(Y, U_d), U_d, SS), (U_\kappa\times U_d \times SS))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Where the last line follows by noting that $D'$ is of size at most $s_{ext}$.  Thus $D$ distinguishes $X$ from all $Y$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\section{Proof of \lemref{lem:security of cons}}
\label{app:security of main cons}
\begin{proof}
\lnote{I need to check this}
Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for point programs over $\mathcal{Z}$.  Let $W$ be a $(q, \alpha = \omega(\log n), \beta)$-unguessable block distribution.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P)\geq H_0(C)- \beta$. % for $\epsilon' = 2\epsilon_{obf} + (\ell - \beta)2^{-(\alpha-1)}$.
Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P) < H_0(C)-\beta$.
By \defref{def:block guessable} there exists a set of indices $J$ such that all blocks within $J$ are unguessable.  Define by $C'$ the distribution of sampling a uniform codeword where all locations outside $J$ are fixed.  Then $C | C_{J^c} \overset{d}=C'$.  By \clref{cl:many locations ent}, we have that $\Hav(C|C_{J^c} )= H_0(C) -\beta$ and thus $\Hav(C'| C_{J^c}) = H_0(C) -\beta$.
Let $D$ a distinguisher of size be some distinguisher of size at most $s_{sec}$ such that
\[
| \expe[D(C, P)] - \expe[D(C', P)] > \epsilon_{sec} = 1/\poly(n).
\]
Define the distribution $X$ as follows:
\[X_j =
\begin{cases}
W_j & C_j = 0\\
R_j & C_j = 1.
\end{cases}\]  By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(P_1,..., P_\ell, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $C$ and $C'$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})) \le (\ell-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $J^c$, the statistical distance is at most $(\ell-\beta)2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then,
\[
\Delta( S^{I_X(\cdot, \cdot)}(c^*, 1^{\ell \log |Z|}), S^{I_X(\cdot, \cdot)}(c', 1^{\ell \log |Z|})) \le ( \ell -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $j\in J$, $\Hav(W_j | View(S))\geq \alpha$.  The only information about the correct value of $c_j^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $j \in J$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not, that is suppose, the probability of at least one nonzero response on index $j$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_j$, the probability of this happening when $c^*_j = 1$ is at most $q/\mathcal{Z}$ or  equivalently $2^{-\log |\mathcal{Z}|+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }j]\nonumber \\
 &= \Pr[c_j^* =1]\Pr[\text{non zero response location }j\wedge c_j^*=1]\nonumber \\&+ \Pr[c_j^*=0] \Pr[\text{non zero response location }j \wedge c_j^*=0]\nonumber \\
&\le 1\times 2^{-\log|\mathcal{Z}|+\log q} + 1\times  \Pr[\text{non zero response location }j \wedge c_j^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that for an unguessable block source the remaining entropy $\alpha\leq \log |\mathcal{Z}|-\log q $:
\begin{claim}
\label{cl:ent bounded away from n}
If $W$ is a $(q, \alpha, \beta)$-block unguessable distribution over $\mathcal{Z}$ then $\alpha \le \log |\mathcal{Z}|-\log q$.
\end{claim}
\begin{proof}
\bnote{Leo I changed some of this under you.  It wasn't clear or probably right as written.}
\bnote{better name for guess, creativity is running low.}
Let $W$ be a $(q, \alpha, \beta)$-block unguessable distribution.  Let $J\subset\{1,..., \ell\}$ the set of good indices.
It suffices to show that there exists an $S$ making $q$ queries such that for some $j\in J, \Hav(W_j | S^{I_{W}(\cdot, \cdot)})\le \log |\mathcal{Z}| - \log q$.  Denote by $guess_{1}, ..., guess_q$ the $q$ most likely outcomes of $W_j$~(breaking ties arbitrarily).  Note that $\sum_{i=1}^q \Pr[W_j = guess_i]\geq q/|\mathcal{Z}|$.  Suppose not, this means that there is some $guess_i$ with probability $\Pr[W_j = guess_i] < 1/|\mathcal{Z}|$.  Since there are $\mathcal{Z} - q $ remaining possible values of $W_j$ for their total probability to be at least $1-q/|\mathcal{Z}|$ at least of these values has probability at least $1/\mathcal{Z}$.  This contradicts the statement $guess_1,..., guess_q$ are the most likely values.  Consider $S$ that queries its oracle on $(j, guess_1),.., (j, guess_q)$.  Denote by $Guess$ the random variable when $W_j\in \{guess_1,.., guess_q\}$  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_j | S^{J_W(\cdot, \cdot)}) &=  -\log \left(\Pr[Guess=1]\times 1+ \Pr[Guess=0]\times \max_{w}\Pr[W_j = w|\not Guess =0]\right)\\
&\leq  -\log \left(\Pr[Guess=1]\times 1\right)\\
&=-\log\left( \frac{q}{|\mathcal{Z}|} \right) = \log|\mathcal{Z}|-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }j \wedge c_j=0] &>2^{-\alpha+1} - 2^{-(\log |\mathcal{Z}|-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_j=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the block unguessability of $W$~(\defref{def:block guessable}).  By the union bound over the indices $j\in J$ the total probability of a $1$ in $J$ is at most $(\ell-\beta)2^{-\alpha+1}$. Recall that $c^*, c'$ match on all indices outside of $J$. Thus, for all $c^*, c'$ the statistical distance is at most $(\ell- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}.
\end{proof}
By averaging over all points in $C'$ we conclude that $\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})) < (\ell -\beta)2^{-(\alpha+1)}$.  This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(P_1,..., P_\ell, C') ]- \expe [S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish}, we have
\begin{align*}
\delta^{D}(( P, C), (P, C'))&\leq |\expe [D(P_1,..., P_\ell, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|})]| \\
&+|\expe[S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|})] - \expe[S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})] |\\
&+|\expe [S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})] - \expe [D(P_1,..., P_\ell, C') ]|\\
&\leq \epsilon_{sec}/3+ (\ell-\beta)2^{-(\alpha-1)}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:security of cons}.
\end{proof}

\section{Proof of \lemref{lem:sampling works}}
\label{sec:proof of sampling lemma}
\begin{proof}
Consider some fixed $i$.
Recall that there a set $J$  of size $\gamma - \beta = \Theta(\gamma)$ such that each $w$ and  block $j\in J$, $\Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}, W_{j+1}=w_{j+1},..., W_\gamma = w_\gamma) \geq \alpha$.  Since this is a worst case guarantee, the entropy of $V_i$ can be deduced from the number of symbols in $V_i$ that come from $J$. Namely, Denote by $X= |\{j_{i, 1},..., j_{i, \eta}\}\cap J|$.

\begin{claim}
\label{cl:vi have entropy}
\[
\Hoo(V_i |\Lambda = \lambda ) \geq \alpha X.
\]
\end{claim}
\begin{proof}
Denote by $j_1,..., j_\eta$ the indices selected by the randomness $\lambda_i$.  We begin by noting that $\Hoo(V_i |\Lambda = \lambda ) = -\log \max_{v\in V_i} \Pr[ V_i =v | \Lambda =\lambda] = -\log \max_{w_{j_1}, ..., w_{j_\eta}} \Pr[W_{j_1} = w_{j_1} \wedge \dots \wedge W_{j_\eta} w_{j_\eta}] $.  Then
\begin{align*}
\max_{w_{j_1},..., w_{j_\eta}} \Pr[ W_{j_1}=w_{j_1} \wedge \dots \wedge W_{j_\eta} = w_{j_\eta}]
&= \max_{w_{j_1},..., w_{j_\eta}} \prod_{k=1}^\eta \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le \prod_{k=1}^\eta \max_{w_{j_1},..., w_{j_\eta}} \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le\prod_{k=1}^\eta \max_{w_1,..., w_\gamma} \Pr[W_{j_k} = w_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1} ]\\
\end{align*}
Taking the negative logarithm of both sides we have that
\begin{align*}
\Hoo(V_i | \Lambda = \lambda) &\ge \sum_{k=1}^\eta \min_{w_1,..., w_\gamma} \Hoo(W_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1})\\
&\ge \sum_{j_k\in J} \alpha = \alpha X
\end{align*}
This completes the proof of \clref{cl:vi have entropy}.
\end{proof}


We note that $X$ is distributed according to the hypergeometric distribution,
and that $\expe[X]=\eta(\gamma-\beta)/\gamma$. Using the tail bounds from~\cite{chvatal1979tail,scala2009hypergeometric}, we can conclude that $\Pr[X\le \expe[X]/2]\le e^{-2((\gamma-\beta)/2\gamma)^2 \eta}=O(e^{-\eta})$.

\bnote{this can also be tightened.}
Thus, setting $\alpha'=\frac{\alpha \eta(\gamma-\beta)}{2\gamma}$ and applying \clref{cl:vi have entropy}, we conclude that
 \[
\Pr[\Hoo(V_i ) \geq \alpha'] \geq 1- O(e^{-\eta}).
\]
\end{proof}

\section{Proof of \lemref{lem:sampling errors}}
\label{sec:sampling errors}

\begin{proof}
Define $\mu = -\frac{1}{2}\log(1-t'/(2\ell)) = \Theta(1)$ and note that $t \leq \mu(\gamma - \eta)/\eta$. Since $\eta = \omega(\log n)$, we will assume
$\eta\ge 2\mu$.
Let
the Bernoulli random variable $X_i=1$ if and only if $v_i \neq v_i'$, and $X=\sum_{i=1}^{\ell} X_i$. We need to show that $\Pr[X>t']=O(2^{-\ell})$.

\begin{align*}
\expe [1-X_i] &=\Pr[w \text{ and } w' \text{ agree on positions } {j_{i,1}},..., j_{i,\eta}]\\
&\geq \prod_{j=0}^{\eta-1}\left(1 - \frac{t}{\gamma-j}\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu(\gamma-\eta)/\eta}{\gamma-j}\right)\\
&\geq  \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu}{\eta}\left(\frac{\gamma-\eta}{\gamma-j}\right)\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu}{\eta}\right) \\
&= \left(1-\frac{\mu}{\eta}\right)^{\eta} =\left( \left(1-\frac{\mu}{\eta}\right)^{\eta/\mu}\right)^\mu\geq \left(\left(\frac{1}{2}\right)^2\right)^\mu\\
& = \left(\frac{1}{2}\right)^{-\log\left(1-\frac{t'}{2\ell}\right)} = 1-\frac{t'}{2\ell}\,.
\end{align*}

Hence, $\expe[X_i]\le t'/(2\ell) = O(1)$, and $\expe[X]\le t'/2$.
By the Chernoff bound, we have \lnote{this can be tightened}
\begin{align*}
\Pr\left[\sum_{i=1}^\ell X_i\geq t'\right]\leq 2e^{-2(t'-\expe[X])^2\ell} \le
2e^{-2(t'/2)^2\ell} = O(e^{- \ell})\,.
\end{align*}
\end{proof}




\end{document} 