\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode} 

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}

\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}

\title{Constant Error Tolerance Computational Fuzzy Extractors}
\author{Ran Canetti \and Benjamin Fuller\footnote{The Lincoln Laboratory portion of this
work was sponsored by the Department of the Air Force under Air Force
Contract
\#FA8721-05-C-0002.  Opinions,
interpretations, conclusions and recommendations are those of the author
and
are not necessarily endorsed by the United States Government.} \and Omer Paneth \and Leonid Reyzin}

\begin{document}
\maketitle


\begin{abstract} 
Fuzzy extractors derive strong keys from noisy sources.  The goal is to reliably convert a high entropy source~(which may have different values on repeated readings) to the same uniformly key distributed key.  Traditionally, their security is defined information-theoretically.  Fuzzy extractors have upper bounds on the length of the derived key~(Dodis et al., J. of Comp 2008).  In particular, the starting entropy of the source must be significantly higher than the logarithm of number of error patterns corrected.  For many practical noisy sources, like biometrics, this condition is not fulfilled, leaving reliable key derivation from these sources as open problem.  

Fuller, Meng, and Reyzin (Asiacrypt 2013) hypothesize that these bounds can be overcome by only providing security against computationally-bounded adversaries.  %They provide a construction of a computational fuzzy extractor that corrects a small number of errors without any loss in key length.  
%They show the feasibility of constructing a computational fuzzy extractor that does not suffer from the information-theoretic bounds.
%Unfortunately, their construction is limited to very high entropy distributions and does not significantly beat information-theoretic bounds.    In addition, we significantly improve on the allowable error rate and the required entropy rate of each block.  In particular we construct the following:
In this work, we provide the first construction of a fuzzy extractor for a large class of distributions where the starting entropy is smaller than the logarithm of the number of correctable error patterns.

\textbf{Construction:} We construct a computational fuzzy extractor based on obfuscation of point functions.  %Let $W = W_1, ..., W_\ell$ be a source distribution composed of blocks over a large alphabet, $Z$. To obfuscate a sample $w$ drawn from $W$.  We flip a coin $c_i$ for each block and either obfuscate the block $w_i$ or a random point in $Z$.  When a close reading $w'$ is given as input, we can recover most of the coins $c_i$.  Error-tolerance is achieved by sampling $c = c_1,..., c_\ell$ from an error-correcting code.  
Our construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~(Eurocrypt 2008).  
%Security holds as long as a large number of blocks $W_i$ of $W$  are unpredictable given equality queries on individual blocks.  
Our construction provides the following novel features:
\begin{itemize}
\item The source entropy is smaller than the log of the number of correctable error patterns.  
\item Constant error tolerance rate.  %Previous constructions of computational fuzzy extractors corrected a logarithmic fraction of errors.  (Information-theoretic constructions support a constant fraction of errors but are subject to bounds described above.)
\item Security for a large and meaningful class of distributions.  %The main requirement for security is that most blocks have a super-logarithmic amount of entropy.  Previous computational fuzzy extractors required the source to be essentially uniform. 
\end{itemize}
\end{abstract}


\section{Introduction}\label{sec:introduction}
Reliable key derivation is a cornerstone of authentication.  However, many sources with sufficient entropy for key derivation are noisy and provide similar but not identical values when the source is read multiple times~(examples include biometrics~\cite{daugman2004} and physically unclonable functions~\cite{pappu2002physical}).  To use noisy physical sources in applications, two problems must be addressed.  The first is ensuring the same value is obtained from every reading of the source.  This must be done in a way that does not eliminate all the entropy from the output~(known as information-reconciliation~\cite{bennett1988privacy}).  The second is converting a source with entropy to a uniform random key~(privacy amplification~\cite{bennett1988privacy}).  Both of these problems have interactive and non-interactive versions.  For a single user~(trying to produce the same key from multiple readings of a physical source) non-interactive solutions are appropriate.  We focus on this setting.  %If both of these tasks are achieved a noisy physical source can be used like a uniformly random private key. 

Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} perform both tasks non-interactively.  They consist of a pair of algorithms: \gen takes a source value $w$, and produces a key $r$ and a public value $p$.  The second algorithm \rep takes this public value $p$ and a close $w'$ to reproduce the original key $r$.  Traditionally, a secure sketch~\cite{DBLP:journals/siamcomp/DodisORS08} performs information-reconciliation and a randomness extractor~\cite{nisan1993randomness} performs privacy amplification.

Unfortunately, fuzzy extractors have not yielded key derivation for all noisy sources.  
This is because there is tension between security and error-tolerance.  The key should be uniform to an observer but nearby $w'$ should map to the same key.  By nearby, we mean that $\dis(w, w')\leq t$ for some metric $\dis$.  In the information-theoretic realm, increasing $t$ means a decrease in the strength of the key $r$.  This is because a fuzzy extractor can be converted into an error correcting code~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08}.  The length of $r$ is dictated by size of the best code that corrects $t$ errors.  Error-correcting codes are well studied and we have tight bounds on this quantity. % how many codewords may be in a code that corrects $t$ errors.  

We consider the Hamming metric.  For the Hamming metric, the key may be no longer than the difference between the starting entropy of the source\footnote{We assume that the fuzzy extractor operates independently of the source $W$.  As we discuss later, it is possible to avoid these losses if the fuzzy extractor has additional knowledge about the distribution.} and logarithm of the number of error patterns to be corrected. For the remainder of this paper we denote this quantity by $gap$.  This represents a real problem as important biometrics~(such as irises~\cite{daugman2004}) have $gap <0$ and current fuzzy extractors provide no security guarantee.
 
 %let $|B_t|$ be the number of points within distance $t$.  The length of the key $|r| < \Hoo(W) - \log |B_t|$~(see~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08} for the precise conditions).  For the remainder of this paper, we denote this quantity by $gap = \Hoo(W) - \log |B_t|$.

However, this does not need to be the case in the computational setting.  Fuller, Meng, and Reyzin define computational fuzzy extractors~\cite{fuller2013computational}.  Computational fuzzy extractors produce pseudorandom instead of truly random keys.  They hypothesize a computational fuzzy extractor may exist with a small or negative $gap$.  We resolve this question and construct a computational fuzzy extractor when $gap<0$.  This is impossible for an arbitrary min-entropy distribution as every point could be within distance $t$.  A necessary condition for security is that a negligible portion of the probability distribution lies within any Hamming ball of radius $t$.  We require a stronger condition that many blocks are unpredictable~(\defref{def:block guessable}).  

The goal of an information-theoretic fuzzy extractor is to output a maximal length key for a source $W$.  
In the computational setting, a key may be expanded once it is long enough to serve as input to a pseudorandom generator.  Our focus is expanding the class of distributions for which we can provide meaningful security, and remember keys can be expanded once they are long enough.

\textbf{Overview of Construction:}  Fuller, Meng, and Reyzin~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational} show that replacing an information-theoretic information-reconciliation component with a computational component is unlikely to be fruitful.  Instead, the authors suggest two alternatives: combine the information-reconciliation and privacy amplification components~(their approach) or instead of recovering $w$ produce a new consistent high-entropy secret.  This is known as a fuzzy conductor~(the entropy is being ``conducted'' from $w$ to a new distribution) and was introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}.  Here, we follow the second approach and use a \emph{computational} fuzzy conductor.

Our construction~(\consref{cons:first construction}) is based on obfuscation of point programs.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$.  Obfuscation for this class of functions is achievable under particular number-theoretic assumptions~\cite{canetti1997towards} and from generic cryptographic hardness assumptions~\cite{wee2005obfuscating}.  

Consider a source $W = W_1,..., W_\ell$ that is split into blocks~(over a large alphabet).  We would like to tolerate errors in $t$ individual blocks.  For each block $i$, we flip a coin $c_i$ and either obfuscate $I_{w_i}$ or pick a random point $r_i$ and obfuscate $I_{r_i}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rec$ then runs the obfuscated program $P_i$ on $w_i'$ and checks whether $P_i(w_i')=1$.  For most locations $i$, \rep can determine whether $w_i$ or a random value was the point obfuscated.  Thus, most bits of $c_i$ are recoverable. To tolerate errors we choose the coins $c$ from an error-correcting code.  Then, $c$ forms an distribution of high entropy.  This construction conducts entropy from $w$ to $c$ and is a computational fuzzy conductor.  It may be converted to a computational fuzzy extractor by either information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors.  Note, it is hard to recover $w$ from the value $c$.   This is necessary to overcome the information-theoretic lower bounds~\cite[Section 3.3]{fuller2013computational}.  
Our construction is inspired by Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  

Our construction provides several advantages over previous constructions of computational fuzzy extractors:
\begin{itemize}
\item The first computational fuzzy extractor where $gap$ may be negative for a large class of distributions.\footnote{There are examples where no loss is necessary in a fuzzy extractor.  If $W$ consists of points from an error-correcting code of sufficient minimum distance, then no public information is necessary and $gap$ may be negative.  This is not in the spirit of a fuzzy extractor as the distribution is already ``error-corrected.''  For arbitrary min-entropy sources, analysis of information-theoretic fuzzy extractors proceeds by arguing an adversary learns at most a bit for each bit necessary for error correction.   For an arbitrary distribution, this is at least the logarithm of the total number of error patterns.  The distributions supported by our construction are not error-correcting codes, we only require they are unpredictable in many dimensions~(see \defref{def:block guessable}).}
\item The first computational fuzzy extractor that allows for a constant fraction of errors.  Previous constructions could correct only  a logarithmic fraction of errors~($O(\frac{\log \ell}{\ell})$)
\item The first computational fuzzy extractor where blocks are not required to be uniformly distributed.  Our construction simply requires a large number of blocks that are unguessable given equality queries.  This subsumes the distributions supported by~\cite[Construction 4.1]{fuller2013computational}.
\end{itemize}

\textbf{Limitations: } The main limitation of our construction is that each block is individually obfuscated.  This means that blocks much be drawn from distributions with super-logarithmic entropy.  Furthermore, our distance is Hamming distance over a large alphabet~(super-polynomial in the security parameter).  Our construction is applicable for physical sources where many bits are expected to err consecutively, followed by many consecutive correct bits~(known as burst errors~\cite{gilbert1960capacity}).  Constructing a computational fuzzy extractor with the above advantages and a small alphabet is an open problem.


\textbf{Connection with Noisy Point Obfuscation: } Ideally, our construction would be a noisy point obfuscation~(which is a stronger object than a computational fuzzy extractor).  Unfortunately, this is not the case.  Each block is individually obfuscated, so an adversary may ask equality queries on individual blocks.  If not all blocks have high entropy an adversary may learn significant information about the source.  Thus, our construction is not a virtual black box~(VBB) obfuscation~(where anything learnable by the circuit is simulatable using input/output behavior).  Dodis and Smith~\cite{DBLP:conf/stoc/DodisS05} construct noisy point obfuscation for $gap>>0$ using a information-reconciliation component that hides partial information.  Constructing VBB obfuscation for sources where $gap <0$ is an open problem.

The remainder is organized as follows: we cover notation and background on obfuscation and error correcting codes in \secref{sec:preliminaries} , describe computational fuzzy extractors in \secref{sec:fuzzy extractors}, present our construction in \secref{sec:construction} and discuss parameters and tradeoffs in \secref{sec:discussion}.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote $X = X_1,..., X_\ell$ as the concatenation of $X_1$ to $X_\ell$.  For a set of indices $\mathcal{I}$,  $X_{\mathcal{I}}$ is the restriction of $X$ to the indices in $\mathcal{I}$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.  
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. 
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  For our sources, we consider the Hamming metric, for two vectors $x, y$ over $\mathcal{Z}^n$ the Hamming distance between $x,y$ is $d(x,y) = \{i | x_i \neq y_i\}$.  For the Hamming metric, $|B_t| = \sum_{i=0}^t {n \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
%\bnote{this is weird here and doesn't fit}  However, we will use codes that only correct one-sided errors~(see \secref{sec:coding theory}).


Usually, we use bold letters for vectors or matrices, capitalized letters for random variables, and lowercase letters for elements in a vector or samples from a random variable. 

\subsection{Coding Theory}
\label{sec:coding theory}
We introduce some notions from the field of binary error correcting codes.  Usually the standard class of errors  is all points within Hamming distance $t$, we will use the Hamming analog of the $Z$-channel~\cite{tallini2002capacity} where there are flips from $0\rightarrow 1$ but no bit flips from $1\rightarrow 0$.  
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$. 
\end{definition}
\textbf{Notes:} Any code that corrects $t$ Hamming errors corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
The class of errors we support~($t$ Hamming errors over a large alphabet) in our source and the class of errors corrected by our code~($t$ $0\rightarrow 1$ errors) are different~(see \consref{cons:first construction} for details).

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \epsilon_{code})$-code if there exists an efficient procedure $\rec$ such that for all $c\in C, \forall c'\in \neigh(c), \Pr[\rec(c') \neq c] \leq \epsilon_{code}$.
\end{definition}

We note that for any code learning a few bits does not inform on the remainder of the bits.  

\begin{claim} 
\label{cl:many locations ent}
Let $C$ be a binary code and let $\mathcal{I}$ be a set of indices of $C$.  Then $\Hav(C | C_\mathcal{I}) = |C| - |\mathcal{I}|$.
\end{claim}
This claim is a direct result of \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}

%need one special property of the code, namely that each output bit is set to $1$ with probability $1/2$.

%\bnote{I don't think we need this!}
%\begin{definition}
%A binary $(\neigh_t, \epsilon_{code})$ code is output equiprobable\bnote{Find the right name for this} $\forall i, |\{C | C_i =1\}| =|C|/2$.
%\end{definition}
%All binary linear codes~(without constant bits) are output equiprobable and there exist constant rate output equiprobable codes that correct errors for $t = O(\ell)$~(first constructed by Justesen~\cite{justesen1972class}).  This is true as long as $C$ contains no constant bits.  Usually, if a code has constant bits, we can truncate those bits before encoding and obtain a code with better parameters.  For the remainder of this work when we say a code, we assume there are no constant bits.
%
%\begin{lemma}
%\label{lem:linear codes independent}
%Let $C$ be a \emph{linear} binary code with no constant bits.  Then $C$ is output equiprobable.
%\end{lemma}
%Proof in \apref{sec:proof of linear indep}.
%
%

\subsection{Obfuscation}
Our construction uses virtual black-box obfuscation of point functions.  This is the following family of functions:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
Here we present the virtual black-box definition of obfuscation.  See \secref{sec:obfuscation} for a more detailed introduction to obfuscation.  This definition is known to be achievable for point functions under various assumptions~\cite{canetti1997towards, wee2005obfuscating}.  We also need the obfuscation to be $\ell$-composable~(\defref{def:obf comp}).

We present the notion of virtual black-box obfuscation with the weakening that the quality of the simulator is an arbitrarily small $1/\poly$ as all known point obfuscations satisfy this definition.

\begin{definition}~\cite{barak2001possibility, goldwasser2005impossibility}  
\label{def:obf} Let $\mathcal{C}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is an obfuscator for $\mathcal{C}$ with dependent auxiliary input if the following conditions are met:
\begin{enumerate}
\item \emph{Approximate Functionality:}  There exists a negligible function $\epsilon$ such that for every $n\in \mathbb{N}$, every circuit $C\in \mathcal{C}_n$, and every $x\in\zo^n$, 
\[
\Pr[\mathcal{O}(C; r)(x) = C(x)] > 1-\epsilon(n),
\]
where the probability is taken over the randomness $r$.  \emph{Almost exact functionality} is a stronger condition that requires $\mathcal{O}(C;r)\equiv C$ with overwhelming probability over the random coin tosses $r$.  Finally, if this probability always equals $1$, then $\mathcal{O}$ has \emph{exact functionality}.
\item \emph{Polynomial Slowdown:}  There exists a polynomial $\psi$ such that for every $n$, every circuit $C\in \mathcal{C}_n$, and every possible $r$, the circuit $\mathcal{O}(C; r)$ run-in time at most $\psi(n)$.
\item \emph{Virtual Black-box:}  For every PPT adversary $A$ and polynomial $\rho$, there exists a PPT simulator $S$ such that for all sufficiently large $n$, for all $C\in \mathcal{C}_n$, for all auxiliary inputs $z\in \zo^*$, 
and for all binary predicates $\pi$, 
\[
|\Pr[A(\mathcal{O}(C), z) = \pi(C, z)] - \Pr[S^C(1^n, z) = \pi(C, z)] | < \frac{1}{\rho(n)}
\]
where the first probability is taken over the coin tosses of $A$ and $\mathcal{O}$, and the second probability is taken over the coin tosses of $S$.  Furthermore, the runtime of $A$ and $S$ must be polynomial in the length of their first input.
\end{enumerate}
\end{definition}


\section{Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

We focus on computational fuzzy extractors introduced by Fuller, Meng, and Reyzin~\cite{fuller2013computational}.  Definitions for information-theoretic fuzzy extractors and a non-interactive information-reconciliation component, known as a secure sketch, can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.  
%\begin{definition}
%\label{def:fuzzy extractor}
%An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$, with the following properties: 
%\begin{enumerate}
%\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\ell$ and a helper string $p\in\{0,1\}^*$.
%\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w,w')\leq t$, if $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$ with probability~(over the coins of $\gen, \rep$) at least $1-\delta$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
%\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
%\end{enumerate}
%A fuzzy extractor is efficient if $\gen$ and $\rep$ run in expected polynomial time.
%\end{definition}
%
%Secure sketches are the main technical tool in the construction of fuzzy extractors.  Secure sketches produce a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
%\begin{definition}
%\label{def:secure sketch}
%An $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$, with the following properties:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|\sketch(W))\geq \tilde{m}$.
%\end{enumerate}
%A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time. 
%\end{definition}
%

%A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}. 
%
%\begin{lemma}
%\label{lem:fuzzy ext construction}
%Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext:\mathcal{M}\times \zo^d \rightarrow \zo^\ell$ be a $(\tilde{m}, \epsilon)$-average-case extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor with error $\delta$:
%\begin{itemize}
%\item $\gen(w):$ generate $x\leftarrow \zo^d$, set $p=(\sketch(w), x), r=\ext(w;x)$, and output $(r,p)$.
%\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $r=\ext(w;x)$.
%\end{itemize}
%\end{lemma}

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$ is known: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness.
The definition of computational fuzzy extractors specifies a family of sources for which the fuzzy extractor is designed to work rather than the family of all sources of a given min-entropy $m$.  With $gap<0$ there are distributions contained in a single Hamming ball, so restricting the family of sources is necessary.

The work of Fuller, Meng, and Reyzin~\cite{fuller2013computational} presents two approaches for constructing a computational fuzzy extractor: analyzing the information-reconciliation and privacy amplifications components together or using a fuzzy conductor and a privacy amplification component.  We follow the second approach.  

In \apref{sec:conductors}, we show that fuzzy conductors are subject to the same lower bounds on entropy loss as fuzzy extractors.  Instead of using a fuzzy conductor, we define a computational version.  
We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has 
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition}

We now define the two primitives for this approach: a computational fuzzy conductor and a (computational)~randomness extractor.  A computational fuzzy conductor is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\bnote{depending on what we get from obfuscation may need this to be relaxed HILL entropy.  Would have to redo lemma below, but it is essentially the same.}
\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' ($\gen'$) and ``reproduce'' ($\rep'$) is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon, s_{sec})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $Y\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(Y, P)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', P) = Y] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $Y$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon, s_{sec}}(Y |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}

A computational extractor is the natural adaption of a randomness extractor to the computational setting.  We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average-case:
%start with the standard notion of an average-case extractor. An average-case extractor is a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}) (in particular, Vadhan~\cite[Problem 6.8]{Vad12} showed that all strong extractors are average-case extractors with a slight loss of parameters):
\begin{definition}
Let $\chi$ be a finite set.
A function $\cext: \zo^\ell \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, s_{sec}, \epsilon)$-average-case computational extractor} if for all pairs
of random variables $X, Y$ over $\zo^\ell, \chi$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{sec}}}((\cext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon$.
\end{definition}

Combining a computational fuzzy conductor and an appropriate randomness extractor yields a computational fuzzy extractor~(proof in \secref{sec:cond and cext}):

\begin{lemma}
\label{lem:cond and cext}
Let $\gen'$, $\rep'$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\ell$.  Let $\cext:\zo^\ell\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, s_{ext}, \epsilon_{ext})$-average case computational extractor.  Define $\gen, \rep$ as:
\begin{itemize}
\item $\gen(w; r, x):$ run $(y, p')= \gen'(w; r)$ and set $r = \cext(y; x)$ and $p = (p', x)$.  Output $(r, p)$.
\item $\rep(w, (p', x)):$ recover $y = \rec'(w'; p')$ and output $r = \cext(y; x)$. 
\end{itemize}
Then $\gen, \rep$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \max\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}


\section{A Constant Rate Computational Fuzzy Extractor}
\label{sec:construction}

Before describing our construction, we recall the problem we are trying address.  The goal is to derive strong keys from noisy sources.  As discussed in the introduction, our goal is to provide security when $gap = \Hoo(W) - \log|B_t|$ is small or even negative.
In the computational setting once a ``long enough'' key may be expanded using a pseudorandom generator.  Thus, we focus on providing enough security when $gap<0$.  We build a computational fuzzy conductor, noting that it can be converted to a computational fuzzy extractor using \lemref{lem:cond and cext}.%If $gap> \omega(\log n)$~(ignoring losses due to randomness extraction), then a key of any length can be formed by using an information-theoretic fuzzy extractor~(which yields nearly this many bits using an optimal code) and expanding the output with a pseudorandom generator~(or using a secure sketch with a computational extractor).  Using an information-theoretic analysis of a fuzzy extractor this seems to be the best construction possible.  As described in Fuller, Meng, and Reyzin~\cite{fuller2013computational}, use of a computational fuzzy extractor may allow a construction when $gap = O(\log n)$.  Their construction is only applicable for high entropy input and yield a result when $gap$ is small.  We will provide the first construction where $gap$ is negative.  

\begin{construction}
\label{cons:first construction}
Let $n$ be a security parameter and let $W = W_1,..., W_\ell$ be a distribution over $\zo^{\ell n}$.  Let $\mathcal{O}$ be a $\ell$-composable obfuscator for the family of point functions over $\zo^n$.  Let $t$ be the desired error-tolerance and let $C\subset \zo^\ell$ %be an output equiprobable 
$(\neigh_t, \epsilon_{code})$-error-correcting code.  
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C, seed\leftarrow \zo^*$.
\item For $i=1,..., \ell$:
\subitem If $c_i = 0$: $p_i = \mathcal{O}(I_{w_i})$.
\subitem If $c_i = 1$: Sample $r_i \leftarrow U_n$. 
\subsubitem Let $p_i = \mathcal{O}(I_{r_i})$.
\item Output $(c, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$ 
\item For $i=1,..., \ell$:
\subitem If $p_i(w_i') = 1$ set $c_i' = 0$.
\subitem Else set $c_i' = 1$.
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage} 
\end{tabular}
\end{center}
\label{cons:informal construction}
\end{construction}

\textbf{Notes:}  We make several observations about the above construction:
\begin{itemize}
\item The input $w$ is hidden in two different ways.  In locations where $c_i=1$ the block $w_i$ is information-theoretically unknown.
In locations where $c_i=0$ it is hard to find $w_i$ given the point obfuscation.  As described in~\cite[Section 3.3]{fuller2013computational} some type of lossiness is necessary to avoid coding bounds.
\item There are two possible reasons for a bit $c_i'$ to be $1$.  Because the true value was $1$~(there is little chance of $1$ being incorrectly decoded as $0$ assuming $w_i'$ is independent of the sketch) and because $w_i \neq w_i'$.  However if a bit $c_i'$ is $0$ this likely means that $w_i=w_i'$ because collisions when the $c_i=0$ are unlikely~(occurring with probability roughly $2^{-n}$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips, instead of a code that corrects all Hamming weight $t$ errors.
\end{itemize}

\subsection{Security of Construction}
\label{sec:sec of construction}
Assuming virtual black-box obfuscation, we argue security in the presence of equality oracles for each block.  The adversary is provided with equality oracles for each block individually.  Security rests on an adversary's inability to tell whether they are working with obfuscations of $w_i$ or random points using \emph{adaptive} equality queries.  For illustrative purposes, we begin with the setting where blocks are independent and each have super-logarithmic entropy and then move to the setting of correlated blocks with some entropy deficient blocks.  Our proof for independent and high entropy blocks is more complicated than necessary, but the proof structure holds for the family of sources described in \secref{sec:supported sources}.  The outline of the proof is to:

\begin{itemize}
\item Show that conditioned on the adversary's view there is a large set of blocks, $\mathcal{I}$, that have a super logarithmic entropy~(\lemref{lem:blocks unguessable after queries}).
\item Show that conditioned on the values of all other indices, $C$ still has many possible values.  That is, $\Hav(C | C_{\mathcal{I}^c})$ is large~(\clref{cl:many locations ent}).
\item Assume that the adversary has perfect knowledge of $C_{\mathcal{I}^c}$.
\item For all codewords $c, c' \in C| C_{\mathcal{I}^c}$, the codewords $c, c'$ are indistinguishable given the adversary's view~(\lemref{lem:codewords in I close}).
%\item The distribution $ C| C_{\mathcal{I}^c} \wedge View(A)$ is close to $C| C_{\mathcal{I}^c}$.  
%\item For all indices in $\mathcal{I}$, the adversary's guess for $c_i, i\in \mathcal{I}$ is close to the prior distribution $C_i | C_{\mathcal{I}^c}$.  
%\item Without seeing a $1$ on a particular block, it is impossible to the distinguish between $c_i=1$ and $c_i=0$.  
%tell if a $1$ or $0$ On all blocks where the probability of a query having returned a $1$ is negligible, it is impossible to tell if you presented with an obfuscation of $W_i$ or a uniform random variable.
% $c_i, i\not \in \mathcal{I}$ there are many possible values of $c\in C$~(\clref{cl:many locations ent}).
%\item Any two codewords, $c_1, c_2\in C | C_{\mathcal{I}^c}$ are statistically close given the adversary's view.
\item The set of codewords $C| View(A) \wedge C_{\mathcal{I}^c}$ has conditional min-entropy~(and thus $C|View(A)$ has conditional min-entropy)~(\corref{cor:avg min after view}).  
\bnote{I think this is wrong as currently written.  We transition from indistinguishability of all pairs to high entropy for the distribution.  This should be true but I think the argument is wrong.}
\item By the security of obfuscation for every adversary $A$, there exists a simulator $S$ such that $C | A^{P_1,..., P_\ell}$ is computationally indistinguishable from $C | S^{\mathcal{O}}$%~(\lemref{lem:obf has hill ent}).
\bnote{I am having a little trouble formalizing this.  Was hoping for HILL entropy but I don't think that's true.  Will we get relaxed HILL entropy?}
\end{itemize}


\begin{lemma}
\label{lem:blocks unguessable after queries}
Let $n$ be a security parameter.  Let $W(n) = W = W_1,..., W_\ell$ be a distribution where each $W_i$ is independent of all $W_j$ and $\forall i, \Hoo(W_i) = \omega(\log n)$.  Then let $\mathcal{A}$ be an adversary making a polynomial number of oracle equality queries for blocks $i$.  %Let $C = c_1,.., c_\ell$ be as in \consref{cons:informal construction}. 
Then $\forall i, \Hav(W_i | View(A))  = \omega(\log n)$.  Furthermore, since $W_i$ are independent $\Hav(W | View(A))= \omega(\ell \log n)$.
\end{lemma}
\begin{proof}
Consider a particular block $i$, we measure how an adversary's queries affect the entropy of this block~(we can ignore the other blocks as they are independent).  
Let $q_w$ be a query asking if the stored value is $a_w$ and its corresponding response.  
Let $Q_{w_1},A_{w_1},..., Q_{w_q}, A_{w_q}$ be the random variables representing the queries and answers for an  adversary $\mathcal{A}$ making $q$ queries.  We assume a deterministic adversary (the adversary is unbounded and thus can compute the best possible queries).  The only dependence between $W_i$ and this view of the adversary is in $A_1,..., A_q$ so we can consider these binary responses.  (There is dependency between $W_i$ and the queries but it is all contained in $A_{w_1},..., A_{w_q}$.)  Our goal is to count the total number of possible responses $A_{w_1},..., A_{w_q}$.  There are two basic cases for $A_{w_1},..., A_{w_q}$: the all zeros string and the case where some query returns $1$.  If some query $Q_{w*}$ returns $1$ then all other $Q_{w_i} = Q_{w*}$ will return $1$ but no other query returns $1$.  These responses can be removed as the response is deterministic.  Let $A'$ represent the sequence with all duplicate queries removed.  This sequence has at most a single $1$.  Thus, the total number of possible responses is $q+1$.  Thus, we have the following,
\begin{align*}
\Hav(W_i | View(A)) &= \Hav(W_i| Q_{w_1}, A_{w_1},..., Q_{w_q}, A_{w_q})\\
&=\Hav(W_i | A_{w_1},..., A_{w_q})\\
&=\Hav(W_i |A') \\
&=\Hoo(W_i) - \log |A'|\\
&= \omega(\log n) - \log |A'|\\
&= \omega(\log n) - \log (q+1) = \omega(\log n) - O(\log(n)) = \omega(\log n)
\end{align*}
Where the fourth line follows from the third by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
This argument holds for all $i$.  This completes the proof.
%There are two possible outcomes to an equality query $1$ in which case $\Hoo(W|Q = q_w) = 0$ and $0$ in which case $\Hoo(W| Q =q_w ) \geq -\log 2^{-\Hoo(W)}(1 - 2^{-\Hoo(W)})$~(every remaining outcome is scaled by the $w$ not being a possible outcome).  Thus,
%\begin{align*}
%\Hav(W | Q_w) &= -\log \left(\Pr[W=w]2^{-0} + \Pr[W\neq w] 2^{-\Hoo(W)}(1 - 2^{-\Hoo(W)})\right)\\
%&\geq -\log \left(2^{-k} + (1-2^{-k})2^{-k}(1-2^{-k})\right)\\
%&\geq -\log \left(2^{-(k)}(1+(1-2^{-k})^2)\right)\geq k-1
%\end{align*}
%Now consider the distribution $W' = W | W\neq w$, by the same argument asking a single query of $W'$ reduces it min-entropy by at most one bit~(we need only consider the case where previous responses were all zeros as the entropy is already $0$ if a previous response is $1$).  Thus, for some polynomial number of queries $n_q$ we have that $\Hav(W | Q_1,..., Q_{n_q}) = k - n_q$.  Thus for $k = \omega(\log n)$ and $n_q = \poly(n)$\bnote{this is too big a loss} %we have that $k-n_q = \omega(\log n)$.  This completes the proof of the lemma. 
\end{proof}
Let $\mathcal{I}$ be the set of all indices $\{1,..., n\}$.  Then $\Hav(C | C_{\mathcal{I}^c}) = \Hoo(C) = \ell$.  We know show that all codewords different in only positions of $\mathcal{I}$ are indistinguishable.
\begin{lemma}
\label{lem:codewords in I close}
Let $c_1, c_2$ be two codewords that agree on all indices $\mathcal{I}^c$.  Then, 
\[
\Delta( ((View(A), C_{\mathcal{I}^c} )| C = c_1),( (View(A), C_{\mathcal{I}^c})  | C= c_2)) < \ngl(n).
\]
\end{lemma}
\begin{proof}
Recall by assumption that for all $i\in \mathcal{I}$, $\Hav(W_i | View(A))\geq \omega(\log n)$.  The only information about $C_i$ is contained in the query responses.  Thus, it suffices to show that whether $c_i= 1$ or $c_i=0$, with overwhelming probability all responses to $c_i$ are $0$.  Suppose not, that is suppose, the probability of a nonzero response on an $i$ query is $1/\poly(n)$.  Since $w, w'$ are independent of $p$, the probability of this happening when $c_i = 1$ is negligible~(occurs with probability at most $q/2^n$).  Thus, it must occur with $1/\poly(n)$ probability when $c_i=0$.  When there is a $1$ response and $c_i=0$ this means that there is no remaining min-entropy.  If this occurs with $1/\poly(n)$ probability this violates the super-logarithmic conditional min-entropy condition.  Thus, for all $c_1, c_2\in C| C_{\mathcal{I}^c}$ all distinguishers are negligibly close.
\end{proof}

%\begin{lemma}
%\bnote{what goes here?}
%Let $\mathcal{I}$ be a set of indices such that for all $i\in \mathcal{I}$, $\Hav(W_i | View(A))$
%\end{lemma}
%\begin{lemma}
%\label{lem:super log indist by equal queries}
%Let $W$ and $U$ be two distributions of super-logarithmic entropy.  Then for all adversaries $A$, $q = poly(n)$ queries, $\Delta(View(A^{\mathcal{O}_W(\cdot)}), View(A^{\mathcal{O}_U(\cdot)}))\leq \ngl(n)$.
%\end{lemma}
%\begin{proof}
%It suffices to show that with the probability of all responses being $0$ is overwhelming.  Clearly, in the case when all responses are $0$ the views are identical, and thus the statistical distance is $0$.  \bnote{Finish this proof}
%\end{proof}

%\begin{lemma}
%\label{lem:code bits statistically indist}
%Let $X_i = W_i$ if $c_i = 1$, otherwise let $X_i = U$.  Then, for every $i$, $\Delta(View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)})| c_i = 1, View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)} | c_i =0) \leq \ngl(n)$.
%\end{lemma}
%\begin{proof}
%\bnote{Do this}
%\end{proof}
%We can extend this lemma to the entire codeword $C$:
%\begin{lemma}
%For all $c_1, c_2 \in C$ we have the following: 
%\[
%\Delta(View(A^{\mathcal{O}_{(X_{c_{1_1}},..., X_{c_{2_\ell}})}(\cdot, \cdot)})| C = c_1, View(A^{\mathcal{O}_{(X_{c_{2_1}},..., X_{c_{2_\ell}})}(\cdot, \cdot)} | C = c_2)) \leq \ngl(n).
%\]
%\end{lemma}
The prior lemma implies that the $View$ does not increase the probability of any codeword by much.
\bnote{As stated above I don't think this proof is right?}
\begin{lemma}
Let $c\in C|C_{\mathcal{I}^c}$, then $\expe_{View(A)} \Pr[C=c | C_{\mathcal{I}^c} \wedge View(A)] \leq \Pr[C=c | C_{\mathcal{I}^c}]+\ngl(n)$
\end{lemma}
\begin{proof}
Let $c\in C|C_{\mathcal{I}^c}$.  Let $P_c =\Pr[C=c | C_{\mathcal{I}^c}]$.  
%Let $e$ be the exponent of the particular negligible value above~(note $e = \omega(1)$).  For $b\in\{0,1\}$, by \lemref{lem:code bits statistically indist}, we have that in expectation each for each $i$, $\Pr[C_i =1 ] = 1/2$. 
\begin{align*}
\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\Pr[C=c | C_{\mathcal{I}^c} \wedge view ] &\leq \Pr[C=c | C_{\mathcal{I}^c} ]+ \ngl(n) = P_c+\ngl(n)\\
\end{align*}
If this was not true~(that is in expectation, the probability of $c$ increased by a polynomial factor, then the statistical distance between $c$ and some other codeword $c'$ would be $1/\poly(n)$ violating \lemref{lem:codewords in I close}.
\end{proof}
\begin{corollary}
\label{cor:avg min after view}
$\Hav(C| View(A))\geq \Hav(C | View (A) \wedge C_{\mathcal{I}^c}) \geq  \Hav(C | C_{\mathcal{I}^c}) - \ngl(n) = |C| - |\mathcal{I}^c| - \ngl(n)$.
\end{corollary}
\begin{proof}
Let $e$ be the exponent of the particular negligible value above~(note $e = \omega(1)$). 
Taking the negative log of the above lemma one has:
\begin{align}
\Hav(C_i |  View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot)}))
 &=\nonumber \\
 -\log \left(\expe_{view \leftarrow View}\max_{c\in C|C_{\mathcal{I}^c}}\Pr[C =c | view \wedge C_{\mathcal{I}^c} ]  \right) &\geq -\log (P_c + \ngl(n))\nonumber \\
&\geq -\log(P_c+ \frac{1}{n^{e}} )= -\log \left(\frac{n^{e}+P_c}{P_c n^{e}}\right) \nonumber \\
%&=-\left(\log (n^w+2) -\log 2 -\log n^w \right)\\
&=-\log P_c-\left(\log (1+\frac{P_c}{n^e}) \right)\nonumber \\
&\geq \Hav(C | C_{\mathcal{I}^c})-\frac{1}{\ln 2}\left(\ln (1+\frac{1}{n^e}) \right) \nonumber \\
&= \Hav(C | C_{\mathcal{I}^c})- \frac{1}{\ln 2}\left(\sum_{i=1}^\infty \frac{(-1)^{i+1}}{i} (\frac{1}{n^e})^i\right) \label{eq:taylor approx} \\
&\geq \Hav(C | C_{\mathcal{I}^c}) - \frac{1}{\ln 2} \frac{1}{n^e} = \Hav(C | C_{\mathcal{I}^c})-\ngl(n) \label{eq:taylor trunc}\\
&= |C| - |\mathcal{I}^c| -\ngl(n) \label{eq:min ent}
\end{align}
\bnote{I think the right thing to say here is that the entropy is $\geq \max\{ |C|, O(1)\}$ since the $\ngl$ term probably dominates the above.}Equation~\ref{eq:taylor approx} is derived using the Taylor expansion for $\ln(1+x)$, Equation~\ref{eq:taylor trunc} is derived by noting it is a geometric series and thus all terms $i=3,....$ are contained in the second term $i=2$ which is positive, and Equation~\ref{eq:min ent} is derived using \clref{cl:many locations ent}.
\end{proof}
%\begin{lemma}
%\label{lem:obf has hill ent}
%For every $s_{sec} = \poly(n)$, there exists some $\epsilon = \ngl(n)$ such that $H^{\hillrlx}_{\epsilon, s_{sec}}(C | P) \geq |C| - \ngl(n)$.
%\end{lemma}
%\begin{proof}
%Suppose not, that is for all $\Hav( X | Y) \geq |C| - \ngl(n)$ there exists some $D$ of size $s_{sec} = \poly(n)$ and $\epsilon = 1/\poly(n)$ such that $\Pr[D(C, P) = 1] - \Pr[D(X, Y)  = 1] = \epsilon$.  Let $ (X, Y)  = (C, 
%\end{proof}
%By similar reasoning we can extend this lemma to the entire codeword:
%\begin{lemma}
%If $C$ is output equiprobable then, $\Hav(C | View(A^{\mathcal{O}_{(X_{c_{2_1}},..., X_{c_{2_\ell}})}})) \geq |C| - \ngl(n)$.
%\end{lemma}
%\begin{lemma}
%$\Hoo(C | View (A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)})) \geq \log |C| - o(\ell \log n)$.
%\end{lemma}

\subsection{Correctness of \consref{cons:first construction}}
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \ell/2^n = \ngl(n)$.
\end{lemma}
\begin{proof}
\bnote{we need to come back to this if we use obfuscation with error}
Recall that $c$ contains at most $\ell$ $1$s.  Since $w'$ is chosen independently of the points $r_i$, this probability is the size of each block, that is $\Pr[r_i =w_i']  = 1/2^n$. Since the $r_i$ are chosen independently, one has,
\begin{align*}
\Pr[\text{no $1\rightarrow 0$ flips}] &\geq 1-\Pr[\text{some }r_i = w_i']\\
&\geq 1-\ell \frac{1}{2^n} = 1-\frac{\poly(n)}{2^n} = \ngl(n)
\end{align*}
\end{proof}

We now consider the number of possible $0\rightarrow 1$ bit flips in $c$.  A $0\rightarrow 1$ flip on index $i$ occurs when two conditions are met $w_i\neq w_i'$ and $c_i = 0$.  Since the first conditioned is only fulfilled at most $t$ times, we have the following lemma:

\begin{lemma}
Let $C$ be a code that corrects $t$ $0\rightarrow 1$ bit errors.  Then when $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then 
\[
\Pr[\rep( w', P) = R] \geq 1-\frac{\ell}{2^n}.
\]
That is, \consref{cons:first construction} is correct with error at most $\ell/2^n$.
\end{lemma}

\textbf{Note: }By using a more structured type of code, the necessary error tolerance may be decreased from $t$ to  $(1/2+O(1))t$ by arguing that roughly half of the mismatches between $w, w'$ occur where $c_i =1$.  However, this requires a codeword where most codewords have Hamming weight close to $1/2$.

%\bnote{even if all $t$ are inserted into the bad block its okay right now.  My goal is to decrease the error tolerance of the code below $t$.}
%We now turn to the question of how many $0\rightarrow 1$ bits flips occur between $c, c'$.  Recall that $\dis(w, w')\leq t$ furthermore recall that $w, w'$ are selected independently of the fuzzy extractor, in particular, independently of $c$.  We break the indices of $c = c_1,..., c_\ell$ into two components those where $c_i=1$ and those $c_i = 0$.  Denote these two components by $c_{I_1}$ and $c_{I_0}$.  Our goal is to bound how many of the places where $w, w'$ are inserted into $c_{I_0}$.  Indices $i$,  where $w_i \neq w_i'$ and $c_i = 1$ will be ``corrected'' as $w_i'$ yield $c_i' = 1$ with overwhelming probably~(see \lemref{lem:no 1 to 0 flips}).  Since $w_{err}$ be set of indices where $w, w'$ differ then for all $j\in w_{err}, \Pr[j\in c_{I_0}] = 1/2$.  This is a binomial distribution with $p=1/2$ and $n=t$.  Denote by $X =|\{j | w_j \neq w_j' \wedge j\in c_{I_1}|$. Thus, $\expe[X] = t/2$.  Using Hoeffding's inequality~\cite{hoeffding1963probability}, one has that 
%\begin{align*}
%\Pr[X\leq t(1/2+\alpha)] \leq 1-e^{-2\alpha^2 t}
%\end{align*}
%This leads us to the following lemma:
%\begin{lemma}
%\label{lem:correctness holds}
%If $t = \omega(\log n)$ and that $\dis(w, w')\leq t$ then for any constant $\alpha>0$, then $\Pr[\dis(c, c')\geq t(1/2+\alpha)] \leq 1-e^{-2\alpha t} = 1-\ngl(n)$.
%\end{lemma}
%\bnote{This analysis is done assuming that the two sets are the same size.  Need to worry about variation in the number of $1$s and $0$s in $c$.  Need to show that for most codes (linear?) the variance in Hamming weight is small.}
%\textbf{Note:} If $t = O(\log n)$ the only error tolerance for $C$ that results in decoding with overwhelming probability is $t$.  However, in most use cases, we expect $t=\omega(\log n)$.  

Together, with the arguments in \secref{sec:sec of construction} we have the construction is secure and correctness for independent sources~(where each block has a super-logarithmic amount of entropy):
\begin{corollary}
Let $\mathcal{W}$ be the family of block distributions where $W_i$ are independent and $\Hoo(W_i) \geq \omega(\log n)$.  Then for $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\zo^{n\times \ell}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon, s_{sec})$ with error $\epsilon_{code} + \ell/2^n$ and resulting entropy $\tilde{m} = |C| - o(n)$.\bnote{Why the transition from $\ngl(n)$ to $o(n)$?  Shouldn't this be $\ngl(n)$?}
\end{corollary}
\begin{proof}
Security is result of arguments in \secref{sec:sec of construction}.  Correctness follows from the arguments above and the fact that $C$ is a $(\neigh_t, \epsilon_{code})$-code.
\end{proof}


\subsection{Supported Sources}
\label{sec:supported sources}
\bnote{need to complete this section and redo the proof in  \secref{sec:sec of construction} with this type of source.}

We now describe a set of high entropy sources appropriate for which the proof outline given in \secref{sec:sec of construction} holds.  We need a distribution where after all of the possible equality queries, a large number of blocks still have super-logarithmic entropy.

\begin{definition}
\label{def:block guessable}
Let $\mathcal{O}_{w_1,..., w_\ell}$ be an oracle that return $\mathcal{O}_{w_1,..., w_\ell}(i, w_i')=\left( w_i\overset{?}=w_i'\right)$.
A source $W = W_1||...|W_\ell$ is a $(q, \alpha, \beta)$-\emph{unguessable block distribution} if for any adversary $A$~(not bounded in time or space) with oracle access to $\mathcal{O}$ making at most $q$ queries there exists a set $\mathcal{I}$ of size at least $\ell -\beta$ such that 
\[
\forall i\in \mathcal{I}, \Hav(W_i |View(A^{\mathcal{O}_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}
As long as $\beta$ is small enough, we can show security of \consref{cons:first construction}~(replace \lemref{lem:blocks unguessable after queries} with an unguessable source, the remaining arguments follow).

\begin{theorem}
Let $\mathcal{W}$ be a family of $(q, \omega(\log n),  \beta)$-unguessable distributions over $\zo^{n\times \ell}$.  Furthermore, let $C$ be a $(\neigh_t, \epsilon_{code})$-code.  Then for $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\zo^{n\times \ell}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon, s_{sec})$ with error $\epsilon_{code} + \ell/2^n$ and resulting entropy $\tilde{m} = \log |C| - \beta - o(n)$.
\end{theorem}

\subsection{Characterizing unguessable block distributions}
\bnote{This section needs to be finished.  The point is to try and give an idea what the above definition means.  What type of distributions fulfill that definition.}
This type of source seems to be inherently adaptive and is difficult to formulate in a clean information-theoretic notion.  However, we can show necessary and sufficient types of sources for a guessable block distribution.  We begin by defining the entropy jump of a block source which will appear in our characterization.

\begin{definition}
Let $W = W_1,..., W_k$ be a source under ordering $i_1,..., i_k$.  The \emph{entropy jump} of a block $i_j$ is $J(i_j) = \Hav(W_{i_j} | W_{i_1},..., W_{i_{j-1}})$.
\end{definition}

\begin{claim}
Let $W$ be a $(q, \alpha, \ell)$-unguessable block distribution.  Then for all orderings $i_1, ...., i_k$ there is some $j\leq k-\ell$ such that $J(i_j)> \log q + \alpha$.
\end{claim}
\begin{proof}
Suppose not, for convenience assume the ordering that violates this condition is $1,..., k$.  We describe a distribution $W$ where $\forall 1\leq j\leq k-\ell$, $J(j)\leq \log q +\alpha$. Define $W$ as follows $W_1 = c_1,...., W_{j-1} = c_{j-1}$ are constants.
\end{proof}
\bnote{keep working}

\section{Discussion}
\label{sec:discussion}
Security of \consref{cons:first construction} relies on a large number of dimensions which are unpredictable.  Correction occurs for a small number of dimensions that are allowed to vary completely.  As described in \secref{sec:supported sources}, \consref{cons:first construction} is not secure for an arbitrary min-entropy source, it must contain a significant number of dimensions that are hard to guess by equality queries~(meaning they have min-entropy).

To show that $gap$ can be negative for \consref{cons:first construction}, we first calculate the size of the Hamming ball.  We allow $t$ errors with an alphabet of size $2^n$.  This means that
\begin{align*}
\log |B_t| &= \log \sum_{i=0}^t {\ell \choose i} (2^n-1)^i\\
&> \log {\ell \choose t} (2^n-1)^t\\
&=\Theta(tn) + \log {\ell\choose t}
% \log (2^n)^{H_{2^n}(t/\ell)\ell - o(\ell)} =n( H_{2^n}(t/\ell)\ell -o(\ell) )
\end{align*}
\bnote{Need a good bound on ${\ell \choose t}$.}
We now ask how much entropy is necessary for security.  The type of source we considered was independent blocks each with super-logarithmic min-entropy.  This yields 
\[
gap = \Hoo(W) - \log |B_t| < \ell \omega(\log n) -\left( \Theta(t n) + \log {\ell \choose t}\right). 
\]
When $t =\Theta(\ell)$, then $gap<0$ and the output entropy is $|C| - \ngl(n)$~(if $C$ is a constant rate, this is $\Theta(\ell)$).
Thus, \consref{cons:first construction} achieves negative $gap$ when $t = \Theta(\ell)$.  Furthermore, not every block must contribute entropy, so security is possible when the total starting entropy is $\omega(\log n)$.  For example, \consref{cons:first construction} is secure when each block is equal and drawn from a distribution of super-logarithmic min-entropy.  So there are supported sources with negative $gap$ for any value of $t$.  We achieve $gap<0$, constant error tolerance, and extract a constant fraction of $n$ when $t = \Theta(\ell)$ and $\ell = O(n)$.
%When $ \ell , t, n$ to be the same order provides a natural balance of parameters. 

\textbf{Limitations of \consref{cons:first construction}:}  There are three major drawbacks to our construction.  The first is that Hamming distance is over an alphabet of exponential size, which precludes use in some applications.  The second is that we leak equality information about particular blocks, this may be sensitive information in the presence of auxiliary information.  Lastly, we obtain only a single bit from each block of $W_i$ which seems sub optimum.
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix
\section{Obfuscation}
\label{sec:obfuscation}
The standard notion of obfuscation is virtual black-box obfuscation~(introduced by Barak et al.~\cite{barak2001possibility}).  This notion is known to be impossible in a black-box way for all polynomial time programs.  Several variants~(best possible obfuscation~\cite{goldwasser2007best}, indistinguishability obfuscation~\cite{barak2001possibility}, differing inputs indistinguishability obfuscation~\cite{barak2001possibility}) have been presented~(see Varia~\cite{varia2010studies} for implications between various definitions).  Indistinguishability obfuscation has recently been shown to be constructible~\cite{garg2013candidate} using multilinear maps~\cite{garg2013multilinear}.  The definition of virtual black box obfuscation is presented in \defref{def:obf}.


In this work, we will use obfuscation of point-programs, 
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}
\]

We present the construction of Canetti~\cite{canetti1997towards}, as an illustration of how point functions can be obfuscated.  Let $G$ be a group:
\begin{enumerate}  
\item Input: string $w\in \zo^n$.
\item Choose a generator $g\overset{\$}\leftarrow G$
\item Compute $h\leftarrow g^w$, where $w$ is viewed as an element of $G$ is some canonical way.
\item Output: circuit that has $g, h$ hardwired, and on input $x$, accepts iff $g^x=h$.
\end{enumerate}
\begin{assumption}[Strong DDH assumption]\label{ass:strong ddh}.  Let $n$ be a security parameter and let $p = 2q+1$ be a randomly chosen $n$-bit safe prime.  Consider the group $Q$ of quadratic residues in $\mathbb{F}_p^*$.  For any $W$ with $\Hoo(W)= \omega(\log n)$ where the domain of $W$ is $\mathbb{F}_q$, for $g\overset{\$}\leftarrow Q, a\leftarrow W, b,c \overset{\$}\leftarrow \mathbb{F}_q$, the ensembles $\langle g, g^a, g^b, g^{ab}\rangle$ and $\langle g, g^a, g^b, g^c\rangle$ are computationally indistinguishable.
\end{assumption}
\begin{theorem}~\cite{canetti1997towards}
The construction above, when instantiated with the group $G = Q$ is a virtual black-box obfuscator for the family of point functions under \assref{ass:strong ddh}.
\end{theorem}

As mentioned above there exist other constructions of point function obfuscation under various assumptions~\cite{lynn2004positive, wee2005obfuscating}.

Lastly, we will need the notion of composable obfuscation.

\begin{definition}[Composable obfuscation~\cite{bitansky2010strong, canetti2008obfuscating,lynn2004positive}]
\label{def:obf comp}
A PPT algorithm $\mathcal{O}$ is a $t$-\emph{composable obfuscator} for the family $\mathcal{C}$ with dependent auxiliary input if functionality and polynomial slowdown hold as before, and the virtual black black-box property holds whenever the adversary and simulator are given up to $t$ circuits in $\mathcal{C}$.  That is, for every PPT $A$ and polynomial $\rho$, there exists a PPT $S$ such that for all sufficiently large $n$, and for all $C_1,..., C_t\in \mathcal{C}_n$, and for all auxiliary inputs $z\in \zo^*$, 
\[
|\Pr[A(\mathcal{O}(C_1), ..., \mathcal{O}(C_t), z) = 1] - \Pr[S^{C_1,..., C_t}(1^n, z) = 1]| < \frac{1}{\rho(n)},
\]
where the probabilities are taken over the random coins tosses of $A, S$ and $\mathcal{O}$.  The runtimes of $A$ and $S$ must be polynomial in the length of their first input.
\end{definition}

\section{Fuzzy Conductors}

\label{sec:conductors}
In this section, we show that fuzzy conductors are subject to the same lower bounds as fuzzy extractors.  We first review the definition of a fuzzy conductor from~\cite{KanukurthiR09}:
\begin{definition}
A $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-\emph{fuzzy conductor} with error $\delta$ is a pair of randomized procedures, ``generate''~(\gen) and ``reproduce''~(\rep), with the following properties:
\begin{enumerate}
\item The generate procedure $\gen$ on input $w\in \mathcal{M}$ outputs a string $y\in\zo^*$ and a helper string $p\in\zo^*$.
\item The reproduction procedure $\rep$ takes an element $w'\in\mathcal{M}$ and a bit string $p\in\zo^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w, w')\leq t$, if $Y, P$ were generated by $(Y, P)\leftarrow \gen(w)$, then $\rep(w', P) = Y$ with probability~(over the coins of $\gen, \rep$) at least $1-\delta$.  If $\dis(w, w')>t$, then no guarantee is provided about the output of $\rep$.
\item The security property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $Y$ is close to a high entropy distribution $Z$.  That is, $\exists Z$ with $\Hav(Z | P ) \geq \tilde{m}$ such that $\mathbf{SD}((Y, P), (Z, P))\leq \epsilon$.
\end{enumerate}
\end{definition}

We borrow from~\cite{DBLP:journals/siamcomp/DodisORS08} the following notation:
\begin{itemize}
\item A $\mathcal{M}, K, t)$ code is a subset of $\mathcal{M}$ where there is a procedure that corrects $t$ errors.
\item $K(\mathcal{M}, t)$ is the largest $K$ for which there exists an $(\mathcal{M}, K, t)$-code.
\item $K(\mathcal{M}, t, S)$ is the largest $K$ such that there exists an $(\mathcal{M}, K, t)$ code all of whose $K$ points belong to $S$.
\item $L(\mathcal{M}, t, m) = \log (\min_{|S| = 2^m} K(\mathcal{M}, t, S))$.  This is finding the best $S$ for encoding $2^m$ points.
\end{itemize}

\begin{lemma}
The existence of a $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-fuzzy conductor implies that $\tilde{m}-1\leq L(\mathcal{M}, t, m) -\log (1-2\epsilon)$.
\end{lemma}
\begin{proof}  Assume $(\gen, \rep)$ is a fuzzy conductor with the parameters stated.  Let $S$ be a set of size $2^m$ in $\mathcal{M}$ and let $W$ be the uniform distribution over $S$.  Define $(Y, P) \leftarrow \gen(W)$.  Then there must exist a distribution $Z$ with $\Hav(Z|P) \geq \tilde{m}$ such that $\mathbf{SD}((Y, P), (Z, P))\leq \epsilon$.  By Markov's inequality, there exists some set $S_P$ such that $\Pr[P\in S_p] \geq 1/2$ and $\forall p\in S_P$ one has $\mathbf{SD}(Y | P = p, p ), (Z | P = p, p)<2\epsilon$.  Now applying Markov's inequality of $\max_{z} \Pr[Z=z | P=p]$, there exists a set $S_P'$ such that $\Pr[P\in S_P']>1/2$, and for all $p\in S_P'$, $\Hoo(Z| P =p ) \geq \tilde{m}-1$.  Denote by $p^*$ a value in $S_P\cap S_P'$~(on value must exist).  Then $\mathbf{SD}((Y | P =p^* , p^*), (Z| P = p^*, p^*))\leq 2\epsilon$ and $\Hoo(Z|P=p^*)\geq \tilde{m}-1$.  Denote by the set $T$ the possible values of $Y$ when $P=p^*$.  For the statistical distance property to hold, $|T| \geq  (1-2\epsilon)2^{\tilde{m}-1}$.  Associate with every $y\in T$ some $w\in S$ which could have produced $y$ with nonzero probability given $P=p^*$, and call this map $C$.  $C$ defines an error correcting code.

\end{proof}


%\section{Proof of \lemref{lem:linear codes independent}}
%\label{sec:proof of linear indep}
%\begin{proof}
%Suppose there is some $i$ that is not equiprobable.  Let $C_{i_1} = \{C | C_i =1\}$ and $C_{i_0} = \{C | C_i = 0\}$. First note that since $C$ does not have any constant bits, it dimension is at least $1$ yielding that $|C| = 2^k$ for some $k$. First suppose that $0<|C_{i_1}| < |C_{i_0}|$.  Let $c\in C_{i_1}$ then by linearity $\forall c'\in C_{i_0}$, $c\oplus c' \in C_{i_1}$.  Furthermore, for all distinct $c_1, c_2\in C_{i_0}$, $c_1\oplus c \neq c_2\oplus c$.  This means that $|C_{i_i}|\geq |C_{i_0}|$ as all $c'\oplus c$ must be distinct and contained in $C_{i_1}$.  
%Now suppose that $|C_{i_1}| > |C_{i_0}| >0$, denote the elements of $C_{i_1}$ by $c_1,..., c_{|C_{i_1}|}$, then by linearity $c_1\oplus c_2, c_1\oplus c_3,..., c_1\oplus c_{|C_{i_1}|}$ are all distinct elements of $C_{i_0}$ this means that $|C_{i_0}|\geq |C_{i_1}| -1 $, for the total size to be even this means that $|C_{i_0}|\geq |C_{i_1}|$.  This is a contradiction and completes the proof.
%\end{proof}

\section{Proof of \lemref{lem:cond and cext}}
\label{sec:cond and cext}
\begin{proof}
It suffices to show if there is some distinguisher $D$ of size $s'$ where 
\[\delta^D((\cext(W; X), U_d, P), (U_\kappa, U_d, P))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D'$ of size $s_{cond}$ where for all $Z$ where $\Hav(Z|P)\geq \tilde{m}$ such that
 \[
 \delta^{D'}((Y, P), (Z, P))\geq \epsilon_{cond}.
 \]
Let $D$ be such a distinguisher.  That is,
\[
\delta^D(\ext(X, U_d)\times U_d \times P, U_\kappa\times U_d\times P)> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D'$ as follows.  On input $y, p$ sample $x\leftarrow U_d$, compute $r\leftarrow \cext(y, x)$ and output $D(r, x, p)$.  Note that $|D'| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D'}((Y, P), (Z, P))&= \delta^D((\cext(Y, U_d), U_d, P), \cext(Z, U_d), U_d, P)\\
&\geq \delta^D((\cext(Y, U_d), U_d, P), (U_\kappa\times U_d \times P)) - \delta^D((\cext(Z, U_d), U_d, P), (U_\kappa\times U_d \times P))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Thus $D'$ distinguishes $Y$ from all $Z$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}


\end{document}