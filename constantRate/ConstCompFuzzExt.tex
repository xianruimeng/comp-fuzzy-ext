\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Vol}{\mathsf{Vol}}

\title{Constant Error Tolerance Computational Fuzzy Extractors}
%\author{Ran Canetti \and Benjamin Fuller\footnote{The Lincoln Laboratory portion of this
%work was sponsored by the Department of the Air Force under Air Force
%Contract
%\#FA8721-05-C-0002.  Opinions,
%interpretations, conclusions and recommendations are those of the author
%and
%are not necessarily endorsed by the United States Government.} \and Omer Paneth \and Leonid Reyzin}

\begin{document}
\maketitle


\begin{abstract}
Fuzzy extractors derive strong keys from noisy sources.  The goal is to reliably convert a high entropy source~(which may differ on repeated readings) to the same uniformly distributed key.  Traditionally, their security is defined information-theoretically.  %Fuzzy extractors have upper bounds on the length of the derived key.  

Fuzzy extractors work for any distribution with enough entropy.  %In this setting, the entropy of the source must be significantly higher than the logarithm of number of error patterns corrected~(Dodis et al., J. of Comp 2008).  
We call the difference between the starting entropy and the logarithm of the number of correctable error patterns the \emph{minimum usable entropy}.  Dodis et al.~(J. of Comp. 2008) show there is some source where the key length cannot exceed the minimum usable entropy.  Therefore, fuzzy extractors that output the same length key for any source with enough entropy cannot output a key longer than the minimum usable entropy.
For many practical noisy sources, like biometrics, the minimum usable entropy is negative, leaving reliable key derivation from these sources as an open problem.

%Traditional fuzzy extractors output a key roughly of length minimum usable entropy.  
To achieve meaningful security when the minimum usable entropy is negative, some restriction on the source is necessary.  Meaningful restrictions (and accompanying constructions) have proved elusive for information-theoretic fuzzy extractors.
Fuller, Meng, and Reyzin (Asiacrypt 2013) define a computationally-secure version of a fuzzy extractor.  It may be easier to construct computational fuzzy extractors for meaningful sources.

In this work, we construct the first (computational) fuzzy extractors that are secure for a large class of distributions where the minimum usable entropy is negative.

\textbf{Constructions:}  We construct computational fuzzy extractors from point function obfuscation.
Our constructions are inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~(Eurocrypt 2008).  
\bnote{this whole paragraph needs work} Our two constructions are similar in spirit but optimize different parameters.  The first requires individual blocks of the source to have super-logarithmic entropy and corrects a constant fraction of errors.  The second construction uses sampling to reduce the required entropy of the source.  Unfortunately, sampling also reduces the effective error tolerance of the construction.  %The first construction works for high-entropy and high-error sources, while the second construction works for distributions with lower entropy and error.
\bnote{need something else to say here}
\end{abstract}


\section{Introduction}\label{sec:introduction}

\paragraph{Fuzzy Extractors}
Cryptography relies on secrets for that are read multiple times for key derivation and authentication. However, many sources with sufficient randomness  provide similar but not identical values at each reading~(prominent examples include biometrics and other human-generated data~\cite{daugman2004,zviran1993comparison,brostoff2000passfaces,ellison2000protecting,mayrhofer2009shake,monrose2002password},
physically unclonable functions~\cite{pappu2002physical,tuyls2006puf,gassend2002silicon,suh2007physical}, 
and quantum information~\cite{bennett1988privacy}). Turning similar readings into identical values is known as \emph{information reconciliation}; further converting those values into uniformly random secret strings is known as \emph{privacy amplification}~\cite{bennett1988privacy}.
Both of these problems have interactive and noninteractive versions.  In this paper, we are interested the noninteractive case, which can be used, in particular, by  a single user trying to produce the same key from multiple readings of a physical source at different times.  The primitive accomplishing information reconciliation and privacy amplification noninteractively is known as a \emph{fuzzy extractor}; it is defined information-theoretically in~\cite{DBLP:journals/siamcomp/DodisORS08}


Fuzzy extractors consist of a pair of algorithms: \gen takes a source value $w$, and produces a key $r$ and a public helper value $p$.  The second algorithm \rep takes this helper value $p$ and a close $w'$ to reproduce the original key $r$.  The security guarantee is that $r$ produces by \gen is close to uniform (information-theoretically \cite{DBLP:journals/siamcomp/DodisORS08} or computationally \cite{fuller2013computational}), even given $p$, as long as $w$ comes from a high-quality distribution (traditionally, any distribution with sufficient min-entropy $m$). The correctness guarantee is that $r$ will be correctly reproduced by \rep as long as $w'$ is no farther than $t$ from $w$ in some metric space (in this paper, we focus on the Hamming metric on strings over some alphabet $\mathcal{Z}$ of length $\ell$).

\paragraph{Limitations of Entropy-Based Approaches}
Constructions of fuzzy extractors are limited by the tension between security and correctness guarantees: if we allow for higher error tolerance $t$, then we also need higher starting entropy $m$. The reason for this tension is quite simple: the adversary who knows $p$ and can guess some $w'$ within distance $t$ of $w$ will be able to easily distinguish $r$ from uniform by running $\rep$ and finding out what $r$ is. If $t$ is higher, then $\rep$ tolerates more $w'$ values, so the adversary's job is easier. In fact, if $t$ is high enough that there are $2^m$ points in a ball of radius $t$, then the \emph{entire distribution} of $w$ may have min-entropy $m$ and yet be contained in a single ball, in which case the adversary can simply use the center of the ball as $w'$. 

More generally, let $B_t$ denote the number of points in a ball of radius $t$. For any $m$ and $t$, one can come up with a distribution of min-entropy $m$ such that the adversary can guess a correct $w'$ with probability $1/\lceil 2^m/B_t) \rceil\approx B_t 2^{-m}$: the distribution consists of all points in several nonoverlapping balls of radius $t$ (we need the space to be sufficiently large for the balls to fit). We thus call $m-\log B_t$ the \emph{minimum usable} entropy, denoted by $\Huse$. The previous paragraph shows that  no fuzzy extractor can handle all distributions of a given min-etropy $m$ if  $\Huse\le 0$.

However, some of the distributions with $\Huse\le 0$ are prime candidates use in authentication. \lnote{examples here}. No current fuzzy extractor construction is known to work for them.

\paragraph{Our Contributions}
We provide the first constructions of computational fuzzy extractors that can be used for a large class of distributions with $\Huse\le 0$ over $\mathcal{Z}^\ell$ for a large alphabet $\mathcal{Z}$.  As explained above, such constructions cannot work without some restrictions on the distribution. 
It is secure when symbols in $w$ 
each have individual superlogarithmic min-entropy, even if they are arbitrarily correlated. Moreover,
a constant fraction of symbols in $w$ may have little entropy, as long as knowledge of their values does not reduce the entropy of the high-entropy symbols too much (see \defref{def:block guessable}).  

We improve the entropy requirement in the second construction, which requires only a constant fraction of the symbols $w$ to have constant min-entropy conditioned on the previous symbols.
However, this improvement comes at a price to error-tolerance: whereas the first construction tolerates a constant fraction of errors, the second construction tolerates $\ell/\omega(\log\ell)$ errors.



\paragraph{Our Approach}
Our constructions are computationally secure. Indeed, known techniques for proving information-theoretically secure fuzzy extractors work for all distributions of a given entropy, and thus cannot be used to prove security for some distributions  with $\Huse\le 0$ (because then they would prove security for all distributions with $\Huse \le 0$, which we know is impossible.)

Most known constructions of fuzzy extractors put sufficient information in $p$ to recover $w$ from a nearby $w'$ during $\rep$ (this procedure is called a \emph{secure sketch}), and then apply a randomness extractor to $w$ to get $r$.
Fuller, Meng, and Reyzin~\cite{fuller2013computational}, in introducing computational fuzzy extractors, show that replacing secure sketches with a similar computational component is unlikely to be fruitful~\cite[Corollary 3.8, Theorem 3.10]{fuller2013computational}.  Instead, they suggest two alternatives: combine the information-reconciliation and privacy amplification components~(their approach) or produce a new consistent secret with computational entropy instead of recovering $w$.  We take the second approach. 

Any procedure that converts a high-entropy input to a high-entropy output is known as a \emph{conductor} \cite{CRVW02}; if it's error-tolerant, then it's a \emph{fuzzy conductor}~\cite{KanukurthiR09}. We define and show two constructions of \emph{computational fuzzy conductors}. 
These constructions may be converted to computational fuzzy extractors using information-theoretic~\cite{nisan1993randomness} or computational~\cite{krawczyk2010cryptographic} extractors~(\lemref{lem:cond and cext}).

Both constructions are based on  obfuscation of point programs~\cite{canetti1997towards}.  A point program $I_w(x)$ outputs $1$ if $x=w$ and $0$ otherwise.  Intuitively, an obfuscated version of a program reveals nothing past its input and output behavior.  For a point program, this means hiding all partial information about the point $w$. 
We need a strong version of point obfuscation, which is secure under composition.  
However, we note that for even this strong version, obfuscation of point programs is much simpler and more efficient than general obfuscation \lnote{omer, could you put in some references?}. In particular, it is achievable under particular number-theoretic assumptions~\cite{bitansky2010strong}. It can be made very efficient under a strong assumption on cryptographic hash functions~\cite{canetti1998perfectly}.

Both constructions are inspired by Canetti and Dakdouk's construction of digital lockers from point obfuscation~\cite{canetti2008obfuscating}.  Let $w=w_1 \dots w_\ell$, for $w_i\in \mathcal{Z}$. In the first construction (\consref{cons:first construction}), for each $i$, $\gen$ flips a coin $c_i$ and either obfuscates $I_{w_i}$ or picks a random point $r_i$ and obfuscates $I_{r_i}$.  This produces $\ell$ obfuscated programs $P_1,..., P_\ell$.  With a close value $w'$, $\rep$ then runs the obfuscated program $P_i$ on $w_i'$ and checks whether $P_i(w_i')=1$.  For most locations $i$, \rep can determine whether $w_i$ or a random value was obfuscated.  Thus, most bits of $c_i$ are recoverable. To tolerate errors,  the set of coins $c_1\dots c_\ell$  is chosen at random from the codewords of an error correcting code. This construction conducts entropy from $w$ to $c$.

Note that obfuscation of point functions provides no security if a point can be guessed; thus, in order for the first construction to be secure, sufficiently many coordinates of $w$ have to be unguessable (even to an adversary who can make equality queries for the values of other coordinates). We relax this requirement in our second construction (\consref{cons:sampling}), called \emph{sample-then-obfuscate}: it converts $w$ to a string of blocks and then applies the first construction. Each block is produced by  sampling several coordinates of $w$ and concatenating them. This reduces the entropy requirement on the individual symbols, but lowers the error-tolerance. This approach is similar to the sample-then-extract paradigm for building locally computable extractors~\cite{lu2002hyper,vadhan2003constructing}.  Note that, unlike in locally computable extractors, we can use the sampler multiple times and not worry about the fact that the same symbols is entering multiple blocks,  because correlations among blocks are not a problem for the first construction. Computational, rather than information-theoretic, analysis seems crucial for achieving this property.

\paragraph{Open Problems}
Both constructions require a large alphabet $\mathcal{Z}$---one whose size is more than polynomial in the security parameter.\footnote{Alternatively, in case of burst errors~\cite{gilbert1960capacity}, multiple symbols from a small alphabet can be combined into a single symbol from a large alphabet}.  Although the sample-then-obfuscate construction remains secure with a small alphabet, because of its lower error tolerance, $\Huse \le 0$ only for large alphabets (for small alphabets, since $\Huse>0$,  there are good information-theoretic constructions known~\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08}).
Constructing a computational fuzzy extractor with the above advantages and a small alphabet is an open problem. 

Using information-theoretic fuzzy extractors with additional privacy properties, Dodis and Smith~\cite[Section 5]{DBLP:conf/stoc/DodisS05} construct program obfuscators for the program $I_w(x)$ that tests if $x$ is within distance $t$ of $w$. The obfuscation is secure as long as $w$ comes from a distribution of sufficient minentropy; in particular, the entropy must be high enough so that $\Huse>0$. Our constructions do not provide obfuscators for proximity queries, because they leak more information than whether $x$ is within distance $t$ of $w$ (for example, they may provide some information about the actual distance or about which coordinates agree). Constructing an efficient obfuscator for proximity queries when $\Huse<0$ is an open problem.

\medskip
The remainder of this paper is organized as follows: we cover notation and background on obfuscation and error correcting codes in \secref{sec:preliminaries}, describe computational fuzzy extractors in \secref{sec:fuzzy extractors}, and present our two constructions in Sections \ref{sec:construction} and \ref{sec:sampling} respectively.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote $X = X_1,..., X_\ell$ as the concatenation of $X_1$ to $X_\ell$.  For a set of indices $\mathcal{I}$,  $X_{\mathcal{I}}$ is the restriction of $X$ to the indices in $\mathcal{I}$.  The set $\mathcal{I}^c$ is the complement of $\mathcal{I}$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$,
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   Let $|W|$ be the size of the support of $W$ that is $|W| = |\{w | \Pr[W=w]>0\}|$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric, for two vectors $x, y$ over $\mathcal{Z}^\ell$ the Hamming distance between $x,y$ is $\dis(x,y) = \{i | x_i \neq y_i\}$.  For the Hamming metric, $|B_t| = \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  Unless otherwise noted logarithms are base $2$.
Usually, we use capitalized letters for random variables and lowercase letters for samples from a random variable.

\subsection{Coding Theory}
\label{sec:coding theory}
We introduce some notions from binary coding theory.  Usually the standard class of errors  is all points within Hamming distance $t$, we will use the Hamming analog of the $Z$-channel~\cite{tallini2002capacity} where there are flips from $0\rightarrow 1$ but no bit flips from $1\rightarrow 0$.
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$.
\end{definition}

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \delta_{code})$-code if there exists an efficient procedure $\rec$ such that for all $c\in C, \forall c'\in \neigh(c), \Pr[\rec(c') \neq c] \leq \delta_{code}$.
\end{definition}

We note that for any code learning a few bits does not inform on the remainder of the bits~(the claim is a direct result of \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}).

\begin{claim}
\label{cl:many locations ent}
Let $C$ be a binary code and let $\mathcal{I}^c$ be a set of indices of $C$.  Then $\Hav(C | C_{\mathcal{I}^c}) = \log |C| - |\mathcal{I}^c|$.
\end{claim}

\textbf{Notes:} 
Any code that corrects $t$ Hamming errors corrects $t$ $0\rightarrow 1$ errors, but more efficient codes  exist for this type of error~\cite{tallini2002capacity}.
 Codes where $\log |C| = \Theta(\ell)$ and $t = \Theta(\ell)$ over the binary alphabet exist for Hamming errors and suffice for our purposes~(first constructed by Justensen~\cite{justesen1972class}).  These codes also yield a constant error tolerance for $0\rightarrow 1$ bit flips.
The class of errors we support in our source~($t$ Hamming errors over a large alphabet) and the class of errors corrected by our code~($t$ $0\rightarrow 1$ errors) are different.  See Constructions~\ref{cons:first construction} and~\ref{cons:sampling} for the translation between the error classes.

\subsection{Obfuscation}
Our construction uses obfuscation for a family of point functions $\mathtt{I} = \{I_w\}_{w \in \zo^*}$ defined as follows:
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}.
\]
The required notion of obfuscation is virtual grey-box (VGB) introduced in \cite{bitansky2010strong}. This notion is weaker then the standard notion of virtual black-box (\cite{barak2001possibility}) as it allows the simulator to run in unbounded time while making at most polynomial number of oracle queries to the function. In the following definition we also require that the obfuscation is composable and secure with respect to auxiliary input. Composable auxiliary-input VGB obfuscators for point functions are constructed in \cite[Theorem 6.1]{bitansky2010strong} from the Strong Vector Decision Diffie-Hellman assumption which is a generalization of the strong DDH assumption of \cite{canetti1997towards} for tuples of points.

\begin{definition}[$\ell$-composable obfuscation VGB obfuscation with auxiliary input \cite{bitansky2010strong}]
\label{def:obf} Let $\mathtt{I}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is a $\ell$-composable VGB obfuscator for $\mathtt{I}$ with auxiliary-input if the following conditions are met:
\begin{enumerate}
\item \emph{Functionality:} for every $ I \in \mathtt{I}$, $\mathcal{O}(I)$ is a circuit that computes the same function as $I$.
\item \emph{Virtual grey-box:}  For every PPT adversary $A$ and polynomial $p$, there exists a (possibly inefficient) simulator $S$ and a polynomial $q$ such that for all sufficiently large $n$, any  sequence of circuits $I^1,\dots,I^\ell \in \mathtt{I}_n$, (where $\ell=\poly(n)$) and for all auxiliary inputs $z\in \zo^*$:
\[
|\Pr_{A,\mathcal{O}}[A(z,\mathcal{O}(I^1),\dots,\mathcal{O}(I^t)) = 1] - \Pr_{S}[S^{(I^1,\dots,I^\ell)[q(n)]}(z, 1^{|I^1|},\dots,1^{|I^\ell|}) = 1] | < \frac{1}{p(n)} \enspace,
\]
where $(I^1,\dots,I^\ell)[q(n)]$ is an oracle that answers at most $q(n)$ queries, and where every query of the form $(i,x)$ is answered by $I^i(x)$.
\end{enumerate}
\end{definition}

\section{Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

In this section we present our paradigm for constructing computational fuzzy extractors.  Definitions for information-theoretic fuzzy extractors can be found in the work of Dodis et al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}~\cite[Definition 2.5]{fuller2013computational}
\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
In the above definition, the errors are chosen before $P$: if the error pattern between $w$ and $w'$ depends on the output of $\gen$ there is no guarantee about the probability of correctness. In both our constructions it is crucial that $w'$ is chosen independently of the outcome of \gen.
The definition of computational fuzzy extractors specifies a family of sources for which the fuzzy extractor works rather than the family of all sources of a given min-entropy $m$.  With $\Huse\le 0$ some restriction is necessary for any meaningful security~(see \apref{sec:minimal conditions}).  Instead of restricting the class of source distributions, the definition could restrict the correctable errors to errors that are ``likely'' to occur in the source.  We leave this as an open problem.

Fuller, Meng, and Reyzin~\cite{fuller2013computational} present two approaches for constructing a computational fuzzy extractor: analyzing the information-reconciliation and privacy amplifications components together or using a fuzzy conductor and a privacy amplification component.  We follow the second approach.
In \apref{sec:conductors}, we show that fuzzy conductors are subject to the same lower bounds on entropy loss as fuzzy extractors.  To overcome these bounds, we use a computational version of a fuzzy conductor.
We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case:
\begin{definition}\protect{~\cite[Definition 3]{DBLP:conf/eurocrypt/HsiaoLR07}}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition}
We now define a computational fuzzy conductor and a (computational)~randomness extractor.  A computational fuzzy conductor is the computational analogue of a fuzzy conductor~(introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).
\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' ($\gen'$) and ``reproduce'' ($\rep'$) is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $X\in\{0,1\}^\ell$ and a helper string $SS\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $SS\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(X, SS)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', SS) = X] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $X$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon_{cond}, s_{cond}}(X |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}
A computational extractor is the adaption of a randomness extractor to the computational setting.  Any information-theoretic randomness extractor is also a computational extractor. We adapt the definition of Krawczyk~\cite{krawczyk2010cryptographic} to the average-case:
\begin{definition}
Let $\chi$ be a finite set.
A function $\cext: \zo^\ell \times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon_{ext}, s_{ext})$-average-case computational extractor} if for all pairs
of random variables $X, Y$ over $\zo^\ell, \chi$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\delta^{\mathcal{D}_{s_{ext}}}((\cext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon_{ext}$.
\end{definition}

Combining a computational fuzzy conductor and an appropriate computational extractor yields a computational fuzzy extractor~(proof in \apref{sec:cond and cext}):

\begin{lemma}
\label{lem:cond and cext}
Let $\gen'$, $\rep'$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{cond}, s_{cond})$-hard with error $\delta$ and outputs in $\zo^\ell$.  Let $\cext:\zo^\ell\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon_{ext}, s_{ext})$-average case computational extractor.  Define $(\gen, \rep)$ as:
\begin{itemize}
\item $\gen(w; seed):$ run $(x, ss)= \gen'(w)$ and set $r = \cext(x; seed)$, $p = (ss, seed)$.  Output $(r, p)$.
\item $\rep(w, (ss, seed)):$ recover $x = \rec'(w'; p')$ and output $r = \cext(x; seed)$.
\end{itemize}
Then $(\gen, \rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{cond}+\epsilon_{ext}, s')$-hard with error $\delta$ where $s' = \min\{s_{cond} - |\cext| -d, s_{ext}\}$.
\end{lemma}


\section{Tolerating a Constant Fraction of Errors when $\Huse\le 0$}
\label{sec:construction}
For the remainder of this work, we consider the Hamming metric over some alphabet $\mathcal{Z}$.  Our goal is to derive strong keys for a large class of sources where $0>\Huse = \Hoo(W) - \log|B_t|$.
In the computational setting a ``long enough'' key may be expanded using a computational extractor~(\lemref{lem:cond and cext}).  We focus on building a computational fuzzy conductor with meaningful output entropy.  Our first construction is inspired by the construction of digital lockers from point obfuscation by Canetti and Dakdouk~\cite{canetti2008obfuscating}.

\begin{construction}
\label{cons:first construction}
Let $n$ be a security parameter, let $\mathcal{Z}$ be an alphabet and let $W = W_1,..., W_\ell$ be a distribution over $\mathcal{Z}^\ell$.  Let $\mathcal{O}$ be an obfuscator for the family of point functions $\mathtt{I}_{\mathcal{Z}}$.  Let  $C\subset \zo^\ell$ be an error-correcting code.
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item If $c_i = 0$: $p_i = \mathcal{O}(I_{w_i})$.
\item Else: Sample $r_i \overset{\$}\leftarrow \mathcal{Z}$.
\subitem Let $p_i = \mathcal{O}(I_{r_i})$.
\end{enumerate}
\item Output $(c, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item If $p_i(w_i') = 1$: set $c_i' = 0$.
\item Else: set $c_i' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.15in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

The input $w$ is hidden in two different ways.  In locations where $c_i=1$ the block $w_i$ is information-theoretically unknown.
In locations where $c_i=0$ it is hard to find $w_i$ given access to the point obfuscation.  
There are two possible reasons for a bit $c_i'$ to be $1$.  Because the true value was $1$ and because $w_i \neq w_i'$.  However if a bit $c_i'$ is $0$ this likely means that $w_i=w_i'$ because collisions when $c_i=0$ are unlikely~(occurring with probability $1/|\mathcal{Z}|$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips.

\consref{cons:first construction} is secure if no distinguisher can tell whether they are working with random obfuscations or obfuscations of $W_i$.  By using the security of point obfuscation, anything learnable from the obfuscation is learnable from oracle access to the function.  More formally, our construction is secure as long as enough blocks are unpredictable after adaptive queries:

\begin{definition}
\label{def:block guessable}
Let $I_w = (I^1,\dots,I^\ell)[q(n)]_{w_1,..., w_\ell}(\cdot, \cdot)$ be an oracle that returns \[(I^1,\dots,I^\ell)[q(n)]_{w_1,..., w_\ell}(i, w_i')=
\begin{cases}
1 & w_i = w_i'\\
0 & \text{otherwise}.
\end{cases}
\]
A source $W = W_1||...|W_\ell$ is a $(q, \alpha, \beta)$-\emph{unguessable block distribution} if there exists a set $\mathcal{I}\subset\{1,..., \ell\}$ of size at least $\ell -\beta$ such that for any unbounded adversary $A$ with oracle access to $I$ making at most $q$ queries
\[
\forall i\in \mathcal{I}, \Hav(W_i |View(A^{I_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}

We discuss unguessable block distributions in \apref{sec:characterize}.  \bnote{there should be probably be something here.  block-sources?}

\begin{theorem}
\label{thm:main thm first cons}
Let $\mathcal{W}$ be a family of $(q,\alpha= \omega(\log n),  \beta)$-unguessable block distributions for any $q = \poly(n)$ where each $W_i$ is over $\mathcal{Z}$ where $|\mathcal{Z}| = \omega(\poly(n))$.  Furthermore, let $C$ be a $(\neigh_t, \delta_{code})$-code.  Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for $\mathtt{I}_{\mathcal{Z}}$.  Then for $s_{sec} = \poly(n)$ there exists some $\epsilon=\ngl(n)$ such that \consref{cons:first construction} is a $(\mathcal{Z}^\ell, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta_{code} + \ell/|\mathcal{Z}|$ and resulting entropy $\tilde{m} =\log |C| - \beta$.
\end{theorem}
\begin{proof}
We argue security in~\lemref{lem:security of cons} and correctness in~\lemref{lem:correct of cons}.
\end{proof}

\begin{lemma}
\label{lem:security of cons}
Let all variables be as in \thref{thm:main thm first cons}.  For every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{obf}}( C | P ) \geq |C| - \beta$.% for $\epsilon' = 2\epsilon_{obf} + (\ell-\beta)2^{-(\alpha - 1)}$.
\end{lemma}

We give a brief outline of the proof here, the proof is in \apref{app:security of main cons}.
\begin{proof}[Outline]
It is sufficient to show that there exists a distribution $C'$ with conditional min-entropy and $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P))\le \ngl(n)$.  Let $\mathcal{I}$ be the set of indices that exists in \defref{def:block guessable}, the distribution $C'$ is defined as a uniform codeword conditioned on the values of $C$ and $C'$ being equal on all indices outside of $\mathcal{I}$.  We first note that $C'$ has sufficient entropy.  That is, $\Hav(C' |P) =\Hav(C' | C_{\mathcal{I}^c} = C'_{\mathcal{I}^c}) = |C| - |\mathcal{I}^c|$.  It is left to show $\delta^{\mathcal{D}_{s_{sec}}}((C, P), (C', P)) \le \ngl(n)$.
%Define the distribution $X$ as follows:
%\[X_i =
%\begin{cases}
%W_i & C_i = 0\\
%R_i & C_i = 1.
%\end{cases}\]
The outline for the rest of the proof is as follows:
\begin{itemize}
\item Let $D$ be a distinguisher between $(C, P)$ and $(C', P)$, since $P$ is a collection of obfuscated programs there exists a simulator, $S$~(outputting a single bit), such that $\Pr[D(C, P)=1]$ is close to $\Pr[S^{\mathcal{O}}(C)=1]$.
\item Show that even an unbounded $S$ making a polynomial number of queries to the stored points cannot distinguish between $C$ and $C'$.  That is, $\Delta(S^{\mathcal{O}}(C),S^{\mathcal{O}}(C'))$ is small.
\item By the security of obfuscation, $\Pr[S^{\mathcal{O}}(C')=1]$ is close to $\Pr[D(C', P)=1]$.
\end{itemize}
\end{proof}
We now argue correctness of \consref{cons:first construction}. 
We begin by showing that the probability of a single $1\rightarrow 0$ bit flip in $c$ is negligible.  
%Most of the effort in showing the correctness of \consref{cons:first construction} is showing that $\dis(w, w')\leq t$ implies $\dis(c, c')\leq t$.  However, we start by justifying our use of unidirectional codes.
\begin{lemma}
\label{lem:no 1 to 0 flips}
Let all variables be as in \thref{thm:main thm first cons}.  
The probability of at least one $1\rightarrow 0$ bit flip~(an obfuscation of a random block being interpreted as the obfuscation of the point) is $ \ell/|\mathcal{Z}| = \ngl(n)$.
\end{lemma}
\begin{proof}
Recall that $c$ contains at most $\ell$ $1$s.  Since $w'$ is chosen independently of the points $r_i$, this probability is the size of each block, that is $\Pr[r_i =w_i']  = 1/|\mathcal{Z}|$. Since the $r_i$ are chosen independently, one has,
\[
\Pr[\text{no $1\rightarrow 0$ flips}] \geq 1-\Pr[\text{some }r_i = w_i']\geq 1-\sum_{i=1}^\ell \Pr[r_i = w_i'] \geq 1-\ell \frac{1}{|\mathcal{Z}|}.
\]
\end{proof}

We now consider the number of possible $0\rightarrow 1$ bit flips in $c$.  A $0\rightarrow 1$ flip on index $i$ occurs when two conditions are met $w_i\neq w_i'$ and $c_i = 0$.  Since the first conditioned is only fulfilled at most $t$ times, we have the following lemma:

\begin{lemma}
\label{lem:correct of cons}
Let all variables be as in \thref{thm:main thm first cons}.  
For $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$
\[
\Pr[\rep( w', P) = R] \geq 1-\left(\frac{\ell}{|\mathcal{Z}|}+\delta_{code}\right).
\]
That is, \consref{cons:first construction} is correct with error at most $\ell/|\mathcal{Z}|+\delta_{code}$.
\end{lemma}

\textbf{Note: }By using a more structured type of code, the necessary error tolerance may be decreased from $t$ to  $(1/2+\Omega(1))t$ by arguing that roughly half of the mismatches between $w, w'$ occur where $c_i =1$.  This requires a code where most codewords have Hamming weight close to $1/2$.

\subsection{Discussion}
\label{sec:discussion}
Security of \consref{cons:first construction} relies on a large number of dimensions which are unpredictable.  Correction occurs for a small number of dimensions that are allowed to vary completely.  \consref{cons:first construction} is not secure for an arbitrary min-entropy source, it must contain a significant number of dimensions that are hard to guess by equality queries.

To show that $\Huse$ can be negative for \consref{cons:first construction}, we first calculate the size of the Hamming ball.  We allow $t$ errors with an alphabet $\mathcal{Z}$ of size $2^n$.  This means that
\begin{align*}
\log |B_t| &= \log \sum_{i=0}^t {\ell \choose i} (|\mathcal{Z}|-1)^i\\
&> \log {\ell \choose t} (|\mathcal{Z}|-1)^t\\
& =\Theta(t\log |\mathcal{Z}|) + \log {\ell\choose t}
% \log (2^n)^{H_{2^n}(t/\ell)\ell - o(\ell)} =n( H_{2^n}(t/\ell)\ell -o(\ell) )
\end{align*}

We consider what entropy is necessary for security.  The simplest type of unguessable block distribution is where each block is independent and has super-logarithmic entropy~(\clref{cl:independent high ent}).  For this type of source the required entropy is $\Hoo(W) = \ell\omega(\log n)$.  This yields:
\[
\Huse = \Hoo(W) - \log |B_t| < \ell \omega(\log n) -\left( \Theta(t\log |\mathcal{Z}|) + \log {\ell \choose t}\right).
\]
When $t =\Theta(\ell)$ and the entropy of each block is $o(\log |\mathcal{Z}|)$, then $\Huse\le 0$ and the output entropy is $|C| -\beta$~(if $C$ is a constant rate, this is $\Theta(\ell)$).
%Thus, \consref{cons:first construction} achieves for security for unguessable block distributions with negative $\Huse$ when $t = \Theta(\ell)$.  %We achieve security when $\Huse\le 0$, constant error tolerance, and extract a constant fraction of $n$ when $t = \Theta(\ell)$ and $\ell = O(n)$.
%When $ \ell , t, n$ to be the same order provides a natural balance of parameters.

\textbf{Limitations of \consref{cons:first construction}:}  There are three major drawbacks to \consref{cons:first construction}.   First, we require blocks to be super-polynomial in size.  Second, we only obtain a single bit from each block.  Third, since each block is individually obfuscated, we leak information about individual blocks.  To compensate for this leakage, we need most blocks to be unguessable given equality queries.  Our second construction addresses this last weakness.

\section{Trading Errors for Entropy}
\label{sec:sampling}
\consref{cons:first construction} is a computational fuzzy conductor with $\Huse\le 0$.  Unfortunately, it requires many blocks of $W$ to have super-logarithmic min-entropy.  This may be an unreasonable requirement.  In this section, we reduce the required entropy of blocks by obfuscating several blocks simultaneously, at the price of  decreasing the effective error tolerance. 
%In this section, we present a generalization where blocks are not individually obfuscated, instead several blocks are obfuscated together~(these blocks are selected using an oblivious sampler). 
The main idea is to sample a random subset of blocks $W_{i_1},..., W_{i_\eta}$ and obfuscate the concatenation of these blocks.  Denote this concatenated value by $V_1$.  This process is repeated to produce $V_1,..., V_\ell$ and the construction proceeds by either obfuscating $V_i$ or a random point as before. For security each value $V_i$ needs to be unguessable.  This will hold as long as enough blocks contribute some entropy:

\begin{definition}
\label{def:partial source}
A distribution $W = W_1,..., W_\gamma$ is an $(\alpha, \beta)$-partial block source if there exists a set of indices $\mathcal{I}$ where $|\mathcal{I}| \geq \gamma - \beta$ such that the following holds:
\[
\forall i\in \mathcal{I}, \forall w_1,..., w_\gamma \in W_1,..., W_\gamma, \Hoo(W_i | W_1 = w_1,..., W_{i-1}=w_{i-1}) \geq \alpha.
\]
\end{definition}

\defref{def:partial source} is a weakening of block sources~(introduced by Chor and Goldreich~\cite{DBLP:journals/siamcomp/ChorG88}) as only some blocks are required to have entropy conditioned on the past.  The need for remaining entropy conditioned on the past is arbitrary.  The precise requirement is that there exists some ordering of indices where most items have entropy conditioned on all previous items in this ordering~(for example, a ``partial'' reverse block source~\cite{vadhan2003constructing}).
Note a partial block source requires worst case entropy while an unguessable block distribution only requires average conditional entropy. \lnote{this comparison is unclear to me}  However, we achieve security for significantly lower entropy levels than for unguessable block distributions.  

We use an algorithm that selects a fixed size random subset of $\{1,..., \gamma\}$.  Let $\sample_{\gamma, \eta}(\cdot)$ be an algorithm that outputs a random subset of $\{1,..., \gamma\}$ of size $\eta$ and let $r_{sam}$ the number of required coins for $\sample_{\gamma, \eta}$.

\begin{construction}[Sample-then-Obfuscate]
\label{cons:sampling}
Let $n$ be a security parameter.
Let $\mathcal{Z}$ be an alphabet, and let $W = W_1,..., W_\gamma$ be a source where each $W_i$ is over $\mathcal{Z}$. %and $\gamma = \Omega(n)$.  Let $\ell = \omega(\log(n))$ and $\eta = o(\gamma)$ be integer parameters (we will set them later). Let $C\subset \zo^\ell$ be a $(\neigh_{t'}, \delta_{code})$. 
Let $\eta$ be a parameter, $C\subset\zo^\ell$ be an error-correcting code and $\mathcal{O}$ be an obfuscator for the family of point functions over $\mathtt{I}_{\mathcal{Z}^\eta}$.  Define $\gen, \rep$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\gamma$
\item Select $c\overset{\$}\leftarrow C$.
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Select $\lambda_i\overset{\$}\leftarrow \zo^{r_{sam}}$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\eta,\gamma}( \lambda_i)$
\item If $c_i = 0$:
\subitem Set $v_i = w_{j_{i,1}},..., w_{j_{i, \eta}}$.
\subitem Set $\rho_i = \mathcal{O}(I_{v_i})$.
\subitem Set $p_i = \rho_i, \lambda_i$.
\item If $c_i = 1$: Select $r_i \overset{\$}\leftarrow \mathcal{Z}^{\eta}$.
\subitem Let $p_i = \mathcal{O}(I_{r_i}), \lambda_i$.
\end{enumerate}
\item Output $(c, p)$, where $p=p_1\dots p_\ell$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$
\item For $i=1,..., \ell$:
\begin{enumerate}[(i)]
\item Parse $p_i$ as $\rho_i, \lambda_i$.
\item Set $j_{i, 1},..., j_{i, \eta}\leftarrow \sample_{\gamma, \eta}(\lambda_i)$.
\item Set $v_i' = w_{j_{i, 1}},..., w_{j_{i, \eta}}$.
\item If $\rho_i(v_i') = 1$ set $c_i' = 0$.
\item Else set $c_i' = 1$.
\end{enumerate}
\item Set $c = \decode(c')$.
\item Output $c$.
\end{enumerate}
\vspace{0.37in}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

The main change in \consref{cons:sampling} is that the obfuscated values are concatenated symbols of $W$.  This paradigm is similar to \emph{sample-then-extract} from the locally computable extractors literature~\cite{lu2002hyper,vadhan2003constructing}.  For this reason we call \consref{cons:sampling} \emph{sample-then-obfuscate}.  A crucial difference is that the use of a computational primitive~(obfuscation) allows us to sample multiple times, because we only need to argue each $v_i$ has only individual entropy, as opposed to the information-theoretic setting, where it would be necessary to argue about the joint entropy of $v_i$s. 

This construction uses a na\"{i}ve sampler that takes truly random samples, but the public randomness may be substantially decreased by using more sophisticated samplers. Goldreich provides an introduction to samplers in~\cite{goldreich1997sample}.


\begin{theorem}
\label{thm:sampling}
Let $n$ be a security parameter.
Let $\mathcal{W}$ be the family of $(\alpha = \Omega(1), \beta\leq \gamma(1-\Theta(1)))$-partial block sources over $\mathcal{Z}^\gamma$ where $\gamma = \Omega(n)$.   Let $\eta$ be such that $\eta = \omega(\log n)$ and $\eta = o(\gamma)$,  and $\ell$ be such that $\ell = O(\poly(n))$ and
$\ell = \omega(\log n)$.  Let $C$ be a $(\neigh_{t'}, \delta_{code})$ where $t' = \Theta(\ell)$.  Then for every $s_{sec} = \poly(n)$ there exists some $\epsilon_{sec} = \ngl(n)$ such that \consref{cons:sampling} is a $(\mathcal{Z}^\gamma, \mathcal{W}, \tilde{m}, t)$-computational fuzzy conductor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ for
\begin{align*}
t&\le \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta} \\
\tilde{m} &=|C|\\
\delta &= O(\ell/|\mathcal{Z}|+2^{-\ell} +\delta_{code}).
\end{align*}
\lnote{we should explain what this t means -- the formula is too complex. Also, we seem to require large alphabet size -- is it only because of $\delta$?}
\end{theorem}


The next two sections are dedicated to proving this theorem.
 For security we argue that each of the $v_i$ values is unguessable.  For correctness we show that the induced error rate in $v$ and $v'$ is a small constant~(with overwhelming probability), so that $c'$ will be corrected to $c$ with overwhelming probability.

\subsection{Security of \consref{cons:sampling}}
In order to show security \consref{cons:sampling}, we 
show that with overwhelming probability, at each of the $\ell$ iterations, the sampler will choose enough coordinates of $W$ that have high entropy, making $V_i$ have sufficient entropy.   We can then argue that $V_1,..., V_\ell$ forms a block-unguessable distribution.  Then \consref{cons:sampling} is just \consref{cons:first construction} applied to $V_1,.., V_\ell$, and security follows by \lemref{lem:security of cons}.  We begin by showing that each $V_i$ is statistically close to a high entropy distribution~(proof in \apref{sec:proof of sampling lemma}).   Let $\Lambda$ represent the random variable of all the coins used by $\sample$ and $\lambda=\lambda_1 \dots \lambda_\ell$
be some particular outcome.

\bnote{think about how to make this work when $\eta = \Theta(1)$.  Will make probabilities gross.  Probably not for submission.}
\begin{lemma}
\label{lem:sampling works}
Let all variables be as in \thref{thm:sampling}.
There exists $\epsilon_{sam} = O(e^{-\eta}) = \ngl(n)$ and $\alpha' = \alpha\eta(\gamma-\beta-\eta)/\gamma = \omega(\log n)$ such that for each $i$,
\[
\Pr_{\lambda\leftarrow \Lambda}[\Hoo(V_i | \Lambda= \lambda) \geq \alpha'] \geq 1- \epsilon_{sam}.
\]
\end{lemma}

\noindent
We can then argue that all $V_i$ simultaneously have individual entropy with good probability:
\begin{corollary}
\label{cor:samp sec}
 Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works}, and all the other variables be as in \thref{thm:sampling}.  Then $\Pr_{\lambda\leftarrow \Lambda}[\forall i, \Hoo(V_i | \Lambda = \lambda)  \ge \alpha'] \leq 1-\ell\epsilon_{sam}$.
%[V_i(V, \Lambda)$ is $(\ell\epsilon_{sam})$-close to a distribution $(V', U_{\ell\times r_{sam}})$ where for $u\in U_{\ell\times r_{sam}}$ for all $i$, $\Hoo(V_i' | U_{\ell\times r_{sam}} =u)\geq \alpha'$.
\end{corollary}
\begin{proof}
Union bound over the probability in \lemref{lem:sampling works}.
%Hybrid argument over the statistical distance in \lemref{lem:sampling works}.
\end{proof}

In ~\clref{cl:all blocks entropy} we show that any distribution where each individual block has super-logarithmic min-entropy forms a unguessable block distribution.  This allows us to conclude:
\begin{corollary}
\label{cor:v are unguessable}
Let $\epsilon_{sam}, \alpha'$ be as in \lemref{lem:sampling works},  and all the other variables be as in \thref{thm:sampling}. Take any $q=\poly(n)$.  For $\alpha'' =\alpha'-1-\log (q+1) =  \omega(\log n)$, with  probability $1-\ell \epsilon_{sam}$ over the choice of $\Lambda=\lambda$, the distribution $V| \Lambda=\lambda$ is a $(q, \alpha'', 0)$-unguessable block distribution.
\end{corollary}


Thus, unless the choice of $\lambda$ is very unlucky,
\consref{cons:sampling} is \consref{cons:first construction} applied to an unguessable block distribution $V_1,..., V_\ell$. This allows us to conclude the following.
\begin{corollary}
\label{cor:samp unguess}
Let all the variables be as in \thref{thm:sampling}.
For every $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} = \ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C | P) \geq |C|$. %for $\epsilon' = (2\epsilon_{obf} + \ell(\epsilon_{sam}+ 2^{-(\alpha''-1)})) = \ngl(n)$.
\end{corollary}
\begin{proof}
Result of \corref{cor:v are unguessable} and \lemref{lem:security of cons} noting that $\ell \epsilon_{sam} = \ngl(n)$.
\end{proof}

\subsection{Correctness of \consref{cons:sampling}}
\label{sec:correct sampling}
The argument that $1\rightarrow 0$ flips from $c$ to $c'$ are unlikely carries over from \consref{cons:first construction}.  This probability is at most $\ell/\mathcal{Z}^\eta$.   Recall that the code $C$ corrects up to $t'$ flips from $0$ to $1$. We now show that $C$ is up to the task with overwhelming probability, i.e., that  $\Pr_{(v, v')\leftarrow (V, V')}[v'\not\in\neigh_{t'}(v)] <\ngl(n)$.  The proof is in \apref{sec:sampling errors}.


\begin{lemma}
\label{lem:sampling errors}
Let all the variables be as in \thref{thm:sampling}.
 Then $\Pr[v'\in\neigh_{t'}(v)]\geq 1-O(2^{-\ell})$, where the probability is over the coins of $\gen$.  %That is, for any $t\leq \mu(\gamma-\eta)/\eta$, \consref{cons:sampling} is correct with overwhelming probability.
\end{lemma}

\begin{corollary}
Let all the variables be as in \thref{thm:sampling}.
When
\[
\dis(w, w')\le \frac{-\log(1-t'/(2\ell))}{2}\frac{\gamma-\eta}{\eta} = \Theta(\gamma/\eta)
\]
and $(c, p)\leftarrow \gen(w)$, then
\[
\Pr[\rep(w', p) = c] \geq 1-\frac{\ell}{|\mathcal{Z}|^\eta} - O(2^{-\ell}) -\delta_{code}= 1-\ngl(n).
\]
That is, \consref{cons:sampling} is correct with overwhelming probability.
\end{corollary}

\subsection{Discussion of \consref{cons:sampling}}
We now show \consref{cons:sampling} can work for partial block sources when $\Huse\le 0$.  The required entropy of  an partial block source is $\alpha\times (1-\beta )\times \gamma = \Theta(\gamma)$.  We are able to correct $O(\gamma/\eta)$ errors.
This yields:
\begin{align*}
\Huse &= \Hoo(W) -\log |B_t| \\
&< \Theta(\gamma)- t \log |\mathcal{Z}|\\
&= \Theta(\gamma) - \Theta(\gamma/\eta) \log |\mathcal{Z}|
\end{align*}
That is, \consref{cons:sampling} achieves $\Huse\le 0$ when the starting alphabet is super polynomial~(noting that for super polynomial size $\mathcal{Z}$ we can set $\eta$ to be super logarithmic and $o(\log |\mathcal{Z}|)$).  If we are willing to accept $\Huse\geq 0$ then \consref{cons:sampling} works for polynomial size alphabet $\mathcal{Z}$.  However, in this setting $\Huse =\Omega(\gamma)$ and known information-theoretic fuzzy extractors provide superior performance.
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

\section{Use of Fuzzy Extractors for Biometrics}
Fuzzy extractors derive stable from noisy sources.  Biometrics represent an important noisy physical source.  Unlike physical unclonable functions, biometrics are fixed distributions.  Biometrics cannot be redesigned to  increase their entropy or reduce their error rates.  Many biometric systems have incorporated fuzzy extractors to improve system security~(see the work of Jain, Nadakumar, and Nagar for a survey~\cite{jain2008biometric}).  Unfortunately, most work measures system security using false accept rate vs. false reject rate.  This assumes an adversary that creates inputs to the \rec algorithm without looking at the public helper data.  False accept rate provides no bound on the information leaked by the helper data.  Indeed the helper data may completely reveal the original input.  In high security applications, measuring false accept rate vs. false reject rate is insufficient to argue system security.  

\label{sec:iris no key}
As an example, we focus on the human iris.  The iris is believed to be the best biometric for high security applications~\cite{prabhakar2003biometric}.  The best estimate for iris entropy is $250$ bits~\cite{daugman2004}.  Daugman uses specialized wavelets to derive a $2048$ bit string called an iris code.  There has been considerable research since the work of Daugman~\cite{daugman2004}.  However, These works do not dramatically alter the entropy estimate or error rate necessary for a reasonable false reject rate.  We use the Daugman's parameters for our calculations.

The precise number of errors that must be tolerated depends on the desired false reject rate (how often the correct key is produced).  For a false reject rate of $\le 80\%$, a $t$ of approximately $205$ is required.%\footnote{Daugman  improves his true accept rate using a masking vector, where bits that are not considered ``trustworthy'' are excluded from the comparison.  The comparison between the two readings is then made using bits not excluded by either image.  It is not clear how to extend into the fuzzy extractor setting.  Even with the masking vector $\Huse$ is negative for irises.}  
We have the following calculation for $\Huse$:
\begin{align*}
\Huse &= \Hoo(W) - \log |B_t|\\
&= 249 - \log \sum_{i=0}^{205} {2048 \choose i} = -707.
\end{align*}
Thus, it is impossible to output a key using a fuzzy extractor that outputs the same strength key for all distributions with the same entropy and error tolerance~(see \apref{sec:minimal conditions} for minimal conditions for fuzzy extractor security).
Irises are believed to be the strongest biometric but applying current fuzzy extractors to irises yields no meaningful security.

\section{Minimal Conditions for Fuzzy Extractor Security}
\label{sec:minimal conditions}

A necessary condition for fuzzy extractor security is that an adversary should not be able to learn the key simply by inputting a point into the \rep algorithm.  This means a negligible portion of the source distribution $W$ lies within any Hamming ball.  We make this intuition formal here:

\begin{definition}
\label{def:fuzzy min-ent}
A distribution $W$ in a metric space $(\mathcal{M}, \dis)$ has $(t, k)$-fuzzy min-entropy, denoted $\Hfuzz(W) \ge k$ if the following holds:
\[
\forall m\in \mathcal{M}, Pr_{w\in W}[\dis(w, m) \leq t] \leq 2^{-k}.
\]
\end{definition}
For a metric space $\mathcal{M}$, let $\max |\mathcal{M}|$ the maximum length required to describe an element $m\in\mathcal{M}$~(for most natural metric spaces this is $\log |\mathcal{M}|$).
\begin{lemma}
\label{lem:fuzz necessary}
Let $n$ be a security parameter and let $W$ be a distribution over $(\mathcal{M}, \dis)$.
If $\Hfuzz (W) = \Theta(\log n)$ there is no $(\mathcal{M}, W, \kappa, t)$-computational fuzzy extractor that is $(\max |\mathcal{M}| +  |\rep|, \epsilon)$-hard for $\epsilon = \ngl(n)$ with error $\delta = \ngl(n)$~(and thus no fuzzy extractor) for $\kappa =\omega(\log n)$.
\end{lemma}
\begin{proof}
Let $W$ be a distribution where $\Hfuzz(W) = \Theta(\log n)$.  This means that there exists a point $m\in \mathcal{M}$ such that $\Pr_{w\in W}[\dis (w, m)\leq t] \geq 1/\poly(n)$.  Consider the following distinguisher $D$:
\begin{itemize}
\item On input $r, p$.
\item If $\rep(m, p) = r$, output $1$.
\item Else output $0$.
\end{itemize}
First note that $|D|$ is of size $\max |\mathcal{M}|+ |\rep|$.  Clearly, $\Pr[D(R, P) = 1]\geq 1/\poly(n) - \delta$, while $\Pr[D(U_\kappa, P)=1 ]\leq 1/2^{-\kappa}$.  Thus, when $\kappa = \omega(\log n)$:
\[
\delta^D((R, P), (U_\kappa, P))\geq \frac{1}{\poly(n)} -\delta -  \frac{1}{2^{-\kappa}} = 1/\poly(n).
\]
\end{proof}
\lemref{lem:fuzz necessary} generalizes to interactive protocols, $D$ only provides an input to the protocol and looks at the output.  This means that fuzzy min-entropy is also a necessary condition for interactive solution.  %The original fuzzy extractors paper of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08} separated the starting entropy of $W$ and the desired error tolerance.  
If we wish to support parameter regimes where $\Huse\le 0$ there is some distribution with $\Hfuzz(W)=0$~(consider some fixed point $m\in\mathcal{M}$ and let $W$ be the uniform distribution over points within distance $t$).  Thus, if the analysis of a fuzzy extractor is for all input distributions with a particular $\Huse$, no key can be output when $\Huse\le 0$.   This motivates our restriction to meaningful classes of source distributions with $\Huse\le 0$.
%This means we inherently must talk about the security and errors together.  

In this work we consider the Hamming distance over some alphabet $\mathcal{Z}$.  There are two minimal types of distributions where $\Hfuzz(W)\geq \omega(\log n)$, the first is where the $(t+1)$-st least entropic block has super-logarithmic entropy~(the $t$ most entropic blocks are essentially free to the adversary).  The other is that there $t+\omega(\log n)$ blocks that contribute some entropy~(there is a continuum between these two types).  Definitions~\ref{def:block guessable} and~\ref{def:partial source} were developed for these types of sources respectively.  However, we do not achieve security when $\Hfuzz(W)= \omega(\log n)$ for either definition, this is an open problem. 

\section{Fuzzy Conductors}

\label{sec:conductors}
Fuzzy extractors  have strong upper bounds on remaining entropy based on the best error correcting codes~(if they provide the same guarantee for all input distributions with the same entropy and error tolerance).  Fuller, Meng, and Reyzin show that computational information-reconciliation techniques are subject to similar bounds~\cite{fuller2013computational}.  They suggest these bounds may be avoided by outputting a fresh random variable.  This is known as a fuzzy conductor~\cite{KanukurthiR09}.
\begin{definition}
A $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-\emph{fuzzy conductor} with error $\delta$ is a pair of randomized procedures, ``generate''~(\gen') and ``reproduce''~(\rep'), with the following properties:
\begin{enumerate}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $x\in\zo^*$ and a helper string $ss\in\zo^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $ss\in\zo^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w, w')\leq t$, if $X, SS$ were generated by $(X, SS)\leftarrow \gen'(w)$, then $\Pr[\rep(w', SS) = X]\geq 1-\delta$~(over the coins of $\gen', \rep'$).  If $\dis(w, w')>t$, then no guarantee is provided about the output of $\rep'$.
\item The security property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $X$ is close to a high entropy distribution $Y$.  That is, $\exists Y$ with $\Hav(Y | SS ) \geq \tilde{m}$ such that $\Delta((X, SS), (Y, SS))\leq \epsilon$.
\end{enumerate}
\end{definition}

\noindent In this section, we show that fuzzy conductors are subject to the same lower bounds as fuzzy extractors.  This motivates our use of a computational fuzzy conductor in \defref{def:comp fuzzy cond}.
We borrow the following notation from the work of Dodis et al.~\cite{DBLP:journals/siamcomp/DodisORS08}:
\begin{itemize}
\item A $(\mathcal{M}, K, t)$ code is a subset of $\mathcal{M}$ of size $K$ where a procedure exists that corrects $t$ errors.
%\item $K(\mathcal{M}, t)$ is the largest $K$ for which there exists an $(\mathcal{M}, K, t)$-code.
\item $K(\mathcal{M}, t, S)$ is the largest $K$ such that there exists an $(\mathcal{M}, K, t)$ code all of whose $K$ points belong to $S$.
\item $L(\mathcal{M}, t, m) = \log (\min_{|S| = 2^m} K(\mathcal{M}, t, S))$.  This is the performance of worst error-correcting code for an arbitrary subset of size $2^m$.
\end{itemize}
Intuitively, $K(\mathcal{M}, t, S)$ is the size of the best code that covers a given subset and $L(\mathcal{M}, t, m)$ is size of the code that covers the hardest subset in the metric space.  
\begin{lemma}
The existence of a $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$-fuzzy conductor implies that $\tilde{m}\leq L(\mathcal{M}, t, m) +1 -\log (1-2\epsilon)$.
\end{lemma}
\begin{proof}  Assume $(\gen', \rep')$ is a fuzzy conductor with the parameters stated.  Let $A$ be a set of size $2^m$ in $\mathcal{M}$ and let $W$ be the uniform distribution over $A$.  Define $(X,SS) \leftarrow \gen'(W)$.  Then there must exist a distribution $Y$ with $\Hav(Y|SS) \geq \tilde{m}$ such that $\Delta((X,SS), (Y, SS))\leq \epsilon$.  By Markov's inequality, there exists some set $B_{SS}$ such that $\Pr[ss\in B_{SS}] \geq 1/2$ and $\forall s\in B_{SS}$ one has $\Delta(X | SS = s, s ), (Y | SS = s, s)<2\epsilon$.  Now applying Markov's inequality to $\max_{Y} \Pr[Y=y | SS=s]$, there exists a set $B_{SS'}$ such that $\Pr[SS\in B_{SS'}]>1/2$, and for all $s\in B_{SS'}$, $\Hoo(Y| SS =s ) \geq \tilde{m}-1$.  Denote by $s^*$ a value in $B_{SS}\cap B_{SS'}$~(one value must exist).  Then $\Delta((X | SS =s^* , s^*), (Y| SS = s^*, s^*))\leq 2\epsilon$ and $\Hoo(Y|SS=s^*)\geq \tilde{m}-1$.  Denote by the set $A'$ the possible values of $X$ when $SS=s^*$.  For the statistical distance property to hold, $|A'| \geq  (1-2\epsilon)2^{\tilde{m}-1}$.  Associate with every $x\in A'$ some $w\in S$ which could have produced $x$ with nonzero probability given $SS=s^*$, and call this map $C$.  $C$ defines an error correcting code with the required parameters.
\end{proof}

\section{Characterizing unguessable block distributions}
\label{sec:characterize}

\defref{def:block guessable} is an inherently adaptive definition and thus a little unwieldy for a distribution.  In this section, we partially characterize sources that satisfy \defref{def:block guessable}.
The majority of the difficulty in characterizing \defref{def:block guessable} is that different blocks may be dependent, so an equality query on block $i$ may shape the distribution of block $j$.  Thus, we begin with the case of independent blocks.  In the examples that follow we denote the adversary by $S$ as we consider security against computationally unbounded adversaries defined in VGB obfuscation~(\defref{def:obf}).

\begin{claim}
\label{cl:independent high ent}
Let $W = W_1,  ... , W_\ell$ be a source in which all blocks $W_i$  are mutually independent.  Let $\alpha$ be a parameter.  Let $\mathcal{I}\subset \{1,..., \ell\}$ be a set of indices such that for all $i\in\mathcal{I}$, $\Hoo(W_i ) =\alpha $.  Then for any $q$, $W$ is a $(q, \alpha - \log (q+1), \ell - |\mathcal{I}|)$-unguessable block distribution.  In particular, when $\alpha = \omega(\log n)$ and $q = \poly(n)$, then $W$ is a $(q, \omega(\log n), \ell - |\mathcal{I}|)$-unguessable block distribution.
\end{claim}
\begin{proof}
It suffices to show that for all $i\in \mathcal{I}, \Hav(W_i |View(S^{I_{W}(\cdot, \cdot)}) = \alpha -\log (q+1)$.
We can ignore queries for all blocks but the $i$th, as the blocks are independent. Furthermore, without loss of generality, we can assume that no duplicate queries are asked, and that the adversary is deterministic (we can hardwire the best coins as nonuniform advice). Let $A_1, A_2, \dots A_q$ be the random variables representing the oracle answers for an  adversary $S$ making $q$  queries about the $i$th block. Each $A_j$ is just a bit, and at most one of them  is equal to 1 (because duplicate queries are disallowed). Thus, the total number of possible responses is $q+1$. Thus, we have the following,
\begin{align*}
\Hav(W_i | View(A^{\mathcal{O}_{W}(\cdot, \cdot)}) &= \Hav(W_i| A_1, \dots, A_q)\\
&=\Hoo(W_i) - |A_1, \dots, A_q|\\
&=\alpha - \log (q+1)\,,
\end{align*}
where the second line follows from the first by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
\end{proof}
\noindent In their work on computational fuzzy extractors, Fuller, Meng, and Reyzin~\cite{fuller2013computational} show a construction for block-fixing sources, where each block is either uniform or a fixed symbol~(block fixing sources were introduced by Kamp and Zuckerman~\cite{KZ07}).  \clref{cl:independent high ent} shows that \defref{def:block guessable} captures, in particular, this class of distributions.
However, \defref{def:block guessable} captures more distributions.  We now consider more complicated distributions where blocks are not independent.

\begin{claim}
\label{cl:each block from single seed}
Let $f:\zo^e \rightarrow \mathcal{Z}^\ell$ be a function.  Furthermore, let $f_i$ denote the restriction of $f$'s output to its $i$th coordinate.  If for all $i, f_i$ is injective then $W = f(U_e)$ is a $( q, e - \log (q+1), 0)$-unguessable block distribution.
\end{claim}
\begin{proof}
Since $f$ is injective on each block, $\Hav(W_i | View(S^{I_{W}(\cdot, \cdot)})) = \Hav(U_e | View(S^{I_{W}(\cdot, \cdot)}))$.  Consider a query $q_j$ on block $i$.  There are two possibilities: either $q_j$ is not in the image of $f_i$,  or $q_j$ can be considered a query on the preimage $f_i^{-1}(q_j)$. Then (by assuming $S$ knows $f$) we can eliminate queries which correspond to the same value of $U_e$.  Then the possible responses are strings with Hamming weight at most $1$ (like in the 
proof of \clref{cl:independent high ent}),
 and by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08} we have for all $i$, $\Hav(W_i | View(S^{I_{W}(\cdot, \cdot)})) \geq \Hoo(W_i) -\log (q+1)$.
\end{proof}

Note the total entropy of a source in \clref{cl:each block from single seed} is $e$, so this is a family of distributions with total entropy $\omega(\log n)$ for which \consref{cons:first construction} is secure.  For these distributions, all the coordinates are as dependent as possible: one determines all others.

We can prove a slightly weaker claim when the correlation between the coordinates $W_i$ is arbitrary:

\begin{claim}
\label{cl:all blocks entropy}
Let $W = W_1,..., W_\ell$ be a source.  Suppose that for all $i$, $\Hoo(W_i)\geq \alpha$, and that $q \le 2^{\alpha}/4$ (this holds asymptotically, in particular, if $q$ is polynomial and $\alpha$ is superlogarithmic). Then  $W$ is a $(q, \alpha-1-\log(q+1), 0)$-unguessable block distribution.
\end{claim}

\begin{proof}
Intuitively, the claim is true because the oracle is not likely to return 1 on any query. Formally, we proceed by induction on oracle queries,
using the same notation as in the proof of   \clref{cl:independent high ent}. Our inductive hypothesis is
that $\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0] \leq (j-1)2^{1-\alpha}$.  If the inductive hypothesis holds, then, for each $i$,  
\begin{equation}
\label{eq:cond-entropy}
\Hoo(W_i | A_1= \dots= A_{j-1}=0) \ge \alpha-1\,.
\end{equation}
This is true for $j=1$ by the condition of the theorem. It is true for $j>1$ because, as a consequence of the definition of $\Hoo$,
for any random variable $X$ and event $E$, $\Hoo(X|E)\ge \Hoo(X)+\log\Pr[E]$; and $(j-1) 2^{1-\alpha}\leq 2 q 2^{-\alpha} \leq 1/2$.  

We now show that $\Pr[A_1\neq 0 \vee \dots \vee A_{j}\neq 0] \leq j2^{1-\alpha}$, assuming that $\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0] \leq (j-1)2^{1-\alpha}$.
\begin{align*}
\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0 \vee A_j\neq 0] & = 
\Pr[A_1\neq 0 \vee \dots \vee A_{j-1}\neq 0]+\Pr[A_1=\dots = A_{j-1}=0 \wedge A_j=1]\\
& \le  (j-1)2^{1-\alpha}+\Pr[A_j=1\,|\,A_1=\dots = A_{j-1}=0]\\
& \le  (j-1)2^{1-\alpha}+\max_i 2^{-\Hoo(W_i | A_1=\dots =A_{j-1}=0)}\\
& \le  (j-1)2^{1-\alpha}+ 2^{1-\alpha}\\
& = j 2^{1-\alpha}
\end{align*}
(where the third line follows by considering that to get $A_j=1$, the adversary needs to guess some $W_i$, and the fourth line follows by~\eqref{eq:cond-entropy}).
Thus, using $j=q+1$ in~\eqref{eq:cond-entropy},
 we know $\Hoo(W_i | A_1= \dots= A_q=0) \ge \alpha-1$.  Finally this means that
\begin{align*}
\Hav(W_i | A_1,\dots, A_q) &\ge -\log \left( 2^{-\Hoo(W_i | A_1= \dots= A_q=0)}\Pr[A_1=\dots=A_q=0]+1\cdot \Pr[A_1\neq 0 \vee \dots \vee  A_q\neq 0] \right)\\
& \ge -\log \left(  2^{-\Hoo(W_i | A_1= \dots= A_q=0)}+q2^{1-\alpha} \right)\\
& \ge -\log \left(  (q+1) 2^{1-\alpha}\right) = \alpha-1-\log(q+1)\,.
\end{align*}
\end{proof}

Claims~\ref{cl:each block from single seed} and~\ref{cl:all blocks entropy} rest on there being no easy ``entry'' point to the distribution.  This is not always the case.  Indeed it is possible for some blocks to have very high entropy but lose all of it after equality queries.

\begin{claim}
Let $p = (\poly(n))$, let $X = U_{\log p\times \ell}$ be a distribution and let $f_1,..., f_{\ell}$ be injective functions where $f_i:\zo^{i\times \log p}\rightarrow \zo^n$.\footnote{Here we assume that $n\ge \ell \times \log p$, that is the source has a small number of blocks.}  Then define the distribution $W_1 = f_1(U_{1,...,\ell}), W_2 = f_2(U_{1,..., 2\ell}),...., W_\ell = f_\ell(U)$.  There is an adversary making $2^e\times \ell = \poly(n)$ queries such that $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{claim}
\begin{proof}
We present an adversary $S$~(running in polynomial time) that completely determines the value $x$.  $S$ computes $y_1^1 = f_1(x_1^1),..., y_1^p = f(x_1^p)$.  Then $S$ queries on $y_1,..., y_p$, exactly one answer returns $1$.  Let this value be $y_1^*$ and its preimage $x_1^*$.  Then $S$ computes $y_2^1 = f_2(x_1^*,x_2^1), ..., y_2^p= f_2(x_1^*, x_2^p)$ and queries $y_2^1,..., y_2^p$.  Again, exactly one of these queries returns $1$.  This process is repeated until all of $x$ is recovered.  The total space complexity of this algorithm can be reduced to a single query~(by computing $y$ as necessary) as its total time is $O(p\times \ell)$.  Once $x$ has been recovered then $\Hav(W | View(S^{I_W(\cdot, \cdot)})) = 0$.
\end{proof}

The previous example relied on an adversaries ability to completely determine a block from the previous blocks.  We formalize this notion next.  We defining the entropy jump of a block source as the remaining entropy when other blocks are known:

\begin{definition}
Let $W = W_1,..., W_\ell$ be a source under ordering $i_1,..., i_\ell$.  The \emph{jump} of a block $i_j$ is $J(i_j) = \max|W_{i_j} |$ conditioned on the values $W_{i_1},..., W_{i_{j-1}}$.
\end{definition}

\noindent
We now show that there must be a super-logarithmic jump early enough.

\begin{claim}
Let $W$ be a distribution and let $q$ be a parameter, if there exists an ordering $i_1,..., i_\ell$ such that for all $j\le \ell-\beta +1$, $J(i_j) = \log q /\ell$, then $W$ is not $(q, 0, \beta)$-unguessable.
\end{claim}

\begin{proof}
For convenience relabel the ordering that violates the condition as $1,..., \ell$.  We describe an unbounded adversary that determines $W_1,..., W_{\ell-\beta+1}$.  As before $A$ queries the $q /\ell$ possible values for $W_1$ and determines $W_1$.  Then $A$ queries the $q/\ell$ possible values for $W_2 | W_1$.  This process is repeated until $W_{\ell-\beta+1}$ is learned.  Note the optimum ordering for the adversary can be encoded nonuniformly but it may take an unbounded amount of time/space to construct the support of $W_i | W_1,.., W_{i-1}$.
\end{proof}

\section{Proof of \lemref{lem:cond and cext}}
\label{sec:cond and cext}
\begin{proof}
It suffices to show if there is some distinguisher $D'$ of size $s'$ where
\[\delta^{D'}((\cext(X; U_d), U_d, SS), (U_\kappa, U_d, SS))>\epsilon_{cond}+ \epsilon_{ext}\]
 then there is an distinguisher $D$ of size $s_{cond}$ where for all $Y$ where $\Hav(Y|SS) \geq \tilde{m}$ such that
 \[
 \delta^{D}((X, SS), (Y, SS))\geq \epsilon_{cond}.
 \]
Let $D'$ be such a distinguisher.  That is,
\[
\delta^{D'}(\cext(X, U_d)\times U_d \times SS, U_\kappa\times U_d\times SS)> \epsilon_{ext}+\epsilon_{cond}.
\]
Then define $D$ as follows.  On input $(y, ss)$ sample $seed\leftarrow U_d$, compute $r\leftarrow \cext(y; seed)$ and output $D(r, seed, ss)$.  Note that $|D| \approx s' + |\cext| +d= s_{cond}$.  Then we have the following:
\begin{align*}
\delta^{D}((X, SS), (Y, SS))&= \delta^{D'}((\cext(X, U_d), U_d, SS), \cext(Y, U_d), U_d, SS)\\
&\geq \delta^{D'}((\cext(X, U_d), U_d, SS), (U_\kappa\times U_d \times SS)) - \delta^{D'}((\cext(Y, U_d), U_d, SS), (U_\kappa\times U_d \times SS))\\
&>\epsilon_{cond}+\epsilon_{ext}- \epsilon_{ext} = \epsilon_{cond}.
\end{align*}
Where the last line follows by noting that $D'$ is of size at most $s_{ext}$.  Thus $D$ distinguishes $X$ from all $Y$ with sufficient conditional min-entropy.  This is a contradiction.
\end{proof}

\section{Proof of \lemref{lem:security of cons}}
\label{app:security of main cons}
\begin{proof}

Let $\mathcal{O}$ be a $\ell$-composable VGB obfuscator with auxiliary input for point programs over $\mathcal{Z}$.  Let $W$ be a $(q, \alpha = \omega(\log n), \beta)$-unguessable block distribution.  Our goal is to show that for all $s_{sec} = \poly(n)$ there exists $\epsilon_{sec} =\ngl(n)$ such that $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P)\geq |C|- \beta$. % for $\epsilon' = 2\epsilon_{obf} + (\ell - \beta)2^{-(\alpha-1)}$.
Suppose not, that is suppose there is some $s_{sec} = \poly(n)$ such that exists $\epsilon_{sec} = \poly(n)$ and $H^{\hill}_{\epsilon_{sec}, s_{sec}}(C|P) < |C|-\beta$.

By \defref{def:block guessable} there exists a set of indices $\mathcal{I}$ such that all blocks within $\mathcal{I}$ are unguessable.  Define by $C'$ the distribution of sampling a uniform codeword where all locations outside $\mathcal{I}$ are fixed.  Then $C | C_{\mathcal{I}^c} \overset{d}=C'$.  By \clref{cl:many locations ent}, we have that $\Hav(C|C_{\mathcal{I}^c} )= |C| -\beta$ and thus $\Hav(C'| C_{\mathcal{I}^c}) = |C| -\beta$.  
Let $D$ a distinguisher of size be some distinguisher of size at most $s_{sec}$ such that 
\[
| \expe[D(C, P)] - \expe[D(C', P)] > \epsilon_{sec} = 1/\poly(n).
\]  
Define the distribution $X$ as follows:
\[X_i =
\begin{cases}
W_i & C_i = 0\\
R_i & C_i = 1.
\end{cases}\]  By the security of obfuscation~(\defref{def:obf}), there exists a unbounded time simulator $S$~(making at most $q$ queries) such that
\begin{align}
\label{eq:dist before}
|\expe [D(P_1,..., P_\ell, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
We now prove $S$ cannot distinguish between $C$ and $C'$.
\begin{lemma}
\label{lem:sim cannot distinguish}
$\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})) \le (\ell-\beta) 2^{-(\alpha+1)}$.
\end{lemma}

\begin{proof}
\noindent It suffices to show that for any two codewords that agree on $\mathcal{I}^c$, the statistical distance is at most $(\ell-\beta)2^{-(\alpha+1)}$.
\begin{lemma}
\label{lem:codewords in I close}
Let $c^*$ be true value encoded in $X$ and let $c'$ a codeword in $C'$.  Then,
\[
\Delta( S^{I_X(\cdot, \cdot)}(c^*, 1^{\ell \log |Z|}), S^{I_X(\cdot, \cdot)}(c', 1^{\ell \log |Z|})) \le ( \ell -\beta) 2^{-(\alpha+1)}.
\]
\end{lemma}
\begin{proof}
Recall that for all $i\in \mathcal{I}$, $\Hav(W_i | View(S))\geq \alpha$.  The only information about the correct value of $c_i^*$ is contained in the query responses.  When all responses are $0$ the view of $S$ is identical when presented with $c^*$ or $c'$.  We now show that for any value of $c^*$ all queries on $i \in \mathcal{I}$ return $0$ with probability $1-2^{-\alpha+1}$.  Suppose not, that is suppose, the probability of at least one nonzero response on index $i$ is $> 2^{-(\alpha+1)}$.  Since $w, w'$ are independent of $r_i$, the probability of this happening when $c^*_i = 1$ is at most $q/2^n$ or equivalently $2^{-n+\log q}$.  Thus, it must occur with probability:
\begin{align}
2^{-\alpha+1}&<\Pr[\text{non zero response location }i]\nonumber \\
 &= \Pr[c_i^* =1]\Pr[\text{non zero response location }i\wedge c_i^*=1]\nonumber \\&+ \Pr[c_i^*=0] \Pr[\text{non zero response location }i \wedge c_i^*=0]\nonumber \\
&\le 1\times 2^{-n+\log q} + 1\times  \Pr[\text{non zero response location }i \wedge c_i^*=0] \label{eq:ways to remove ent}
\end{align}
We now show that $n-\log q \geq \alpha$:
\begin{claim}
\label{cl:ent bounded away from n}
$\alpha \le n-\log q$
\end{claim}
\begin{proof}
It suffices to show that there exists a simulator $S$ making $q$ queries such that the remaining entropy in any block is at most $n-\log q$.  Let $W_i$ be a distribution, consider $S$ that asks the $q$ most likely outcomes, the total probability of this block must be at least $2^{-n+\log q}$.  After these queries the remaining min-entropy is at most:
\begin{align*}
\Hav(W_i | View(S)) &\leq  -\log \left(\Pr[\text{some }q_i=1]\times 1+ \Pr[\text{no }q_i=1]\times \Pr[\text{most likely outcome}|q_1,...,q_q]\right)\\
&\leq  -\log \left(\Pr[\text{some }q_i=1]\times 1\right)\\
&=-\log\left( 2^{-n+\log q} \right) = n-\log q
\end{align*}
This completes the proof of \clref{cl:ent bounded away from n}.
\end{proof}
\noindent
Rearranging terms in Equation~\ref{eq:ways to remove ent}, we have:
\begin{align*}
 \Pr[\text{non zero response location }i \wedge c_i=0] &>2^{-\alpha+1} - 2^{-(n-\log q)}=  2^{-\alpha}
 \end{align*}
 When there is a $1$ response and $c_i=0$ this means that there is no remaining min-entropy.  If this occurs with over $2^{-\alpha}$ probability this violates the block unguessability of $W$~(\defref{def:block guessable}).  By the union bound over the indices $i\in\mathcal{I}$ the total probability of a $1$ in $\mathcal{I}$ is at most $(\ell-\beta)2^{-\alpha+1}$. Recall that $c^*, c'$ match on all indices outside of $\mathcal{I}$. Thus, for all $c^*, c'$ the statistical distance is at most $(\ell- \beta)2^{-\alpha+1}$.  This concludes the proof of \lemref{lem:codewords in I close}
\end{proof}
By averaging over all points in $C'$ we conclude that $\Delta(S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|}), S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})) < (\ell -\beta)2^{-(\alpha+1)}$.  This completes the proof of \lemref{lem:sim cannot distinguish}.
\end{proof}

\noindent Now by the security of obfuscation we have that
\begin{align}
\label{eq:dist after}
|\expe [D(P_1,..., P_\ell, C') ]- \expe [S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})] |\leq \epsilon_{sec}/3.
\end{align}
Combining Equations~\ref{eq:dist before} and~\ref{eq:dist after} and \lemref{lem:sim cannot distinguish}, we have
\begin{align*}
\delta^{D}(( P, C), (P, C'))&\leq |\expe [D(P_1,..., P_\ell, C)] - \expe [S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|})]| \\
&+|\expe[S^{I_X(\cdot, \cdot)}(C, 1^{\ell \log |Z|})] - \expe[S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})] |\\
&+|\expe [S^{I_X(\cdot, \cdot)}(C', 1^{\ell \log |Z|})] - \expe [D(P_1,..., P_\ell, C') ]|\\
&\leq \epsilon_{sec}/3+ (\ell-\beta)2^{-(\alpha-1)}+\epsilon_{sec}/3 \\
&\leq 2\epsilon_{sec}/3 + \ngl(n) < \epsilon_{sec}.
\end{align*}
This is a contradiction and completes the proof of \lemref{lem:security of cons}.
\end{proof}

\section{Proof of \lemref{lem:sampling works}}
\label{sec:proof of sampling lemma}
\begin{proof}
Consider some fixed $i$.
Recall that there a set $\mathcal{I}$  of size $\gamma - \beta = \Theta(\gamma)$ such that each $w$ and  block $j\in \mathcal{I}$, $\Hoo(W_j | W_1 = w_1,..., W_{j-1}=w_{j-1}, W_{j+1}=w_{j+1},..., W_\gamma = w_\gamma) \geq \alpha$.  Since this is a worst case guarantee, the entropy of $V_i$ can be deduced from the number of symbols in $V_i$ that come from $\cal{I}$. Namely, Denote by $X= |\{j_{i, 1},..., j_{i, \eta}\}\cap \mathcal{I}|$.

\begin{claim}
\label{cl:vi have entropy}
\[
\Hoo(V_i |\Lambda = \lambda ) \geq \alpha X.
\]
\end{claim}
\begin{proof}
Denote by $j_1,..., j_\eta$ the indices selected by the randomness $\lambda_i$.  We begin by noting that $\Hoo(V_i |\Lambda = \lambda ) = -\log \max_{v\in V_i} \Pr[ V_i =v | \Lambda =\lambda] = -\log \max_{w_{j_1}, ..., w_{j_\eta}} \Pr[W_{j_1} = w_{j_1} \wedge \dots \wedge W_{j_\eta} w_{j_\eta}] $.  Then 
\begin{align*}
\max_{w_{j_1},..., w_{j_\eta}} \Pr[ W_{j_1}=w_{j_1} \wedge \dots \wedge W_{j_\eta} = w_{j_\eta}]
&= \max_{w_{j_1},..., w_{j_\eta}} \prod_{k=1}^\eta \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le \prod_{k=1}^\eta \max_{w_{j_1},..., w_{j_\eta}} \Pr[W_{j_k} = w_{j_k} | W_{j_{k-1}} = w_{j_{k-1}} \wedge ... \wedge W_{j_1} = w_{j_1}]\\
&\le\prod_{k=1}^\eta \max_{w_1,..., w_\gamma} \Pr[W_{j_k} = w_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1} ]\\
\end{align*}
Taking the negative logarithm of both sides we have that 
\begin{align*}
\Hoo(V_i | \Lambda = \lambda) &\ge \sum_{k=1}^\eta \min_{w_1,..., w_\gamma} \Hoo(W_{j_k} | W_1 = w_1 \wedge ... \wedge W_{{j_k}-1} = w_{{j_k}-1})\\
&\ge \sum_{j_k\in \mathcal{I}} \alpha = \alpha X
\end{align*}
This completes the proof of \clref{cl:vi have entropy}.
\end{proof}


We note that $X$ is distributed according to the hypergeometric distribution,
and that $\expe[X]=\eta(\gamma-\beta)/\gamma$. Using the tail bounds from~\cite{chvatal1979tail,scala2009hypergeometric}, we can conclude that $\Pr[X\le \expe[X]/2]\le e^{-2((\gamma-\beta)/2\gamma)^2 \eta}=O(e^{-\eta})$.

\bnote{this can also be tightened.}
Thus, setting $\alpha'=\frac{\alpha \eta(\gamma-\beta)}{2\gamma}$ and applying \clref{cl:vi have entropy}, we conclude that 
 \[
\Pr[\Hoo(V_i ) \geq \alpha'] \geq 1- O(e^{-\eta}).
\]
\end{proof}

\section{Proof of \lemref{lem:sampling errors}}
\label{sec:sampling errors}

\begin{proof}
Define $\mu = -\frac{1}{2}\log(1-t'/(2\ell)) = \Theta(1)$ and note that $t \leq \mu(\gamma - \eta)/\eta$. Since $\eta = \omega(\log n)$, we will assume
$\eta\ge 2\mu$.
Let
the Bernoulli random variable $X_i=1$ if and only if $v_i \neq v_i'$, and $X=\sum_{i=1}^{\ell} X_i$. We need to show that $\Pr[X>t']=O(2^{-\ell})$.

\begin{align*}
\expe [1-X_i] &=\Pr[w \text{ and } w' \text{ agree on positions } {j_{i,1}},..., j_{i,\eta}]\\
&\geq \prod_{j=0}^{\eta-1}\left(1 - \frac{t}{\gamma-j}\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu(\gamma-\eta)/\eta}{\gamma-j}\right)\\
&\geq  \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu}{\eta}\left(\frac{\gamma-\eta}{\gamma-j}\right)\right)\geq \prod_{j=0}^{\eta-1}\left( 1-\frac{\mu}{\eta}\right) \\
&= \left(1-\frac{\mu}{\eta}\right)^{\eta} =\left( \left(1-\frac{\mu}{\eta}\right)^{\eta/\mu}\right)^\mu\geq \left(\left(\frac{1}{2}\right)^2\right)^\mu\\
& = \left(\frac{1}{2}\right)^{-\log\left(1-\frac{t'}{2\ell}\right)} = 1-\frac{t'}{2\ell}\,.
\end{align*}

Hence, $\expe[X_i]\le t'/(2\ell) = O(1)$, and $\expe[X]\le t'/2$.
By the Chernoff bound, we have \lnote{this can be tightened}
\begin{align*}
\Pr\left[\sum_{i=1}^\ell X_i\geq t'\right]\leq 2e^{-2(t'-\expe[X])^2\ell} \le
2e^{-2(t'/2)^2\ell} = O(e^{- \ell})\,.
\end{align*}
\end{proof}




\end{document} 