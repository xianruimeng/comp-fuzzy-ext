\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}

\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode} 

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}

\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\rnote}[1]{{\authnote{Ran}{#1}}}
\newcommand{\onote}[1]{{\authnote{Omer}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}

\title{Constant Rate Computational Fuzzy Extractors}
\author{Some people}
\begin{document}
\maketitle


%\begin{abstract} 
%We make computational fuzzy extractors better.
%\end{abstract}


\section{Introduction}\label{sec:introduction}
Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} derive stable keys from noisy data.  They are composed of two algorithms: \gen takes a source value $w_0$, and produces a key $r$ and a public value $p$, and \rep which takes a nearby value $w_1$ and $p$ producing the original key $r$.  Intuitively, they are two functions: information reconciliation~(to provide the same value with \emph{noisy} readings) and key derivation or privacy amplification~(to transform an entropic source into a uniform key).  Traditionally, these two functions were performed by two separate primitives: a secure sketch~\cite{DBLP:journals/siamcomp/DodisORS08} and a randomness extractor~\cite{nisan1993randomness}.

For a fixed length key, there are two main parameters: the desired error tolerance and the required entropy of the source.  In the information-theoretic setting these two parameter are at odds.  Any increase error-tolerance requires a similar increase to the entropy of the source.  However, this does not need to be the case in the computational setting.  The notion of computational fuzzy extractors was introduced by Fuller, Meng, and Reyzin~\cite{fuller2013computational}.\footnote{The only difference between a computational fuzzy extractor and a fuzzy extractor is the key is pseudorandom instead of statistically random. }  With computational security, an error tolerance \emph{larger} than the starting entropy may be possible.  We provide the first construction of a fuzzy extractor where the entropy of the source distribution is smaller than the desired error-tolerance.\footnote{Security is impossible if all points of the source distribution are close together.  In the fuzzy extractor setting, a necessary condition for security is that a negligible portion of the source distribution lies in within any error-tolerance region.   This is both a necessary and sufficient condition for security in the interactive setting using two-party computation.}

Our construction is based on obfuscation.  While black-box obfuscation of polynomial time functions is known to be impossible~\cite{barak2001possibility}, we use the obfuscation of point functions.  This class is known to be achievable under particular number-theoretic assumptions~\cite{canetti1997towards} and generically from generic cryptographic hardness assumptions~\cite{wee2005obfuscating}.  

\textbf{Overview of Construction:}  Consider a source $w = w_1,..., w_\ell$ that is split into blocks.  We would like to construct tolerate Hamming errors for individual blocks.  The main idea is for each block, $i$: either the $w_i$ is obfuscated or a random point $r_i$ is obfuscated~(this idea is similar to the construction of digital lockers from point obfuscation).  Then with a close value $w'$ the recovery value knows where random values were obfuscated and where $w_i$ was obfuscated.  To tolerate errors, the choice of obfuscating $w_i$ or a random point is made using bits of an error correcting code.

\textbf{Connection with Noisy Point Obfuscation: } Ideally, our construction would be a noisy point obfuscation~(which is a strictly stronger object than a computational fuzzy extractor).  Unfortunately, this is not the case.  Since each block is individual obfuscated, equality of individual blocks is learned~(for sources where not all blocks have high entropy).  This precludes virtual black-box~(VBB) security where no partial information about the input may be learned.  Dodis and Smith~\cite{DBLP:conf/stoc/DodisS05} constructed noisy point obfuscation when the source entropy is significantly higher than the error-tolerance.  Constructing VBB obfuscation for sources where the error tolerance is smaller than the starting entropy is an important open question.

\textbf{Our Results:} Specifically this paper presents the following:
\begin{itemize}
\item The first computational fuzzy extractor that supports a constant fraction of errors.
\item The first fuzzy extractor where the number of errors corrected is larger than the entropy of the starting distribution.
\end{itemize}
\bnote{More details here}

\bnote{Provide an outline of the rest of the paper}

\section{Preliminaries}
\label{sec:preliminaries}
For a random variable $X = X_1||...|| X_n$ where each $X_i$ is over some alphabet $\mathcal{Z}$, we denote by $X_{1,..., k} = X_1||...|| X_k$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.  
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. 
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t(\cdot)|$ the size of a ball of radius $t$.  For our sources, we consider the Hamming metric, for two vectors $x, y$ over $\mathcal{Z}^n$ the Hamming distance between $x,y$ is $d(x,y) = \{i | x_i \neq y_i\}$.  For the Hamming metric, $|B_t(\cdot)| = \sum_{i=0}^t {n \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.  However, we will use codes that only correct one-sided errors~(see \secref{sec:coding theory}).


Usually, we use bold letters for vectors or matrices, capitalized letters for random variables, and lowercase letters for elements in a vector or samples from a random variable. 

\subsection{Randomness Extractors and Computational Fuzzy Extractors}
\label{sec:fuzzy extractors}

Here we present relevant extractor definitions.  We start with the standard notion of an average-case extractor. An average-case extractor is a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}) (in particular, Vadhan~\cite[Problem 6.8]{Vad12} showed that all strong extractors are average-case extractors with a slight loss of parameters):
\begin{definition}
Let $\chi_1$, $\chi_2$ be finite sets.
A function $\ext: \chi_1\times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(m, \epsilon)$-average-case extractor} if for all pairs
of random variables $X, Y$ over $\chi_1, \chi_2$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\Delta((\ext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon$.
\end{definition}

We focus on computational fuzzy extractors introduced by Fuller, Meng, and Reyzin~\cite{fuller2013computational}.  Definitions for information-theoretic fuzzy extractors and secure sketches can be found in the work of Dodis et. al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}.  The definition of computational fuzzy extractors allows for a small probability of error, as described in~\cite[Sections 8]{DBLP:journals/siamcomp/DodisORS08}.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.  

%\begin{definition}
%\label{def:fuzzy extractor}
%An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$, with the following properties: 
%\begin{enumerate}
%\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\ell$ and a helper string $p\in\{0,1\}^*$.
%\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w,w')\leq t$, if $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$ with probability~(over the coins of $\gen, \rep$) at least $1-\delta$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
%\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
%\end{enumerate}
%A fuzzy extractor is efficient if $\gen$ and $\rep$ run in expected polynomial time.
%\end{definition}
%
%Secure sketches are the main technical tool in the construction of fuzzy extractors.  Secure sketches produce a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
%\begin{definition}
%\label{def:secure sketch}
%An $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$, with the following properties:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|\sketch(W))\geq \tilde{m}$.
%\end{enumerate}
%A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time. 
%\end{definition}
%

We now present the definition of a computational fuzzy extractor~(from~\cite{fuller2013computational}):

%A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}. 
%
%\begin{lemma}
%\label{lem:fuzzy ext construction}
%Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext:\mathcal{M}\times \zo^d \rightarrow \zo^\ell$ be a $(\tilde{m}, \epsilon)$-average-case extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor with error $\delta$:
%\begin{itemize}
%\item $\gen(w):$ generate $x\leftarrow \zo^d$, set $p=(\sketch(w), x), r=\ext(w;x)$, and output $(r,p)$.
%\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $r=\ext(w;x)$.
%\end{itemize}
%\end{lemma}

\begin{definition}[Computational Fuzzy Extractor]\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\kappa$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\kappa, P))\leq \epsilon$.
\end{itemize}
\end{definition}
Any efficient fuzzy extractor is also a computational fuzzy extractor with the same parameters.  
Note that in the above definition of fuzzy extractors, the errors are chosen before $P$ is known: if the error pattern between $w$ and $w'$ depends on the output of $\gen$, then there is no guarantee about the probability of correctness.
For notational convenience, we the definition of computational fuzzy extractors specifies a general class of sources for which the fuzzy extractor is designed to work, rather than the class of sources that consists of all sources of a given min-entropy $m$~(of course, this modification can also be applied to prior definitions of information-theoretic secure sketches and fuzzy extractors).


Our main question is: for what type of sources can we build fuzzy extractors?  Of particular interest are sources where information-theoretic bounds do not allow meaningful constructions~\cite[Lemmas C.1 and C.2]{DBLP:journals/siamcomp/DodisORS08}.  The main limiting factor is the difference between the starting entropy and the size of the ball to be corrected or $gap = \Hoo(W) - \log |B_t(\cdot)$.  When $gap$ is large, information-theoretic fuzzy extractors provide a good solution.  We ask whether security is possible when $gap$ is small or negative.
%In this paper, we ask whether a smaller entropy loss can be achieved by considering a computational fuzzy extractor with a computational security requirement.  %We therefore relax the security requirement of \defref{def:fuzzy extractor} to require a pseudorandom output instead of a truly random output.  

As in the information-theoretic realm~\cite[Lemma 4.1]{DBLP:journals/siamcomp/DodisORS08}, a computational fuzzy extractor can be constructed by combining a correction component and an privacy-amplication component.  As shown in Fuller, Meng, and Reyzin~\cite[Corollary 3.8 and Theorem 3.10]{fuller2013computational}, the ``right'' correction component is probably not a computational version of a secure sketch~\cite[Definition 3]{DBLP:journals/siamcomp/DodisORS08}.  Instead, we will use a computational fuzzy conductor~(this is a computational analogue of the notion introduced by Kanukurthi and Reyzin~\cite{KanukurthiR09}).  We use the common notion of HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99} extended to the conditional case~(see \defref{def:hill ent}): 

\begin{definition}
\label{def:comp fuzzy cond}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$.  A pair of randomized procedures ``generate'' (\gen') and ``reproduce'' (\rep') is a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t$)-computational fuzzy conductor that is $(\epsilon, s_{sec})$-hard with error $\delta$ if $\gen'$ and $\rep'$ satisfy the following properties:
\begin{itemize}
\item The generate procedure $\gen'$ on input $w\in \mathcal{M}$ outputs a string $Y\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure $\rep'$ takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(Y, P)\leftarrow \gen'(w)$, then $\Pr[\rep'( w', P) = Y] \geq 1-\delta$ where the probability is over the randomness of $(\gen', \rep')$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of $\rep'$.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $Y$ has high HILL entropy conditioned on $P$, that is $H^{\hill}_{\epsilon, s_{sec}}(Y |P)\geq \tilde{m}$.
\end{itemize}
\end{definition}

Combining a computational fuzzy conductor and an appropriate randomness extractor yields a computational fuzzy extractor:

\begin{lemma}
\label{lem:cond and ext}
Let $\epsilon_{ent} >0$ be a parameter.  Let $\gen'$, $\rep'$ be a $(\mathcal{M}_1, \mathcal{W}, \tilde{m}, t)$-computational fuzzy extractor that is $(\epsilon_{sec}, s_{sec})$-hard with error $\delta$ and outputs in $\mathcal{M}_2$.  Let $\ext:\mathcal{M}_2\times \zo^d\rightarrow \zo^\kappa$ be a $(\tilde{m} - \log 1/\epsilon_{ent}, \epsilon_{ext})$-average case extractor.  Define the $\gen, \rep$:
\begin{itemize}
\item $\gen(w; r, x):$ run $(y, p')= \gen'(w; r)$ and set $p = (p', x)$ and $r = \ext(y; x)$,  and output $(r, p)$.
\item $\rep(w, (p', x)):$ recover $y = \rec'(w'; p')$ and output $r = \ext(y; x)$. 
\end{itemize}
Then $\gen, \rep$ is a $(\mathcal{M}_1, \mathcal{W}, \kappa, t)$-computational fuzzy extractor that is $(\epsilon_{sec}+\epsilon_{ext}+\epsilon_{ent}, s_{sec} - |\ext|)$-hard with error $\delta$.
\end{lemma}
\begin{proof}
Result of extractibility of conditional HILL entropy~\cite[Lemma 5]{DBLP:conf/eurocrypt/HsiaoLR07}.\bnote{I don't think you need this $\epsilon_{ent}$ should be able to just use average case extractor directly}
\end{proof}

\subsection{Coding Theory}
\label{sec:coding theory}
We introduce some notions from the field of binary error correcting codes.  Usually the standard class of errors  is all points within Hamming distance $t$, we will use the Hamming analog of the $Z$-channel where there are flips from $0\rightarrow 1$ but no bit flips from $1\not\rightarrow 0$.
\begin{definition}
\label{def:hamming z channel}
For a point $c\in \zo^\ell$ define $\neigh_t(c) $ as the set of all points where at most $t$ bits $c_i$ are changed from $0$ to $1$. 
\end{definition}
\textbf{Note:} Any code that corrects $t$ Hamming errors corrects $t$ $0\rightarrow 1$ errors, but more efficient codes may be exist for these errors.

\begin{definition}
Let $\neigh_t(c)$ be as in \defref{def:hamming z channel}.  Then a set $C$~(over $\zo^\ell$) is a $(\neigh_t, \epsilon)$-code if there exists an efficient procedure $\rec$ such that for all $c\in C, \forall c'\in \neigh(c), \Pr[\rec(c') \neq c] \leq \epsilon$.
\end{definition}

We need one special property of the code, namely that each output bit is set $1$ with probability $1/2$.

\begin{definition}
A binary $(\neigh_t, \epsilon_{code})$ code is output equiprobable\bnote{Find the right name for this} $\forall i, |\{C | C_i =1\}| =|C|/2$.
\end{definition}
All binary linear codes~(without constant bits) are output equiprobable and there exist constant rate output equiprobable codes that correct errors for $t = O(\ell)$~(first constructed by Justesen~\cite{justesen1972class}).  This is true as long as $C$ contains no constant bits.  Usually, if a code has constant bits, we can truncate those bits before encoding and obtain a code with better parameters.  For the remainder of this work when we say a code, we assume there are no constant bits.

\begin{lemma}
Let $C$ be a \emph{linear} binary code with no constant bits.  Then $C$ is output equiprobable.
\end{lemma}
\begin{proof}
Suppose there is some $i$ that is not equiprobable.  Let $C_{i_1} = \{C | C_i =1\}$ and $C_{i_0} = \{C | C_i = 0\}$. First note that since $C$ does not have any constant bits, it dimension is at least $1$ yielding that $|C| = 2^k$ for some $k$. First suppose that $0<|C_{i_1}| < |C_{i_0}|$.  Let $c\in C_{i_1}$ then by linearity $\forall c'\in C_{i_0}$, $c\oplus c' \in C_{i_1}$.  Furthermore, for all distinct $c_1, c_2\in C_{i_0}$, $c_1\oplus c \neq c_2\oplus c$.  This means that $|C_{i_i}|\geq |C_{i_0}|$ as all $c'\oplus c$ must be distinct and contained in $C_{i_1}$.  
Now suppose that $|C_{i_1}| > |C_{i_0}| >0$, denote the elements of $C_{i_1}$ by $c_1,..., c_{|C_{i_1}|}$, then by linearity $c_1\oplus c_2, c_1\oplus c_3,..., c_1\oplus c_{|C_{i_1}|}$ are all distinct elements of $C_{i_0}$ this means that $|C_{i_0}|\geq |C_{i_1}| -1 $, for the total size to be even this means that $|C_{i_0}|\geq |C_{i_1}|$.  This is a contradiction and completes the proof.
\end{proof}



\subsection{Obfuscation}
The standard notion of obfuscation is virtual black-box obfuscation~(introduced by Barak et al.~\cite{barak2001possibility}).  This notion is known to be impossible in a black-box way for all polynomial time programs.  Several variants~(best possible obfuscation~\cite{goldwasser2007best}, indistinguishability obfuscation~\cite{barak2001possibility}, differing inputs indistinguishability obfuscation~\cite{barak2001possibility}) have been presented~(see Varia~\cite{varia2010studies} for implications between various definitions).  Indistinguishability obfuscation has recently been shown to be constructible~\cite{garg2013candidate} using multilinear maps~\cite{garg2013multilinear}.

We present the notion of virtual black-box obfuscation with the weakening that the quality of the simulator is an arbitrarily small $1/\poly$ as all known point obfuscations satisfy this definition.

\begin{definition}~\cite{barak2001possibility, goldwasser2005impossibility}  Let $\mathcal{C}$ be a family of polynomial-size circuits.  A PPT algorithm $\mathcal{O}$ is an obfuscator for $\mathcal{C}$ with dependent auxiliary input if the following conditions are met:
\begin{enumerate}
\item \emph{Approximate Functionality:}  There exists a negligible function $\epsilon$ such that for every $n\in \mathbb{N}$, every circuit $C\in \mathcal{C}_n$, and every $x\in\zo^n$, 
\[
\Pr[\mathcal{O}(C; r)(x) = C(x)] > 1-\epsilon(n),
\]
where the probability is taken over the randomness $r$.  \emph{Almost exact functionality} is a stronger condition that requires $\mathcal{O}(C;r)\equiv C$ with overwhelming probability ver the random coin tosses $r$.  Finally, if this probability always equals $1$, then $\mathcal{O}$ has \emph{exact functionality}.
\item \emph{Polynomial Slowdown:}  There exists a polynomial $\psi$ such that for every $n$, every circuit $C\in \mathcal{C}_n$, and every possible $r$, the circuit $\mathcal{O}(C; r)$ run-in time at most $\psi(n)$.
\item \emph{Virtual Black-box:}  For every PPT adversary $A$ and polynomial $\rho$, there exists a PPT simulator $S$ such that for all sufficiently large $n$, for all $C\in \mathcal{C}_n$, for all auxiliary inputs $z\in \zo^*$, 
and for all binary predicates $\pi$, 
\[
|\Pr[A(\mathcal{O}(C), z) = \pi(C, z)] - \Pr[S^C(1^n, z) = \pi(C, z)] | < \frac{1}{\rho(n)}
\]
where the first probability is taken over the coin tosses of $A$ and $\mathcal{O}$, and the second probability is taken over the coin tosses of $S$.  Furthermore, the runtime of $A$ and $S$ must be polynomial in the length of their first input.
\end{enumerate}
\end{definition}

In this work, we will use obfuscation of point-programs, 
\[
I_w(x):\begin{cases} 1 & x=w\\0 & \text{otherwise}\end{cases}
\]

We provide the construction of Canetti~\cite{canetti1997towards}, as an illustration of how point functions can be obfuscated.  Let $G$ be a group:
\begin{enumerate}  
\item Input: string $w\in \zo^n$.
\item Choose a generator $g\overset{\$}\leftarrow G$
\item Compute $h\leftarrow g^w$, where $w$ is viewed as an element of $G$ is some canonical way.
\item Output: circuit that has $g, h$ hardwired, and on input $x$, accepts iff $g^x=h$.
\end{enumerate}
\begin{assumption}[Strong DDH assumption]\label{ass:strong ddh}.  Let $n$ be a security parameter and let $p = 2q+1$ be a randomly chosen $n$-bit safe prime.  Consider the group $Q$ of quadratic residues in $\mathbb{F}_p^*$.  For any $W$ with $\Hoo(W)= \omega(\log n)$ where the domain of $W$ is $\mathbb{F}_q$, for $g\overset{\$}\leftarrow Q, a\leftarrow W, b,c \overset{\$}\leftarrow \mathbb{F}_q$, the ensembles $\langle g, g^a, g^b, g^{ab}\rangle$ and $\langle g, g^a, g^b, g^c\rangle$ are computationally indistinguishable.
\end{assumption}
\begin{theorem}~\cite{canetti1997towards}
The construction above, when instantiated with the group $G = Q$ is a virtual black-box obfuscator for the family of point functions under \assref{ass:strong ddh}.
\end{theorem}

As described above there exist other constructions of point function obfuscates under various assumptions~\cite{lynn2004positive, wee2005obfuscating}.

Lastly, we will need the notion of composable obfuscation.

\begin{definition}[Composable obfuscation~\cite{bitansky2010strong, canetti2008obfuscating,lynn2004positive}].  A PPT algorithm $\mathcal{O}$ is a $t$-\emph{composable obfuscator} for the family $\mathcal{C}$ with dependent auxiliary input if functionality and polynomial slowdown hold as before, and the virtual black black-box property holds whenever the adversary and simulator are given up to $t$ circuits in $\mathcal{C}$.  That is, for every PPT $A$ and polynomial $\rho$, there exists a PPT $S$ such that for all sufficiently large $n$, and for all $C_1,..., C_t\in \mathcal{C}_n$, and for all auxiliary inputs $z\in \zo^*$, 
\[
|\Pr[A(\mathcal{O}(C_1), ..., \mathcal{O}(C_t), z) = 1] - \Pr[S^{C_1,..., C_t}(1^n, z) = 1]| < \frac{1}{\rho(n)},
\]
where the probabilities are taken over the random coins tosses of $A, S$ and $\mathcal{O}$.  The runtimes of $A$ and $S$ must be polynomial in the length of their first input.
\end{definition}

\section{A Constant Rate Computational Fuzzy Extractor}

Before describing our construction, we recall the problem we are trying address.  The goal is to derive strong keys from noisy sources.  As discussed in the introduction, the important parameter is the difference between the total entropy and the number of errors being corrected.  We call this value the entropy gap or $gap= \Hoo(W) - \log|B_t(\cdot)|$.  The results of Dodis et al.~\cite[Lemmas C.1 and C.2]{DBLP:journals/siamcomp/DodisORS08} show that gap is an upper bounded for the length of the derived key. 

If $gap> \omega(\log n)$~(ignoring losses due to randomness extraction), then a key of any length can be formed by using an information-theoretic fuzzy extractor~(which yields nearly this many bits using an optimal code) and expanding the output with a pseudorandom generator~(or using a secure sketch with a computational extractor).  Using an information-theoretic analysis of a fuzzy extractor this seems to be the best construction possible.  As described in Fuller, Meng, and Reyzin~\cite{fuller2013computational}, use of a computational fuzzy extractor may allow a construction when $gap = O(\log n)$.  Their construction is only applicable for high entropy input and yield a result when $gap$ is small.  We will provide the first construction where $gap$ is negative.  

\subsection{First $gap<0$ construction}
\begin{construction}
\label{cons:first construction}
Let $n$ be a security parameter and let $W = W_1,..., W_\ell$ be a distribution over $\zo^{\ell n}$.  Let $\mathcal{O}$ be a $\ell$-composable obfuscator for the family of point functions over $\zo^n$.  Let $t$ be the desired error-tolerance and let $C\subset \zo^\ell$ be an output equiprobable $(\neigh_t, \epsilon_{code})$-error-correcting code.  Let $\ext : \zo^n\times \zo^* \rightarrow \zo^r$ be a $(\tilde{m}, \epsilon_{ext})$-extractor where $\tilde{m}= XXX$. \bnote{fill this in}
We describe $\gen, \rep$ as follows:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w = w_1,..., w_\ell$
\item Sample $c\leftarrow C, seed\leftarrow \zo^*$.
\item For $i=1,..., \ell$:
\subitem If $c_i = 0$: $p_i = \mathcal{O}(I_{w_i})$.
\subitem If $c_i = 1$: Sample $r_i \leftarrow U_n$. 
\subsubitem Let $p_i = \mathcal{O}(I_{r_i})$.
\item Let $r = \ext(c, seed)$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p, seed)$ 
\item For $i=1,..., \ell$:
\subitem If $p_i(w_i') = 1$ set $c_i' = 0$.
\subitem Else set $c_i' = 1$.
\item Set $c = \decode$.
\item Output $r  = \ext (c', seed)$.
\end{enumerate}
\vspace{0.45in}
\end{minipage} 
\end{tabular}
\end{center}
\label{cons:informal construction}
\end{construction}

\textbf{Notes:}  We make several observations about the above construction:
\begin{itemize}
\item If $C$ is output equiprobable, in expectation, half of $w$ is information-theoretically unknown conditioned on $p$.  As described in~\cite[Section 3.3]{fuller2013computational} some type of lossiness is necessary to avoid coding bounds.
\item There are two possible reasons for a bit of $c_i'$ to be $1$.  Because the true value was $1$~(there is little chance of $1$ being incorrectly decoded as $0$ assuming $w_i'$ is independent of the sketch) and because $w_i \neq w_i'$.  However if a bit of $c_i'$ is $0$ this likely means that $w_i=w_i'$ because collisions when the $c_i=0$ are unlikely~(occurring with probability roughly $2^{-n}$).  This is the reason for the use of a code that only corrects $0\rightarrow 1$ flips, instead of a code that corrects all Hamming weight $t$ errors.
\item The extractor can be removed from the construction leaving a computational fuzzy conductor~(\defref{def:comp fuzzy cond}).
\end{itemize}

\subsection{Security of Construction}
Assuming virtual black-box obfuscation, we can argue security in the presence of equality oracles for each block.  The adversary is provided with equality oracles for each block individually.  Security rests on an adversaries inability to learn the values using only \emph{adaptive} equality queries.  For illustrative purposes, we begin with the setting where blocks are independent and each have super-logarithmic entropy and then move to the setting of correlated blocks with some entropy deficient blocks.

\begin{lemma}
Let $n$ be a security parameter.  Let $W(n) = W = W_1,..., W_t$ be a distribution where each $W_i$ is independent of all $W_j$ and $\forall i, \Hoo(W_i) = \omega(\log n)$.  Then let $\mathcal{A}$ be an adversary making a polynomial number of oracle equality queries for blocks $i$.  %Let $C = c_1,.., c_\ell$ be as in \consref{cons:informal construction}. 
Then $\forall i, \Hav(W_i | View(A))  = \omega(\log n)$.
\end{lemma}
\begin{proof}
Consider a particular block $i$, we measure how an adversaries queries affect the entropy of this block~(we can ignore the other blocks as they are independent).  
Let $q_w$ be a query asking if the stored value is $a_w$ and its corresponding response.  
Let $Q_{w_1},A_{w_1},..., Q_{w_q}, A_{w_q}$ be the random variables representing the queries and answers for an  adversary $\mathcal{A}$ making $q$ queries.  We assume a deterministic adversary (the adversary is unbounded and thus can compute the best possible queries).  The only dependence between $W_i$ and this view of the adversary is in $A_1,..., A_q$ so we can consider these binary responses.  (There is dependency between $W_i$ and the queries but it is all contained in $A_{w_1},..., A_{w_q}$.)  Our goal is to count the total number of possible responses $A_{w_1},..., A_{w_q}$.  There are two basic cases for $A_{w_1},..., A_{w_q}$: the all zeros string and the case where some query returns $1$.  If some query $Q_{w*}$ returns $1$ then all other $Q_{w_i} = Q_{w*}$ will return $1$ but no other query returns $1$.  These responses can be removed as the response is deterministic.  Let $A'$ represent the sequence with all duplicate queries removed.  This sequence has at most a single $1$.  Thus, the total number of possible responses is $q+1$.  Thus, we have the following,
\begin{align*}
\Hav(W_i | View(A)) &= \Hav(W_i| Q_{w_1}, A_{w_1},..., Q_{w_q}, A_{w_q})\\
&=\Hav(W_i | A_{w_1},..., A_{w_q})\\
&=\Hav(W_i |A') \\
&=\Hoo(W_i) - \log |A'|\\
&= \omega(\log n) - \log |A'|\\
&= \omega(\log n) - \log (q+1) = \omega(\log n) - O(\log(n)) = \omega(\log n)
\end{align*}
Where the fourth line follows from the third by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}.
This argument holds for all $i$.  This completes the proof.
%There are two possible outcomes to an equality query $1$ in which case $\Hoo(W|Q = q_w) = 0$ and $0$ in which case $\Hoo(W| Q =q_w ) \geq -\log 2^{-\Hoo(W)}(1 - 2^{-\Hoo(W)})$~(every remaining outcome is scaled by the $w$ not being a possible outcome).  Thus,
%\begin{align*}
%\Hav(W | Q_w) &= -\log \left(\Pr[W=w]2^{-0} + \Pr[W\neq w] 2^{-\Hoo(W)}(1 - 2^{-\Hoo(W)})\right)\\
%&\geq -\log \left(2^{-k} + (1-2^{-k})2^{-k}(1-2^{-k})\right)\\
%&\geq -\log \left(2^{-(k)}(1+(1-2^{-k})^2)\right)\geq k-1
%\end{align*}
%Now consider the distribution $W' = W | W\neq w$, by the same argument asking a single query of $W'$ reduces it min-entropy by at most one bit~(we need only consider the case where previous responses were all zeros as the entropy is already $0$ if a previous response is $1$).  Thus, for some polynomial number of queries $n_q$ we have that $\Hav(W | Q_1,..., Q_{n_q}) = k - n_q$.  Thus for $k = \omega(\log n)$ and $n_q = \poly(n)$\bnote{this is too big a loss} %we have that $k-n_q = \omega(\log n)$.  This completes the proof of the lemma. 
\end{proof}
\begin{lemma}
\label{lem:super log insist by equal queries}
Let $W$ and $U$ be two distributions of super-logarithmic entropy.  Then for all adversaries $A$, $q = poly(n)$ queries, $\Delta(View(A^{\mathcal{O}_W(\cdot)}), View(A^{\mathcal{O}_U(\cdot)}))\leq \ngl(n)$.
\end{lemma}
\begin{proof}
It suffices to show that with the probability of all responses being $0$ is overwhelming.  Clearly, in the case when all responses are $0$ the views are identical, and thus the statistical distance is $0$.  \bnote{Finish this proof}
\end{proof}

\begin{lemma}
\label{lem:code bits statistically indist}
Let $X_i = W_i$ if $c_i = 1$, otherwise let $X_i = U$.  Then, for every $i$, $\Delta(View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)})| c_i = 1, View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)} | c_i =0) \leq \ngl(n)$.
\end{lemma}
\begin{proof}
\bnote{Do this}
\end{proof}
We can extend this lemma to the entire codeword $C$:
\begin{lemma}
For all $c_1, c_2 \in C$ we have the following: 
\[
\Delta(View(A^{\mathcal{O}_{(X_{c_{1_1}},..., X_{c_{2_\ell}})}(\cdot, \cdot)})| C = c_1, View(A^{\mathcal{O}_{(X_{c_{2_1}},..., X_{c_{2_\ell}})}(\cdot, \cdot)} | C = c_2)) \leq \ngl(n).
\]
\end{lemma}
\begin{lemma}
If $C$ is output then, for all $i$, $\Hav(C_i |View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)}))\geq 1 - \ngl(n)$.
\end{lemma}
\begin{proof}
Since $C$ is output equiprobable we know that for each $i$, $\Pr[C_i =1 ] = 1/2$.  Let $w$ be the exponent of the particular negligible value above~(note $w = \omega(1)$).  For $b\in\{0,1\}$, by \lemref{lem:code bits statistically indist}, we have that in expectation each for each $i$, $\Pr[C_i =1 ] = 1/2$. 
\begin{align*}
\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\Pr[C_i =b | view ] &\leq 1/2 + \ngl(n)\\
\end{align*}
If this was not true~(that is in expectation, there was some $b$ that was likely with probability greater than $1/2+1/\poly(n)$), there there would exist an unbounded adversary outputting that $b$, contradicting \lemref{lem:code bits statistically indist}).  
Taking the negative log of each side one has:
\begin{align*}
\Hav(C_i |  View(A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot)}))
 &=\\-\log \left(\expe_{view \leftarrow View(A^{\mathcal{O}_{(X_1,..., X_\ell)}})}\max_{b\in\{0,1\}}\Pr[C_i =b | view ]  \right) &\geq -\log (1/2 + \ngl(n))\\
&\geq -\log(\frac{1}{2} + \frac{1}{n^{w}} )= -\log \left(\frac{n^{w}+2}{2n^{w}}\right)\\
%&=-\left(\log (n^w+2) -\log 2 -\log n^w \right)\\
&=1-\left(\log (1+\frac{2}{n^w}) \right)\\
&=1-\frac{1}{\ln 2}\left(\ln (1+\frac{2}{n^w}) \right)\\
&= 1- \frac{1}{\ln 2}\left(\sum_{i=1}^\infty \frac{(-1)^{i+1}}{i} (\frac{2}{n^w})^i\right) \\
&\geq 1 - \frac{1}{\ln 2} \frac{2}{n^w} = 1-\ngl(n)
\end{align*}
Here the second to last line is derived using the Taylor expansion for $\ln(1+x)$ and the last line is derived by noting it is a geometric series and thus all terms $i=3,....$ are contained in the second term $i=2$ which is positive.
\end{proof}
By similar reasoning we can extend this lemma to the entire codeword:
\begin{lemma}
If $C$ is output equiprobable then, $\Hav(C | View(A^{\mathcal{O}_{(X_{c_{2_1}},..., X_{c_{2_\ell}})}})) \geq |C| - \ngl(n)$.
\end{lemma}
%\begin{lemma}
%$\Hoo(C | View (A^{\mathcal{O}_{(X_1,..., X_\ell)}(\cdot, \cdot)})) \geq \log |C| - o(\ell \log n)$.
%\end{lemma}
\begin{corollary}
\consref{cons:first construction} is a $XXXXX$ computational fuzzy extractor.
\end{corollary}
\begin{proof}
Result of security of point obfuscation and \lemref{lem:cond and ext}.
\end{proof}

\subsection{Correctness of Construction}
\section{Supported Sources}
We now describe a set of high entropy sources appropriate for our construction.  After giving the description of the distribution, we give examples of distributions that fit in this class and provide additional characterizations.

\begin{definition}
Let $\mathcal{O}_{w_1,..., w_k}$ be an oracle that return $\mathcal{O}_{w_1,..., w_k}(i, w_i')=\left( w_i\overset{?}=w_i'\right)$.
A source $W = W_1||...|W_k$ is a $(q, \alpha, \ell)$-\emph{guessable block distribution} if for any adversary $A$~(not bounded in time or space) with oracle access to $\mathcal{O}$ making at most $q$ queries there exists a set $S$ of size at least $\ell$ such that 
\[
\forall i\in S, \Hav(W_i |View(A^{\mathcal{O}_{W}(\cdot, \cdot)}))\geq \alpha.
\]
\end{definition}
This type of source seems to be inherently adaptive and is difficult to formulate in a clean information-theoretic notion.  However, we can show necessary and sufficient types of sources for a guessable block distribution.  We begin by defining the entropy jump of a block source which will appear in our characterization.

\begin{definition}
Let $W = W_1,..., W_k$ be a source under ordering $i_1,..., i_k$.  The \emph{entropy jump} of a block $i_j$ is $J(i_j) = \Hav(W_{i_j} | W_{i_1},..., W_{i_{j-1}})$.
\end{definition}

\begin{claim}
Let $W$ be a $(q, \alpha, \ell)$-guessable block distribution.  Then for all orderings $i_1, ...., i_k$ there is some $j\leq k-\ell$ such that $J(i_j)> \log q + \alpha$.
\end{claim}
\begin{proof}
Suppose not, for convenience assume the ordering that violates this condition is $1,..., k$.  We describe a distribution $W$ where $\forall 1\leq j\leq k-\ell$, $J(j)\leq \log q +\alpha$. Define $W$ as follows $W_1 = c_1,...., W_{j-1} = c_{j-1}$ are constants.
\end{proof}
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix
\section{Computational Entropy}
HILL entropy is a commonly used computational notion of entropy \cite{DBLP:journals/siamcomp/HastadILL99}.  It was extended to the conditional case by Hsiao, Lu, Reyzin~\cite{DBLP:conf/eurocrypt/HsiaoLR07}. 

\begin{definition}
\label{def:hill ent}
Let $(W, S)$ be a pair of random variables.  $W$ has 
\emph{HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, S)$, such that $\Hav(X|S)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,S))\leq \epsilon$.
\end{definition}


\end{document}