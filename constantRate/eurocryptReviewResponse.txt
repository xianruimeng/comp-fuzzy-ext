Thank you for the reviews and feedback.  We will incorporate your comments into our work.

Q: The second and third construction use obfuscation (for point functions) to add "reusability" 
A: The second and third construction also secure different types of sources.  The first construction requires many blocks to contribute fresh entropy conditioned on previous blocks (for some order of blocks).  The second construction requires that sampling produces a high entropy string (this class is larger than supported sources for the first construction).  The third construction allows all blocks to be determined by a single block as long as most blocks are hard to guess.

Q: Missing is an answer to the question what happens if an attacker maliciously modifies helper data in order to force the extractor to output a different key.
A: This question is known as a robust fuzzy extractor (Boyen et al., Eurocrypt 2004).  It is a separate question than reusability of a fuzzy extractor. Robust fuzzy extractors are easy in the random oracle model.  In the standard model, constructions are more fragile and utilize one-time MACs (Dodis et al., Crypto 2006).

Q:  I would like to suggest the authors to add some detailed discussion whether the result of this work can be used in general applications, where the condition of a super-constant size of alphabet might does not naturally hold.  Is it a hard challenge to find an alternative of removing this condition? 
Also the constructions are somewhat unrelated: how do they compare in terms of the classes of sources they work for, how do they apply to those sources
(biometric, PUF examples) where standard fuzzy extractors fail? It would be
great to gain intuition about whether these construction can be improved or
not.
A: The second construction provides reusability regardless of alphabet size.  (It only corrects more errors than entropy on a super-constant size alphabet.)  The constructions support different types of sources, the reusable construction does support a superset of the information-theoretic construction. 

Supporting more errors than entropy on a constant size alphabet remains a challenge.  As the reviewers note, we crucially make use of two metric spaces, the input metric and a second metric space where we correct errors.  This is to avoid stronger lower bounds that arise when working on a single space.  Known techniques for working on a single space recover the original reading (known as a secure sketch) and are subject to stronger negative results.  It is not clear how to make use of multiple metric spaces when the starting alphabet is constant size.

We have run some initial experiments and the second construction appears promising for irises where the distance is binary Hamming over 2048 bit strings.  However, there is still considerable work to verify that irises the second construction is secure on irises.  The main work is showing that  sampling producing high entropy strings.  Indeed, the main barrier to applicability is showing that known PUFs and biometrics satisfy the necessary input distribution of our constructions.

Subsequent work (Herder et al., ePrint 2014/938) increased error-tolerance by using confidence information that may be available for some PUFs and biometrics.  It may be possible to incorporate such information to our constructions to improve applicability.