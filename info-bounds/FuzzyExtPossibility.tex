 \documentclass[11pt]{article}
 
\def\shownotes{1}
\def\blinded{0}


\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode}

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}
\DeclareMathOperator*{\argmax}{arg\,max}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\key}{\ensuremath{\class{key}}\xspace}
\newcommand{\Key}{\ensuremath{\class{Key}}\xspace}

\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\crust}{\ensuremath{\class{Crust}}\xspace}
\newcommand{\inter}{\ensuremath{\class{Inter}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}
\newcommand{\guess}{\mathsf{guess}}
\newcommand{\eqdef}{\stackrel{\rm def}{=}}


\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Hfuzz}{\mathrm{H}^{\mathtt{fuzz}}_{t,\infty}}
\newcommand{\Huse}{\mathrm{H}_{\mathtt{usable}}}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\cext}{\ensuremath{\mathtt{cext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}
\newcommand{\sd}{\ensuremath{\mathbf{SD}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi

\ifnum\blinded=0
\newcommand{\blind}[1]{{#1}}
\else
\newcommand{\blind}[1]{}
\def\shownotes{0}
\fi


\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\anote}[1]{{\authnote{Adam}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Vol}{\mathsf{Vol}}

\newcommand{\Succ}{\mathsf{Succ}}
\newcommand{\Adv}{\mathbf{Adv}}
 \newcommand{\Exp}{\mathbf{Exp}}

\title{When are Fuzzy Extractors Possible?}
\blind{
\author{
Benjamin Fuller\footnote{Email: {\tt bfuller@cs.bu.edu}.  Boston University and MIT Lincoln Laboratory. The Lincoln Laboratory portion of this work was sponsored by the Department of the Air Force under Air Force Contract \#FA8721-05-C-0002.  Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government.}  \and Leonid Reyzin\footnote{Email: {\tt reyzin@cs.bu.edu}.  Boston University.} \and Adam Smith\footnote{Email: {\tt asmith@cse.psu.edu}.  Pennsylvania State University; work performed while at Boston University's Hariri Institute for Computing and RISCS Center, and Harvard University's
``Privacy Tools'' project.}}
}
\begin{document}
\maketitle

\begin{abstract}
Fuzzy extractors (Dodis et al., Eurocrypt 2004) convert repeated noisy readings of a high-entropy secret into the same uniformly distributed key. A minimum condition for the security of the key is the hardness of guessing a value that is similar to the secret, because the fuzzy extractor converts such a guess to the key.
We define \emph{fuzzy min-entropy} to quantify this property of a noisy source of secrets.

High fuzzy min-entropy is necessary for the existence of a fuzzy extractor; moreover, there is evidence that it may be sufficient when only computational security is required. Nevertheless, information-theoretic fuzzy extractors are not known for many practically relevant sources of high fuzzy min-entropy. In this work, we ask: \emph{is fuzzy min-entropy sufficient to build information-theoretic fuzzy extractors?} 
%\lnote{Or it seems sufficient under strong computational assumptions and, under weaker assumptions, in the interactive setting.}  
%\lnote{may be use``analyzing this quantity may be more fruitful than analyzing min-entropy and error-tolerance separately''}

We give a positive answer to this question when the fuzzy extractor knows the precise distribution of the physical source.  On the other hand, because it is imprudent to assume precise knowledge of a complicated distribution,  fuzzy extractors are typically designed to work for families of sources. We show that this uncertainty is an impediment to security by building a family of high fuzzy min-entropy sources for which no fuzzy extractor can exist.

We provide similar but stronger results for secure sketches, whose goal is not to derive a consistent key, but to recover a consistent reading of the secret.

\bnote{mention secure sketches?}
\bnote{keep editing}
\bnote{nearby or close?  be consistent}
\bnote{I renamed section titles to get rid of distribution aware and oblivious.  don't quite like them}
\lnote{mention computational setting?}
\lnote{mention the typical analysis that separates min-entropy and error-tolerance?}
\end{abstract}

\newpage
\tableofcontents\newpage

\section{Introduction}
Sources of reproducible secret random bits are necessary for many cryptographic applications.  In many situations these bits are not explicitly stored for future use, but are obtained by repeating the same process  (such as reading a biometric or a physically unclonable function) that generated them the first time.
However, bits obtained this way present a problem: noise \cite{daugman2004,zviran1993comparison,brostoff2000passfaces,ellison2000protecting,mayrhofer2009shake,monrose2002password,pappu2002physical,tuyls2006puf,gassend2002silicon,suh2007physical,bennett1988privacy}.  That is, when a secret is read multiple times, readings are close~(according to some metric) but not identical.  To utilize such sources, it is often necessary to remove noise, in order to derive the same value in subsequent readings.

The same problem occurs in the interactive setting, in which the secret channel used for transmitting the bits between two users is noisy and/or leaky~\cite{wyner1975wire}. Bennet, Brassard, and Robert~\cite{bennett1988privacy} identify two fundamental tasks.  The first, called information reconciliation, removes the noise without leaking significant information. The second, known as privacy amplification, converts the high entropy secret to a uniform random value.  In this work, we consider the noninteractive version of these problems, in which these tasks are performed together with a single message.

The noninteractive setting is modeled by a primitive called a fuzzy extractor~\cite{DBLP:journals/siamcomp/DodisORS08}, which consists of two algorithms.  %Consider some distribution $W$.%The goal of a fuzzy extractor is to derive stable keys from a distribution $W$ whenever the two readings~(denoted $w$ and $w'$ respectively) are within distance $t$.   T
The  generate algorithm ($\gen$)  takes an initial reading $w$ and produces an output $\key$ along with a nonsecret helper value $p$.  The reproduce~($\rep$) algorithm takes the subsequent reading $w'$ along with the helper value $p$ to reproduce $\key$.   The correctness guarantee is that the key is reproduced precisely as long as the distance between $w$ and $w'$ is at most $t$. 

The security requirement for fuzzy extractors is that $\key$ is uniform even to a (computationally unbounded) adversary who has observed $p$.   This requirement is  harder to satisfy as the allowed error tolerance $t$ increases, because it becomes easier for the adversary to guess $\key$ by guessing a $w'$ within distance $t$ of $w$ and running $\rep(w',p)$.


\paragraph{Fuzzy Min-entropy}
We introduce a new entropy notion that precisely measures how hard it is for the adversary to guess a value within distance $t$ of the original reading $w$, thus subverting the security of $\key$ by running $\rep$. Suppose $w$ is sampled from a distribution $W$.   To have the maximum chance that $w'$ is within distance $t$ of $w$, the adversary would want to maximize the total probability mass of $W$ within the ball $B_t(w')$ of radius $t$ around $w'$.
We  therefore define \emph{fuzzy min-entropy} $\Hfuzz(W) \eqdef -\log \max_{w'} \Pr[W\in B_t(w')]$.  Observe that this quantity can be bounded in terms of min-entropy: $\Hoo(W) \ge \Hfuzz(W) \ge \Hoo(W)-\log |B_t|$.

Superlogarithmic fuzzy min-entropy  is \emph{necessary} for nontrivial key extraction~(\propref{prop:fuzz necessary} formalizes the above intuition). 
However, existing constructions do not measure their security in terms of fuzzy min-entropy; instead, their security is shown to be  $\Hoo(W)$ minus some loss that is at least $\log |B_t|$ due to error-tolerance. Since $\Hoo(W)-\log |B_t| \le \Hfuzz(W)$, it is natural to ask whether this loss is necessary. This question is particularly relevant when the gap between the two sides of the inequality is high.  As an example, iris scans appear to have significant $\Hfuzz(W)$ (because iris scans for different people appear to be well-spread in the metric space~\cite{daugman2006probing}) but negative $\Hoo(W) -\log |B_t|$ \cite[Section 5]{blanton2009biometric}. We therefore ask: \emph{is fuzzy min-entropy sufficient for fuzzy extraction?} There is evidence that it may be when the security requirement is computational rather than information-theoretic---see \secref{sec:related settings}. 



\paragraph{Tight Characterization for the Case of a Known Distribution}
We show that for every source $W$ with superlogarithmic $\Hfuzz(W)$, it is possible to construct a fuzzy extractor with a superlogarithmic length $\key$ (\corref{cor:extension to fuzz ext}). We thus show that $\Hfuzz(W)$ is a necessary and sufficient condition for building a fuzzy extractor for a \emph{known distribution} $W$.  It is important to emphasize that these constructions incorporate the knowledge of the complete distribution of $W$ (and, in particular, they are not polynomial-time).


This characterization justifies $\Hfuzz(W)$ as the measure of the quality of a noisy distribution,  better than cruder measures such as $\Hoo(W)-\log |B_t|$.



\paragraph{Impossibility of Fuzzy Extractors for Families of Distributions}
Assuming full knowledge of a distribution is often unrealistic. Indeed, high-entropy distributions can never be fully observed directly and must therefore be modeled. It is imprudent to assume that the designer's model of a distribution is completely accurate---the adversary, with greater resources, would likely be able to build a better model. Therefore, fuzzy extractor designs cannot usually be tailored to one  particular source. Existing designs work for a family of sources (for example, all sources of min-entropy at least $m$ with at most $t$ errors). Thus, the design is fixed before the distribution is fully known, and the adversary may therefore know more about the distribution than the designer of the fuzzy extractor.

We show that this extra adversarial knowledge can be devastating  (\thref{thm:imposs fuzz ext}). 
Specifically, we show a family of distributions $\mathcal{W}$ such that not even a 2-bit fuzzy extractor can be secure for most distributions in  $\mathcal{W}$.  We emphasize that each distribution $W\in \mathcal{W}$ has superlogarithmic fuzzy min-entropy---in fact, $\Hfuzz(W)=\Hoo(W)$, because all points in $W$ are distance at least $t$ apart. Our proof relies on high dimensionality of $W$ and on perfect correctness of the $\rep$ procedure.


\begin{table}[h]
\begin{center}
\begin{tabular}{c  l l }
 & Known Distribution & Family of Distributions\\
\hline
Secure Sketch & Yes~(\thref{cons:leveling}) & No~(\thref{thm:imposs sketch})\\
\hline
Fuzzy Extractor & Yes~(\corref{cor:extension to fuzz ext}) & No~(\thref{thm:imposs fuzz ext}) 
\end{tabular}
\end{center}
\caption{Is fuzzy min-entropy sufficient to extract a superlogarithmic length key?  Results are information-theoretic.}
\label{tab:main results}
\end{table}


\paragraph{Stronger Results on Information Reconciliation (Secure Sketches)}
Traditionally, fuzzy extractors use a secure sketch to perform information reconciliation~(mapping $w'$ back to $w$), followed by randomness extractor~\cite{nisan1993randomness} to transform $w$ into a uniform key.  The security losses incurred in the first of these two steps dominate for typical sources and, indeed, this step is less well understood.\footnote{Randomness extractors have matching upper and lower bounds on the security loss: for every extra two bits of output key, they lose one bit of security} Formally, a secure sketch performs non-interactive information reconciliation via pair of algorithms: $\sketch$ takes $w$ and produces a nonsecret value $ss$, while $\rec$ takes a value $w'$ within distance $t$ of $w$ and uses $\sketch$ to output the original reading $w$.  

We show comparable, but stronger, results for secure sketches.  Namely, we show in \thref{cons:leveling} that secure sketches are possible if the distribution $W$ is precisely known. (In fact, we obtain our fuzzy extractors for the case of a known distribution from this result by applying a randomness extractor.) \bnote{should be citing same thing for sketch and fuzzy extractor.  its the same corollary} 

On the other hand, there is a family of sources with superlogarithmic $\Hfuzz(W)=\Hoo(W)$ for which no secure sketch correcting even a few errors is possible~(\thref{thm:imposs sketch}). The impossibility result applies even when $\rec$ is allowed to be incorrect with probability up to $1/4$ (as opposed to our fuzzy extractor impossibility result).

\subsection{Our Techniques}

\paragraph{Techniques for Positive Results for Known Distributions} We now explain how to construct a secure sketch for an arbitrary known distribution $W$ (we already explained how to construct a fuzzy extractor from it).  We begin with distributions in which all points in the support have roughly the same probability.  We call distributions of this type ``nearly flat.''  Consider some subsequent reading $w'$. To achieve correctness, the sketch algorithm must disambiguiate which point $w\in W$ within distance $t$ of $w'$ was sketched. Disambiguating multiple points can be accomplished by universal hashing, as long as the number of possible hash outputs is slightly greater than the number of possible points. Thus, our sketch is computed via a universal hash of $w$. To determine the length of that sketch, consider the heaviest (according to $W$) ball of radius $t$. Because the distribution is nearly flat, it is also the ball with the most points of nonzero probability. Thus, the length of the sketch needs to be slightly greater than the logarithm of the number of non-zero probability point in that ball. Since $\Hfuzz(W)$ is determined by the weight of that ball, the number of points cannot be too high and there will be entropy left after the sketch is published.

For an arbitrary distribution, we cannot afford to disambiguate points in the ball with the greatest number of points, because there could be too many low-probability points in a single ball despite a high $\Hfuzz(W)$.  We solve this problem by splitting the arbitrary distribution into a number of nearly flat distributions we call ``levels.''  We then write down, as part of the sketch, the level of the original reading $w$ and apply the above construction considering only points in that level.  We call this construction \emph{leveled hashing}.%: it separates the distribution $W$ into levels that are nearly flat and then hashes to disambiguate the most likely ball at that level.

\paragraph{Techniques for Negative Results for Distribution Families}
We construct a family of distributions $\mathcal{W}$ and prove impossibility for a uniformly random $W\in \mathcal{W}$~(instead of proving impossibility for a worst-case $W$).
We start by observing the following asymmetry: $\gen$  sees only the sample $w$ (obtained via $W\leftarrow \mathcal{W}$ and $w\leftarrow W$), while
the adversary knows $W$.   To exploit the asymmetry, we construct $\mathcal{W}$ so that conditioning on the knowledge of $W$ reduces the distribution to a single affine line, but conditioning on $w$ leaves the rest of the distribution uniform on the entire space. \bnote{this isn't quite accurate, the first coordinate is known.  you have no info on the rest of the support}

Then we show how the adversary can exploit the knowledge of the affine line to reduce the uncertainty about $w$ (in the secure sketch case) or $\key$ (in the fuzzy extractor case). 
In the secure sketch case, $ss$ can be used to find fixed points of $\rec(\cdot, ss)$ which, by the correctness requirement of the sketch, must be separated by minimum distance $t$. This means there aren't too many of them, so few can lie on an average line, permitting the adversary to guess one easily.

In the fuzzy extractor case, the nonsecret value $p$ partitions the metric space into regions that produce a consistent value under $\rep$ (preimages of each $\key$ under $\rep(\cdot, p)$).  For each of these regions, the adversary knows that possible $w$ lie $t$-far from the boundary of the region.  However, in the Hamming space, the vast majority of points lie near the boundary (this follows by combining the isoperimetric inequality~\cite{harper1966optimal} showing that the ball has the smallest boundary and Hoeffding's inequality~\cite{hoeffding1963probability} for bounding the volume that is $t$-away from this boundary).  This allows the adversary to rule out so many possible $w$ that, combined with the adversarial knowledge of the affine line, most regions become empty, leaving $\key$ far from uniform.

The result for fuzzy extractors is delicate.  The fact that $p$ partitions the space~(into non overlapping regions) relies on perfect correctness.  Extending this result to imperfect correctness seems challenging and is an interesting open problem.
The result also relies on the majority of points lying near boundaries of a set. Indeed, fuzzy extractors for arbitrary distributions are known in metric spaces where the boundary of a set is not the majority of its area.\bnote{cite cite cite!!!}

\subsection{Sufficiency of $\Hfuzz(W)$ in related settings}
\label{sec:related settings}
\bnote{this needs work}
We have evidence that fuzzy min-entropy is sufficient in settings related to information-theoretic fuzzy extractors.  
%We first consider the interactive setting.  
First, an alternate definition fuzzy extractors and secure sketches may provide computational security instead of information-theoretical security~\cite{fuller2013computational}.  In this setting, under the strong assumption of semantically-secure multilinear maps, fuzzy extractors and secure sketches can be constructed for any distribution $W$~\cite{BitanskyCKP14}.  We note that the secure sketch of~\cite{BitanskyCKP14} provides computational unpredictability of $W |p$.  Our negative result that implies that $W|p$ cannot have pseudoentropy~(discussion in \secref{sec:feas comp sec sketch}).

Second, we can weaken the computational assumption necessary for $\Hfuzz(W)$ to be sufficient by moving to an interactive setting.
Consider a user, Alice, using a noisy secret to authenticate with a remote server, Bob.  In this setting, Alice could initially send the value $w$ to Bob and then authenticate with subsequent value $w'$.  If the server holds $w$ and the user $w'$ the two parties can agree on a key using an interactive protocol.
%\footnote{It is dangerous to allow a remote server access to original reading $w$ as they can then impersonate the user.  Ideally, the server would keep a ``hashed'' value of $w$.  This is  done for noiseless secrets like passwords.}  
In this world, secure two-party computation can produce a key whenever the distribution $W$ has fuzzy min-entropy.  The minimal assumption necessary for two party computation is oblivious transfer~\cite{yao1986generate,lindell2009proof}.\lnote{OT, right?; we can ask Ran about their paper with Tal R. or read the paper}


\paragraph{Organization} The reminder of the paper is organized as follows.  In \secref{sec:preliminaries}, we cover preliminaries and fuzzy extractor definitions.  In \secref{sec:known distributions}, we construct a fuzzy extractor for every known distribution with fuzzy min-entropy.  In Sections~\ref{sec:family of dist} and~\ref{sec:imposs fuzz ext} we provide negative results for secure sketches and fuzzy extractors for families of distributions respectively.


\section{Preliminaries}
\label{sec:preliminaries}
%For a random variables $X_i$ over some alphabet $\mathcal{Z}$ we denote by $X = X_1,..., X_\gamma$  the tuple $(X_1,\dots, X_\gamma)$. % For a set of indices $J$, $X_{J}$ is the restriction of $X$ to the indices in $J$.  The set $J^c$ is the complement of $J$. 
Usually, we use capitalized letters for random variables and corresponding lowercase letters for their samples.
 Unless otherwise noted logarithms are base $2$.
The {\em min-entropy} of $W$ is $\Hoo(W) = -\log(\max_w \Pr[W=w])$,
and the {\em average (conditional)} min-entropy of $W$ given $P$ is  $\Hav(W|P) = -\log(\expe_{p\in P} \max_{w} \Pr[W=w|P=p])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.   Let $H_0(W)$ be the logarithm of the size of the support of $W$,  that is $H_0(W) = \log |\{w | \Pr[W=w]>0\}|$.  We use an average case notion~(to the best of our understanding this has not defined used before):\bnote{has it?}
the conditional Hartley entropy of $W |P $ is $\tilde{H}_0(W |P) = \log ( \expe_{p\in P} |\{w | \Pr[W=w |P=p]>0\}|)$.

The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\sd(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$.
For a distinguisher $D$ we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$ (we extend it to a class of distinguishers $\mathcal{D}$ by taking the maximum over all distinguishers $D\in\mathcal{D}$).  We denote by $\mathcal{D}_{s}$ the class of randomized circuits which output a single bit and have size at most $s$.

For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $w$} is the set of all points within radius $t$, that is, $B_t(w) = \{w'| \dis(w, w')\leq t\}$.  If the size of a ball in a metric space does not depend on $w$, we denote by $|B_t|$ the size of a ball of radius $t$.  We consider the Hamming metric over vectors in $\mathcal{Z}^\gamma$, defined via $\dis(w,w') = \{i | w_i \neq w'_i\}$ where $\mathcal{Z}$ is some alphabet.  For this metric, $|B_t| = \sum_{i=0}^t {\gamma \choose i} (|\mathcal{Z}|-1)^i $.  $U_\kappa$ denotes the uniformly  distributed random variable on $\{0,1\}^\kappa$.  Throughout this work, we consider a sequence of metric spaces $\mathcal{M}_n$ parameterized by $n$, but we write $\mathcal{M}$ for notational convenience.

\subsection{Fuzzy Extractors for families of distributions}\label{sec:fuzz extractor}

In this section, we define fuzzy extractors and secure sketches.  Definitions and lemmas are drawn from the work of Dodis et. al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08} with modifications.  First we allow for error, as discussed in \cite[Section 8]{DBLP:journals/siamcomp/DodisORS08}.  Second, in the \emph{family of distributions} setting we consider an arbitrary family $\mathcal{W}$ of distributions instead of families containing all distributions of a given min-entropy.
%\footnote{Our negative results rule out a family that is a subset of all distributions with given min-entropy and thus rules out all distributions of a given entropy.}  
%This is a generalization of the definition of Dodis et al. which require a fuzzy extractor to work for all distributions of a particular entropy.  
We discuss known distribution algorithms in \secref{sec:known distributions def}.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}
\label{def:fuzzy extractor}
An $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$. \gen on input $w\in \mathcal{M}$ outputs an extracted string $\key \in\{0,1\}^\kappa$ and a helper string $p\in\{0,1\}^*$. \rep takes $w'\in\mathcal{M}$ and $p\in\{0,1\}^*$ as inputs.   $(\gen, \rep)$ have the following properties:
\begin{enumerate}
\item \emph{Correctness:} if $\dis(w, w')\leq t$ and $(\key, p)\leftarrow \gen(w)$, then $\Pr[\rep( w', p) = \key] \geq 1-\delta$.
\item \emph{Security:} for any distribution $W\in\mathcal{W}$, if $(\Key,P)\leftarrow\gen (W)$, then $\sd((\Key,P),(U_\kappa,P))\leq \epsilon$.
\end{enumerate}
\end{definition}

\noindent
Fuzzy extractors perform two tasks, information-reconciliation and privacy amplification.  The standard construction is \emph{sketch-and-extract:} the uniform key is extracted from $w$~(using a randomness extractor~\cite{nisan1993randomness}) and the error-tolerance is obtained by using a secure sketch~\cite[Lemma 4.1]{DBLP:journals/siamcomp/DodisORS08}.  Secure sketches produce a string $ss$ that minimally decreases the entropy of $w$, while mapping nearby $w'$ to $w$:
\begin{definition}
\label{def:secure sketch}
An $(\mathcal{M},\mathcal{W}, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$.  \sketch on input $w\in\mathcal{M}$ returns a bit string $ss\in\{0,1\}^*$.  \rec takes an element $w'\in\mathcal{M}$ and $ss\in\{0,1\}^*$.  $(\sketch, \rec)$ have the following properties:
\begin{enumerate}
\item \emph{Correctness}: $ \forall w, w'\in\mathcal{M}$ if $\dis(w,w')\leq t$ then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta.$
\item \emph{Security}: for any distribution $W\in\mathcal{W}$, $\Hav(W|\sketch(W))\geq \tilde{m}$.
\end{enumerate}
\end{definition}

\noindent In the above definitions, the errors are chosen before $ss$ (resp., $p$) is known: if the error pattern between $w$ and $w'$ depends on the output of $\sketch$ (resp., $\gen$),  there is no correctness guarantee.  
A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}:

\begin{definition}
Let $\mathcal{M}$, $\chi$ be finite sets.
A function $\ext: \mathcal{M}\times \{0,1\}^d \rightarrow \{0,1\}^\kappa$ a \emph{$(\tilde{m}, \epsilon)$-average-case extractor} if for all pairs
of random variables $X, Y$ over $\mathcal{M}, \chi$ such that
$\tilde{H}_\infty(X|Y) \ge \tilde{m}$, we have $\sd((\ext(X, U_d), U_d, Y), U_\kappa\times
U_d \times Y) \le \epsilon$.
\end{definition}

\begin{lemma}
\label{lem:fuzzy ext construction}
Assume $(\sketch, \rec)$ is an $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext:\mathcal{M}\times \zo^d \rightarrow \zo^\kappa$ be a $(\tilde{m}, \epsilon)$-average-case extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon)$-fuzzy extractor with error $\delta$:
\begin{itemize}
\item $\gen(w):$ generate $x\leftarrow \zo^d$, set $p=(\sketch(w), x), r=\ext(w;x)$, and output $(r,p)$.
\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $r=\ext(w;x)$.
\end{itemize}
\end{lemma}

\subsection{Fuzzy min-entropy: a necessary condition}
\label{sec:minimal conditions}

In an ideal world, $p$ is not useful to an adversary.  The adversary's only attack method is to execute $\rep(\cdot, p)$ on different $w'$.  In this world, $\rep$ outputs $\key$ if the input is close enough and provides no other  information.  The security of a fuzzy extractor in this world is the maximum weight of $W$ inside a ball of radius $t$:

\begin{definition}
\label{def:fuzzy min-ent}
The $t$-fuzzy min-entropy of a distribution $W$ in a metric space $(\mathcal{M}, \dis)$ is:
\[
\Hfuzz(W) = -\log \left(\max_{w'}  \sum_{w\in W | \dis(w, w')\le t} \Pr[W=w] \right)
\]
\end{definition}
\noindent
Fuzzy min-entropy is a necessary condition for security~(proof in \apref{sec:proof fuzz necessary}):
\begin{proposition}
\label{prop:fuzz necessary}
Let $W$ be a distribution over $(\mathcal{M}, \dis)$ and let $n= \log |\mathcal{M}|$.
If $\Hfuzz (W) = \Theta(\log n)$ there is no $(\mathcal{M}, W, \kappa, t)$-fuzzy extractor %that is $(\max |\mathcal{M}| +  |\rep|, \epsilon)$-hard for $\epsilon = \ngl(n)$
 with error $\delta = \ngl(n)$ for $\kappa =\omega(\log n)$.
\end{proposition}

%Even if we consider a two party protocol where Alice holds $w$ and Bob holds $w'$, 
%\lemref{lem:fuzz necessary} still applies.  
\noindent
There are many distributions with $\Hfuzz$ with no known fuzzy extractor~(or corresponding impossibility result).
Our goal is to provide a superlogarithmic length key when provided with superlogarithmic fuzzy min-entropy.  We now describe the known distribution setting.

\subsection{Known Distribution Algorithms}
\label{sec:known distributions def}
%To show that fuzzy min-entropy is the right notion for fuzzy extractors and secure sketches we show that it is sufficient for fuzzy extraction if the algorithm has complete knowledge of the distribution $W$.  
We call an algorithm with complete information on $W$ \emph{known distribution}.  We provide a definition for secure sketches~(extendible to fuzzy extractors):

\begin{definition}
Let $W$ denote a distribution.  A pair of algorithms, $\sketch, \rec$ is a $(\sketch_W, \rec_W)$ is called a $(\mathcal{M}, W, \tilde{m}, t)$-\emph{known distribution secure sketch} with error $\delta$ if the correctness and security properties hold for the distribution $W$.  That is, 
\begin{enumerate}
\item \emph{Correctness:} 
$
\forall w\in W$, $\forall w' \in\mathcal{M}| \dis(w,w')\le t$, $\Pr[\rec(w',\sketch(w))=w]\ge 1-\delta.$\\
Correctness is guaranteed only for the support of $W$.
\item \emph{Security:} $\Hav(W|\sketch(W))\geq \tilde{m}$.
\end{enumerate}
\end{definition}
$(\sketch, \rec)$ know the description of $W$ and security only holds for $W$.  We assume known distribution algorithms are \emph{inefficient}.  Finding a natural model of specifying distributions that allows for efficient~(yet generic) known distribution constructions of sketches and extractors is an interesting problem.

\section{Sufficiency of $\Hfuzz(W)$ for Known Distribution Algorithms}
\label{sec:known distributions}
In this section, we show it is possible to build known distribution secure sketches~(and thus fuzzy extractors through \lemref{lem:fuzzy ext construction}) whenever $\Hfuzz(W)= \omega(\log n)$.
We first consider flat distributions and show that hashing maintains fuzzy min-entropy and suffices to disambiguate points.  We then turn to arbitrary distributions.
%\subsection{Well-Spread Distributions}
%\bnote{can remove this section if we need space}
%We call a distribution well-spread if the distance between any two original readings $w_0, w_1$ is large.  Well-spread distributions should be the easiest type of distribution to handle as given a particular $w'$ there is a unique $w$ that could be the original reading.  Error-correction is ``easy'' when all points of $W$ are far apart.  Intuitively, the fuzzy extractor/secure sketch does not need to disambiguate points.  %Indeed for any error-correcting code, a fuzzy extractor can be designed~(where the design of the fuzzy extractor depends on the supported distribution).  However, this intuition becomes more difficult when a fuzzy extractor should work for all well-spread distributions.  
%We begin by defining a well-spread distribution.
%
%\begin{definition}
%A distribution $W$ is called \emph{$t_{cor}$-well spread} if for all $w, x\in W, \dis(w, x)\ge t_{cor}$.
%\end{definition}
%Intuitively, it should be easy to build a fuzzy extractor for well-spread distributions when the desired error-tolerance is less than $t_{cor}/2$~(assuming the distribution has superlogarithmic min-entropy).  Indeed, this is the case when the fuzzy extractor is allowed to depend on $W$.\footnote{This result is intuitive.  However, our negative results for the family of distributions setting will be for families of well-spread distributions.  This result illustrates how different the known distribution and family of distributions settings are.}
%\begin{lemma}
%\label{lem:nosketchwellspread}
%Let $W$ be a $t_{cor}$ well-spread distribution with $\Hoo(W)\ge k$ there exists a $(\mathcal{M}, W, m, \lfloor( t_{cor}-1)/2\rfloor)$-secure sketch with no error that works for $W$.  In particular, $\sketch(w) = \perp$, and $\rec(w')$ finds the nearest $w\in W$.  \rec is efficient if there exists efficient decoding for the points in $W$.  By \lemref{lem:fuzzy ext construction} there also exists a $(\mathcal{M}, W,m -2\log 1/\epsilon, \lfloor (t_{cor}-1)/2\rfloor, \epsilon)$-fuzzy extractor with no error.
%\end{lemma}  
%\begin{proof}
%It suffices to show that for any $w'$ there exists a unique $w\in W$ such that $\dis (w, w')\le \lfloor (t_{cor}-1)/2\rfloor$.  Since $W$ is $t_{cor}$-well spread, for all $w_0, w_1 \in W, \dis (w_0, w_1)\ge t_{cor}$.  Then by the triangle inequality,
%\[
%t_{cor}\ge \dis(w_0, w_1) \ge \dis(w_0, w') + \dis(w', w_1).\]
%This is only true if at least one of $\dis(w_0, w')$ or $\dis(w', w_1)$ is greater than $\lfloor (t_{cor}-1)/2\rfloor$.
%\end{proof}

\subsection{Secure Sketches for Flat distributions}
A distribution is flat if all points in its support have the same probability:
\begin{definition}
A distribution $W$ is \emph{flat} if for all $w_0, w_1 \in W$, $\Pr[W=w_0] = \Pr[W=w_1]$.  
\end{definition}

Fuzzy min-entropy is a lower bound of the adversaries success probability for any scheme.  We denote the ball with the largest number of points as $B_{t, max}$.  Fuzzy min-entropy is the weight of the maximum probability ball~(\defref{def:fuzzy min-ent}).  For flat distributions, this is proportional to the largest number of points in any ball.  More precisely, 
\begin{align}
\Hfuzz(W) &= -\log \left(\max_{w' \in \mathcal{M}} \left| \{w | w\in W \wedge \dis(w, w')\le t\} \right|* \Pr[W=w]\right) \nonumber \\
&= -\log\left( \max_{w' \in \mathcal{M}} |\{w | w\in W \wedge \dis(w, w')\le t\}| *2^{-\Hoo(W)}\right) \nonumber \\
&=\Hoo(W) -\log\left( \max_{w' \in \mathcal{M}}| \{w | w\in W \wedge \dis(w, w')\le t\} |\right)\nonumber \\
&= \Hoo(W) -\log |B_{t, max}|.\label{eq:fuzz for flat}
\end{align}

\noindent We use universal hashes to construct secure sketches for flat distributions.  Skoric et al. constructed secure sketches from universal hashes to correct a polynomial number of error patterns~\cite{skoric2009efficient}.


\begin{definition}
Let $F : \mathcal{K} \times \mathcal{M} \to R$ be a function.  We say that $F$ is \emph{universal} if for all distinct $x_1, x_2 \in \mathcal{M}$:
\[
 \Pr_{K \leftarrow \mathcal{K}}[F(K, x_1) = F(K, x_2)] = \frac{1}{|R|} \;.
\]
\end{definition}

\begin{construction}
\label{cons:universal hash}
Let $F :\mathcal{K}\times \mathcal{M}\rightarrow R$ be a universal hash function.  Let $W$ be a distribution.  Define $\sketch_W, \rec_W$ as:

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{$\sketch_W$}
\begin{enumerate}
\item \underline{Input}: $w$.
\item Sample $K\leftarrow \mathcal{K}$.
\item Set $p = F(K, w), K$.
\end{enumerate}
\vspace{.3in}
\end{minipage} &
\begin{minipage}{3in}
\textbf{$\rec_W$}
\begin{enumerate}
\item \underline{Input}: $(w', p = y, K)$
\item Create $W^* = \{w \in W | \dis(w, w')\le t\}$.
\item For $w^*\in W^*$, if $F(K, w^*) = y$, \\ output $w^*$.
\item Output $\perp$.
\end{enumerate}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\begin{lemma}
\label{lem:flat hashing} (Proof in \apref{sec:proof of flat hashing})
Let $W$ be a flat distribution with $\Hoo(W)\ge m$.  Then
\consref{cons:universal hash} is a $(\mathcal{M}, W, m - \log |R|, t)$-known distribution secure sketch with error $\delta \le \frac{|B_{t, max}|-1}{|R|}$. 
\end{lemma}
\begin{corollary}
Let $n = \log |\mathcal{M}|$.  If $|R| \ge |B_{t, max}|* n^{\omega(1)}$ then \consref{cons:universal hash} is correct with overwhelming probability.  That is, setting $\log |R| = \log |B_{t, max}| + \omega(\log n)$ suffices.
\end{corollary}

\noindent
\consref{cons:universal hash} writes down enough information to disambiguate any ball of points.  The remaining entropy for this construction is 
$
\Hav(W |\sketch(W)) = \Hoo(W) - \log |B_{t, max}| - \omega(\log n).
$
For a flat distribution this is within a superlogarithmic factor of optimal~(see Equation~\eqref{eq:fuzz for flat}). By choosing $\delta$ based on $\Hfuzz(W)$ we build $(\sketch, \rec)$ such that $\Hav(W | \sketch(W)) = \omega(\log n)$.

\subsection{Secure sketches for all distributions with $\Hfuzz(W)$}
The worst-case hashing approach does not work for arbitrary sources.  The reason is that some balls may have  many points but low total weight.  Let $W$ be a distribution, denote by $B^1_t$ a ball with $2^{\Hoo(W)}$ points with probability $\Pr[W\in B^1_t] =2^{-\Hoo(W)}$.  Let $B^2_t,..., B^{2^{-\Hoo(W)}}_t$ be balls with one point each with probability $\Pr[W\in B^i_t] = 2^{-\Hoo(W)}$.  Then the hashing algorithm needs to write down $\Hoo(W)$ bits to achieve correctness.  However, with probability $1-2^{-\Hoo(W)}$ the initial reading is outside of $B^1_t$ and the hash completely reveals the point.  

Dealing with non-flat distributions requires a new strategy. 
Many solutions for manipulating high entropy distributions leverage a solution for flat distributions and use the fact that high entropy distributions are convex combinations of flat distributions.  However, a distribution with high fuzzy min-entropy may be formed from component distributions with little or no fuzzy min-entropy.  It is unclear how to leverage the convex combination property in this setting.  \bnote{I took out the appendix on convex combs, didn't think it was helping}%In \apref{sec:convex comb}, we show that it is difficult to exploit this property in our scenario.  The issue is that a distribution with high fuzzy min-entropy may be formed from component distributions each of which have no fuzzy min-entropy.

The main obstacle in the arbitrary setting is distinguishing between a setting where a ball has a few high probability points and a large number of low probability points.
To overcome this problem, we write the probability of $w\in W$ in the sketch output.  To ensure this information does not completely reveal $w$ we write down $\lfloor \log \Pr[W=w] \rfloor$. We then use a universal hash whose output length is proportional to the number of close points of the same probability as $w$.  This construction divides the distribution $W$ into probability levels.  Each level is nearly flat.  

\begin{construction}
\label{cons:leveling}
Let $\mathcal{M}$ be a metric space and let $n =\log |\mathcal{M}|$. Let $W$ be a distribution with $\Hoo(W)= m$.  Let $\ell\in\mathbb{Z}^+$ be a parameter.  Let $L_i = (2^{-(i+1)}, 2^{-i}]$ for $i=m,..., m+\ell$.  Let $F_i :\mathcal{K}_i\times \mathcal{M}\rightarrow R_i$ be a parameterized family of universal hash functions.  Define $\sketch_W, \rec_W$ as: 
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{$\sketch_W$}
\begin{enumerate}
\item \underline{Input}: $w$.
\item If $\Pr[W=w] \le 2^{-(m+\ell)}$. Set $p=0,w$.
\item Else
\begin{enumerate}
\item Find $i$ such that $\Pr[W=w]\in L_i$.
\item Sample $K\leftarrow \mathcal{K}_i$.
\item Set $ss =1,  i, F_i(K, w), K$.
\end{enumerate}
\end{enumerate}
\vspace{.4in}
\end{minipage} &
\begin{minipage}{3in}
\textbf{$\rec_W$}
\begin{enumerate}
\item \underline{Input}: $(w', ss)$
\item If $ss_0 = 1$, output $ss_{1,..., |y|}$.
\item Else
\begin{enumerate}
\item Parse $(i, y, K) = ss_{1,..., |y|}$.
\item $W^* = \{w \in W | \dis(w, w')\le t\}$.
\item For $w^*\in W^*$, if $F(K, w^*) = z$, \\ output $w^*$.
\item Output $\perp$.
\end{enumerate}
\end{enumerate}
\end{minipage}
\end{tabular}
\end{center}
\end{construction}

\noindent
We extend our notation for the maximum likelihood ball to the leveled case.  Let $B_{t, i, max}$ be the ball that contains the most points in $L_i$.  That is,
\[
B_{t, i, max} = \max_{w' \in \mathcal{M}} \{w | w\in W \wedge \dis(w, w')\le t \wedge \Pr[W=w]\in L_i\}.
\]

\begin{theorem}
\label{thm:layered hashing}
Let $W$ be a distribution over $\mathcal{M}$ where $n =\log \M$.  Let $\delta>0$ be an function of $n$.  Let $F_i: \mathcal{K}_i \times \mathcal{M}\rightarrow R_i$ be a parameterized family of universal hash functions where $|R_i| = (|B_{t, i, max}|-1) /\delta$.  When $\ell = n$ \consref{cons:leveling} is a $(\mathcal{M}, W, \tilde{m}, t)$-secure sketch with error $\delta$ for $\tilde{m} = \Hfuzz(W) - \log n - \log 1/\delta - 3$.

\end{theorem}
We provide a proof in \apref{sec:proof of layered hashing}.  The main idea is to show that an adversary that knows the level of $w$ cannot perform much better than an adversary without this information.  

\begin{corollary}
\label{cor:extension to fuzz ext}
Let $\mathcal{M}$ be a metric space where $n = \log |\M|$.
For any distribution $W$ over $\mathcal{M}$ with $\Hfuzz(W)=\omega(\log n)$, there exists a $(\mathcal{M}, W, \tilde{m}, t)$-known distribution secure sketch with $\tilde{m} = \omega(\log n)$ and $\delta = \ngl(n)$.  (Extendible to a fuzzy extractor using~\lemref{lem:fuzzy ext construction}.)
\end{corollary}

\noindent
In our positive results we considered an arbitrary finite metric space.  The relevant parameters were the size of balls and the size of the metric space.  Our negative results are specific to the Hamming metric.
\section{Insufficiency of $\Hfuzz(W)$ to secure sketch from distribution families}
\label{sec:family of dist}

In the previous section, we showed the sufficiency of $\Hfuzz$ for known distribution algorithms.  Unfortunately, it is unrealistic to assume that $W$ is completely known.  Traditionally, algorithms deal with this uncertainty by providing security for a family of distributions $\mathcal{W}$.  

In this section, we show uncertainty of $W$ comes at a real cost. 
We will show a family $\mathcal{W}$~(where each member has fuzzy min-entropy) where any secure sketch for $\mathcal{W}$ leaves almost no entropy.  A key aspect is formalizing the difference between the views of the fuzzy extractor~(which knows the family $\mathcal{W}$ and the sample $w$) and the view of the adversary~(knows the particular distribution $W$ but not the sample $w$).

\subsection{A definitional equivalence}
\label{sec:def equiv}
%We refer to $\mathcal{W}$ as the \emph{family}, the distribution $W$ as a \emph{source} and the particular outcome taken $w\in W$ as the \emph{sample}.   

The security game of a fuzzy extractor for a family of distributions $\mathcal{W}$ can be thought of as a two stage process: 1) the challenger specifies $(\sketch, \rec)$, 2) the adversary specifies a source $W\in \mathcal{W}$.  The challenger wins if the adversary cannot guess the outcome of $W$ with high probability, and the adversary wins otherwise.  Let $V$ is a distribution over elements of $\mathcal{W}$.  The minimax theorem says we can reverse which of these actions is announced first~\cite{neumann1928theorie} if $\mathcal{A}$ announces a distribution $V$ instead of a single element $W$.  That is, the following two player games have the same equilibrium:

%More formally, let $\mathcal{A}, \mathcal{C}$ be the adversary and challenger respectively. We provide two formulations of a two player game.\footnote{We denote the outcome of the game as a reward to each player.  This is opposed to the standard formulation saying that $\mathcal{A}$ wins or loses.  This is to apply results from game theory.}

\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\begin{tabbing}
123\=123\=123\=123\=123\=\kill
\textbf{Experiment} $\Exp^{\mathcal{W}}_1(\mathcal{A}, \mathcal{C}, \tilde{m})$: \\
$(\sketch, \rec)\leftarrow \mathcal{C}(\mathcal{W})$\\
$W \leftarrow \mathcal{A}(\mathcal{W}, \sketch, \rec)$\\
If $W\not\in \mathcal{W}$ reward $(1, -1)$.\\
If $\Hav(W | \sketch(W))\ge \tilde{m}$ reward $(1, -1)$\\
Else reward $(-1, 1)$.
\end{tabbing} 
\vspace{.065in}
\end{minipage}  &
\begin{minipage}{3in}
\begin{tabbing}
123\=123\=123\=123\=123\=\kill
\textbf{Experiment} $\Exp^{\mathcal{W}}_2(\mathcal{A}, \mathcal{C}, \tilde{m})$: \\
$V \leftarrow \mathcal{A}(\mathcal{W})$\\
$(\sketch, \rec)\leftarrow \mathcal{C}(V, \mathcal{W})$\\
$W \leftarrow V$\\
If $W\not\in \mathcal{W}$ reward $(1, -1)$.\\
If $\Hav(W | \sketch(W))\ge \tilde{m}$ reward $(1, -1)$\\
Else reward $(-1, 1)$.
\end{tabbing}
\end{minipage}
\end{tabular}
\end{center}

%The difference between these two games is which player announces their action first, in $\Exp^{\mathcal{W}}_1$, the challenger announces a pair of algorithms and the adversary specifies a source.  In $\Exp^{\mathcal{W}}_2$, the adversary specifies a distribution $V$ over sources, and the challenger then specifies their algorithm.  

\begin{lemma}
\label{lem:quant switch fuzz}
Let $\mathcal{W}$ be a family of distributions.   Let $(\sketch, \rec)$ be a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-secure sketch.  Let $V$ be a distribution over $\mathcal{W}$.  Then there exists $(\sketch', \rec')$ that is a $(\mathcal{M}, V, \tilde{m}, t)$-secure sketch on the mixed distribution $V$ when the adversary knows the sampled distribution $W$.\footnote{The other direction of this lemma is true as well.  If it is possible to construct a fuzzy extractor for an arbitrary distribution over a family then it is possible to construct a fuzzy extractor for every element of the family.}
\end{lemma}

In our negative results, the adversary that specifies a distribution $V$ over $\mathcal{W}$.  Importantly, the adversary sees the distribution $W$ and $\sketch(W)$ when they are trying to recover the particular outcome.  An alternate view is that $(\sketch, \rec)$ sees a distribution $V$ while the adversary sees side information $Z$~(corresponding to the distribution $W$).

\begin{corollary}
\label{cor:no fuzz for dist}
Let $\mathcal{W}$ be a family of distributions.  Let $V$ be an arbitrary distribution over elements of $\mathcal{W}$.  If there does not exist a $(\mathcal{M}, V, \tilde{m}, t)$-secure sketch on the mixed distribution $V$, then there is no $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-secure sketch.
\end{corollary}

Both \lemref{lem:quant switch fuzz} and \corref{cor:no fuzz for dist} apply for fuzzy extractors as well as secure sketches.  Additionally they generalize to imperfect correctness.

\subsection{Insufficiency of $\Hfuzz(W)$ to secure sketch from families of distributions}
We now show a family of distributions $\mathcal{W}$ that does not admit a secure sketch.  

\begin{theorem}
\label{thm:imposs sketch}
Let $n$ be a security parameter.  There exists a family of distributions $\mathcal{W}$ such that for each element $W\in \mathcal{W}$, $\Hfuzz(W)= \omega(\log n)$ and there is no $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-secure sketch with error $\delta < 1/4$ for $\tilde{m} \ge 2$ for $t\ge 4$.  
\end{theorem}

\begin{proof}
We first describe a family $\mathcal{W}$.  Let $\mathbb{F}$ be some field of size $q =\omega(\poly(n))$.  
Let $\mathcal{W}$ be the set of all distributions of the form 
\[W =  \begin{pmatrix} \vec{1} \\a_2  \\ \vdots \\ a_{\gamma} \end{pmatrix} W_1 + \begin{pmatrix} 0  \\ 
b_2\\ \vdots \\ b_{\gamma} \end{pmatrix} 
\]
where $W_1$ is uniform and $W_i = a_i W_1 + b_i$ for $2\le i \le \gamma$ and $a_i, b_i\in\mathbb{F}, a_i\neq 0$.  
This type of distribution is an affine line in space $\mathbb{F}^\gamma$.  Define $V$ as the uniform distribution over $\mathcal{W}$.  The adversary sees $\sketch(V)$ and $Z$.  $Z$ is the description of the line $Z = a_2, b_2, ..., a_\gamma, b_\gamma$. The algorithms $\sketch, \rec$ never see $Z$.
Fix some $4\le t < \gamma$.
We show the following~(in \apref{sec:proof secure sketch imposs}):

\begin{itemize}
\item \propref{prop:each element good}: for all $W\in \mathcal{W}$, $\Hfuzz(W) = \omega (\log n)$. That is, $\forall z, \Hfuzz(V | Z=z) = \omega(\log n)$.
\item \propref{prop:distribution uniform}: the distribution $V$ is uniform.
\item \lemref{lem:secure sketch entropy loss}: for any secure sketch on $V$, the support size of $V | \sketch(V)$ decreases significantly.  Here we show the minimum distance of $V|\sketch(V)$ is at least $t$.
\item \lemref{lem:side info determines sketch}:  for most lines $Z$, the intersection of the support of $V|Z$ and $V | \sketch(V)$ is small.  \\That is, $\tilde{H}_0(V | \sketch(V), Z) < 2$.
\item By \corref{cor:no fuzz for dist}, since no secure sketch can support the uniform distribution $V$ over $\mathcal{W}$, there is no secure sketch for the family $\mathcal{W}$.\end{itemize}
\end{proof}
\bnote{should note we can decrease field size at the cost of distance.}
\subsubsection{Implications for Computational secure sketches}
\label{sec:feas comp sec sketch}
Fuller et al. showed that computational secure sketches that provide pseudoentropy imply information-theoretic secure sketches with almost the same parameters~\cite[Corollary 3.8]{fuller2013computational}.  %This means that ruling out any information-theoretic secure sketches also rules out a computational secure sketch.  
The definitional of Fuller et al. uses a weak version of pseudoentropy~\cite{DBLP:journals/siamcomp/HastadILL99} due to Gentry and Wichs~\cite{gentry2011separating}.

\begin{definition}
\label{def:relaxed hill}
Let $(W, S)$ be a pair of random variables.  $W$ has 
\emph{relaxed HILL entropy} at least $\tilde{m}$ conditioned on $S$,
denoted $H^{\hillrlx}_{\epsilon_{sec}, s_{sec}}(W|S)\geq \tilde{m}$ if there exists a joint distribution $(X, Y)$, such that $\tilde{H}_\infty(X|Y)\geq \tilde{m}$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,Y))\leq \epsilon_{sec}$.
\end{definition}

A \emph{HILL secure sketch} is obtained by replacing the security condition in \defref{def:secure sketch} with HILL entropy~(adding parameters $\epsilon_{sec}, s_{sec}$ parameters to the definition, representing the distinguishing circuit).  By the contrapositive of~\cite[Corollary 3.8]{fuller2013computational}, no sketch can retain HILL entropy for the same family of distributions:

\begin{corollary}
\label{cor:imposs comp sketch}
Let $n$ be a security parameter and let $\mathcal{M} = |\mathbb{F}|^\gamma$.  There exists a family of distributions $\mathcal{W}$ over $\mathcal{M}$ such that for each element $W\in \mathcal{W}$, $\Hfuzz(W)= \omega(\log n)$ and for any $(\mathcal{M}, \mathcal{W}, \tilde{m}, t)$-HILL secure sketch $(\sketch, \rec)$ that is $(s_{sec}, \epsilon_{sec})$-hard and error $\delta$.  If 
$s_{sec}\ge t(|\rec| + \gamma \log |\mathbb{F}|)$, 
$t\ge 4$, and 
 $\epsilon_{sec} + t\delta < 1/16$,
 then $\tilde{m} =O(1)$.
\end{corollary}


%\begin{corollary}
%\label{cor:rec yields sketch}
%Let $\mathcal{Z}$ be an alphabet. Let $(\sketch', \rec')$ be an $(\epsilon,s_{sec})$-HILL-entropy $(\mathcal{Z}^n, n\log |\mathcal{Z}|, \tilde{m}, t)$-secure sketch with error $\delta$ for the Hamming metric over $\mathcal{Z}^n$, with $\rec'$ of circuit size $s_{rec}$.
%If $s_{sec}\geq t(s_{rec} + n\log |\mathcal{Z}|)$, then there exists a   $(\mathcal{Z}^n, n\log |\mathcal{Z}|, \tilde{m}-2,t)$ (information-theoretic) secure sketch with error
%$4(\epsilon+t\delta)$. 
%\end{corollary}

Secure sketches that provide computational unpredictability are implied by virtual-grey box obfuscation of proximity functions~\cite{BitanskyCKP14}.  Our impossibility result says nothing about this weaker form of a secure sketch.  Extracting from unpredictability requires a extractor with a reconstruction property~\cite{barak-computational, DBLP:conf/eurocrypt/HsiaoLR07}.

\section{Insufficiency of $\Hfuzz(W)$ to fuzzy extract from distribution families}
\label{sec:imposs fuzz ext}
In the previous section, we showed a family of distributions that does not admit a secure sketch.  We provide a similar result for fuzzy extractors.  

\begin{theorem}
\label{thm:imposs fuzz ext}
Let $n$ be a security parameter.  There exists a family of distributions $\mathcal{W}$ over $\zo^n$ such that for each element $W\in \mathcal{W}$, $\Hfuzz(W)= \omega(\log n)$ and there is no $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon)$-fuzzy extractor with error $\delta = 0$, for $\kappa \ge 2$ such that $\epsilon \le 1/8 - \ngl(n)$ for $t = \omega(n^{1/2}\log n)$. \end{theorem}

\begin{proof}[Proof Outline]
Our counterexample uses a slightly different family of distributions $\mathcal{W}$ than the counterexample for secure sketches.  In particular, we need to assume a larger error tolerance.  Let $\nu = \omega(\log n)$ and $\nu = o(n^{1/2}/\log n)$.  Let $t=4\nu n^{1/2}$ and note that $n/\nu >t$.  

We can't work directly on the Hamming distance over a large alphabet~(as we did for secure sketches).  We embed this larger space to the binary Hamming metric.
Let $x_1,..., x_\nu \in \zo^\nu$.  Let $\mathbb{F}$ denote the field of size $2^{\nu}$.  Let $a_2,..., a_{n/\nu}\in\mathbb{F}$ such that $a_i\neq 0$ and let $b_2,..., b_{n/\nu}\in\mathbb{F}$.  
Interpret $x_1,..., x_{\nu}$ as a element $x\in \mathbb{F}$ and let 
\[w =  \begin{pmatrix} \vec{1} \\a_2  \\ \vdots \\ a_{n/\nu} \end{pmatrix} x + \begin{pmatrix} 0  \\ 
b_2\\ \vdots \\ b_{n/\nu} \end{pmatrix} .
\]
The multiplication is in $\mathbb{F}$.
Define the distribution $W$ as the uniform distribution over values of $x$ for a particular value of $a_2,..., a_{n/\nu}$, $b_2,..., b_{n/\nu}$.  
Let $\mathcal{W}$ be the set of all such $W$.  Let $V$ be the uniform distribution over elements $\mathcal{W}$ and $Z$ is the description of the line $Z = a_2,..., a_{n/\nu}, b_2, ..., b_{n/\nu}$.
We now present an outline of the proof~(proofs in \apref{sec:fuzz ext proof}):
\begin{itemize}
\item \propref{prop:dist fuzzy ent fuzz}: for all $W\in \mathcal{W}$, $\Hfuzz(W) = \omega (\log n)$. That is, $\forall z, \Hfuzz(V | Z=z) = \omega(\log n)$.
\item \propref{prop:dist uniform fuzz}: the distribution $V$ is uniform.
\item \lemref{lem:fuzz can't get key}: for many values of $Z$ and for half of the $\key \in\zo^\kappa$, there are no viable $v\in\mathcal{M}$.  In more detail,
\begin{itemize}
\item Half the keys have at most $2^{n- \kappa}$ pre images in the metric space~(this is at most half the metric space).  Denote this set as $R_{small}$.  
\item Consider some $\key \in R_{small}$.  %Our goal is to show there are few values of $v \in V|Z$ that could produce this key.  
Consider the set of $V_{\key } = \{w | \rep(w, p) = \key \}$.  All points in $V | \sketch(V)$ are distance $t$ from a boundary of $V_\key$~(the functionality of $\rep$ guarantees that for the true $w$ all nearby points map to the same $\key$).  We show that most of $V_\key$ is near a boundary.  A result of Frankel and F{\"u}redi says that the boundary of a region is minimized by a ball containing the same number of points~\cite{frankl1981short} .  Hoeffding's inequality says that most of a ball lies near its boundary~\cite{hoeffding1963probability}.  Together these two results imply that $V_\key$ is small.
\item As before, there are many possible values for $z_1, z_2$ for the side information $Z$~(and these possible values are equally likely).  Furthermore, the distributions $V|Z=z_1 $ and $V| Z=z_1$ have disjoint support outside of $v$.
\item For most values of possible $Z$, the intersection between the viable pre images of $V|Z$ and $V_\key$ contains at most one point~(the received point $v$).  Thus, for most values of $Z$, keys lying in $R_{small}$ can be labeled as ``random''.
%The previous two propositions were similar to the secure sketch counterexample.  However, the structure on the metric space imposed byThe remainder of the proof is significantly different.  We show for any $(\gen, \rep)$ many keys have few possible values that could produce them in the metric space once $p$ is published.  Then an adversary that sees $z$ is able to eliminate many possible keys and thus effectively distinguish the key from random.
\end{itemize}
\item By \corref{cor:no fuzz for dist}, since no fuzzy extractor can support the uniform distribution over $\mathcal{W}$, there is no fuzzy extractor for the family $\mathcal{W}$.
\end{itemize}

\end{proof}

\noindent
\textbf{Note:} As stated in \secref{sec:related settings}, using strong computational assumptions it is possible to avoid this result.  We note that the specific family $\mathcal{W}$, Canetti et al.~\cite[Construction 5.3]{canetti2014key} construct computational fuzzy extractors  for this family of distributions when $\mathbb{F}$ is large enough under weaker assumptions.  (Their construction is stated with imperfect correctness.  A construction with perfect correctness is obtained by using a code that corrects $t$ bidirectional errors instead of a code that corrects $t$ unidirectional errors.)

\paragraph{Comparison with \thref{thm:imposs sketch}} The parameters in this result are weaker than those in \thref{thm:imposs sketch}.  This result requires: 1) higher error tolerance $t= \omega(n^{1/2}\log n)$ 2) the fuzzy extractor must have perfect correctness.  Note ruling out a fuzzy extractor also rules out a secure sketch~(as a secure sketch can be transformed to a fuzzy extractor).  
\bnote{improve this}\bnote{do we need this or is it covered in the intro?}
\bibliographystyle{alpha}
\bibliography{crypto}

\appendix

%\section{Leveraging convex combinations?}
%\label{sec:convex comb}
%\bnote{be careful uncommenting this, some notation may be out of date}
%In many situations solving a problem for min-entropy distributions can be reduced to solving the problem for flat distributions and then using the fact that any min-entropy distribution is a convex combination of flat distributions.  For example, consider the following hypothetical extension to \consref{cons:universal hash}.  Let $W$ be some distribution that is a finite convex combination of flat sources $W_1,..., W_\alpha$ where each $\forall i, \Hoo(W_i)\ge k$.\footnote{A distribution may be an  convex combination of an infinite number of distributions.  However, the problem arises even when considering a finite combination so we restrict ourselves to this simpler case.}  For convenience we assume that the coefficients on all $W_i$ are the same and distributions may be repeated. 
%
%We now describe a possible modification to the hashing construction.  Upon receiving an input $w$, sample a distribution $W_i$ from the set of distributions where $w$ has nonzero probability, write $i$ and perform the sketch as before for distribution $W_i$.  Intuitively, writing down a distribution $i$ should not do any harm as each of distributions $W_i$ has the same min-entropy as the starting distribution.  Unfortunately, this proposal may remove all entropy.  %Furthermore, each of these distributions is flat and thus, \consref{cons:universal hash} should apply.  However, this is not the case.
%
%%In the flat case, we used a hash whose length was proportional to the number of points in the largest ball.  The residual entropy in this case was within a superlogarithmic factor of $\Hfuzz(W)$.  However, in the setting where $W$ is a convex combination of flat sources this does not hold.
%
%Consider set of distributions $W_i$ where each $W_i$ has two components, first $W_i$ contains a distinguished point $w^*$ where $\Pr[W=w^*] = 2^{-k}$ and $w^*$ has no neighbors and $W_i$ contains a set $B_i$ of $2^{-k}-1$ points within distance $t$ each with probability $2^{-k}$.  This distribution is illustrated in \figref{fig:convex comb}.  
%
%\begin{figure}
%
%\centering
%    \includegraphics[width=.9\textwidth]{convexCombExample.png}
%    \caption{A convex combination of $W_1,..., W_\alpha$ has high min and fuzzy min-entropy, but sketching by using the component flat distributions removes all entropy.}
%\label{fig:convex comb}    
%\end{figure}
%
%
%The fuzzy min-entropy of $W_i$ is almost $0$ as the overwhelming majority of the probability space lies in $B_i$.  Now consider the distribution $W$ that is a convex combination of the $W_i$~(we assume that $B_i$ do not intersect).  Note that $\Hoo(W)=k$ and that $\Hfuzz(W)\approx \min\{\alpha, k\}$ where $\alpha$ is the number of distributions $W_1,..., W_\gamma$.  The value $\alpha$ may be quite large and $k$ may be the dominant factor.
%
%Consider an input point $w\in W$.  One of two cases occurs either $w=w^*$ or $w\in B_i$ for some $i$.  It should be clear in either case that the selected distribution $W_i$ is uniformly distributed from $\{1,..., \alpha\}$.  For any of the selected distributions, there is a ball with $\approx 2^{-k}$ points and thus the hash based construction must write down approximately $k$ points.  This means that $\Hav(W | \sketch(W))  = \Hav(W_i | \sketch(W_i)) \approx 0$.  This is despite the fact $\Hfuzz(W) \approx \min\{\gamma, k\}$.  Thus, the ability to sketch flat distributions does not easily admit a technique to sketch non-flat distributions.  
\bnote{I removed the discussion of convex combs for the submitted version}

\section{Proof of \propref{prop:fuzz necessary}}
\label{sec:proof fuzz necessary}
\begin{proof}
Let $W$ be a distribution where $\Hfuzz(W) = \Theta(\log n)$.  This means that there exists a point $w' \in \mathcal{M}$ such that $\Pr_{w\in W}[\dis (w, w')\leq t] \geq 1/\poly(n)$.  Consider the following distinguisher $D$:
\begin{itemize}
\item Input $r, p$.
\item If $\rep(w', p) = r$, output $1$.
\item Else output $0$.
\end{itemize}
Clearly, $\Pr[D(\Key, P) = 1]\geq 1/\poly(n) - \delta$, while $\Pr[D(U_\kappa, P)=1 ]= 1/2^{-\kappa}$.  Thus, when $\kappa = \omega(\log n)$:
\[
\delta^D((R, P), (U_\kappa, P))\geq \frac{1}{\poly(n)} -\delta -  \frac{1}{2^{-\kappa}} = 1/\poly(n).
\]
Note that $D$ only provides an input and looks at the output, thus it extends to an interactive protocol.  Also, $D$ is of size $\max |\mathcal{M}|+ |\rep|$ where $\max |\mathcal{M}|$ is the longest description of an item in the metric space.  Thus, $D$ is also a distinguisher in the computational setting.
\end{proof}

\section{Proof of \lemref{lem:flat hashing}}
\label{sec:proof of flat hashing}
\begin{proof}
We first argue security.  Fix some $W\in\mathcal{W}$. Since $\mathcal{K}$ and $W$ are independent $\Hav(W | \mathcal{K}) = \Hoo(W) = m$.  Then by \cite[Lemma 2.2b]{DBLP:journals/siamcomp/DodisORS08}, $\Hav(W | \mathcal{K}, F(\mathcal{K}, W)) \ge \Hoo(W) - \log |F(\mathcal{W}, W)| \ge m - \log |R|$.

We now argue correctness.  Fix some $w, w'$.  Let $W^*$ denote the set of elements in $W$ within distance $t$ of $w'$.  The size of $W^*$ is at most $B_{t, max}$.  Since $w, w'$ are independent of $\sketch$ this set is independent of the choice of $\mathcal{K}$.  The algorithm  $\rec$ will never output $\perp$ as the correct $w$ will match the hash.  The probability that another element $w^*$ collides is:
\begin{align*}
\Pr[\exists w^* \in W^* |w^* \neq w \wedge F(K, w^*) = F(K, w)] &\le \sum_{w^*\in W^* | w^*\neq w} \Pr[F(K, w^*) = F(K, w)] \\
 &= \sum_{w^*\in W^* | w^*\neq w} \frac{1}{|R|} \le \frac{|B_{t, max}|-1}{|R|}
\end{align*}
The inequality proceeds by union bound. The first equality proceeds by the universality of $F$ and the second inequality proceeds by noting the number of wrong neighbors is bounded by $|B_{t, max}|-1$.  This completes the proof.
\end{proof}

\section{Proof of \thref{thm:layered hashing}}
\label{sec:proof of layered hashing}
\begin{proof}
Throughout the proof we assume that $\ell = n$ is the number of levels.  The proof can be carried out for an arbitrary $\ell$ but it leads to a complicated theorem statement.

\noindent
\textbf{Correctness:}  Fix some $w, w'$.  If $\Pr[W=w]\le 2^{-(m+\ell)} = 2^{-(m+n)}$, then $w$ is simply transmitted to $\rec$ and correctness is clear.  When $\Pr[W=w]> 2^{-(m+n)}$ let $L_i^*$ be the level of $\Pr[W=w]$.

Let $W^*$ denote the set of elements of $W$ in $L_i$ within distance $t$ of $w'$.  The size of $W^*$ is at most $B_{t, i, max}$. The choice of $w, w'$ is independent of $\sketch$, so this set is independent of $\mathcal{K}_i$~(it does effect the value of $i$ but not the particular outcome from $\mathcal{K}_i$).  The probability that another element $w^*$ matches the hash is:
\begin{align*}
\Pr[\exists w^* \in W^* |w^* \neq w \wedge F(K, w^*) = F(K, w)] &\le \sum_{w^*\in W^* | w^*\neq w} \Pr[F(K, w^*) = F(K, w)] \\
 &= \sum_{w^*\in W^* | w^*\neq w} \frac{1}{|R_i|} \le \frac{|B_{t, i, max}|-1}{|R_i|} = \delta
\end{align*}
The inequality is by union bound. The first equality follows from the universality of $F$.  The second inequality follows since the number of neighbors is bounded by $|B_{t, i, max}|$.  

\noindent
\textbf{Ideal Adversary with access to Level Information:} To aid in the argument in security, we show the level information on its own is not too harmful.

The best strategy for an adversary that receives $i$ as is to guess a point that has the most nearby weight in that level.  The adversary chooses \[w^*= \argmax_{w' \in \mathcal{M}}\Pr_{w\in W | 2^{-(i+1)}< \Pr[W=w]\le 2^{-i}\wedge \dis(w, w^*)}[W=w].\] The success of this adversary is at least $2^{-(i+1)}|B_{t,i, max}|$ as there at $B_{t, i, max}$ nearby points in that layer each with probability at least $2^{-(i+1)}$.  There are $n$ outcomes for $i$. The overall success of such an adversary is at most $n$ better than an adversary without such input~(by~\cite[Lemma 2.2]{DBLP:journals/siamcomp/DodisORS08}).  That is, 
\begin{align}
\expe_{i | m\le i \le m+n}2^{-(i+1)}|B_{t, i, max}|&\le \expe_{i | m\le i \le n+m}\left( \max_{w^*\in W}\sum_{w\in W| 2^{-(i+1)}< \Pr[W=w]\le 2^{-i} \wedge \dis(w, w^*) \le t}\Pr [W=w]\right) \nonumber\\
&\le n \left(\max_{w^*\in W} \sum_{w\in W | \dis(w, w^*)\le t} \Pr[W=w]\right)\nonumber \\
&= n 2^{-\Hfuzz(W)}\label{eq:link fuzz 3}
\end{align}
\textbf{Security:}
We now argue security.  First note that the total weight of points whose probability is less than $2^{-(n+m)}$ is at most $2^{-m}$~(there are at most $2^n$ points in the distribution).  Let $1_{\text{low}}$ be the indicator random variable for $\Pr[W=w]\le 2^{-(n+m)}$.  Then 
\begin{align*}
\Hav(W | \sketch(W)) = -\log \left(\Pr[1_{\text{low}}=1] * 1 + \Pr[1_{\text{low}} =0]   2^{-\Hav(W | \sketch(W) \wedge 1_{\text{low}} = 0)}\right)\\
-\log\left( 2^{-m} + (1-2^{-m})2^{-\Hav(W | \sketch(W) \wedge 1_{\text{low}} = 0)}\right)
\end{align*}
For the remainder of the proof, we seek a bound on 
\[
2^{-\Hav(W | \sketch(W) \wedge 1_{\text{low}} =0} = \max_{w\in W | 2^{-(n+m)}<\Pr[W=w]}\Pr[W=w | \sketch(W)].
\]
We separate out this quantity into levels:
\begin{align*}
\max_{w\in W | \Pr[W=w]>2^{-(m+n)}}\left(\Pr[W=w | \sketch(W)]\right) &= \expe_{i | m\le i \le m+n} \left(\max_{w\in W | \Pr[W=w]\in L_i} \Pr[W=w | \sketch(W), i]\right)\\
&= \expe_{i | m\le i \le m+n} \left(\max_{w\in W | \Pr[W=w]\in L_i} \Pr[W=w]*2^{|\sketch(W)|i|}\right)\\
&\le \expe_{i | m\le i \le m+n} \left(\max_{w\in W | \Pr[W=w]\in L_i} \Pr[W=w]*2^{H_0(\sketch(W) | i)}\right)\\
&\le \expe_{i | m\le i \le m+n} \left(2^{-i}*|B_{t, i, max}|/\delta\right)\\
&\le\frac{ \expe_{i | m\le i \le m+n} \left(2^{-(i+1)}*|B_{t, i, max}|\right)}{2\delta}\\
%&=\delta \sum_{i | m\le i \le m+n} \Pr[W\in L_i]\left(2^{-i}*|B_{t, i, max}|\right)
%\end{align*}
%Now consider This allows us to conclude that 
%\begin{align*}
%\max_{w\in W | \Pr[W=w]>2^{-n}\epsilon}\left(\Pr[W=w | \sketch(W)]\right) 
&= \frac{n 2^{-\Hfuzz(W)}}{2\delta}.
\end{align*}
Where the last line follows by Equation~\eqref{eq:link fuzz 3}.
Combining both cases we have:
\begin{align*}
\Hav(W | \sketch(W)) &= -\log \left(2^{-m}+\frac{(1-2^{-m})(n)2^{-\Hfuzz(W)}}{2\delta}\right)\\
&\ge -\log \min\{2^{-m}, \frac{(1-2^{-m}) n2^{-\Hfuzz(W)}}{2\delta}\})-1\\
&\ge \Hfuzz(W) - \log n + \log \delta - \log (1-2^{-m}) - 2\\
&\ge \Hfuzz(W) - \log n + \log \delta - 3\\
\end{align*}
Where the third line follows from the second because $\Hfuzz(W)\le \Hoo(W) = m$. The last line follows from the fourth because if $m\ge 1$ then $\log (1-2^{-m})\le 1$ and if $m< 1$ the entire bound is vacuous as $\Hfuzz(W)< 1$.
\end{proof}

\section{Proof of \thref{thm:imposs sketch}}
\label{sec:proof secure sketch imposs}
Let $c'\leftarrow \neigh_t(c)$ sample a uniform point within distance $t$ of $c$.  
The proof of \thref{thm:imposs sketch} uses the definition of a Shannon code: 
\begin{definition}
\label{def:shannon-code}
Let $C$ be a set over space $\mathcal{M}$.  We say that $C$ is an $(t,\delta)$-\emph{Shannon code} if there exists a procedure $\rec$ such that for all $t'\le t$ and for all $c\in C$, $\Pr[c'\leftarrow \neigh_t(c) \wedge \rec(c') \neq c]\le \delta$. %To distinguish it from the average-error Shannon code defined below, we will sometimes call it a \emph{maximal-error} Shannon code.
\end{definition}

\noindent
We now prove item in the outline of \thref{thm:imposs sketch}.

\begin{proposition} 
\label{prop:each element good} For each $W\in\mathcal{W}$, $\Hfuzz(W) = \omega(\log n)$.
\end{proposition}
\begin{proof}
Consider some $W\in\mathcal{W}$.  The value $w_1$ is uniform in a field of size $\omega(\poly(n))$, so $\Hoo(W) =\omega(\log n)$.  We now show that for any $w, w'\in W$, $\dis(w, w') = \gamma>t$.  This shows that $\Hfuzz(W) = \Hoo(W)$.  Fix some $w, w'\in W$.  Clearly, $w_1 \neq w_1'$, for any $i$, $w_i = a_i w_1 + b_i$ and $w_i' = a_i w_1' + b_i$.  Since $a_i\neq 0$, $a_iw_1 \neq a_iw_1'$ and thus $a_iw_1+b_i \neq a_iw_1'+b_i$.  That is, $\dis (w, w')  =\gamma$.
\end{proof}

\begin{proposition}
\label{prop:distribution uniform} $V$ is the uniform distribution over $\mathbb{F}^\gamma$.
\end{proposition}
\begin{proof}
Consider some $w\in V$.  Then $w$ was drawn from some intermediate distribution $W$ with coefficients $a_2, b_2, ..., a_\gamma , b_\gamma$.  The value $w_1$ is uniformly random and $w_i$ are uniformly random since $b_2,..., b_\gamma$ are uniformly random.
\end{proof}


\begin{lemma}
\label{lem:secure sketch entropy loss}
Fix some $\sketch, \rec$ algorithm with error $\delta < 1/4$, then $\tilde{H}_0(V | \sketch(V)) \le (\gamma-t+1)\log |\mathbb{F}|+1$.\footnote{This result is an extension of lower bounds from~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08}.  Dealing with imperfect correctness makes the bound more complicated.  In particular, we can only argue about the average remaining support size.}
\end{lemma}
\begin{proof}
We assume that $\rec$ is deterministic in our analysis.  Any randomness necessary for the \rec algorithm can be provided by \sketch.  We do not consider these coins in our security analysis~(and thus, they can be considered private).  \bnote{is this really okay?  I know its just another markov but it makes me nervous}
By the definition of correctness for $(\sketch, \rec)$, 
\[
\forall w, w', \Pr_{ss\leftarrow \sketch(w)} [\rec(w', ss) \neq w] < 1/4.
\]
%For most $p$, $\rec$ works on most neighbors of $w$.  
Fix some $w$.  
By Markov's inequality, there exists a set $A_{ss}$ such that $\Pr[ss\in A_{ss}]\ge 1/2$ and $\forall ss\in A_{ss}$, 
\[
\{w' | \dis (w', w)\le t \wedge \rec(w', p) \neq w\}\le 2\delta < 1/2.\]

Consider some $ss^*\in A_{ss}$.  We now show that $H_0(V | \sketch(V) = ss^*) \le (\gamma-t+1)\log |\mathbb{F}|$.  For the sketched value $w$, $\{w' | \dis(w, w') \le t \wedge \rec(w', p) \neq w] \le 2\delta$.  

For every value in $V|\sketch(V) = ss^*$ this is also true.  This makes the support of $V|\sketch(V)=ss^*$ a $(t, 2\delta)$-Shannon code~(see \defref{def:shannon-code}).  This implies that for all $w_1, w_2 \in V|\sketch(V)=ss^*$, $\dis(w_1, w_2)\ge t$~(since $2\delta< 1/2$).  That is $V|\sketch(V)=ss^*$ is a set with minimum distance at least $t$.  

By the Singleton bound, this implies that $H_0(V |\sketch(V)=ss^*) \le (\gamma -t+1 )|\mathbb{F}|$.  Averaging over $\sketch(V)=ss^*$ one has that $\tilde{H}_0(V|P) \le (\gamma -t +1) \log|\mathbb{F}| +1$.
\end{proof}

\begin{lemma}
\label{lem:side info determines sketch}
$\Hav(V | \sketch(V), Z) <2$.
\end{lemma}
\begin{proof}
Recall that $Z$ consists of $2\gamma$ coefficients and there are $(|\mathbb{F}|-1)^{\gamma-1} |\mathbb{F}|^{\gamma-1}$ equally likely values for $Z$.
 As described above, the view of $\sketch, \rec$ is a uniform distribution $V$.  %Having seen $V$ there are many possible values for $Z$.  Furthermore, the distributions $V|Z$ have disjoint support outside of the observed point.  
 The only information seen by $\sketch$ algorithm is in the point $V=v$.  The length of this point is $|\mathbb{F}|^\gamma$.  Conditioned on this information there are still many possible values for $Z$.  That is, 
 \[
 \forall v, H_0(Z | V=v) =\log \left(\frac{(|\mathbb{F}|-1)^{\gamma-1} |\mathbb{F}|^{\gamma-1}}{|\mathbb{F}|^\gamma}\right) = \log \left( (|\mathbb{F}|-1)^{\gamma-1}/|\mathbb{F}|\right).
 \]
Consider two possible $z_1, z_2$ that are possible values of $Z$~(having seen $v$).  The distributions $V| Z=z_1$ and $V | Z=z_2$ intersect at one point~(namely $v$).  

We now show for any sketch algorithm there are few possible values of $V|Z$ in the support of $V |\sketch(V)$.  The distributions $V | Z=z_1$ and $V| Z=z_2$ for possible $z_1, z_2$~(having seen $v$) overlap only at the point $v$.  This means for any $v^*\in V| \sketch(V)$ (other than the true $v$) there is at most one $z$ such that $v^*\in V | \sketch(V), Z=z$.  

The optimum strategy is to include these values uniformly from different $Z$ values.
We show this across different sketch values.  Consider some fixed sketch value $s$ and let $h_s= H_0(V | \sketch(V) = s)$.  %That is, there are $2^{h_s}$ possible values for $V$ conditioned on $\sketch(V) = s$.  
Recall that 
\[
\tilde{H}_0(V | \sketch(V)) =  \log \expe_{s\in \sketch(V)} 2^{H_0(V | \sketch(V) = s)}  = \log \expe_{s\in \sketch(V)} 2^{h_s} %\le   (\gamma-t+1)\log |\mathbb{F}|+1.
\]  
Conditioned on seeing the point $V$ there are $(|\mathbb{F}|-1)^{\gamma-1}/|\mathbb{F}|$ possible values for $Z$ with disjoint support outside of the sketched point.  Consider these possible values for $Z$ as bins to be filled with the $2^{h_{ss}}$ balls~(possible values of $V | \sketch(V)=ss$).  Each bin receives automatically receives one free point~(all the distributions share $v$).  The average number of balls in each bin is maximized when the bins are filled equally.  That is, the average number of balls in each bin is bounded by the number of balls divided by the number of bins.  That is, 
\begin{align*}
\tilde{H}_0(V |Z  , \sketch(V) = ss) &\le \log \left(\frac{\text{\# balls}+\text{\# bins}}{\text{\# bins}}\right)\\
&= \log \left(\frac{2^{h_{ss}}|\mathbb{F}|}{(|\mathbb{F}|-1)^{\gamma-1}} +1 \right)
\end{align*}
Then averaging over the possible values of $s$, we have the following as long as $t\ge 4$~(using  \lemref{lem:log-minus-one}, which appears below):
\begin{align*}
\tilde{H}_0(V |Z , \sketch(V) ) &= \log \expe_{s\in \sketch(V)} 2^{\tilde{H}_0(V |  \sketch(V) =ss , (Z| \sketch(V) =ss) )}\\
&= \log\expe_{s\in \sketch(V)} \left(\frac{2^{h_s}|\mathbb{F}|}{(|\mathbb{F}|-1)^{\gamma-1}} +1\right)\\
&\le \max\left\{ \log \left(\frac{|\mathbb{F}|}{(|\mathbb{F}|-1)^{\gamma-1}} \expe_{s\in \sketch(V)} 2^{h_s}\right)+1, 1\right\}.
\end{align*}
Where the inequality follows because $\log x+1 \le \max\{ 1+ \log x,1\}$.
The left operand to $\max$ is bounded by $2$~(bounded the $\max$ by $2$):
\begin{align*}
\log \left(\frac{|\mathbb{F}|}{(|\mathbb{F}|-1)^{\gamma-1}} \expe_{s\in \sketch(V)} 2^{h_s}\right)+1
&=\log |\mathbb{F}| - (\gamma -1)\log (|\mathbb{F}|-1) + \log \left(\expe_{s\in \sketch(V)} 2^{h_s}\right) +1\\
&=\log |\mathbb{F}| - (\gamma -1)\log (|\mathbb{F}|-1) + \tilde{H}_0(V | \sketch(V)) +1 \\ 
&\le \log |\mathbb{F}| - (\gamma -1)\log (|\mathbb{F}|-1) + (\gamma-t+1)\log |\mathbb{F}|+2\\
&\le (\gamma-t+2)\log |\mathbb{F}| - (\gamma-1) \log (|\mathbb{F}|-1)+2\\
&< (\gamma-t+2)\log |\mathbb{F}| - (\gamma-2) \log |\mathbb{F}| +2 \ \ \ \ \mbox{(by \lemref{lem:log-minus-one})}\\
&\le (4-t)\log |\mathbb{F}| +2< 2\,.
\end{align*}
\end{proof}

\lnote{need for \lemref{lem:log-minus-one}:  $\gamma-1\le |F|$ and $|F|\ge 4$}
\lnote{change the any needed theorem statements it affects in light of the tightening}

\lnote{this lemma should probably live elsewhere since it is also needed for the FE case}
\bnote{fine with it being here as the FE case lives in the appendix too}
\begin{lemma}
\label{lem:log-minus-one}
For any real numbers $\alpha \leq \beta$ with $\beta \ge e+1$ (in particular, $\beta\ge 4$ suffices), the following holds:
$\alpha \log (\beta-1) > (\alpha-1)\log \beta$. 
\end{lemma}

\begin{proof}
Because $\beta-1$ is positive, and $1+x<e^x$ for positive $x$,
$$1+\frac{1}{\beta-1} < e^{\frac{1}{\beta -1}}\,.$$  Therefore, 
$$\left(1+\frac{1}{\beta-1}\right)^{\alpha-1} < e^{\frac{\alpha-1}{\beta-1}}\le e < \beta-1$$ (since $\alpha\le \beta$). Multiplying both sides by $(\beta-1)^{\alpha-1}$, we obtain
$$\beta^{\alpha-1} < (\beta-1)^\alpha\,.$$
Taking the logarithm of both sides yields the statement of the lemma.
\end{proof}


\section{Proof of \thref{thm:imposs fuzz ext}}
\label{sec:fuzz ext proof}
\begin{proposition} 
\label{prop:dist fuzzy ent fuzz}
For each $W\in\mathcal{W}$, $\Hfuzz(W) = \omega(\log n)$.
\end{proposition}
\begin{proof}
Consider some fixed $W\in\mathcal{W}$.  The bits $w_{1,..., \nu}$ are uniform, so $\Hoo(W) =\omega(\log n)$.  Recall that $t=o (n/\nu)$. 
 %We now show that for any $w, w'\in W$, $\dis(w, w') \ge n/\nu$ and thus $\Hfuzz(W) = \Hoo(W)$.  
Fix some $w, w'\in W$.  Denote by $x, x'$ the values that produce $w, w'$ respectively.  Clearly, $x\neq x'$.  Thus, for any $i$, $a_i x + b_i \neq a_i x' + b_i$.  This implies that $w_{i\nu+1,...., (i+1)\nu} \neq w'_{i\nu+1,..., (i+1)\nu}$. That is, at least one of the bits in each block differs between $w$ and $w'$, and so $\dis(w, w') \ge n/\nu$. Since no two values in the support of $W$ lie in the same ball of radius $t$, we have $\Hfuzz(W) = \Hoo(W)= \omega(\log n)$.
\end{proof}

\begin{proposition}\label{prop:dist uniform fuzz}
$V$ is the uniform distribution over $\mathbb{F}^\gamma$.
\end{proposition}
\begin{proof}
Consider some $w\in V$ over $\zo^n$.  Then $w\leftarrow W$ with coefficients $a_2, b_2, ..., a_\gamma , b_\gamma$.  The value $w_{1,...,\nu} =x $ is uniformly random and $w_{i\nu+1,...,(i+1)\nu}$ are uniformly random since $b_2,..., b_\gamma$ are random.
\end{proof}

\begin{lemma}
\label{lem:fuzz can't get key}
Fix some $(\gen, \rep)$ algorithm with $\kappa \ge 2$.  There exists an information-theoretic distinguisher between $(R, P, Z)$ and $(U_\kappa, P, Z)$ with advantage $\epsilon = 1/4-\ngl(n)$.
\end{lemma}
\begin{proof}
We assume that $\rep$ is deterministic.  Any randomness necessary for the \rep algorithm can be provided by $\gen$.  We do not consider these coins in our security analysis~(and thus, they can be considered private).  Denote by $(\Key, P) \leftarrow \gen(V)$.

%By the definition of correctness for $(\sketch, \rec)$, 
%\[
%\forall w, w', \Pr_{p\leftarrow \sketch(w)} [\rec(w', p) = w] >1-\delta.
%\]
%For most $p$, $\rec$ works on most neighbors of $w$.  
%Fix some $w$.  
By Markov's inequality, there exists a set $A_{p}$ such that $\Pr[p\in A_{p}]\ge 1/2$ and $\forall p\in A_{p}$, 
\[
(\Key |P =p, P = p ) \approx_{2\epsilon} (U_\kappa , P =p).
\]
%\{w' | \dis (w', w)\le t \wedge \rec(w', p) \neq w\}\le 2\delta < 1/2.\]

Consider some $p^*\in A_{p}$.  %Since $(R | P=p^*, p^*)\approx_{2\epsilon} (U, p^*)$ this means that $R|P=p^*$ is at least $(1-2\epsilon)2^\kappa$.  
The distribution $\Key|P=p^*$ is the set of possible keys.
The distribution $\Key|P=p^*$ induces a partition on the metric space.  That is, for every $w\in\mathcal{M}$, there exists a unique value $\key$ such that $\rep(w, p^*) =\key$.  Denote this partition by $Q_{p^*,\key} = \{w | \rep(w, p^*) = \key\}$.  

There exists a set $R_{small}$  where $|R_{small} | \ge 2^{\kappa-1}$ such that for all $\key\in R_{small}$,  $|Q_{p^*, r}|\le \mathcal{M}/2^{\kappa} = 2^{n-\kappa }$.  If not, then $\cup_{\key} |Q_{p^*, \key}| > |\mathcal{M}|$.
For the remainder of the proof we restrict ourselves to elements in $R_{small}$.  Only points that are distance $t$ from points outside of $Q_{p^*, r}$ are viable points in the metric space.  These are the interior of $Q_{p^*, r}$:
\begin{align*}
\inter(Q_{p^*, \key}) = \{w | \rep(w, p^*) = \key \wedge \forall w', \dis(w, w') \le t \wedge \rep(w', p^*) =\key\},\\
%\crust(Q_{p^*, r}) = \{w | \rep(w, p^*) = r \wedge \exists w', \dis(w, w')\le t \wedge \rep(w', p^*) \neq r\}.
\end{align*}

%Note that $\crust(Q_{p^*, r}) \cap \inter(Q_{p^*,r}) =\emptyset$.  
We will use the term deficient ball\footnote{In most statements of the isoperimetric inequality, this type of set is simply called a ball.  We use the term deficient ball for emphasis.}:\bnote{use a term other than deficient}
\begin{definition}
A set $S$ is a $\beta$-deficient ball if there exists a point $x$ such that $B_{\beta-1}(x) \subseteq S \subseteq B_{\beta}(x)$.
\end{definition}

Consider some $\key^*\in R_{small}$.  
We now proceed to show that the interior of each $Q_{p^*, \key^*}$ is small:
%Recall that $B_{n-\kappa-t}$ denotes the ball of radius $n-\kappa -t$.

\begin{lemma}
$|\inter(Q_{p^*, \key^*})| \le 2^{n-4\nu}$.
\end{lemma}
\begin{proof}
By the isoperimetric inequality on the Hamming space~(we use a version due to~\cite[Theorem 1]{frankl1981short}, the original result is due to Harper~\cite{harper1966optimal}), there exists a $\beta$-deficient ball $S_{p^*, \key^*}$ centered at $0$ and a set $D$ such that $|S_{p^*, \key^*}| = |\inter(Q_{p^*, \key^*})|$, $|D| = |Q_{p^*, \key^*}^\complement|$ and $\forall s\in S_{p^*, \key^*}, d\in D$, $\dis(s, d) \ge t$~(alternatively, the distance between the sets is $t$).  Furthermore, note that $S_{p^*, \key^*} \cup D$ is a deficient ball~(and its radius is $\beta+t$).
We now find bound the size of $S_{p^*, \key^*}$.

Recall that $|S_{p^*, \key^*} \cup D| = |Q_{p^*, \key^*} | \le 2^{n-\kappa}\leq |\mathcal{M}|/2$.  Since this set contains less than half the points in the metric space we know its radius at most $n/2$.  This means that $|S_{p^*, \key^*}|$ is a deficient sphere of radius at most $n/2-t$.  Let $X$ denote a uniform string on $\zo^n$.  We use Hoeffding's inequality~\cite{hoeffding1963probability}:

\begin{align*}
|S_{p^*, \key^*}| \le \{ x | \dis (x, 0)\le n-t\} &= 2^n \Pr[ wt(X) \le (1/2-t/n)n] \le 2^n e^{-n ((t/n)^2)} = 2^n e^{-4\nu} \le 2^{n - 4\nu}
\end{align*}
\end{proof}

We have shown that $|\inter(Q_{p^*, \key^*})| \le 2^{n-4\nu}$.  
To complete the proof it suffices to show that for most values of the auxiliary information $Z$ there are many parts $Q_{p^*, \key^*}$ that do not receive any points.  
Recall that $Z$ consists of $2n/\nu$ coefficients and there are $(2^{n/\nu}-1)^{\nu-1} 2^{n-\nu}$ equally likely values for $Z$.
 As described above, the view of $\gen, \rep$ is a uniform distribution $V$.  We know show there are many possible values for $Z |P=p^*$.  The only information about $Z$ is contained in the point  $V=v$.  The length of this point is $2^n$.  Conditioned on this information there are still many possible values for $Z$.  That is, 
 \begin{align*}
 \forall v, H_0(Z | V=v) &=\log \left(\frac{(2^{n/\nu}-1)^{\nu-1} 2^{n-\nu}}{2^n} \right)\\
  &= \log \frac{(2^{n/\nu}-1)^{\nu-1}}{2^{\nu}} \\
  &>\log  \frac{(2^{n/\nu})^{\nu-2}}{2^{\nu}} \ \ \ \ \mbox{(by \lemref{lem:log-minus-one})}\\
  &=\log \frac{2^{(n-2\nu))}}{2^\nu} = n -3\nu.
 \end{align*}
Consider two possible $z_1, z_2$ that are possible values of $Z$.  The distributions $V| Z=z_1$ and $V | Z=z_2$ intersect at one point~(namely $v$).  

This means that the $\gen$ algorithm may include points for possible $Z$ values into parts $Q_{p^*, \key^*}$~(other than $v$) and these values are disjoint.  The optimum strategy is to include these values uniformly from different $Z$ values.  Consider the set of all preimages of $R_{small}$ denoted $Q_{small} = \cup_{\key\in R_{small}} \inter(Q_{\key, p^*})$.  Note that $Q_{small} \le 2^{n-4\nu}|R_{small}|$.  We now show that the intersection between $Q_{\key, p^*}$ is small for most possible values $z$.  As before each bin~(the values of $z$)  receives one ball for free~(the point $v$).
\begin{align*}
\expe_z |Q_{small} \cap (V | P=p^* \wedge Z=z) | &\le \left(\frac{\text{\# balls}+\text{\# bins}}{\text{\# bins}}\right)\\
&\le \frac{2^{n-4\nu}|R_{small}|}{2^{n - 3\nu}}+1\\
&=\frac{|R_{small}|}{2^{\nu}}+1
\end{align*}
In expectation across $Z$, 
\[\frac{\frac{|R_{small}|}{2^{\nu}}+1}{|R_{small}|} \le \frac{1}{2^\nu}+\frac{1}{|R_{small}|} \] fraction of $R_{small}$ receive any support.  %Thus, at most $1+1/2^{\nu} = \ngl(n)$ keys in $R_{small}$ have any support  conditioned on $p^*$ and $Z$~(note this is an expectation across the values of $Z$).  
We now present a distinguisher $D_{p^*}$ for a particular $p^*$:
\begin{enumerate}
\item On input $x, z$.
%\item If $x\not \in R_{small}$ output random bit $b$.
\item Compute $V|P=p^* \wedge Z=z$ and $Q_{p^*, x}$. 
\item If $(Q_{p^*, x} \cap V|P=p^* \wedge Z=z) =\emptyset$ output $b=0$.
\item Else output $b=1$.
\end{enumerate}

The distinguisher $D(x, p, z)$ is formed by calling $D_p(x, z)$ when $p\in A_p$ and outputting a random bit otherwise.  The advantage of $D$ is 
\begin{align*}
\Pr[D(\Key, P, Z) = 1] &- \Pr[D(U, P, Z) =1]\\
&=(\Pr[D(\Key, P, Z) = 1| P\in A_p] - \Pr[D(U, P, Z) =1 | P\in A_p])\Pr[P\in A_p]\\
&\ge \sum_{p^*\in A_p} \Pr[P=p^*] \left(1 - \Pr[D_{p^*}(U, Z)=1]\right)\\
&\ge \sum_{p^*\in A_p} \Pr[P=p^*] \left(1- \Pr[D_{p^*}(U, Z)=1 | U\in R_{small}]\Pr[U\in R_{small}] - \Pr[U\not\in R_{small}]\right)\\
&\ge \sum_{p^*\in A_p} \Pr[P=p^*] \left(1- \left(\left(\frac{1}{|R_{small}|}+\frac{1}{2^\nu}\right)\Pr[U\in R_{small}]\right) - \Pr[U\not\in R_{small}]\right)\\
&\ge \sum_{p^*\in A_p} \Pr[P=p^*] \left(1- \frac{1}{2^{\nu}} -\frac{1}{2}\Pr[U\in R_{small}] - \Pr[U\not \in R_{small}]\right)\\
&\ge \sum_{p^*\in A_p} \Pr[P=p^*] \left(1- \frac{1}{2^{\nu}} -\frac{1}{2}\Pr[U\in R_{small}] - \Pr[U\not \in R_{small}]\right)\\
&\ge \sum_{p^*\in A_p} \Pr[P=p^*] \left(1- \frac{1}{2^{\nu}} -1+\frac{1}{2}\Pr[U\in R_{small}] \right)\\
&\ge \sum_{p^* \in A_p} \Pr[P=p^*]\left(1/4-\ngl(n)\right) \ge \frac{1}{8}-\ngl(n).
\end{align*}
The sixth line follows since $R_{small} \ge 2^{\kappa-1}\ge 2$.  The eighth line follows because $\Pr[U\in R_{small}]\ge 1/2$.  The last inequality proceeds because $\Pr[P\in A_p]\ge 1/2$.
This completes the proof of \lemref{lem:fuzz can't get key}.
\end{proof}

\end{document}











