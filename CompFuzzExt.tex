\documentclass[11pt]{article}
\def\shownotes{1}

\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{algpseudocode} 

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\textbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\D}{\mathcal{D}}

\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\xnote}[1]{{\authnote{Xianrui}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\ignore}[1]{}

\title{\textbf{Computational Fuzzy Extractors}}
\author{Benjamin Fuller%\footnote{The work of Benjamin Fuller is sponsored by the United States Air Force under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.}
 \and Xianrui Meng \and Leonid Reyzin}
\begin{document}
\maketitle

\begin{abstract} 
One-message information reconciliation protocols called secure sketches remove noise
from secrets without revealing too much about them.  They are a crucial ingredient in
fuzzy extractors, which provide an information-theoretic mechanism for reliable key derivation from noisy sources.

Secure sketches necessarily reveal some information about the noisy secret, and thus the derived key in fuzzy extractors has lower entropy than the source.  The minimal entropy loss is dictated by bounds on error-correcting codes.

We overcome these bounds by considering computational, rather than information-theoretic, secure sketches and fuzzy extractors, and building them based on the Learning with Errors (LWE) problem.  We thus obtain the first secure sketch with no loss in entropy.   More specifically, our contributions are as follows:

\begin{itemize}
\item \textbf{New Definitions:} In seeking the right computational relaxation of the information-theoretic definition of a secure sketch, we rule out most natural one, which uses HILL pseudoentropy.  We do so by demonstrating that a secure sketch defined via HILL pseudoentropy is subject to the same coding-based  bounds as the information-theoretic secure sketch.  We then find a different relaxation, using unpredictability entropy, and show that it can be used to construct computational fuzzy extractors.

\item \textbf{New Construction:} We build a lossless secure sketch for uniform sources and certain other sources of high entropy (namely, symbol-fixing sources, in which each dimension is either uniform or fixed),  based on the hardness of the LWE problem.  This construction requires us to show that the decision version of LWE is secure when a small number of dimensions have no error---a result that may be of independent interest.
\end{itemize}
\end{abstract}

\newcommand{\M}{\mathcal{M}}
\section{Introduction}\label{sec:introduction}

Authentication generally requires secrets.  Unfortunately, many useful  sources that contain sufficient entropy for a secret  are also noisy, and provide similar, but not identical secret values at each invocation (examples of such sources include biometrics~\cite{daugman2004}, human memory~\cite{zviran1993comparison}, pictorial passwords~\cite{brostoff2000passfaces}, measurements of capacitance~\cite{tuyls2006puf}, timing~\cite{suh2007physical}, motion~\cite{castelluccia2005shake},  quantum information~\cite{bennett1988privacy} etc.).  Information reconciliation protocols~\cite{bennett1988privacy} remove the noise without revealing the secrets.

We will specifically focus on a one-round information reconciliation  mechanism called ``secure sketch,''  which, aside from its immediate application to authentication, is also
a crucial ingredient in the construction of fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} (as well as in other contexts, e.g.,~\cite{Boyen05secureremote,dodisWichs2009,chandran2010privacy}).
It consists of  two algorithms: Sketch (used once) and Recover (used subsequently).  The Sketch ($\sketch$) algorithm takes an input $w$ and produces a sketch $s$.  This information allows
the Recover ($\rec$) algorithm to recover $w$ given $s$ and some value $w'$ that is close (according to some predefined metric, such as Hamming distance) to $w$. 
Crucially for security,  knowledge of $s$ should not reveal $w$; that is, $w$ should still have entropy  conditioned on $s$.  This feature is needed because $s$ is not secret: for example, in a single-user setting (where the user wants to recover the original reading $w$ from a subsequent reading $w'$), it would be stored in the clear; and in a key agreement application~\cite{Boyen05secureremote} (where two parties have $w$ and $w'$, respectively), it would be transmitted between the parties.
As defined in \cite{DBLP:journals/siamcomp/DodisORS08}, the entropy loss of a secure sketch is the difference in the entropy of $w$ and the entropy of $w$ conditioned on $s$.  

Secret entropy of convenient sources is often at a premium; a goal of secure sketch constructions is to minimize the entropy loss, increasing the security of the resulting application. For example, in the application of secure sketches to fuzzy extractors (whose goal is to reliably produce a uniformly random secret key from a noisy input), the resulting secret key may be too short to be useful if the entropy loss is too high. However, because secure sketches are defined as information-theoretic objects, some entropy loss is inherent in any secure sketch construction, because $s$ contains enough information to recover $w$ from any value $w'$ that is close to it.  

The minimum necessary  entropy loss has been quantified in past work.  Even if we don't require perfect correctness (i.e., allow $\rec$ to err with small probability~\cite[Section 8]{DBLP:journals/siamcomp/DodisORS08}), the entropy loss of a secure sketch that can reconcile $t$ errors with high probability is bounded by the redundancy of
the best code that can correct $t$ random errors with high probability~\cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}
%(the argument is the same as in \cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08}\lnote{probably we should double-check that this is correct}).

\paragraph {Our Contributions}
To see if entropy loss can be improved, we explore secure sketches with the relaxed secrecy requirement that is computational rather than information-theoretic.  The definition of  \cite{DBLP:journals/siamcomp/DodisORS08} uses min-entropy as the information-theoretic measure of secrecy. 
 Finding the right computational measure of secrecy requires us to consider different possible computational analogs of min-entropy.

The natural relaxation of this secrecy requirement is to require HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99}~(namely, that the distribution of $w$ conditioned on $s$ be \emph{indistinguishable} from a high-min-entropy distribution).  Under this definition, using the paradigm for constructing fuzzy extractors from~\cite{DBLP:journals/siamcomp/DodisORS08} would yield a pseudorandom key.  

On the negative side, we show defining sketches using HILL entropy is unlikely to be fruitful. We prove in \corref{cor:rec yields sketch} that the entropy loss of such secure sketches is subject to the same coding bounds as the ones that constrain information-theoretic secure sketches.
%Furthermore, we have (asymptotic)~constructions that meet these bounds.

On the positive side, we show that a different relaxation of the security requirement can in fact lead to a \emph{lossless} construction of secure sketches, in which the knowledge of $s$ does not reduce entropy of $w$ (see Theorem~%s~%\ref{thm:lossless secure sketch} and
~\ref{thm:lossless secure sketch log})).  This relaxation requires that $w$ have high \emph{unpredictability} entropy conditioned on $s$ (unpredictability  entropy is a computational notion originally defined in \cite[Section 5]{DBLP:conf/eurocrypt/HsiaoLR07}).  We demonstrate how to build such lossless sketches based on the Learning with Errors~(LWE) assumption due to Regev~\cite{regev2005LWE, regevLWEsurvey},  which says that decoding a random linear code is computationally difficult. 

We show that this relaxation of the security requirement is  useful.  Specifically, we also define the notion of computational fuzzy extractors, in which the extracted key is required to be pseudorandom---i.e., to look random to any computationally bounded observer.
We show that our relaxed notion of secure sketches is sufficient to construct computational  fuzzy extractors via a simple and efficient construction, utilizing reconstructive extractors~\cite{barak-computational}.  The resulting fuzzy extractor produces a key that is as long as the entropy of $w$, minus only the loss necessary in any extractor construction.
\lnote{it would be good to find another application} 

Both our lossless secure sketch and the resulting fuzzy extractor are built via the code-offset construction~\cite{JW99},\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} already used in prior work.  To be able to apply the LWE assumption, we use a random linear code (unfortunately, our decoding algorithm can only reconcile a logarithmic number of differences).  We utilize the recent result of D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} that shows security of LWE when the error vector comes from the uniform distribution, with each coordinate ranging over a small interval.  The LWE error vector, in our case, is the input $w$ itself.  To utilize the result of~\cite{dottling2012}, we need to assume that the input comes from the uniform distribution. We are able to relax this limitation somewhat and allow $w$ to come from a symbol-fixing source~\cite{KZ07} (where each dimension is either uniform or fixed). This relaxation requires new results about the hardness of LWE when samples have a fixed error vector, which may be of independent interest~(\thref{thm:blockLWE}).

\section{Preliminaries}
\label{sec:preliminaries}
We begin by recalling a few basic entropy definitions.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
%the {\em (worst-case) conditional min-entropy} of $X$ given $Y$ is  $\Hoo(X|Y) = -\log(\max_{x,y} \Pr[X=x|Y=y])$, and
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite{DBLP:journals/siamcomp/DodisORS08}.  
%The {\em collision probability} of  $X$ is 
%$\col(X) = \sum_{x} \Pr[X=x]^2$.
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. We write $X \approx_{\epsilon} Y$ if $\Delta(X,Y) \leq \epsilon$, and when $\epsilon$ is negligible (in the appropriate parameter, as clear from the context) then we say $X$ and $Y$ are \emph{statistically close}.  For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized $\{0,1\}$ distinguishers of size at most $s_{sec}$.
For a metric space $(\mathcal{M}, \dis)$ be define the \emph{(closed) ball of radius $t$ around $x$} as the  set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.

\subsection{Fuzzy Extractors and Secure Sketches}
\label{sec:fuzzy extractors}

We now recall definitions and lemmas from the work of Dodis et. al.~\cite[Sections 2.5--4.1, 8]{DBLP:journals/siamcomp/DodisORS08}.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.% \lnote{$d$ is overloaded -- it's also a seed length for an extractor and also a constant in the parameter setting.  can we change this $d$ to $\mathsf{dis}$?}	

\begin{definition}%\protect{\cite[Definition 5]{DBLP:journals/siamcomp/DodisORS08}}
An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} is a pair of randomized procedures, ``generate'' (\gen) and ``reproduce'' (\rep), with the following properties:
\begin{enumerate}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that if $\dis(w,w')\leq t$ and $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
\end{enumerate}
\end{definition}
A fuzzy extractor can be produced from a \emph{average-case randomness extractor} %(all randomness extractors are average-case extractors with a small loss, see~\cite[Chapter 6]{vadhan2012}) 
and a \emph{secure sketch}.  
An average-case extractor is simply a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}):

\begin{definition}
Let $\chi_1$, $\chi_2$ be finite sets.
An extractor $\ext$ is a \emph{$(k, \epsilon)$-average-case extractor} if for all pairs
of random variables $X, Y$ over $\chi_1, \chi_2$ such that
$\tilde{H}_\infty(X|Y) \ge k$, we have $\Delta((\ext(X, U_d), Y), U_m\times
U_d \times Y) \le \epsilon$.
\end{definition}

Secure sketches are the main technical tool in the construction of fuzzy extractors.  They produces a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
\begin{definition}%\protect{\cite[Definition 3]{DBLP:journals/siamcomp/DodisORS08}}
\label{def:secure sketch}
An $(\mathcal{M},m, \tilde{m}, t, \epsilon)$-\emph{approximately correct secure sketch} is a pair of randomized procedures, ``sketch'' (\sketch) and ``recover'' (\rec), with the following properties:
\begin{enumerate}
\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',SS(w))=w]\geq 1-\epsilon$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|SS(W))\geq \tilde{m}$.
\end{enumerate}
A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time.  If $\epsilon=0$ we omit $\epsilon$ and say that $(\sketch, \rec)$ is a $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch}.
\end{definition}
For an approximately correct secure sketch we assume that errors are drawn independently of $\sketch(w)$.  If the error pattern between $w$ and $w'$ depends on $\sketch(w)$ there is no guarantee about the probability of correctness.
%In \secref{sec:defCompFuzzyExtractors} we will use a slightly weaker notion, in which \rec is allowed to fail with small probability.\lnote{Can we add a comment that this corresponds to errors that can depend on the input but not the sketch? It's not obvious from the way this is written.} \lnote{can we just have a single definition --- the one below --- instead of two?}
%\begin{definition}
%We say that pair of procedures $(\sketch, \rec)$ is $(\mathcal{M}, m, \tilde{m}, t, \epsilon)$ is an \emph{approximately correct secure sketch} if \defref{def:secure sketch} holds with correctness replaced with the following property, $\forall w, w'$ such that $\dis(w, w')\leq t$:
%\[
%\Pr[\rec(w', \sketch(w)) = w]\geq 1-\epsilon.
%\] where the probability is taken over the coins of \sketch and \rec.
%\end{definition}
A fuzzy extractor is usually formed by combining a randomness extractor with a secure sketch.
\begin{lemma}%\protect{\cite[Lemma 4.1]{DBLP:journals/siamcomp/DodisORS08}}
\label{lem:fuzzy ext construction}
Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch, and let $\ext$ be an average-case $(n, \tilde{m}, \ell, \epsilon)$-strong extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor~(for $(r, x)$ drawn from the uniform distribution):
\begin{itemize}
\item $\gen(w; r, x):$ set $P=(\sketch(w; r), x), R=\ext(w;x)$, and output $(R,P)$.
\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $R=\ext(w;x)$.
\end{itemize}
\end{lemma}
The main parameter we will be concerned with is the entropy loss of the construction, specifically, the entropy loss in the definition of the secure sketch~($m-\tilde{m}$).  %The cause for the entropy loss in the secure sketch is the additional bits added for error correction.  In the definition of a fuzzy extractor this is the input min-entropy less the size of the output: $m-\ell$.   
%The entropy loss in the fuzzy extractor is due to the use of an extractor (where the loss is inversely proportional to the statistical distance of the output distribution from uniform) and the use of the secure sketch.


%=============================================================
\section{Defining Computational Secure Sketches}
\label{sec:defCompFuzzyExtractors}
We first recall a commonly used computational notion of entropy \cite{DBLP:journals/siamcomp/HastadILL99}, as extended to the conditional case by Hsiao, Lu, Reyzin~\cite{DBLP:conf/eurocrypt/HsiaoLR07}.

\begin{definition}
Let $(X, Y)$ be a pair of random variables.  $X$ has 
\emph{conditional HILL entropy} at least $k$ conditioned on $Y$,
denoted $H^{\hill}_{\epsilon, s_{sec}}(X|Y)\geq k$ if there exists a collection of
distributions $Z_y$ for each $y\in Y$, giving rise to a joint distribution
$(Z, Y)$, such that $\tilde{H}_\infty(Z|Y)\geq k$ and for all (randomized, binary output) circuits $D$ of size $s_{sec}$, $\delta^D((X, Y),(Z,Y))\leq \epsilon$.
\end{definition}

Intuitively, conditional HILL entropy is as good as average min-entropy for all computationally-bounded observers.  Thus, redefining secure sketches using HILL entropy is a  natural relaxation of the original information-theoretic definition; in particular, the construction in \lemref{lem:fuzzy ext construction} would yield pseudorandom outputs if the secure sketch ensured high HILL entropy.  However, as we show in Section~\ref{sec:imp HILL sketch}, such secure sketches can't perform much better than the information-theoretic ones.

Instead, in \secref{sec:unp secure sketch}, we define computational secure sketches using unpredictability, and work with that definition for the rest of the paper.  In particular, in \secref{sec:comp fuzzy extractors}, we show a natural construction of computational fuzzy extractors from secure sketches defined using unpredictability.
%The size of the circuits becomes important, we would like the \rec procedure to be computable by a circuit significantly smaller than the \hill entropy circuit size.  We simply include both of these parameters in the definition.  We review other definitions of computational entropy in Appendix~\ref{sec:notionsEntropy}.


\subsection{Bounds on Secure Sketches using HILL entropy}
\label{sec:imp HILL sketch}
It seems natural to replace a sketch that retains information theoretic entropy with a sketch that retains HILL entropy.  Then the paradigm of secure sketch and average case randomness extractor could be used. However, we show that using this definition it is impossible to beat information theoretic bounds on secure sketches.  We omit formal definitions for the moment and recall the two informal properties we seek:
\begin{itemize}
\item \emph{Correctness}: if $\dis(w, w')\leq t$ then $\rec(w', \sketch(w)) = w$.
\item \emph{Security}: For all $W$ where $H_\infty(W)\geq m$, $H^\hill_{\epsilon, s}(W|\sketch(W))  = \tilde{m}$.
\end{itemize}
%We will concentrate on the case where $gap = m-\tilde{m}=0$ however, any setting where  $gap$ is better than the best information theoretic sketch is still a meaningful result. % As before, we desired $\rec$ to be efficiently computable.  We start with this stronger definition to show the difficulty of constructing such an object. 

Unfortunately, we will show if $H^{\hill}_{\epsilon, s}(W|SS(W))= \tilde{m}$ this implies an error correcting code with approximately $2^{\tilde{m}}$ points.  We provide intuition for the argument below and then present the formal argument.  Consider a function $\rec$ where $\rec (w', \sketch(w)) = w$ when $\dis(w, w')\leq t$.  This means in order for $H^{\hill}_{\epsilon, s}(W|SS(W))= \tilde{m}$, there must be a distribution $Y$ where $\Hoo(Y)\geq \tilde{m}$ such that $\Pr_{y\in Y} [y'\leftarrow B_t(y) \wedge \rec(y', \sketch(y)) = y ]\geq 1-\epsilon$.  This means the points in $Y$ form an error correcting code and $\rec$ is an efficient decoder.  We can then use standard arguments to turn this code into an information theoretic sketch.  This means lower bounds on the entropy loss of an information theoretic sketch must apply.  There are several technical conditions required to make this intuition formal, we present them below.
%any function that is able to recover correctly on a large space is an decoder for an error correcting code formed by that space.  If there is a distribution $Y$ with many points such that $\rec(y', \sketch(w)) =y\in Y$ when $\dis(y, y')\leq t$.  Then $\rec$ is a decoder for the error correcting code formed by $Y$.
% and make no assumption about the behavior of $\rec $ outside of this ball.  The intuition is quite simply, on a point $w\leftarrow W$, the \rec oracle will always return $w$ within the $B_t(w)$.  If $W|\sketch(W)$ is indistinguishable from $Y|\sketch(W)$ then the $\rec$ oracle must also correct for points in $Y$.  However, this implies a code~(the points in $Y$) with $\rec$ as an efficient decoding algorithm. 
%\subsubsection{The Uniform Distribution}
%and extend to an arbitrary high-entropy distribution.  
%We begin by showing that any given no distribution $W$ can be indistinguishable from the uniform distribution given a sketch.
%We need two technical requirements on our metric space: 1) there is an efficient algorithm to sample neighbors 2) each point is regular~(has the same number of neighbors).  These requirements hold for several natural metric spaces including Hamming and symmetric difference metrics.
%The only other requirement we make is for the distribution and its neighborhood to be efficiently samplable.
%\begin{definition}
%A distribution $W$ is $s-$\emph{efficiently-samplable} if there exists a randomized circuit of size at most $s$ that outputs samples according to $W$.
%Furthermore, we say $W$ is \emph{efficiently-samplable} if there exists a randomized circuit of polynomial size that outputs samples according to $W$.
%\end{definition}
%We will be working with a metric space $\mathcal{M}, d$.  We will need to efficiently sample nearby points in the metric space.
We assume that sampling a random neighbor of a point is efficient:
\begin{definition}
We say a metric space $(\mathcal{M}, \dis)$ is $(s_{sam}, t)$-\emph{neighborhood efficently-samplable} if there exists a randomized circuit, $\sample$ of size $s_{sam}$ that for all $t'\leq t$, $\sample (x, t')$ outputs a random point at distance $t'$ of $x$.  
%That is $\exists \sample$ of size at most $s$ such that $\forall x\in \mathcal{M},$
%\[
%\Delta(\sample(x, t') , \{x'| \dis(x,x') = t'\})=0
%\]
Furthermore, there is a procedure that outputs a random point of distance at most $t$.  For convenience we denote this procedure by $\sample(X)$.
\end{definition}
We require that \sample returns a point different from $x$ to ensure that each $B_t(x)$ contains at least one other point.  While we could consider metric spaces where balls only contain their center, this is not very interesting in the context of fuzzy extractors~(as \gen can always output the empty string for $P$).

%We will concentrate on metrics where the size of each ball is fixed.  We say a metric space that obeys this property is regular~(using terminology from graph theory).  This will allow us to show that the neighbor of a uniformly chosen point is also uniformly random.  
%\begin{definition}
%A metric space $(\mathcal{M},d)$ is \emph{$(t, c)-$distance regular} if for all $x\in \mathcal{M}, |B_t(x)| = c$. 
%\end{definition}
%For a distribution $X$ we define the distribution $N_t(X)$ as the process of sampling a random neighbor of a random point in $X$.  That is $\Pr[N_t(X) = z] = \sum_{x\in X}\Pr[X=x \wedge y \leftarrow B_t(x) \wedge z=y]$.
%\begin{lemma}\label{lem:uniformNeighbor}
%Let $(\mathcal{M}, d)$ be a $(t, c)-$distance regular and let $U$ be uniform over $\mathcal{M}$.  Then $U \overset{d}= N_t(U)$.
%
%\end{lemma}
%\begin{proof}
%Note that $\forall z, \Pr[U = z] =1/|\mathcal{M}|$.  Thus, it suffices to show that $\forall z , \Pr[N_t(U) =z] =1/|\mathcal{M}|$.
%
%Fix an arbitrary $z\in \mathcal{M}$.  Then, 
%\begin{align*}
%\sum_{x\in U} \Pr[U=x \wedge y\leftarrow B_t(x) \wedge y=z] &=
%\sum_{x\in U} \Pr[U = x \wedge x\in B_t(z)] \Pr[y\leftarrow B_t(x) \wedge y=z | x\in B_t(z)]\\
%&=\frac{c}{|\mathcal{M}|} \frac{1}{c} = \frac{1}{\mathcal{M}}.
%\end{align*}
%%The uniform distribution over $\mathcal{M}$ is a random bit string of length $n$.  Let $x\leftarrow \mathcal{M}$, then $\sample (x) = z = x\oplus y$ where $y$ has at weight at least $1$ and at most $t$.  Since $x$ was uniformly random $x\oplus y$ is uniformly random.
%\end{proof}

Finally, we will dealing with an arbitrary secure sketch that only needs to satisfy the correctness property of a secure sketch~(see \defref{def:secure sketch}).  

\begin{definition}
A pair of functions $(\sketch, \rec) $ is a $(s_{rec}, t)$-\emph{recover functionality for a metric space $\mathcal{M}, d$} if $\sketch : \mathcal{M}\rightarrow \{0,1 \}^*$ is a randomized function and $\rec: \mathcal{M}\times \{0, 1\}^*\rightarrow \mathcal{M}$ is a function computable by a circuit of size at most $s_{rec}$, such that $\rec (w', \sketch (w)) = w$ if $\dis(w, w')\leq t$ for all $w, w' \in \mathcal{M}$.
\end{definition}

\ignore{For any recover functionality, there is an inherent tradeoff between the ability to keep points stationary and to perform error correction.

\begin{theorem}\label{thm:compSketchImpUniform}
Let $\mathcal{M}, d$ be a metric space that is $(s_{neigh}, t)$-neighborhood samplable and $(t, c)$-distance regular.
Furthermore, let $(\sketch, \rec)$ be any $(s_{rec}, t)$-recover functionality for $\mathcal{M}, d$.  Then $H^\hill_{\epsilon, s}( U |\sketch(U))\not \geq \Hoo(U)$ for $s \approx (s_{neigh} + 2s_{rec})$ and $\epsilon=1/2$~(if $s_{neigh}, s_{rec}$ are polynomial so is $s$).
\end{theorem}
%\emph{Intuition:  }The goal that a recover functionality cannot be both stable~(maps points to themselves) and correcting~(maps nearby points back to themselves).  If a recovery functionality is correcting then we can check if the input point maps to itself.  If a recovery functionality is stable then we can see if recovery succeeds with nearby points.  

\begin{proof}
Let $U_\mathcal{M}$ be the uniform distribution over $\mathcal{M}$ and let $d$ a  metric.  Furthermore assume $\mathcal{M}$ is $(s_{neigh}, t)$-neighborhood samplable.  Let $(\sketch, \rec)$ be a $(s_{rec}, t)$-recover functionality for $\mathcal{M}, d$.  Let $X\overset{d}= Y\overset{d}= U_\mathcal{M}$ be i.i.d..  It suffices to construct a distinguisher $D$~(whose size is $s\approx s_{neigh}+2s_{rec}$) such that $\expe[D(X, SS(X))]- \expe[D(Y, SS(X))]\geq \epsilon = 1/2$.  
%Let $q$ be a polynomial.  
$D$ is defined as follows:
\begin{enumerate}
\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*$. 
\item Sample $b\leftarrow \{0,1\}, z'\leftarrow \sample(z)$.
\item If $b=1$:
\subitem If $\rec(z', s) = z\wedge \rec(z, s) = z$ output $1$.
\item If $b=0$:
\subitem If $\rec(z, s) = z \wedge \rec(z', s)=z$ output $1$.
\item Output $0$.
\end{enumerate}
First note that $D$ is of size $O(s_{neigh}+2s_{rec})$ and thus in polynomial in $s_{sam}, s_{neigh}$.  It remains to show that $D$ separates $(X,\sketch(X))$ and $(Y,\sketch(X))$.  We being by noting that on input $(X, \sketch(X)), D$ outputs $1$.
By \lemref{lem:uniformNeighbor} we have:
\begin{align*}
0&=\delta([Y, \sample(Y), \sketch(X)],& &[\sample(Y), Y, \sketch(X)])\\
&=\delta([Y, \sketch(X), \sample(Y), \sketch(X)],& &[\sample(Y), \sketch(X), Y, \sketch(X)])\\
&=\delta([\rec(Y,\sketch(X)), \rec(\sample(Y),\sketch(X))],& &[\rec(\sample(Y), \sketch(X)), \rec(Y, \sketch(X))])
\end{align*}
Thus, the output of $\rec$ is the same when given $(\sample(Y), Y)$ or $(Y, \sample(Y))$\footnote{This is also true for a stronger class of recover functionalities that is able to keep state between queries.}.  This means it can properly recover $Y$ with probability at most $1/2$ over the ordering of inputs.  Thus we have, 
\begin{align*}
\Pr[D'(X, \sketch(X))=1] - \Pr[D'(Y, \sketch(X))=1] &=\\
1 - \frac{1}{2}(\Pr[\rec(Y, \sketch(X)) = Y \wedge \rec(\sample(Y),\sketch(X))=Y]&+\\\Pr[\rec(\sample(Y), \sketch(X)) = Y \wedge \rec(Y, \sketch(X)) = Y])&\geq 
1-\frac{1}{2}\left(1\right) =\frac{1}{2}
\end{align*}

%\begin{enumerate}
%\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*$.
%\item Initialize $correct, stable = 0$.
%\item For $i=1...q$:
%\subitem $x_i \leftarrow U$, if $\rec(x_i, s) = x_i, stable +=1/q$.
%\subitem $x_i' \leftarrow \sample (x_i)$, if $\rec(x_i', s) = x_i, correct+=1/q$.
%\item If $\rec(z, s)\neq z$ output $0$.
%\item If $stable<1/2$ output $1$
%\item Else $z'\leftarrow \sample(z)$.
%\subitem If $\rec(z', s) = z$ output $1$.
%\subitem Else output $0$.
%\end{enumerate}
%
%We begin by noting that $D$ is of size $O(q(s_{sam}+s_{neigh}+2s_{rec})+2s_{rec}+s_{neigh})$ and thus is polynomial in $s_{sam}, s_{neigh}, s_{rec}$.  It remains to show that $D$ separates $(X, SS(X))$ and $(Y, SS(X))$ by a noticeable $\epsilon$.  Let $STABLE = \Pr[\rec(U, \sketch(X)) = U]$.  We begin by noting that $\Pr[|stable-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).  That is, after step (3) $D$ has a good approximation
%of $STABLE$.  Similarly, let $CORRECT = \Pr[\rec(\sample(U), \sketch(X)) = U]$.  As before $\Pr[|stable-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).
%Thus, after step (3), $\Pr[|CORRECT-correct|>\ngl \wedge |STABLE-stable|>\ngl|]<\ngl$.  We proceed assuming, that $CORRECT$ and $STABLE$ are known exactly at step (4), and note the output of $D$ is negligibly close to the output of a distinguisher that knows $CORRECT$ and $STABLE$ exactly~(we label this distinguisher $D'$).
%\begin{claim}
%$CORRECT+STABLE\leq 1$.
%\end{claim}
%\begin{proof}
%\begin{align*}
%\Pr[\rec(U, \sketch(X) = U] + \Pr[\rec(\sample(U), \sketch(X)) = U]&\leq\\
%\Pr[\rec(U, \sketch(X) = U]  + \Pr[\rec(Y), \sketch(X)\neq Y] &=1
%\end{align*}
%Where the first inequality proceeds because of \lemref{lem:uniformNeighbor} and the fact that \sample never returns itself.  The equality proceeds as these events as these events are complementary events after renaming. 
%\end{proof}
%We now show that when $(\sketch, \rec)$ is stable or correcting the distinguisher has nonnegligible advantage:
%\begin{itemize}
%\item Case 1: $STABLE<1/2$.
%\begin{align*}
%\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1] &=\\
%\Pr[\rec(X, \sketch(X) = X] - \Pr[\rec(Y, \sketch(X) = Y] &\geq
% 1- 1/2  = 1/2
%\end{align*} 
%\item Case 2: $STABLE\geq 1/2, CORRECT \leq 1/2$.
%\begin{align*}
%\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1]&=\\
%\Pr[\rec(X, \sketch(X)) = X \wedge \rec(\sample(X), \sketch(X)) = X] &-\\\Pr[\rec(Y, \sketch(X) = Y) \wedge \rec(\sample(Y), \sketch(X)) = Y)]&\geq\\
%1 - \Pr[\rec(\sample(Y), \sketch(X)) = Y)]= 1-1/2 = 1/2
%\end{align*}
%\end{itemize}
%Thus, in both cases we have $\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1]\geq 1/2$.  As noted above, $CORRECT, STABLE$ are only known approximately and thus:
%\[
%\Pr[D(X, \sketch(X) )= 1] - \Pr[D(Y, \sketch(X) )= 1]\geq 
%\Pr[D'(X, \sketch(X) )= 1] - \Pr[D'(Y, \sketch(X) )= 1] - \ngl \geq 1/2 -\ngl
%\]
This completes the proof.
\end{proof}

This proof relied on two crucial facts: 1) that the neighbor of a uniform point was uniformly distributed 2) for an point uncorrelated with the value of the secure sketch it was impossible to distinguish between a point and its neighbor.  We can extend this result to distributions other than the uniform distribution but by drawing on more general results from coding theory.  

\subsection{Indistinguishable sketches imply information theoretic sketches}
In this section we show that the results of \thref{thm:compSketchImpUniform} can be extended to whenever $X$ is indistinguishable from a high min-entropy distribution.  }

We show that any recovery functionality that retains high HILL entropy implies a information theoretic secure sketch.  We begin by reviewing definitions of Shannon codes~\cite{shannon1949mathematical}:
\begin{definition}
Let $\mathcal{C}$ be a set over space $\mathcal{M}$.  We say that $\mathcal{C}$ is an $(t,\epsilon)$-\emph{Shannon code} if there exists an efficient procedure $\rec$ such that for all $c\in \mathcal{C}$, $\Pr[\rec(\sample(c, t)) \neq c]<\epsilon$.
\end{definition}
This is a slightly nonstandard formulation, we only require the code to correct exactly $t$ errors\footnote{In the standard formulation, the code must correct up to $t$ errors.  For codes that are monotone~(if decoding succeeds on a set of errors, it succeeds on all subsets), these formulations are equivalent.  However, we work with an arbitrary recover functionality that is not necessarily monotone.}.  Shannon codes work for all codewords, we can also consider a formulation that works for an ``average'' codeword.
\begin{definition}
Let $W$ be a distribution over space $\mathcal{M}$.  We say that $W$ is an $(t,\epsilon)$-\emph{average error Shannon code} if there exists an efficient procedure $\rec$ such that 
$\Pr_{w\leftarrow W}[\rec(\sample(w, t)) \neq w]<\epsilon$.
\end{definition}
An average error Shannon code is convertible to a (maximal error)~Shannon code with a small loss.  We use the following pruning argument from Cover and Thomas~\cite[Pages 202-204]{cover2006elements}~(we provide a proof in \secref{sec:proof of average to maximal error}):
\begin{lemma}
\label{lem:averageToMaximalError}
Let $W$ be a distribution with $\Hoo(W)\geq k$ and let $W$ be a $(t, \epsilon)$-average error Shannon code with recovery procedure $\rec$.  There is a set $W'$ where $|W'|\ge2^{k-1}$ and $W'$~(drawing a codeword uniformly) is a $(t, 2\epsilon)$-(maximal error) Shannon code with recovery procedure $\rec$.
\end{lemma}

With this conversion we can show a surprising result: recover functionalities that retain HILL entropy imply Shannon error correcting codes~(proof in \secref{sec:proof of thm sketch implies code}).
\begin{theorem}\label{thm:impSketchArbitraryW}
Let $(\mathcal{M}, \dis)$ be a metric space that is $(s_{neigh}, t)$-neighborhood samplable.  Let $(\sketch, \rec)$ be an $(s_{rec}, t)$-recover functionality over $\mathcal{M}$.  If $H^{\hill}_{\epsilon, s_{sec}}(X|SS(X))\geq k$ then there exists $x$ and a set $W'$ where $|W'|\geq 2^{k-1}$ such that for all $1\leq t'\leq t$, $W'$ is a $(t', 2\epsilon)$-Shannon code with recovery procedure $\rec(\cdot, \sketch(x))$ for $s_{sec}= \Omega(t(s_{neigh}+s_{rec}))$.
%Let $W$ be a flat distribution~(all points in the support of $W$ have the same probability) over $\mathcal{M}$ where $\Hoo(W)\geq k$ and   Let $X$ be arbitrarily but independently distributed over $\mathcal{M}$.  If $\delta^D((X, \sketch(X)), (W, \sketch(X)))<\epsilon$ for negligible $\epsilon$ and $D$ of size at least $O(s_{neigh}+s_{rec})$, then $\exists x, W'$ where $|W'|\geq |W|/2 \geq 2^{k-1}$ such that $W'$ is a $(t, 2\epsilon)$ Shannon code with recovery procedure $\rec(\cdot, \sketch(x))$.
\end{theorem}
%The code provided in \thref{thm:impSketchArbitraryW} is weaker than a Shannon error correcting code~\cite{shannon1948}.  In a Shannon code, for all codewords $c, \Pr[c'\leftarrow \sample(c) \wedge \rec(c') \neq c]<\epsilon$.  Bounds for this type of code are well known~\cite{shannon1948} and efficient constructions exist~\cite{forney1966}.  Requiring proper decoding for only a random codeword is a significant restriction particularly in the case where the probability of outcomes in $W$ differs significantly~(e.g. where $\exists w_1,w_2\in W$ such that $\Pr[W=w_1]/\Pr[W=w_2] = \ngl$).  However, this is still a meaningful set of error correcting codes.  A similar question is considered in Appendix C of \cite{DBLP:journals/siamcomp/DodisORS08}.  They are able to achieve results for Hamming codes~(as they are not restricted to sampling a polynomial portion of the space)\bnote{expand on this explanation}.


We can use techniques from information reconciliation~\cite{smith2007scrambling} and privacy amplification~\cite{bennett1988privacy} to convert any  Shannon code into an information theoretic security sketch~(see \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08}):
\begin{lemma}
\label{lem:shannon to sketch}
Let $t$ be a parameter.  Let $\mathcal{C}$ over $\{0,1\}^n$ be a set such that for all $1\leq t'\leq t$, $\mathcal{C}$ is a $(t', \epsilon)$ Shannon code with recovery procedure $\rec'$.  Let $W$ be a distribution with $\Hoo(W)\geq k$.  Define the following procedures
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\sketch}
\begin{enumerate}
\item Input $w\leftarrow W$.
\item Sample $\pi$ from the space of all permutations from $\pi: [n]\rightarrow [n]$.
\item Sample $c\leftarrow \mathcal{C}$.
\item Apply $\pi$ bitwise to get $x_i = w_{\pi(i)}$.
\item Output $p = x \oplus c, \pi$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rec}
\begin{enumerate}
\item Input $(w', p, \pi)$
\item Apply $\pi^{-1}$ bitwise to get $x_i' = w_{\pi(i)}$.
\item Compute $c' = p \oplus x'$.
\item Compute $c = \rec'(c)$.
\item Output $p\oplus c$.
\\
\end{enumerate}
\end{minipage} 
\end{tabular}
\end{center}
Then $(\sketch, \rec)$ is a $(\{0,1\}^n, k, k-(n-\log |\mathcal{C}|), t, \epsilon)$ approximately correct secure sketch. 
%satisfies the following properties~(probability over the choice of $\pi$ in $\sketch$):
%\begin{itemize}
%\item Correctness: $\forall w, w'$ where $dis(w, w')\leq t$, $\Pr[\rec(w', \sketch(w))\neq w]<\epsilon$.
%\item Security: $\Hav(W |\sketch(W)) \geq \Hoo(W) - (n-\log |C|)$.
%\item Efficiency: $\sketch$ is efficient if sampling from $C$ is efficient and $\rec$ is efficient if $\rec'$ is efficient.
%\end{itemize}
\end{lemma}
%\begin{construction}
%Let $\delta >0$ be a parameter.
%Let $C$ be a $(t, \epsilon)$ Shannon code over $\{0,1\}^n$.  Choose a permutation $\pi:\mathcal{M}rightarrow \mathcal{M}$ that is $\beta$-almost independent $\ell = \delta n/\log(n)$-wise permutation.  Thefn for $\beta = 2^{-\ell}
%\end{construction}
Putting together \thref{thm:impSketchArbitraryW} and \lemref{lem:shannon to sketch} gives our main negative result.  Any recover functionality that retains HILL entropy implies an information theoretic sketch:
\begin{corollary}
\label{cor:rec yields sketch}
Let $(\sketch', \rec')$ be a $(s_{rec}, t)$-recover functionality over $\{0,1\}^n$ such that $H^{\hill}_{\epsilon, s_{sec}}(X | \sketch'(X))\geq k$.  If $s_{sec}=\Omega(t(s_{rec} + n))$,  then for all $W$ with $\Hoo(W)\geq m$ there exists a $(\{0,1\}^n, m, m-(n-k+1), 2\epsilon)$ (information-theoretic) approximately correct secure sketch.
%Let $(\mathcal{M}, d)$ be a metric space that is $(s_{neigh}, t)$-neighborhood samplable.  If no $(t, \epsilon)$-average error Shannon code over $\mathcal{M}$ exists of size $2^k$ then, $\forall (s_{rec}, t)$ recovery procedures $(\sketch, \rec)$ $H^{\hill}_{\epsilon, s}(X|\sketch(X))\not\geq k$ for $s=O(s_{rec}+s_{neigh})$.
\end{corollary}
\textbf{Note:} In \corref{cor:rec yields sketch} we make no claim about the feasibility of constructing $(\sketch, \rec)$.  Several steps in \thref{thm:impSketchArbitraryW} are not constructive so there may not be an efficient procedure for $\sketch$.

\corref{cor:rec yields sketch} shows that sketches that retain HILL entropy can be converted into an information-theoretic sketch.  Unfortunately, we have lower bounds for the entropy drop for information-theoretic sketches:

\begin{proposition}\protect{\cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}}
\label{prop:lower bound entropy drop}
For any error rate $1\leq t\leq n$, any secure sketch which corrects $t$ random errors with probability at least $2/3$ has entropy loss at least $n(h(t/n) -o(1))$; that is, $\Hav(W|\sketch(W))\leq n (1-h(t/n) - o(1))$ when $W$ is drawn uniformly from $\{0,1\}^n$.  Here $h(\cdot)$ is the binary entropy function.
\end{proposition}

Together \corref{cor:rec yields sketch} and \propref{prop:lower bound entropy drop} say the entropy drop for a ``HILL sketch'' must be the same as the entropy drop of a sketch that corrects random errors.  This entropy drop is tied to the Shannon capacity.  Furthermore, we have information theoretic constructions that meet these bounds.  Thus, defining a computational secure sketch using HILL entropy is unlikely to be useful.


%\begin{theorem}
%Let $\mathcal{M}, d$ be a finite metric space that is $s_{sam}$ efficiently samplable and $s_{ball}, t$ neighborhood efficiently samplable.  Furthermore, let $(\sketch, \rec)$ be any $(s_{rec}, t)$-recover functionality for $\mathcal{M}, d$.  Let $U$ be the uniform distribution over $\mathcal{M}$.  If $s_{ball}, s_{rec}, s_{sam}$ are of polynomial size, then $H^\hill_{\epsilon, s}( U |\sketch(U))\not \geq \Hoo(U)$ for polynomial $s$ and noticeable $\epsilon$.
%\end{theorem}
%\emph{Intuition:  }We will show that a recover functionality cannot be both stable~(maps points to themselves) and correcting~(maps nearby points back to themselves).  If a recovery functionality is correcting then we can check if the input point maps to itself.  If a recovery functionality is stable then we can see if recovery succeeds with nearby points.  
%
%\begin{proof}
%Let $\mathcal{M}, d$ be a finite metric space that is $s_{sam}$ efficiently samplable and $s_{ball}, t$ neighborhood efficiently samplable~(by $\sample$).  Let $(\sketch, \rec)$ be a $s_{rec}, t$-recover functionality for $\mathcal{M}, d$.  Let $X, Y, U$ be i.i.d. and be the uniform distribution over $\mathcal{M}$.  By assumption $s_{ball}, s_{sam}, s_{rec}$ be of polynomial size.  It suffices to construct a polynomial size distinguisher $D$ such that $\expe[D(X, SS(X))]- \expe[D(Y, SS(X)]> \epsilon$ for noticeable $\epsilon$.  Let $q$ be a polynomial.  $D$ is defined as follows:
%
%\begin{enumerate}
%\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*$.
%\item Initialize $correct, stable = 0$.
%\item For $i=1...q$:
%\subitem $x_i \leftarrow U$, if $\rec(x_i, s) = x_i, stable +=1/q$.
%\subitem $x_i' \leftarrow \sample (x_i)$, if $\rec(x_i', s) = x_i, correct+=1/q$.
%\item If $\rec(z, s)\neq z$ output $0$.
%\item If $stable<2/3$ output $1$
%\item Else $z'\leftarrow \sample(z)$.
%\subitem If $\rec(z', s) = z$ output $1$.
%\subitem Else output $0$.
%\end{enumerate}
%
%We begin by noting that $D$ is of size at most $q(s_{sam}+s_{ball}+2s_{rec})+2s_{rec}+s_{ball}$ and thus is polynomial assuming that $q, s_{sam}, s_{ball}, s_{rec}$ are of polynomial size.  It remains to show that $D$ separates $X, SS(X)$ and $Y, SS(X)$ by a noticeable $\epsilon$.  Let $STABLE = \Pr[\rec(U, \sketch(X)) = U]$.  We begin by noting that $\Pr[|stable-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).  That is, after step (3) $D$ has a good approximation
%of $STABLE$.  Similarly, let $CORRECT = \Pr[\rec(\sample(U), \sketch(X)) = U]$.  As before $\Pr[|correct-STABLE|>\ngl]<\ngl(n)$ assuming enough queries~(\bnote{fill in this information}).
%
%\begin{claim}
%$CORRECT+STABLE\leq 1$.
%\end{claim}
%\bnote{finish proof}
%
%\end{proof}

\subsection{Defining Secure Sketches using unpredictability entropy}
\label{sec:unp secure sketch}
In \secref{sec:imp HILL sketch}, we showed that defining a computational secure sketch using HILL entropy connects the parameters to the best information-theoretic sketches.  The basic problem is that an adversary had access to both $X$ and $SS(X)$ and could test if neighborhoods correctly decoded.  However, if we define the secure sketch using unpredictability entropy~\cite[Section 5]{DBLP:conf/eurocrypt/HsiaoLR07}, this oracle is not available to the adversary.  We begin by defining unpredictability entropy:

\begin{definition}
\label{def:unp entropy}
For a distribution $(X, Z)$, we say that $X$ has unpredictability entropy at least $k$ conditioned on $Z,$ denoted by $H^{\unp}_{\epsilon, s_{sec}} (X|Z) \geq k$, if there exists a collection of distributions $Y_z$ (giving rise to a joint distribution $(Y, Z)$) such that $\forall D\in \mathcal{D}_{s_{sec}}, \delta^D((X, Z),(Y, Z))\leq \epsilon$, and for all circuits $C$ of size $s_{sec}$,
\[
\Pr[C(Z) = Y ] \leq 2^{-k}
.\]
\end{definition}
We now provide a definition of a computational secure sketch using unpredictability entropy:
\begin{definition}\label{def:comp secure sketch}
An $(\mathcal{M},m, \tilde{m}, \epsilon, s_{sec}, s_{sketch}, s_{rec}, t)$-\emph{computational secure sketch} is a pair of randomized procedures, sketch (\sketch) and recover (\rec), with the following properties:
\begin{enumerate}
\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$ and runs in expected time $s_{sketch}$.
\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $d(w,w')\leq t$, then $\rec(w',SS(w))=w$ and \rec runs in expected time $s_{rec}$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.  
\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ is unpredictable given $\sketch(W)$.  That is, $H^{\unp}_{\epsilon, s_{sec}}(W|\sketch(W))\geq \tilde{m}$.
\end{enumerate}
We say a construction is \emph{lossless} if $m=\tilde{m}$ and we omit the parameter $\tilde{m}$.
\end{definition}
We make $s_{sketch}, s_{rec}$ explicit in the definition as we are considering computationally bounded adversaries and the definition is only meaningful if both $s_{rec}$ and $s_{sketch}$ are significantly smaller than $s_{sec}$.  In \secref{sec:comp fuzzy extractors}, we show a natural construction of a computational fuzzy extractor from a computational secure sketch that meets \defref{def:comp secure sketch}.  We show in the next section that a lossless construction is achievable for the uniform distribution using the LWE problem.

\section{Lossless sketch based on \class{LWE}}
\label{sec:fuzzyCompExt}

In this section we describe our main construction.  The secure sketch is computed via the code-offset construction~\cite{JW99},\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} instantiated with a random linear code over a finite field $\Fq$.   Let $\mathcal{I}_t$ be an algorithm that decodes a random linear code with at most $t$ errors~(not necessarily efficient).  We provide an informal description below.
%Please refer to \consref{cons:informal construction} for the description of the construction.  

\begin{construction}
Let $n$ be a security parameter and let $m>n$.  %Let $W = W_1,..., W_m$ and let $b$ be a constant and let $W_i \in \zo^b$ for all $i$.  
Define $\sketch, \rec$ as follows:%The following construction is a computational secure sketch:
%Code-offset construction of a secure sketch using a random linear code over a finite field $\Fq$. $\mathcal{I}_t (\vA, \vect{D})$ denotes an algorithm that decodes up to $t$ errors, i.e., an algorithm that finds $X'\in \Fq^n$ such that $\vect{A}X'$ is of Hamming distance at most $t$ from $\vect{D}$. $m > n$ are parameters to be figured out later.
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\sketch}
\begin{enumerate}
\item \underline{Input}: $w\leftarrow W$ (where $W$ is some distribution over $\Fq^m$).
\item Sample $\vA\in\Fq^{m\times n}, X\in\Fq^n$ uniformly.
\item Output $s = (\vA, \vA X+w)$.
\\
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rec}
\begin{enumerate}
\item \underline{Input}: $(w', s)$ (where the Hamming distance between $w'$ and $w$ is at most $t$).
\item Parse $s$ as $(\vA, \vect{C})$; let $\vect{D}=\vect{C}-w'$.
\item Set $X' = \mathcal{I}_t(\vA, \vect{D}) $. 
\item Output $w = \vect{C}-\vA X'$.
\end{enumerate}
\end{minipage} 
\end{tabular}
\end{center}
\label{cons:informal construction}
\end{construction}


Intuitively, security comes from the computational hardness of decoding random linear codes from a high number of errors (introduced by $w$).  
In fact, we know that decoding a random linear code is NP-hard~\cite{berlekamp1978}; however, this statement is not sufficient for our security goal, which is to show  $H^{\unp}_{\epsilon, s_{sec}}(W|\sketch(W)) \approx \Hoo(W)$.  Efficiency comes from the fact that up to some number of errors $t$, decoding is easy and thus $\mathcal{I}_t$ can be made polynomial-time.

The rest of this section is devoted to making these intuitive statements precise.
 We describe the \class{LWE} problem and the security of our construction in \secref{subsec:LWE}.
 %a recent variant due to D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} in \secref{subsec:LWE}.  
We describe conditions for efficient decoding in \secref{sec:time main construction}.  Finally, in \secref{sec:lossless sketch} we describe parameter settings where our construction yields a lossless secure sketch.\lnote{revise after going through the rest}.



\subsection{Security of \consref{cons:informal construction}}
\label{subsec:LWE}
The $\LWE$ problem was introduced by Regev \cite{regev2005LWE, regevLWEsurvey} as a generalization of ``learning parity with noise." For positive integers $n$ and $q \ge 2$, a vector $\vx \in \Fq^n$, and a probability distribution $\chi$ on $\Fq$, let $A_{\vect{x}, \chi}$ be the distribution obtained by choosing $\vA \overset{\$}\leftarrow \Fq^{m\times n}$ uniformly at random and a noise term $\ve \overset{\$}\leftarrow \chi^m$, and outputting $(\vA, \vA\vx+\ve)$. 
%The Learning With Errors ($\lwe$) is defined as follows: given access to the oracle that outputs a sample from the distribution $A_{\vx, \chi}$ for a uniform $\vx$, and output $\vx$ with high probability. 

\begin{definition}[Decisional $\lwe$]\label{def:dist-LWE}
Let $m = m(n) = \poly(n)$ be an integer, $q = q(n) = \poly(n)$. The decisional version of the $\LWE$ problem, denoted \class{dist}-$\LWE_{n, m, q, \chi}$, is to distinguish between $m$ samples chosen according to $A_{\vx, \chi}$ for a uniformly random $\vx$ from samples chosen according to the uniform distribution over $(\Fq^{m\times n}, \Fq^m)$. 

We say that $\distLWE_{n, m, q, \chi}$ is $(\epsilon, t)$-hard if no (probabilistic) distinguisher running in time $t$ can distinguish the $\lwe$ instances from uniform except with the probability $\epsilon$.
\end{definition}

Denote by $\bar{\Psi}_\alpha$ the discretized Gaussian distribution over $\Fq$ with variance $(\alpha q)^2/(2\pi)$, Regev\cite{regev2005LWE} and Peikert \cite{peikert2009latticereduction} shows that, when $\chi = \bar{\Psi}_{\alpha}$, $\class{dist}$-$\lwe_{n, m, q, \chi}$ is hard under the worst-case lattice problems.
%The matrix $\vA$ and secret $\vx$ are drawn from the uniform distribution, while the error term is drawn from a discretized Gaussian distribution $\bar{\Psi}_{\alpha}$.  
Furthermore, Regev \cite{regev2005LWE} shows a decision to search equivalence for $q=\poly(n)$.
%We state the decisional version in terms of unpredictability entropy~(see \defref{def:unp entropy} for the definition of unpredictability entropy and \secref{sec:LWEoverview} for an equivalence to the standard formulation of decisional LWE):


\begin{proposition} 
\label{assume:entropy LWE}
Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be a prime and let $\bar{\Psi}_\alpha$ be the discretized Gaussian distribution with variance $(\alpha q)^2/2\pi$.  If GAPSVP and SIVP are hard to approximate within polynomial factors for quantum algorithms, then $\distLWE_{n, m, q, \chi}$ is $(\epsilon, \poly(n))\mbox{-}hard$ for all probabilistic distinguisher $D$, where $\epsilon = \ngl(n)$.

%H^{\unp}_{\epsilon, s_{sec}}(X | \vA, \vA X+E) =H^{\unp}_{\epsilon, s'_{sec}}(E| \vA, \vA X+E) \geq n\log q $ for $s'_{sec}\approx s_{sec}$.  
\end{proposition}

We cannot immediately adapt the construction in \consref{cons:informal construction} to the LWE problem.  Standard formulations of LWE require the error term to come from the discretized Gaussian distribution.  
%\subsection{LWE with uniform errors}
%\label{subsec:LWE uniform error}
Recent work D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} shows the security of LWE with the uniform distribution over an interval reduces to the security of LWE where the noise comes from the discretized Gaussian.  This allows us to directly encode $w$ as the error term in an LWE problem by splitting it properly into $m$ blocks.
%As stated above this has two advantages for Construction~\ref{cons:LWESecureSketch}: 1) the bits of $W$ can be used directly and \rep can output $w$ instead of $sam_w$ 2) there is a fixed number of bits required to sample each dimension.  
We present the formulation of D\"{o}ttling and M\"{u}ller-Quade: 
%\begin{lemma}~\protect{~\cite[Theorem 5]{dottling2012}}
%\label{lem:uniform LWE}
%Let $n$ be a security parameter.  Let $q = q(n))$ and $m = m(n) = \poly(n)$ be integers.  Let $\rho = \rho(n) \in (0,1)$ be such that $\rho q\geq 2n^2m$.  Assume there exists a PPT-algorithm that solves the LWE decision problem where $\vA,X$ are drawn uniformly at random and $E$ is drawn from the interval $[-\rho q, \rho q]$ with non-negligible probability.  Then there exists an efficient quantum-algorithm that approximates the decision-version of the shortest vector problem (GAPSVP) and the shortest independent vectors problem (SIVP) to within $\tilde{O}(n^{5/2}m/\rho)$ in the worst case.
%\end{lemma}

%This problem can be stated in the language of indistinguishability using the Decision-to-search reduction of Regev or Peikert:
\begin{lemma}~\protect{~\cite[Corollary 10]{dottling2012}}
\label{lem:uniform LWE decision}
Let $n$ be a security parameter.  Let $q = q(n) = \poly(n)$ be a prime integer and $m = m(n) = \poly(n)$ be an integer. Let $\gamma\in (0, 1)$ be a constant and let $\rho\in (0,1)$ such that $\rho q \geq 2n^{1/2+3\gamma}m$.  Assume there exists a PPT-distinguisher $D$ such that $\delta^D((\vA, \vA X+E), (\vA, U))>\epsilon$ for non-negligible $\epsilon$ and $\vA\in \Fq^{m\times n}, X\in \Fq^{n}, U\in \Fq^m$ drawn uniformly and $E$ drawn uniformly from $[-\rho q, \rho q]$.  Then there exists an efficient quantum-algorithm that approximates the decision-version of the shortest vector problem (GAPSVP) and the shortest independent vectors problem (SIVP) to within $\tilde{O}(n^{1/2+3\gamma}m/\rho)$ in the worst case.
\end{lemma}

We will restate any $\distLWE_{n, m, q, \chi}$ formulation using unpredictability entropy~(recall \defref{def:unp entropy}).  We have the following statement about the unpredictability of $\LWE$ problems~(proof in \secref{sec:proof of unpred conversion}).
\begin{lemma}
\label{lem:conversion to unpredictability}
Assume that $\distLWE_{n-k, m, q, \chi}$ is $(\epsilon, s_{sec})$-hard and $m\geq n$ then  $H^{\unp}_{\epsilon', s_{sec}'}(E|\vA, \vA X+E) \geq k\log q$
for $\epsilon' \approx \epsilon, s_{sec}'\approx s_{sec}$.
\end{lemma}
%\begin{lemma}
%\label{lem:dmq with unp entropy}
%Let $n$ be a security parameter.  Let $q = q(n) = \poly(n)$ be a prime integer and $m = m(n) = \poly(n)$ be an integer. Let $\gamma\in (0, 1)$ be a constant and let $\rho\in (0,1)$ such that $\rho q \geq 2n^{1/2+3\gamma}m$.  Assume that approximating GAPSVP and SIVP using a polynomial time quantum algorithm is hard within a factor of $\tilde{O}(n^{1/2+3\gamma}m/\rho)$.  Let $\vA\in \Fq^{m\times n}, X\in\Fq^{n}, E\in [-\rho q, \rho q]^m$ be drawn uniformly.  Then for $s_{sec} = \poly(n)$ and $\epsilon = \ngl(n)$
%\[
%H^{\unp}_{\epsilon, s_{sec}}(E| AX+E) = \min\{|X|, |E|\} = \min \{ n\log q, m\log \rho q\}
%\]
%\end{lemma}
With \lemref{lem:uniform LWE decision} we can use the code-offset construction in \consref{cons:informal construction} and retain unpredictability entropy $n\log q$.  We provide a formal description of our algorithm in the next section.
%\lemref{lem:uniform LWE decision} has two advantages: 1) We can use the code-offset construction from \consref{cons:informal construction}. 2) The error term is distributed over a smaller interval $[-\rho q, \rho q]$ instead of $\Fq$.

%\subsection{Computational Secure Sketch based on \class{LWE}}
%\label{subsec:fuzzyExtLWE}
%We now give a formal description of our sketch based on LWE.  We now consider a source $W$~(this fulfills the role of $E$ in standard LWE notation):
%
%\begin{construction}[Computation Secure Sketch based on LWE]
%\label{cons:LWESecureSketch} Let $n$ be a security parameter and let $m = m(n) = \poly(n), q = q(n)\geq 2$ be integers.  Let $\gamma\in (0,1)$ be a constant and let $\rho \in (0,1)$ such that $\rho q\geq 2n^{1/2+3\gamma}m$.  Let $\mathcal{I}_t$ be an algorithm~(not necessarily efficient) that inverts an LWE instance when no more than $t$ of $m$ dimensions have non-zero error.  Let $W$ be a distribution over $\{0,1\}^{(\log \rho q)\times m}$\,.  Then the following is a computational secure sketch:%We present the construction in 
%%\begin{figure}
%%\label{fig:construction figure}
%%\caption{Computational Secure Sketch based on the Learning with Errors problem}
%\begin{center}
%\begin{tabular}{c|c}
%\begin{minipage}{3in}
%\textbf{\sketch}
%\begin{enumerate}
%\item Input $w\leftarrow W$.
%\item Sample $\vA\in\Fq^{m\times n}, x\in\Fq^n$ uniformly.
%\item Output $p = (\vA, \vA x+w)$.\\
%\end{enumerate}
% \end{minipage} &
%\begin{minipage}{3in}
%\textbf{\rec}
%\begin{enumerate}
%\item Input $(w', p)$
%\item Parse $p$ as $(\vA, \vect{C})$
%\item Set $x' = \mathcal{I}_t(\vA, \vect{C}-w') $. 
%\item Output $w = \vect{C}-\vA x'$.
%\end{enumerate}
%\end{minipage} 
%\end{tabular}
%\end{center}
%%\end{figure}
%%\textbf{\gen}
%%\begin{enumerate}
%%\item Input $w\leftarrow W$.
%%\item Sample $A\in\Fq^{m\times n}, x\in\Fq^n$ uniformly at random.
%%\item Use $w$ as the randomness for the sampling algorithm, \\$\sample$, for $\chi$.  Set $E\leftarrow  \sample(w)$.
%%\item Output $p = (A, AX+E)$.
%%\end{enumerate}
%%
%%
%%\textbf{\rep}
%%\begin{enumerate}
%%\item Input $(w', p)$
%%\item Parse $p$ as $(A, C)$
%%\item Compute $E' \leftarrow \sample (w')$.
%%\item Set $X' = \mathcal{I}_t(A, C-E') $. 
%%\item Output $sam_w = C-AX'$.
%%\end{enumerate}
%\end{construction}

\subsection{Efficiency of \consref{cons:informal construction}}
\label{sec:time main construction}
%\subsection{Analysis of Construction~\ref{cons:LWESecureSketch}}
\consref{cons:informal construction} is only useful if $\mathcal{I}_t$ can be efficiently implemented.  We now present a particular $\mathcal{I}_t$ and consider its efficiency:

\begin{construction}
\label{cons:decoding algorithm} Suppose that for a $(n, m, q, \chi)$ that the LWE errors problem is a unique witness relation except with negligible probability.  Let $R$ be the relation that decides whether $X$ is a witness for an instance $\vA, \vA X+W$.  We describe inverter $\mathcal{I}_t$:
\begin{enumerate}
\item Input $\vA , \vect{C} = \vA X + E - E'$
\item Randomly select rows without replacement $i_1,..., i_n\leftarrow [1,m]$.  
\item Restrict $\vA, \vect{C}$ to rows $i_1,...,i_n$, denote this is $A_{i_1,...,i_n}, C_{i_1,...,i_n}$.
\item Compute $x = \vA^{-1}_{i_1,...,i_n}\vect{C}_{i_1,...,i_n}$.  If $R(x, \vA, \vect{C}) \neq 1$.  Go to step (2).
\item Output $x$.
\end{enumerate}
\end{construction}

$\mathcal{I}_t$ is an efficient inverter/recovery procedure when $t$ is small enough.  Our inverter will only succeed if it can verify it has the proper $X$. Consider a relation $R(X, (\vA, \vA X+W-W'))$ that verifies if $X$ is the point encoded in $\vA X+W-W'$.   If $n$ is large enough and if $|W-W'|_1$ is small enough, $R$ is a unique witness relation with high probability~(see \defref{def:unique witness relation}) and computable in time $O(n^3)$.
We consider the setting where the running time of $\mathcal{I}_t$ grows with $t$.
%  We provide the results in \lemref{lem:i t constant time} and \lemref{lem:i t poly time} respectively~(proofs in Sections~\ref{sec:proof lem i t constant time},~\ref{sec:proof lem i t poly time}).  \lnote{can we combine these lemmas into one?  It's hard for the reader to compare line-by-line to see where the differences are.  More errors seems better; we could simply point out that constant number of erros leads to better running time, right in the statement of the lemma or right after it}
%\begin{lemma}[Efficiency of $\mathcal{I}_t$ when $t\leq m/n-1$]
%\label{lem:i t constant time}
%Let $\vA, \vA X+E$ be a  $(n,m, q, \chi)$-LWE instance that is a unique witness relation~(except with negligible probability).  Assume $d(E, E')\leq t$ where $t \leq (m-n)/n$.  Then $\mathcal{I}_t$ runs in expected time $O(n^3\log q+ s_{ver})$ on input distribution $\vA, \vA X+E - E'$ and outputs $X' =X$ for all but a negligible fraction of inputs over $\vA, \vA X+E - E'$.
%\end{lemma}
\begin{lemma}[Efficiency of $\mathcal{I}_t$ when $t\leq d\log n (m/n-1)$]
\label{lem:i t poly time}
Let $\vA, \vA X+W$ be a  $(n,m, q, \chi)$-LWE instance that is a unique witness relation~(except with negligible probability).  Assume that $W$ is split into $m$ blocks of length $b$ and let $\dis$ be the Hamming distance over alphabet $2^b$.  Let $d$ be a constant and assume that $\dis(W, W')\leq t$ where $t\leq d\log n(m/n-1)$.  Then $\mathcal{I}_t$ runs in expected time $O(n^{2d}(n^3\log q))$ on input distribution $\vA, \vA X+W - W'$ and outputs $W$ for all but a negligible fraction of inputs over $\vA, \vA X+W - W'$.
\end{lemma}
\textbf{Note:} If we consider the setting where $t\leq m/n-1$, then $\mathcal{I}_t$ will run in expected time $O(n^3\log n)$.  Our main theorems apply when $t\leq m/n$ with slightly better parameters.  We concentrate on the setting $t\leq d\log n(m/n-1)$.  Also even though we are correcting $O(\log n)$ errors, this a super polynomial number of error patterns and the running time of $\mathcal{I}_t$ is significantly better than exhaustive search over these error patterns.

%\textbf{Problems:} Construction~\ref{cons:LWESecureSketch} does not meet Definition~\ref{def:comp secure sketch}.  This is because the sampling algorithm may not be invertible.  Furthermore, in standard $\LWE$ the sampling algorithm is Gaussian and there is no fixed number of bits used to sample error in any dimension.  Approximating the Gaussian distribution using a fixed number of bits may be possible but there are still two error patterns which require a significantly different number of bits.  We will address both of these problems by sampling the error from a uniform distribution.

\subsection{Lossless Secure Sketch}
\label{sec:lossless sketch}
We now state a setting of parameters that yields a lossless sketch.  For a more detailed explanation of the various parameters and constraints see \secref{sec:parameter settings}.  There exist efficient lossless secure sketches based on \consref{cons:informal construction}:

%\begin{theorem}
%\label{thm:lossless secure sketch}
%Let $n$ be a security parameter and let $t$ be a constant.  Consider the Hamming metric with block length $b = \log 2n^3(t+1)$.  Let $W$ be uniform over $\zo^{n(t+1)b}$.  If GAPSVP and SIVP are hard to approximate within polynomial factors, there is a 
%\[
%(\zo^{n(t+1)b}, |W|,|W|, \epsilon, s, s_{sketch}, s_{rec}, t)\text{-computational secure sketch}
%\]
% for $\epsilon = \ngl(n), s = \poly(n), s_{sketch} = O(n^3\log n )$ and $s_{rec}= O(n^3 \log n)$.
%\end{theorem}

\begin{theorem}
\label{thm:lossless secure sketch log}
Let $n$ be a security parameter and let $t = c\log n$ for some constant $c$.  Let $d\in\Fq^+$ be a constant and consider the Hamming metric with block length $b = \log (2n^3 (c+d)/d)$.  Let $W$ be uniform over $\zo^{(c+d)/d(n\log b)}$.  If GAPSVP and SIVP are hard to approximate within polynomial factors using quantum algorithms, then \consref{cons:informal construction} is a lossless secure sketch.  That is, $H^{\unp}_{\epsilon, s_{sec}}(W|\sketch(W)) = H_\infty(W)$
% for following parameters:
%\[
%(\zo^{(c+d)/dn\log b}, |W|, \epsilon, s_{sec}, s_{sketch}, s_{rec}, t)
%\] 
with 
$\epsilon = \ngl(n), s_{sec} = \poly(n)$,  sketch time $s_{sketch} = O(n^3\log n)$, and recover time $s_{rec}= O(n^{2d+3} \log n)$.
\end{theorem}
\begin{proof}
Security follows by combining Lemmas~\ref{lem:uniform LWE decision} and~\ref{lem:conversion to unpredictability}, efficiency follows by \lemref{lem:i t poly time}.
\end{proof}

\thref{thm:lossless secure sketch log} shows that a computational secure sketch can be built without incurring any entropy loss.  This is essentially due to $\vA X+W$ looking like a uniformly random point $B$ and thus providing no information about either $X$ or $W$.  A natural question to ask is whether security degrades gracefully when $W$ is not the uniform distribution.  We begin to provide an answer in \secref{sec:LWE block fixing sources}.  

\section{\consref{cons:informal construction} with Symbol Fixing Sources}
\label{sec:LWE block fixing sources}
In this section, we show the security of~\consref{cons:informal construction} is preserved for certain high entropy distributions.  Showing security of a computational secure sketch for arbitrary high min-entropy distributions is an open problem.  First we recall the notion of a symbol fixing source~(from~\cite{KZ07}): 
\begin{definition}
Let $W = (W_1,..., W_m)$ be a distribution where $W_i$ takes values over $\{0,1\}^b$, we say that a distribution is a $(m-\alpha)$-\emph{symbol fixing source} if $H_\infty(W)\geq (m- \alpha)b$ and for all $W_i$ one of the following conditions is met:
\begin{itemize}
\item $W_i \overset{d}= U_b$
\item There exists a value $c_i$ such that $\Pr[W_i = c_i] =1$.
\end{itemize}
\end{definition}
%A block fixing source is an extension of a entropy source where each bit is independently random or fixed to larger alphabets.  

We now study \consref{cons:informal construction} with a symbol fixing source.  Obviously, if $\alpha \geq n$ then the adversary knows $n$ equations with no error and $W$ can be recovered with Gaussian elimination.  We show that security degrades exponentially as symbols are fixed.
\ignore{
\begin{assumption}[LWE w/ entropic errors]
Let $n$ be a security parameter and define $m = \poly(n)$ and $q\geq 2$ be integers.  Let $W = W_1||...||W_{m+\alpha}$ be a $m$-block fixing distribution over $\{0,1\}^{(m+\alpha)\ell}$. For a vector 
Let $A\in\Fq^{(m+\alpha)\times (n+\alpha)}$ and let $x\in\Fq^{n+\alpha}$.  No PPT algorithm given $Ax+W$ can recover $x$ with probability greater than nonnegligible.
\end{assumption}
}
We now define the decisional $\LWE$ problem with symbol fixing errors is defined as follows: 
\begin{problem}[$\distLWE$ w/ symbol fixing errors]
Let $n$ be a security parameter and define $m = \poly(n)$ and $q\geq 2$ be integers.  Let $W = W_1||...||W_{m+\alpha}$ be a $m$-symbol fixing distribution over $\{0,1\}^{(m+\alpha)b}$. For a vector $X \leftarrow \Fq^n$, we denote $A_{X, W}$ be the distribution obtained by choosing a uniform $\vA\in\Fq^{(m+\alpha)\times (n+\alpha)}$ and output $\vA X + W$. The decisional learning with error problem $\distLWE_{n+\alpha, m+\alpha, q, W}$ is to distinguish samples chosen according $A_{X, W}$ for a uniformly random $X \in \Fq^{n+\alpha}$ from samples chosen according to the uniform distribution over $\Fq^{n+\alpha} \times \Fq$.
\end{problem}

The dist-$\LWE_{n, m, q, W}$ with symbol fixing sources is implied by standard $\distLWE_{n, m, q, \chi}$~(proof in \secref{sec:proof of block theorem}).
\vspace{.1in}
\begin{theorem}
\label{thm:blockLWE}
Let $n$ be a security parameter and define $q, m = \poly(n)$.  Let $W$ be a $m$-symbol fixing source over $\{0,1\}^{(m+\alpha)b}$ where $\Hoo(W) \geq mb$.  Then the $\distLWE_{n, m,q, W}$ assumption implies the $\distLWE_{n+\alpha, m+\alpha, q, W}$ w/ symbol fixing sources problem.
\end{theorem}

\thref{thm:blockLWE} allows us to construct a computational secure sketch from block-fixing sources: 

%\begin{theorem}
%\label{thm:lossless block sketch}
%Let $n$ be a security parameter and let $t$ be a constant.  Consider the Hamming metric with block length $b = \log 2n^3(t+1)$.  Let $W$ be an $\alpha$-symbol fixing source over $\zo^{((t+1)n+\alpha)b}$.  There is a \lnote{instead of ``there is'' can we actually point to the to construction?}
%\[
%(\zo^{((t+1)n+\alpha)b}, \Hoo(W),\Hoo(W), \epsilon, s, s_{sketch}, s_{rec}, t)\text{-computational secure sketch}
%\]
% for $s = \poly(n), s_{sketch} = O(n^3\log n )$ and $s_{rec}= O(n^3 \log n)$.
%\end{theorem}

\begin{theorem}
\label{thm:lossless block sketch log}
Let $n$ be a security parameter and let $t = c\log n$ for some constant $c$.  Let $d\in\Fq^+$ be a constant and consider the Hamming metric with block length $b = \log (2n^3 (c+d)/d)$.  Let $W$ be $\alpha$-symbol fixing source over $\zo^{((c+d)n/d+\alpha)b}$.  Assume GAPSVP and SIVP are hard to approximate within polynomial factors using quantum algorithms.   \consref{cons:informal construction} is a lossless secure sketch.  That is, $H^{\unp}_{\epsilon, s_{sec}}(W|\sketch(W)) = H_\infty(W)$ for %the following parameters:
%\[
%(\zo^{((c+d)n/d+\alpha)b}, \Hoo(W),\Hoo(W), \epsilon, s_{sec}, s_{sketch}, s_{rec}, t)\text{-computational secure sketch}
%\]
$\epsilon = \ngl(n), s_{sec} = \poly(n)$, sketch time $s_{sketch} = O(n^3\log n)$, and recover time $s_{rec}= O(n^{2d+3} \log n)$.
\end{theorem}



\section{Computational Fuzzy Extractors}
In this section, we show that any computational secure sketch implies a computationally fuzzy extractor using a paradigm similar to \lemref{lem:fuzzy ext construction}.  We begin with a formal definition of a computational fuzzy extractor:
\label{sec:comp fuzzy extractors}
\begin{definition}[Computational Fuzzy Extractor]\label{def:comp fuzzy extractor}
An $(\mathcal{M}, m, \ell, t, s_{rec}, s_{sec}, \epsilon)$-\emph{computational fuzzy extractor} is a pair of randomized procedures, ``generate'' (\gen) and ``reproduce'' (\rep), with the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\rep( w', P) = R$.  Furthermore \rep is computable by a circuit of size at most $s_{rec}$.  If $d(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ has full HILL entropy conditioned on $P$, that is $H^\hill_{\epsilon, s_{sec}}(R | P) = \ell$.
\end{itemize}
\end{definition}
Any efficient fuzzy extractor is also a computational fuzzy extractor with the same parameters ($s_{rec}$ is the size of the circuit that computes \rec and $s_{sec}$ is unbounded).  

\textbf{Note: }A computational fuzzy extractor can be constructed by making the extractor computational~(using a standard extractor and then running a pseudorandom generator on its output).  However, the entropy loss due to error correction may mean there is not be enough remaining bits to run a pseudorandom generator.  Depending on the application, it may be appropriate to make the sketch and/or extractor computational.

Recall, we defined computational secure sketches using unpredictability entropy~(\defref{def:comp secure sketch}).  It is not known if unpredictability entropy can be extracted from, so we cannot use the construction of \lemref{lem:fuzzy ext construction}.  However, unpredictability entropy implies Yao entropy~(\defref{def:yao entropy}) and this can be extracted from using a reconstructive extractor~(defined in~\cite{barak-computational}).  
\begin{definition}
An $(\ell, \epsilon)$-reconstruction for a function $\rext:\{0,1\}^n\times \{0,1\}^d\rightarrow \{0,1\}^m\times\{0,1\}^d$~(where the last $d$ outputs are equal to the last $d$ input bits) is a pair of machines $C$ and $D$, where $C:\{0,1\}^n\rightarrow \{0,1\}^\ell$ is a randomized Turing machine and $D^{(\cdot)}:\{0,1\}^\ell\rightarrow \{0,1\}^n$ is a randomized oracle Turing machine which runs in time polynomial in $n$.  Furthermore, for every $x$ and $T$, if $|\Pr[T(\rext(x, U_d)) =1] -\Pr[T(U_m\times U_d)=1]|>\epsilon$, then $\Pr[D^T(C^T(x))=x]>1/2$~(the probability is over the random choices of $C$ and $D$).
\end{definition}
\begin{lemma}~\cite[Lemma 6]{DBLP:conf/eurocrypt/HsiaoLR07}
\label{lem:unp extraction} 
Let $X$ be a distribution with $H^{\unp}_{\epsilon, s_{sec}}(X|Z)\geq k$, and let $\rext$ be an extractor with a $(k-\log 1/\epsilon, \epsilon)$-reconstruction $(C,D)$.  Then $\delta^{\mathcal{D}_{s'}}((\rext(X, U_d), Z), U_m\times U_d\times Z)\leq 5\epsilon,$ where $s'=s_{sec}/(|C|+|D|)$.
\end{lemma}
We are now ready to provide a computational analogue of \lemref{lem:fuzzy ext construction}:
\begin{lemma}
Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, s_{sec}, s_{sketch}, s_{rec}, t)$-computational secure sketch, and let $\rext:\mathcal{M}\times \{0,1\}^d\rightarrow \{0,1\}^\ell \times \{0, 1\}^d$ be an reconstructive extractor with $(\tilde{m}-\log 1/\epsilon, \epsilon)$-reconstruction $(C, D)$.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, s_{sec}/(|C|+|D|), s_{rec}, 5\epsilon)$-computational fuzzy extractor:
\begin{itemize}
\item $\gen(w;r, x)$: set $P= (\sketch(w; r), x), R = \rext(w, x)$, and output $(R,P)$.
\item $\rep(w', (s, x)):$ recover $w = \rec(w', s)$ and output $R = \rext(w;x)$.
\end{itemize}
\end{lemma}
\begin{proof}
Consequence of \defref{def:comp secure sketch} and \lemref{lem:unp extraction}.
\end{proof}

%For completeness, we describe a computational fuzzy extractor based on Construction~\ref{cons:LWESecureSketch} in \secref{sec:fuzzy extractor phrasing}.

\ignore{
Additionally, it is simple to use a pseudorandom generator to produce a computational fuzzy extractor from a standard fuzzy extractor:
\begin{proposition}\label{prop:prg comp fuzzy extractor}
Let \gen, \rep be a $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor~(where \rep is computable by a  $s_{rec}$ size circuit) and let $prg: \{0,1\}^\ell\rightarrow \{0, 1\}^{\ell'}$ be a $(\epsilon_{prg}, s)$ pseudorandom generator (computable by a circuit of size $s_{peg}$) from $\ell$ bits to $\ell'$ bits.  Let \gen' and \rep' be defined as:
\begin{itemize}
\item \gen': $(R, P) \leftarrow \gen(w)$, output $(prg(R), P)$
\item \rep': Output  $prg(\rep (w', P))$.
\end{itemize}
Then $(\rep', \gen')$ is a $(\mathcal{M}, m, \ell', t, s_{rec}+s_{prg}, s, \epsilon+\epsilon_{prg})$-computational fuzzy extractor.
\end{proposition}
Recall, our goal is to construct fuzzy extractors where the entropy drop due to an information theoretic construction were too high.  However, it seems difficult to definitionally exclude this type of construction.  For example, one could require that $R$ have information theoretic entropy in the absence of $P$.  This does not solve the problem:
\begin{proposition}\label{prop:xor comp fuzzy extractor}
Let \gen, \rep be a $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor~(where \rep is computable by a  $s_{rec}$ size circuit) and let $prg: \{0,1\}^\ell\rightarrow \{0, 1\}^{\ell'}$ be a $\epsilon_{prg}, s$ pseudorandom generator (computable by a circuit of size $s_{peg}$) from $\ell$ bits to $\ell'$ bits.  Let \gen' and \rep' be defined as:
\begin{itemize}
\item $\gen'(w)$
\subitem Sample $K\leftarrow \{0,1 \}^{\ell'}$
\subitem $(R, P) \leftarrow \gen(w)$
\subitem $H= prg(R)\oplus K$
\subitem Output $(K, (H, P))$
\item $\rep'(w', (H, P))$
\subitem Output $K' = H\oplus prg(\rep (w', P))$.
\end{itemize}
Then (\rep', \gen') is a $(\mathcal{M}, m, \ell', t, s_{rec}+s_{prg}, s, \epsilon+\epsilon_{prg})$-computational fuzzy extractor.  Furthermore $H_\infty(K) = \ell'$.
\end{proposition}

Both of these constructions rely on a crucial property $\ell$ was long enough to provide a seed to a pseudorandom generator.  We are interested in the case where the remaining entropy is small and may not sufficient for pseudorandom generator.  We leave Definition~\ref{def:comp fuzzy extractor} as stated and note constructing a computational fuzzy extractor is desirable when a standard fuzzy extractor cannot be used. 

In the information theoretic setting, Dodis et. al.~\cite{DBLP:journals/siamcomp/DodisORS08} construct fuzzy extractors by using a secure sketch to perform (secure)~error correction and a randomness extractor to convert the distribution to close to uniform.  A similar paradigm seems desirable in the computational case: can a computational fuzzy extractor can be produced by a ``computational'' secure sketch and a randomness extractor~(which convert conditional HILL entropy to pseudorandom bits~\cite[Lemma 5]{DBLP:conf/eurocrypt/HsiaoLR07}).
}
\bibliographystyle{alpha}
\bibliography{crypto}
\appendix
\section{Background}
\subsection{Notions of Computational Entropy}
\label{sec:notionsEntropy}
In this sections we provide some further definitions of computational entropy.  HILL entropy is defined in \secref{sec:defCompFuzzyExtractors}.  HILL entropy generalizes the notion of indistinguishability.  Yao entropy~\cite{DBLP:conf/focs/Yao82a, barak-computational} generalizes the notion of compressibility.  Here we present the computational definition due to~\cite{DBLP:conf/eurocrypt/HsiaoLR07}:
\begin{definition}
\label{def:yao entropy}
For a distribution $X, Z$ we say that $X$ has Yao entropy at least $k$ conditioned on $Z$, denoted $H^{\yao}_{\epsilon, s}(X|Z)\geq k$, if for every pair of circuits $c,d$ of total size at most $s$ with the outputs of $c$ having length $\ell$,
\[
\Pr_{(x,z)\leftarrow (X,Z)}[d(c(x,z),z) = x]\leq 2^{\ell-k}+\epsilon
\]
\end{definition}
It should be clear that HILL entropy implies Yao entropy in both the unconditional and conditional case.  In the conditional case it is possible for Yao entropy to be significantly larger than HILL entropy~\cite{DBLP:conf/eurocrypt/HsiaoLR07}.  We also use a less common notion called unpredictability entropy.  This measures an adversaries ability to recover a secret given some side information, see \defref{def:unp entropy}.
Unpredictability entropy is a notion between HILL and Yao entropy:
\begin{lemma}~\cite[Lemmas 8, 9]{DBLP:conf/eurocrypt/HsiaoLR07}\label{lem:hillimplyunpimplyyao}
\[ H^{\hill}_{\epsilon, s}(X|Z)\geq k \Rightarrow H^{\unp}_{\epsilon, s}(X|Z)\geq k\Rightarrow H^{\yao}_{\epsilon, s}(X|Z)\geq k.
\]
\end{lemma} 
\subsection{Unique Witness Relations}

We will need the ability to verify if we have a correct solution in~\consref{cons:informal construction}.  

\begin{definition}
Let $R$ be a relation.  We say that $R$ is $s_{ver}$-\emph{unique witness verifiable} if $\forall y$ there exists a unique $x$ such that $R(x, y)=1$ and $R(x, y) =1$ is computable in time $s_{ver}$.
\end{definition}
We can relax this definition slightly to allow a relation that is unique witness verifiable with all but negligible probability~(when $x, y$ grow with the security parameter).

\begin{definition}
\label{def:unique witness relation}
Let $R:\mathcal{M}_1\times \mathcal{M}_2\rightarrow \zo$.  We say that $R$ is $(\epsilon, s_{ver})$-\emph{unique witness verifiable} if over $y\in \mathcal{M}_2$, drawn uniformly, the probability that $\exists x_1, x_2, x_1\neq x_2$ such that $R(x_1, y) =1 = R(x_2, y)$ is at most $\epsilon$ and $R$ is computable in time $s_{ver}$.
\end{definition}

\ignore{
\section{Possibility of Constructing Computational Fuzzy Extractors}\label{ssec:constructing comp fuzzy extractors}
In the \secref{sec:imp HILL sketch}, we showed that defining a computational secure sketch using HILL entropy connects the parameters to the best constructible codes.  Thus, some entropy drop seems necessary when considering $H^\hill_{\epsilon, s}(X | SS(X))$.  However, in \propref{prop:prg comp fuzzy extractor}, we showed that a computational fuzzy extractor exists using a pseudorandom generator.  A natural question is whether this is necessary.  Do there exist clever constructions of computational fuzzy extractors that rely on the hardness of some cryptographic primitive that is easy within some distance $t$.  We begin by recalling the important parameters of a computational fuzzy extractor:
\begin{itemize}
\item The input entropy $m$.
\item The length of the output string $\ell$.  This output string is required to be $(\epsilon, s_{sec})$-pseudorandom.
\item The accepting distance $t$.
\end{itemize}
We consider the possibility of constructing a computational fuzzy extractor based on the remaining entropy of $W$ conditioned on the string $P$~(generated by $(R, P)\leftarrow \gen(W)$).  We first define a parameter $k_{prg}$.  Let $k_{prg}$ be the smallest integer such that there exists a pseudorandom generator from $k_{prg}$ to at least $k_{prg}+1$ bits.  More precisely, let $k_{prg}$ be the smallest integer such that there exists a deterministic function $prg:\{0,1\}^{k_{prg}}\rightarrow \{0,1 \}^{k_{prg}+1}$ such that $\forall D\in \mathcal{D}_s$: $\delta^D(prg(U_{k_{prg}}), U_{k_{prg}+1})<\epsilon$ for some negligible $\epsilon$ and super polynomial $s$.  We now classify the existence of computational fuzzy extractors based on this parameter.  Let $CODE$ be some error-correcting code with parameters, $(n, k, 2t+1)$.  We define $|CODE| = n-k$.

\begin{tabular}{c | c}
Remaining Entropy & Constructible?\\\hline
$CODE$ is computationally computable. & Yes, use \propref{prop:prg comp fuzzy extractor} \\
$H_\infty (W) - |CODE|\geq k_{prg}$\\\hline
$CODE$ exists, not necessarily efficiently computable. & Seems impossible to disprove, efficient\\
$H_\infty(W) - |CODE|\geq k_{prg}$ & construction of $CODE$ admits \propref{prop:prg comp fuzzy extractor}.\\\hline
Let $CODE$ be the best code. & Unknown, main area of interest\\
$H_\infty(W) - |CODE|< k_{prg}\leq H_\infty(W)$\\\hline 
$H_\infty(W)< k_{prg}$ & Seems to imply existence \\
&of improved $prg$ by using $\gen$.
\end{tabular}

We begin by focusing on the last case where $H_\infty(W) < k_{prg}$.  We first introduce two concepts, efficiently-generatable sources and sources that are $prg$ qualified.

\begin{definition}
A distribution $W$ over $\mathcal{M}$ is \emph{$(s_{gen}, \ell)$-efficiently-generatable} if there exists a deterministic circuit $G$ of size at most $s_{gen}$ such that $W \overset{d}= G(U_{\ell})$.
\end{definition}
\begin{definition}
A distribution $W$ over $\mathcal{M}$ is \emph{$(\epsilon, s, s_{f})$ - prg-qualified} if there exists a deterministic function $f:\mathcal{M}\rightarrow \{0,1\}^d$~(computable in space $s_{f}$) such that for all $D\in\mathcal{D}_s$, $\delta^D(f(W), U)<\epsilon$ and $d\geq k_{prg}$\footnote{We can extend this definition to allow $f$ to have public-randomness.  The definition is not meaningful if $f$ has private randomness as $f$ can simply generate a fresh seed of length $d$.}.
\end{definition}
We use the term prg-qualified as the composition of $f$ and a $prg$ gives pseudorandom bits:
\begin{proposition}
Let $W$ over $\mathcal{M}$ be a $(\epsilon_{W}, s_W, s_{f})$-prg-qualified distribution with function $f$ to $d$ bits.  Furthermore let $prg:\{0,1\}^k\rightarrow \{0,1\}^{k+1}$ be a $(\epsilon_{prg}, s_{sec})$- pseudorandom-generator~(computable by a circuit of size $s_{prg}$) where $k\leq d$.  Then $\forall D\in\mathcal{D}_{\min\{s_W-s_{prg}, s_{sec}\}}$
\[
\delta^D(prg(f(W)_{1,..., k}), U_{k+1})< \epsilon_W + \epsilon_{prg}.
\]
\end{proposition}
\begin{proof}
Let $W$ be a $(\epsilon_{W}, s_W, s_{f})$-prg-qualified distribution to $d$ bits.  Furthermore let $prg:\{0,1\}^k\rightarrow \{0,1\}^{k+1}$ be a $(\epsilon_{prg}, s_{sec})$- pseudorandom-generator where $k\leq d$.  We proceed by contradiction.  Suppose that $\exists D$ of size at most $s= \min\{s_W -s_{prg}, s_{sec}\}$ such that $\delta^D(prg(f(W)), U_{k+1})\geq \epsilon_W+\epsilon_{prg}$.  By the triangle inequality, one has:
\begin{align*}
\epsilon_W+\epsilon_{prg}&\leq \delta^D(prg(f(W)_{1,...,k}), U_{k+1})\\
&\leq \delta^D(prg(f(W)_{1,...,k}, prg(U_k)) + \delta^D(prg(U_k), U_{k+1})
\end{align*}
Thus, either $\delta^D(prg(f(W)_{1,...,k}, prg(U_k))\geq \epsilon_{W}$ or $\delta^D(prg(U_k), U_{k+1})\geq \epsilon_{prg}$.

In the first case, we construct a distinguisher $D'$ that contradicts the $(\epsilon_W, s_W, s_{f})$-qualification of $W$.  $D'$ upon receiving input $x\in \{0,1\}^d$ computes $D(prg(x))$ and outputs $D$'s answer.  $D'$ is of size at most $s + s_{prg} \leq s_W-s_{prg}+s_{prg} = s_W$ and has advantage at least $\epsilon_W$.  This is a contradiction.

In the second case, $D$ immediately contradicts the $(\epsilon_{prg}, s_{sec})$ security of $prg$.  Thus, we have a contradiction in both cases.  This completes the proof.
\end{proof}

We now return to the question of whether we can build a computational fuzzy extractor when $H_\infty(W)<k_{prg}$.
\begin{theorem}

\end{theorem}
\begin{proof}
\bnote{Do the proof!}
\end{proof}
}
%We now provide a candidate construction of fuzzy extractor based directly on a computational problem~(the LWE problem introduced by Regev~\cite{regevLWEsurvey}).
%\begin{definition}
%An $(\mathcal{M}, m, \tilde{m}, t, s_{dist}, \epsilon)$-\emph{computational secure sketch} is a pair of randomized algorithms, ``sketch'' (\sketch) and ``recover'' (\rec) with the following properties:
%\begin{enumerate}
%\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns two strings, $(x,s)\in\{0,1\}^*$ where $x$ is drawn from a distribution $X$ and $\Hoo(X)\geq m$.
%\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of a computational secure sketches guarantees that if $d(w,w')\leq t$, then $Rec(w', SS(w)_2) = SS(w)_1$.  
%\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, observing $SS(W)_2$ does not significantly help the adversary.  That is, $H^\hill_{\epsilon, s_{dist}}(X | SS(W)_2)\geq \tilde{m}$.
%\end{enumerate}
%A secure sketch is \emph{efficient} if \sketch runs in expected polynomial time and \rec runs in expected polynomial time if $d(w', w) < t$.
%\end{definition}
%This definition has two significant changes from the definition of a secure sketch.  First, we do not require that the distribution $X$ is equal to the input distribution $W$.  This is because constructions will utilize computationally hard problems that may be difficult to invert.  However, we can consider the special case where $X=W$.  None of the power of a secure sketch is lost when $X\neq W$.

%\textbf{Note: }  The entropy of $X$ is somehow limited by the entropy of $X$ we cannot completely hide some very high entropy string, using a low entropy string.  This would have to be reflected in the parameters of \hill entropy but I am not yet sure how to formalize this.
%
% Furthermore, note there are two conditions on the distribution $X$, $\Hoo(X)\geq m$ and $H^\hill_{\epsilon, s}(X|SS(W)_2)\geq \tilde{m}$.  This means that if an adversary does not see the helper value $s$ it has true entropy and its computational entropy is decrease by seeing $s$.  

%Second, we now require that the value $X$ only has conditional \hill entropy.  Considering the special case where $m=\tilde{m}$ is interesting but constructions that improve on the $\tilde{m}$ achieved in secure sketch are interesting.  Furthermore, it seems natural to consider the case where $|\rec|<s$ however, \rec has a significantly more difficult task, \rec must recover all of $x$ while the distinguisher in the conditional \hill entropy definition only needs to distinguish $x$ from a high average min-entropy distribution.
%
%Thus, we can define a stronger definition where all of these desiderata are achieved.  
%
%\begin{definition}
%A $(\mathcal{M}, m, \tilde{m}, t, s, \epsilon)$-computational secure sketch is strong if the following properties hold:
%\begin{enumerate}
%\item Seeing $s$ does not decrease the conditional $\hill$ entropy of $X$.  That is, $m=\tilde{m}$.
%\item There exists a circuit of \rec' where the size of \rec' is polynomial, $|\rec'|<s_{dist}$ and \rec' satisfies the correctness property.
%\end{enumerate}
%\end{definition}
\ignore{
\subsection{Learning With Errors (\class{LWE})}
\label{subsec:LWE in detail}
The LWE problem was introduced by Regev \cite{regev2005LWE, regevLWEsurvey} as a generalization of ``learning parity with noise". For positive integers $n$ and $q \ge 2$, a vector $\vect{s} \in \Fq$, and a probability distribution $\chi$ on $\Fq$, let $A_{\vect{s}, \chi}$ be the distribution obtained by choosing a vector $\vect{a} \overset{\$}\leftarrow \Fq^n$ uniformly at random and a noise term $e \overset{\$}\leftarrow \chi$, and outputting $(\vect{a}, \langle \vect{a}, \vect{s}\rangle + e) \in (\Fq^m, \Fq)$. A formal definition follows.


\label{sec:LWEoverview}
\begin{problem}[\class{LWE}]\label{prob:LWE}
Let $n$ be the security parameter. For a integer $q = q(n)$ and the error distribution $\chi = \chi(n)$ over $\Fq$, the Learning With Error problem $\class{LWE}_{n, m, q,\chi}$ is defined as follows: given access to an oracle that outputs $m$ samples from $A_{\vect{s}, \chi}$ for a uniformly random $\vect{s}$, output $\vect{s}$ with high probability.
\end{problem}

We will use the fixed form of the LWE assumption where a matrix is prepared ahead of time.  The matrix form is as follows: $\class{LWE}$ find $\vect{x}$ given $(\vect{A},\vect{Ax+e})$, where $\vect{A}\leftarrow \Fq^{m \times n}$ is chosen uniformly and the error vector $\vect{e} \leftarrow \chi$.
Results of Regev~\cite{regev2005LWE} and Peikert~\cite{peikert2009latticereduction} connect the hardness of the LWE problem to solving worst-case lattice problems.  The matrix $\vA$ and secret $\vx$ are drawn from the uniform distribution, while the error term is drawn from a discretized Gaussian distribution $\bar{\Psi}_\alpha$~(following the notation of~\cite{regev2005LWE}).  We denote by $\bar{\Psi}_\alpha$ the discretized Gaussian distribution over $\Fq$ with variance $(\alpha q)^2/(2\pi)$.  Here we present the theorem of~\cite{regev2005LWE}.

\begin{theorem}~\cite[Theorem 3.1]{regev2005LWE}
Let $n$ be a security parameter and $q = q(n) \in\Fq^+$, let $\alpha = \alpha(n)\in (0,1)$ be such that $\alpha q > 2\sqrt{n}$.  Let $\vA\in\Fq^{m\times n}, X\in \Fq^n$ be chosen uniformly at random and $E$ be chosen according to $\bar{\Psi}_\alpha$.  If there exists a PPT-algorithm $\mathcal{I}$ such that $\Pr_{\vA, X, E}[X\leftarrow  \mathcal{I}(\vA, \vA X+E) ]>\epsilon$ for some noticeable $\epsilon$, then there exists an efficient quantum-algorithm that approximates the decision-version of the shortest vector problem~(GAPSVP) and the shortest independent vectors problem~(SIVP) to within $\Omega(n/\alpha)$ in the worst case.
\end{theorem}

We will consider a generalized assumption of \probref{prob:LWE}:
\begin{assumption}
\label{assume:general LWE}
Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ and $q=q(n)\geq 2$ be integers and let $\chi$ be a distribution on $\Fq$.  If $X\in\Fq^n, \vA\in\Fq^{m\times n}$ are chosen uniformly at random and $E$ is chosen according to $\chi$, then for all $\mathcal{I}$ of size at most $s$
\[
\Pr_{\vA, X, E}[X\leftarrow \mathcal{I}(\vA, \vA X+E)]<\epsilon
\]
\end{assumption}
Furthermore, Regev shows a decision to search equivalence for $q=\poly(n)$, so we can consider an equivalent indistinguishability formulation:
\begin{assumption}
\label{assume:indist LWE}
Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be prime and let $\chi$ be a distribution on $\Fq$.  If $X\in\Fq^n, A\in\Fq^{m\times n}, B\in\Fq^m$ are chosen uniformly at random and $E$ is chosen according to $\chi$, then for all $\mathcal{D}$ of size at most $s$:
\[
\Pr[D(\vA, \vA X+E)=1]-\Pr[D(\vA, B)=1]<\epsilon
\]
\end{assumption}
We can restate Assumption~\ref{assume:indist LWE} in terms of unpredictability entropy~(see Definition~\ref{def:unp entropy}).  
}

\section{\lemref{lem:conversion to unpredictability}}
\label{sec:proof of unpred conversion}
In this section we show that for an indistinguishable LWE instance, the unpredictability entropy of $E$ conditioned on $\vA X+E$ remains high.  %We first note that any inverter that can recover $X$ can recover $E$ and vice versa by using simple a single addition and multiplication~(and vice versa).  Thus, we have the following claim:
%\begin{claim}
%\label{lem:unp of x and e}
%Let $A, X, E$ be as above, then:
%\begin{itemize}
%\item $H^{\unp}_{\epsilon, s}(X| \vA, \vA X+E)\geq k \Rightarrow H^{\unp}_{\epsilon, s'}(E| \vA, \vA X+E) \geq k$ for $s'\approx s$.
%\item $H^{\unp}_{\epsilon, s}(E | \vA, \vA X+E) \geq k \Rightarrow H^{\unp}_{\epsilon', s'}(X | \vA, \vA X+E) \geq k'$ for $k'\approx k, \epsilon' \approx \epsilon, s'\approx s$. 
%\end{itemize}
%\end{claim}
%In the body of the text we use \lemref{lem:unp of x and e} to present  \assref{assume:indist LWE} in terms of unpredictability entropy~(\assref{assume:entropy LWE}).
%We now restate Assumption~\ref{assume:indist LWE} in terms of unpredictability entropy:
%\begin{assumption}
%\label{assume:entropy LWE}
%Let $n$ be a security parameter, let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be prime and let $\chi$ be a distribution on $\Fq$.  If $X\in\Fq^n, A\in\Fq^{m\times n}$ are chosen uniformly at random and $E$ is chosen according to $\chi$, $H^{\unp}_{\epsilon, s}(X | A, AX+E) =H^{\unp}_{\epsilon, s'}(E| A, AX+E) \geq m\log q $ for $s'\approx s$.  
%\end{assumption}
%For clarity, we delay the proof of \clref{lem:unp of x and e} until the end of this section~(the proof is quite straightforward).
We first recall a result of Akavia, Goldwasser, and Vaikuntanathan~\cite{akavia2009}.  This says that if $\distLWE$ is hard then $X$ has many hardcore bits.  We state their result for a general error distribution~(noting their proof does not consider the error distribution anywhere):
\begin{lemma}\protect{\cite[Lemma 2]{akavia2009}}
\label{lem:many hardcore bits}
Let $n>0$ be an integer, $q\geq 2$, let $\chi$ be an error distribution over $\zq$ and $k\leq n$ be an integer.  Let $\vA\in \zq^{m\times n} , X\in\zq^n$ and $E$ be distributed according to $\chi^m$.  Then $H^{\hill}_{\epsilon, s_{sec}}(X_{1,..., k}| \vA, \vA X+E )= k\log q$ assuming $\distLWE_{(n-k, m, q, \chi)}$.
\end{lemma}

\begin{proof}[Proof of \lemref{lem:conversion to unpredictability}]
Assume that $\distLWE_{(n-k, m, q, \chi)}$ is $(\epsilon, s_{sec})$ hard and that $m\geq n$.  Let $\vA\in \zq^{m\times n}, X\in \zq^n$ and $E$ be distributed according to $\chi^m$.  Then according to \lemref{lem:many hardcore bits} we know that $H^{\hill}_{\epsilon, s_{sec}}(X_{1,..., k} | \vA , \vA X+E)\geq k\log q$.  By \lemref{lem:hillimplyunpimplyyao}, we know that $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k} | \vA, \vA X+E)\geq k\log q$.  To expand to $E$ we use the following claim:

\begin{claim}
Let $\vA, X, E$ be distributed as in \lemref{lem:conversion to unpredictability} and $m\geq n$ and let $k\leq n$.  Then 
\[
H^{\unp}_{\epsilon', s_{sec}'} ( E | \vA X+ E) \geq H^{\unp}_{\epsilon, s_{sec}} ( X_{1,..., k} | \vA X+E)
\]
\end{claim}
%\begin{claim}
%If $H^{\unp}_{0, s}(X_{1,..., k} | Z) \geq k$ then $H^{\unp}_{0, s}(X|Z)\geq k$.
%\end{claim}
%\begin{proof}
%Suppose that $H^{\unp}_{0, s}(X|Z)< k$ that is there exists a circuit $C$ of size at most $s$ such that $\Pr[C(Z) = X]> 2^{-k}$.  Then let $C'$ be a circuit that simply takes the output of $C$ and returns the first $k$ bits.  Then $\Pr[C'(Z) = X_{1,...,k}]\geq \Pr[C(Z) = X] > 2^{-k}$.  This completes the claim.
%\end{proof}

%\begin{claim}
%If $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k} | Z) \geq k$ then $H^{\unp}_{\epsilon, s_{sec}'}(X|Z)\geq k$ for $s_{sec} \approx s_{sec}'$.
%\end{claim}
%\begin{proof}
%Assume that $H^{\unp}_{\epsilon, s_{sec}}(X_{1,...,k}|Z)\geq k$.
%Suppose that $H^{\unp}_{\epsilon, s_{sec}'}(X|Z)< k$.  Let $Y$ be a distribution such that $\delta^{\mathcal{D}_{s_{sec}'}}((X, Z), (Y, Z))\leq \epsilon$.  Furthermore, let $C$ be a circuit of size at most $s_{sec}'$ where $\Pr[C(Z) = Y] > 2^{-k}$.  Define the circuit $C'$ that outputs the first $k$ bits of $C$'s output.  Thus, $\Pr[C'(Z) = Y_{1,..., k}] \geq \Pr[C(Z) = Y] > 2^{-k}$.  Note that $\delta^{\mathcal{D}_{s_{sec}}}((X_{1,...,k}, Z), (Y_{1,..., k}, Z))<\delta^{\mathcal{D}_{s_{sec}'}}((X, Z), (Y, Z))\leq \epsilon$.  Together, these two facts contradict $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k}|Z)\geq k$.
%This completes the claim.
%\bnote{This claim isn't accurate because we can't show for all $Y_{1,..., k}$.  In order to complete the proof we need to show that $\forall Y_{1,..., k}$ such that $(X_{1,..., k}, Z), (Y_{1,...k}, Z)<\epsilon$ there exists an extension which is also indistinguishable.}
% that is there exists a circuit $C$ of size at most $s$ such that $\Pr[C(Z) = X]> 2^{-k}$.  Then let $C'$ be a circuit that simply takes the output of $C$ and returns the first $k$ bits.  Then $\Pr[C'(Z) = X_{1,...,k}]\geq \Pr[C(Z) = X] > 2^{-k}$.  This completes the claim.
%\end{proof}

%\begin{claim}
%If $H^{\yao}_{\epsilon, s_{sec}}(X_{1,..., k} | Z)\geq k$ then $H^{\yao}_{\epsilon , s_{sec}'}(X|Z)\geq k$ for $s_{sec}\approx s_{sec}'$.
%\end{claim}
%Suppose that $H^{\yao}_{\epsilon, s_{sec}}(X_{1,..., k} |Z ) \geq k$.  This implies $\exists c, d$ of total size $s_{sec}$ where $c$ has outputs of length $\ell$ such that $\Pr[d(c(X_{1,..., k}, Z), Z) = X_{1,..., k}] \leq 2^{\ell -k} + \epsilon$.  Furthermore, suppose $\forall c', d'$ of total size $s_{sec}$ where $c$ has output length $\ell$ and $\Pr[d(c(X), Z), Z) = X] > 2^{\ell-k} +\epsilon$.
%
%
%Thus, we know that $H^{\unp}_{\epsilon, s_{sec}'}(X | \vA, \vA X+E)\geq k\log q$.  By \lemref{lem:unp of x and e} we know that $H^{\unp}_{\epsilon, s_{sec}'} (E | \vA, \vA X+E)\geq k\log q$.  This completes the proof of \lemref{lem:conversion to unpredictability}.
%\end{proof}
\begin{proof}
Suppose that $H^{\unp}_{\epsilon', s_{sec}}( E | \vA X+E) < \ell$.  We will show that $H^{\unp}_{\epsilon, s_{sec}}(X_{1,..., k} | \vA X+E) < \ell$.  That is, $\forall Y$ such that $\delta^{\mathcal{D}_{s_{sec}'}}((E, \vA X+E), (Y, \vA+E))\leq \epsilon$, there exists $C$ of size $s_{sec}'$ such that $\Pr[C(\vA, \vA X+E) = Y]> 2^{-\ell}$.  Fix one such $Y$.  Let $\mathcal{E}$ be the event that $A_{1,..., n}^{-1}$ exists.  Note that $\mathcal{E}$ occurs with probability $1-\gamma$.

Define the distribution $Y' =A^{-1}_{1,...,n}(\vA X+E -Y)$.  Then one has that 
\begin{align*}
\delta^{\mathcal{D}_{s_{sec}}}((Y', \vA, \vA X+E ), (X, \vA, \vA X+E))&\leq \delta^{\mathcal{D}_{s_{sec}}}((Y', \vA, \vA X+E ), (X, \vA, \vA X+E) |\mathcal{E} )+ \gamma\\
&\leq \delta^{\mathcal{D}_{s_{sec}}}(\vA^{-1}_{1,..., n}(\vA X+E - Y), \vA, \vA X+E), (X, \vA, \vA X+E))+\gamma\\
&\leq \delta^{\mathcal{D}_{s_{sec}'}}((Y, \vA, \vA X+E), (E, \vA, \vA X+E))+\gamma\leq \epsilon' + \gamma
\end{align*}
Where the second to last inequality follows by applying the transform $\vA X+E - \vA(\cdot)$ to the first argument.  Note  that $\delta^{\mathcal{D}_{s_{sec}}}((Y'_{1,...k}, \vA, \vA X+E ), (X_{1,..,k}, \vA, \vA X+E))\leq \epsilon' + \gamma$. Otherwise there would be a distinguisher for the entire distribution by just ignoring the last $n-k$ dimensions.

We now show that $\exists C'$ such that $\Pr[C'(\vA, \vA X+E) = Y']> 2^{-k}$.  Define $C'$ as follows:
\begin{enumerate}
\item Input $(\vA, \vA X+E)$.
\item Run $y\leftarrow C(\vA, \vA X+E)$.
\item Try to compute $\vA^{-1}_{1,.., n}$, if $\vA^{-1}_{1,..., n}$ does not exist output $\perp$.
\item Output $z = \vA^{-1}_{1,..., n}(\vA X+E -y)$
\end{enumerate}
Observer that $\Pr[C'(\vA, \vA X+E) = Y']> 2^{-k} - \gamma$.  Finally note that $\exists C''$ that simply outputs the first $k$ dimensions of the output of $C'$.  Thus, $H^{\unp}_{\epsilon, s_{sec}}( X_{1,..., n}| \vA , \vA X+E)< 2^{-k} - \gamma$.
\end{proof}

The statement of \lemref{lem:conversion to unpredictability} follows by the statement of the claim.
%\begin{proof}[Proof of \clref{lem:unp of x and e}]
%First suppose that $H^{\unp}_{\epsilon, s}(X| \vA, \vA X+E) < k$.  That is, $\exists I \in\mathcal{D}_s$ such that for exists $Y$ where $\delta^{\mathcal{D}_s}((X, \vA, \vA X+E), (Y, \ A, \vA X+E)\leq \epsilon$, $\Pr[I(\vA, \vA X+E) = Y] > 2^{-k}$.  Define $I'$ by the following:
%\begin{enumerate}
%\item Input $(\vA, \vA X+E)$.
%\item Run $y\leftarrow I(\vA, \vA X+E)$.
%\item Output $z = \vA y - \vA X+E$.
%\end{enumerate}
%Note that $|I'|\approx |I|$.
%It suffices to show that $\delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (Z, A, \vA X+E))\leq \epsilon$.  One has the following:
%\begin{align*}
%\delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (Z, \vA, \vA X+E))&=
%\delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (\vA Y-\vA X+E, \vA, \vA X+E))\\
%&\leq \delta^{\mathcal{D}_s'}((\vA X, \vA, \vA X+E), (\vA Y, \vA, \vA X+E))\\
%&\leq \delta^{\mathcal{D}_s'}((X, \vA, \vA X+E), (Y, \vA, \vA X+E)) \leq \epsilon.
%\end{align*}
%Now suppose that $H^\unp_{\epsilon, s}(E | A, \vA X+E)<k$.  That is, $\exists I \in\mathcal{D}_s$ such that for exists $Y$ where $\delta^{\mathcal{D}_s}((E, \vA, \vA X+E), (Y, \vA, \vA X+E)\leq \epsilon$, $\Pr[I(\vA, \vA X+E) = Y] > 2^{-k}$.  Define $I'$ by the following:
%\begin{enumerate}
%\item Input $(A, \vA X+E)$.
%\item Run $y\leftarrow I(\vA, \vA X+E)$.
%\item Try to compute $\vA^{-1}$, if $\vA^{-1}$ does not exist output $\perp$.
%\item Output $z = \vA^{-1}(\vA X+E-y)$.
%\end{enumerate}
%Note that $|I'|\approx |I|$.  For sufficient parameter sizes the probability of $A^{-1}$ not existing is negligible.  Denote this probability by $\gamma$.  Then note that $\Pr[I'(\vA, \vA X+E) = Z] > 2^{-k}-\gamma = 2^{-k'}$.  Then one has the following~(conditioned on the event that $\vA^{-1}$ exists):
%\begin{align*}
%\delta^{\mathcal{D}_s'} ((X, \vA, \vA X+E), (Z, \vA, \vA X+E))&=
%\delta^{\mathcal{D}_s'} ((X, \vA, \vA X+E), (\vA^{-1}(\vA X+E-Y), \vA, \vA X+E)) \\
%&\leq \delta^{\mathcal{D}_s'} ((\vA^{-1}E, \vA, \vA X+E), (\vA^{-1}Y, \vA, \vA X+E))\\
%&\leq \delta^{\mathcal{D}_s'} ((E, \vA, \vA X+E), (Y, \vA, \vA X+E))\leq \epsilon
%\end{align*}
%Since $A^{-1}$ exists with probability $1-\gamma$ we obtain total distance at most $\epsilon(1-\gamma)=\epsilon'$.
%\end{proof}

\end{proof}

\section{Proof of Theorem \ref{thm:blockLWE}}
\label{sec:proof of block theorem}

\begin{proof}
We assume that all of the fixed blocks are located at the end and their fixed value is $0$, that is, $W_{m+1}, ..., W_{m+\alpha} =0$.  If the blocks are fixed to some other value the reduction is essentially the same.

%It should be clear to the reader where the reduction differs for another block fixing source.\lnote{I don't like the phrasing; maybe we should say that if the blocks are fixed to some non-0 value, the reduction is essentially the same}

Let $\D$ be a distinguisher that can distinguish $(m+\alpha)$ $\LWE$ instances from uniform where $\alpha$ of the equations have no error added.  That is, let $\vA$ be the uniform distribution over $\Fq^{(m+\alpha)\times(n+\alpha)}$, let $\vx'$ be uniform over $\Fq^{(n+\alpha)}$, $\ve= (e_1,..., e_{m+\alpha})$ where $e_1,..., e_m$ are generated from error distribution $\chi$ and $e_{m+i} =0$ for $i=1,..,\alpha$. Furthermore, let $\vu$ be the uniform distribution over $\Fq^{m+\alpha}$.  We assume that, 
\[
|\Pr[\D(\vA, \vA\vx+\ve) = 1] - \Pr[\D(\vA, \vu) =1]|> \epsilon.
\]

We build a distinguish $\D'$ that distinguishes standard LWE instances from uniform.  Let $\vA'$ be uniform over $\Fq^{m\times n}$, $\vx'$ be uniform over $\Fq^n$ and $\ve'$ be drawn from $\chi^m$.  Our goal is to prepare a random block fixing instance from the random instance we are given.  The code for $\D'$ is given in \figref{fig:perfectLWEreduction}.

It suffices to show that $\D'$ properly prepares a random instance for the distinguisher.  We show this in three claims: 
\begin{enumerate}
\item If $\vA'$ is a random matrix then $\vA$ is a random matrix.
\item If $\vb'$ is uniformly distributed then $\vb$ is uniformly distributed.
\item If $\vb' = \vA'\vx'+\ve'$ then $\exists x$~(uniformly distributed) such that $\vb = \vA \vx + \ve'$ where the last $\alpha$ coordinates of $\ve$ are $0$.
\end{enumerate}
\begin{figure}
\begin{enumerate}
\item Input $\vA', \vb'$, where $\vA' \overset{\$} \leftarrow \Fq^{m\times n}$ and $\vb'$ is either uniform over $\Fq^m$ or $\vb' = \vA'\vx' +\ve$.
\item Randomly generate $\vect{R} \overset{\$}\leftarrow \Fq^{\alpha \times n}$.
\item Let $\vb^* = (\vb', b^*_{m+1}, \ldots,b^*_{m+\alpha})$, where $(b^*_{m+1}, \ldots, b^*_{m+\alpha} )\overset{\$} \leftarrow \Fq^\alpha$.

\item Initialize $\vA^* = (\va^*_1, \ldots, \va^*_{m+\alpha})^T = \left(\begin{array}{c | c}\vA' & \mathbf{0}\\\hline \vect{R} & \vect{I}\end{array}\right)$.
\item \label{step:randomization}
For {$i=1\ldots m+\alpha$}:
\subitem Randomly generate $(\gamma_{i,1},\ldots, \gamma_{i,\alpha}) \leftarrow \Fq^\alpha$.
 \subitem For {$j = 1 \ldots n+\alpha$}:
\subsubitem Set $A_{ij} \leftarrow A^*_{ij}+\sum_{k=1}^\alpha \gamma_{i, k} A^*_{(m+k)j}$
\subsubitem Set $b_i \leftarrow b^*_i +  \sum_{k=1}^\alpha\gamma_{i, k} b^*_{m+k}$
\item Run $\D$ on input $\vA, \vb$.
\item Output $\D$'s output.
\end{enumerate}
\caption{A PPT $\mathcal{D'}$ that distinguishes LWE using distinguisher for LWE w/ block fixing source}
\label{fig:perfectLWEreduction}
\end{figure}

\begin{claim}
\label{cl:randomMatrixDist}
The matrix $\vA$ generated after Step \ref{step:randomization} is a random matrix.
\end{claim}
\begin{proof}
In the randomized matrix $\vect{A}$, for $1\le i \le m$ and $n+1\le j \le n+\alpha$, $A_{ij}$ is assigned the value $\gamma_{i,(j - n)}$, which is a random value.  Similarly, for $m+1\le i \le m+\alpha$ and $n+1\le j \le n+\alpha$, each entry $A_{ij}$ is random due to the original construction of the matrix $\vA^*$.  Then before step \ref{step:randomization} the matrices $\vA', \vect{R}$ are random submatrices.  Thus, it remains to show that $\vA', \vect{R}$ remain random after step 5. For $1\le i \le m+\alpha$ and $1\le j \le n$, by construction $A_{ij} = A_{ij}'+\sum_{k=1}^\alpha \gamma_{i, k} R_{kj}$, which is a truly random number. Overall, each row vector $\va_{i}$ in $\vA$ is the sum of a random vector and a random linear combination independent vectors. Therefore, the entire matrix $\vA$ is a truly random matrix.  This completes the claim.
\end{proof}
\begin{claim}
\label{cl:random ax+e}
If $\D'$ is provided with input distributed as $\vA', \vA'\vx'+\ve'$ then $\vb = \vA \vx+\ve$ where $x_i = x_i'$ for $1\leq i \leq n$ and $x_{n+i} = b_{m+i}^*$ is uniformly generated otherwise and  $e_i = e_i'$ for $1\leq i\leq m$ and $e_i = 0$ for $m<i\leq m+\alpha$.
\end{claim}
\begin{proof}
Before step \ref{step:randomization} the claim is satisfied.  Denote by $\vx'$ the value such that $\vA'\vx'+\ve = \vb$.  
Then define $\vx^*$ as:
\[
x^*_i = \begin{cases}
x_i', & \text{if }1\leq i\leq n \\
b_{m-n+i}^*-\sum_{j=1}^n R_{i, j} x_j', & n+1\leq i\leq n+\alpha\,.
\end{cases}
\]
Then $\vx^*$ is uniformly random and $\vb^* = \vA^*\vx^* + \ve^*$~(where $\ve^*$ is $\ve'$ with $\alpha$ 0s appended).  Thus, it remains to show that step~\ref{step:randomization} preserves a solution.
%For convenience, denote by $\vx^*$ the vector where $x_i^* = x_i'$ for $1\leq i \leq n$ and $x_{n+i}^* = b_{m+i}^*$ otherwise.  
For a matrix $\vA$ we will denote the $i$-th row by $\va_i$~(similarly for the matrix formed by $\gamma_{i,j}$).  Thus it suffices to show for all rows $i$, if $b_i^* = \va^*_i \vx^*+e^*_i$  then $b_i = \va_i \vx^* + e^*_i$.
%, we have added $\alpha$ equations as well as $\alpha$ unknowns, furthermore, those $\alpha$ equations have no error.  The number of unknowns we added are $x_{n+i} = b^*_{m+i} - \sum^{n}_{j=1}R_{ij}x_j$ for $i=n+1,..., n+\alpha$.
We have the following for all $i=1,..., m+\alpha$:
\begin{align*}
\va_i \vx^* + e_i^* &=\va^*_i\vx^*+\sum_{k=1}^\alpha \gamma_{i} \va^*_{m+k}\vx^*+e_i^*\\
&= \va_i^*\vx^* + e_i^* + \sum_{k=1}^\alpha \mathbf{\gamma}_{i} \va^*_{m+k}\vx^*\\
&= b_i^* + \sum_{k=1}^\alpha\gamma_i \va_{m+k}^* \vx^*\\
&= b_i^* + \sum_{k=1}^\alpha \gamma_i \left(\left( \sum_{j=1}^n \vR_{i, j}x_j^* \right) + x_{n+k}^*\right)\\
&= b_i^* +\sum_{k=1}^\alpha\gamma_i b_{m+k}^*= b_i
\end{align*}
%\begin{align*}
%\vA_i \vx + e_i &= \sum_{j=1}^{n+\alpha} A_{ij} x_j +e_i = \sum_{j=1}^{n+\alpha}(A^*_{ij}x_j+\sum_{k=1}^\alpha \gamma_{i, k} A^*_{(m+k)j}x_j)+e_i\\
%& = \sum_{j=1}^{n+\alpha}(A^*_{ij}x_j)+e_i +\sum_{k=1}^\alpha \gamma_{i, k} \sum_{j=1}^{n+\alpha} (A^*_{(m+k)j}x_j)\\
%& = b^*_i +  \sum_{k=1}^\alpha\gamma_{i, k} b^*_{m+k} = (\va_i^* \vx + e_i) + \sum_{k=1}^\alpha\gamma_{i, k} (\va_{m+k}^* \vx + e_i)\\
%& = b_i \label{reductionEq}
%\end{align*}
\end{proof}
\begin{claim} 
\label{clm:random b}
If $\D'$ is provided with input distributed as $\vA', \vb'$ where $\vb'$ is uniformly random then $\vb$ is uniformly random.
\end{claim}
\begin{proof}
As before this is clearly satisfied before step~\ref{step:randomization}.  That is, each $b_i^*$ is uniformly and independently random.  After step~\ref{step:randomization} each $b_i$ is a fresh random linear combination of random values:
\[
b_i = b_i^* + \sum_{k=1}^\alpha \gamma_i b_{m+k}^*.
\]
Thus, each of these values is uniformly and independently random.
\end{proof}

%Let $\tilde{\vA} = (\sum_{k=1}^\alpha \gamma_{1, k} \va^*_{m+1}, \ldots, \sum_{k=1}^\alpha \gamma_{1, k} \va^*_{m+1})$ and $\tilde{\vb} = (\sum_{k=1}^\alpha\gamma_{1, k}b^*_{m+1}, \ldots, \sum_{k=1}^\alpha\gamma_{1, k} b^*_{m+\alpha})$. 
%
%The equation above gives us $\vA = \vA' + \tilde{\vA}$ and $\vb = \vA'\vx + \ve + \tilde{\vb}$. Given a uniform $\vu \overset{\$}\leftarrow\Fq^m$ and a $\LWE$ instance $\vA'\cdot \vx + \ve$, the probability that the distinguisher $\D'$ can distinguish a uniform instance from a $\LWE$ instance, when given access to the distinguisher $\D$ is given below:
%\begin{align*}
%&\left|\Pr\left[\D'(\vA', \vu') = 1\right]- \Pr\left[\D'(\vA', \vA'\vx + \ve)=1\right]  \right|\\
%& =\left|\Pr\left[\D(\vA'+ \tilde{\vA}, \vu' + \tilde{\vb})=1\right]- \Pr\left[\D(\vA' + \tilde{\vA}, \vA'\vx + \ve + \tilde{\vb})=1\right]  \right|\\
%&= \left| \Pr\left[\D(\vA, \vu)=1\right]- \Pr\left[\D(\vA, \vA\vx + \ve)=1\right]  \right| \ge \epsilon
%\end{align*}
Together Claims~\ref{cl:randomMatrixDist},~\ref{cl:random ax+e}, and~\ref{clm:random b} show that $\D'$ properly prepares the instance thus, 
\begin{align*}
&\left|\Pr\left[\D'(\vA', \vu') = 1\right]- \Pr\left[\D'(\vA', \vb'=\vA'\vx + \ve)=1\right]  \right|\\
& =\left|\Pr\left[\D(\vA, \vu)=1\right]- \Pr\left[\D(\vA, \vb)=1\right]  \right|\geq \epsilon
\end{align*}
%==========
\ignore{
Before randomized:
$$\vA'\vx' + \ve = \vb'$$
For $i = 1\ldots m$,
$$\sum^{n+\alpha}_{j=1}A_{ij}x_j + e_i = b_i$$
For $i = 1\ldots \alpha$,
$$\sum^{n+\alpha}_{j=1}A_{(m+i)j}x_j = \sum^{n}_{j=1}F_{ij}x_j + x_{n+i} = b_{m+i}$$
}
\end{proof}


\section{Additional Proofs}

\subsection{Proof of \lemref{lem:averageToMaximalError}}
\label{sec:proof of average to maximal error}
\begin{proof}
Let $W$ be a distribution with $\Hoo(W)\geq k$ and let $W$ be a  $(t,\epsilon)$-average error Shannon code with recovery procedure $\rec$.  This implies that 
\[
\sum_{w\in W} \Pr[W=w]\Pr[ w'\leftarrow \sample (w) \wedge \rec(w') \neq w]\leq \epsilon.
\]
For $w$ denote by $\epsilon_w = \Pr[w'\leftarrow \sample(w) \wedge \rec(w') \neq w]$.  Then by Markov inequality:
\[
\Pr_{w\in W}[ \epsilon_w \leq 2\expe_{w\leftarrow W} [\epsilon_w ] = 2\epsilon]\geq \frac{1}{2}
\]
Now suppose that for all $W'\subset W$ where $|W'|\geq 2^{k-1}$ there exists a $w\in W'$ such that $\Pr[ w'\leftarrow \sample (w) \wedge \rec(w') \neq w]> 2\epsilon$.  This implies that there exists a set $V$of size $V\geq |W|-2^{k-1}+1$ where $\forall v\in V, \epsilon_v >2\epsilon$.  Then note that $\Pr[W\in V]> 1/2$ as $W\setminus V$ contains at most $2^{k-1}-1$ points each of which has probability at most $1/2^k$ so $\Pr[W\in (W\setminus V)]\leq (2^{k-1}-1)/2^k < 1/2$.  Thus,
\begin{align*}
\Pr_{w\in W}[ w'\leftarrow \sample (w) \wedge \rec(w') \neq w]&\geq \sum_{w\in V}  \Pr[W=w]\Pr[ w'\leftarrow \sample (w) \wedge \rec(w') \neq w]\\
&\geq \sum_{w\in V} \Pr[W=w] 2\epsilon   = 2\epsilon \Pr[W\in V]> 2\epsilon\left(\frac{1}{2}\right)= \epsilon
\end{align*}
% where $\forall d\in D, \Pr[d'\leftarrow \sample(d) \wedge \rec(d') > 2\epsilon$.  In turn this implies that  
%\begin{align*}
%\Pr_{c\in C}[ c'\leftarrow \sample (c) \wedge \rec(c') \neq c]&=\frac{1}{|C|} \sum_{c\in C}  \Pr[ c'\leftarrow \sample (c) \wedge \rec(c') \neq c]\\
%&\geq \frac{1}{|C|} \sum_{d\in D}  \Pr[ d'\leftarrow \sample (d) \wedge \rec(d') \neq d]\\
%&\geq \frac{1}{|C|}\sum_{d\in D} 2\epsilon = \frac{1}{|C|}\left(\frac{|C|}{2}+1\right)2\epsilon = \epsilon + 2\epsilon/|C|> \epsilon
%\end{align*}
This is a contradiction.  The statement of~\lemref{lem:averageToMaximalError} follows directly.
\end{proof}

\subsection{Proof of \thref{thm:impSketchArbitraryW}}
\label{sec:proof of thm sketch implies code}
\begin{proof}
Let $\mathcal{M},d$ be a metric space that is $(s_{neigh},t)$-neighborhood samplable.  Let $(\sketch, \rec)$ be an $(s_{rec}, t)$-arbitrary recover functionality over $\mathcal{M}$.  Let $X$ be an arbitrary distribution.  Let $W$ be a distribution over $\mathcal{M}$ with $\Hoo(W)\geq k$ such that 
\[
\delta^D((X, \sketch(X)), (W, \sketch(X)))<\epsilon.
\]  
For all $D$ of size at most $s = O(t(s_{neigh}+s_{rec}))$.  One such $W$ must exist by the definition of conditional HILL entropy.
%Let $X$ be arbitrarily but independently distributed over $\mathcal{M}$. Let $\delta^D((X, \sketch(X)), (W, \sketch(X)))<\epsilon$ for negligible $\epsilon$ and $D$ of size at least $s=O(s_{neigh}+s_{rec})$.  This implies that $H^{\hill}_{\epsilon, s}(X|\sketch(X))\geq k$.  
Define $D$ as:
\begin{enumerate}
\item Input $z\in\mathcal{M}, s \in\{0, 1\}^*, t$.
\item For all $1\leq t'\leq t$: 
\subitem Sample $z'\leftarrow \sample(z, t')$.
\subitem If $\rec(z', s) \neq  z$ output $0$.
\item Output $1$.
\end{enumerate}
  As before we know that $D(X, \sketch(X)) = 1$.  Since 
$\delta^D((X, \sketch(X)), (W, \sketch(X)))<\epsilon.$ then $D(W, SS(X))>1-\epsilon$.  This means at least one $x'$ where this equation holds.  Fix one such $x'$.  
%This implies that $D(W, \sketch(x'))\geq 1-\epsilon$.  
Thus, for all $t', 1\leq t'\leq t$ on a random $w$
\[ \Pr_{w\leftarrow W}[w'\leftarrow \sample(w, t') \wedge \rec(w', \sketch(x')) = w]\geq 1-\epsilon.\]  
%Thus, $\rec(\cdot, \sketch(x'))$ is a decoding procedure that succeeds on a random neighbor of a random codeword with probability all but $\epsilon$.  
Thus, for all $1\leq t'\leq t$, $W$ is a $(t', \epsilon)$-average error Shannon code with recovery $\rec(\cdot, \sketch(x'))$.  The statement of the theorem follows by application of \lemref{lem:averageToMaximalError}.  
\end{proof}

%\subsection{Proof of \lemref{lem:i t constant time}}
%\label{sec:proof lem i t constant time}
%\begin{proof} We consider the expected number of iterations in step 2 when $t+1\leq m/n$.  Steps 3 and 4 can be accomplished in time $n^3\log q +s_{ver}$.  An iteration of $\mathcal{I}_t$ succeeds when all selected rows of $\vA, \vect{C}$ have no error.  The probability of each selected row having an error is at most $\frac{t}{m - i}$ where $i$ is the number of rows already selected.  That is,
%\begin{align*}
%\Pr[i_1,..., i_n\text{ have no errors}]&\geq \prod_{i=0}^{n-1}\left(1 - \frac{t}{m-i}\right)\geq  \prod_{i=0}^{n-1}\left( 1-\frac{m-n}{n(m-i)}\right)\\
%&\geq \prod_{i=0}^{n-1}\left( 1-\frac{1}{n}\right)\geq \left(1-\frac{1}{n}\right)^n \geq 1/4.
%\end{align*}
%Note in the last step we assume that $n\geq 2$.  The minimum of $(1-1/n)^n$ is achieved at $n=2$ and it quickly converges to $1/e$.  Thus, the expected number of iterations is at most $4$ giving the stated running time for $\mathcal{I}_t$.
%\end{proof}

\subsection{Proof of \lemref{lem:i t poly time}}
\label{sec:proof lem i t poly time}
\begin{proof}
We first note that $R(X, (\vA, \vA X+W))$ is computable in time $O(n^3\log q)$ by computing $AX+(W-W')- AX$ and checking if all dimensions have magnitude at most $|W|_\infty$ and at most $t$ dimensions have nonzero magnitude. We consider the expected number of iterations in step 2 when $t+1\leq m/n$.  Steps 3 and 4 can be accomplished in time $n^3\log q $.  An iteration of $\mathcal{I}_t$ succeeds when all selected rows of $\vA, \vect{C}$ have no error.  The probability of each selected row having an error is at most $\frac{t}{m - i}$ where $i$ is the number of rows already selected.  That is,
\begin{align*}
\Pr[i_1,..., i_n\text{ have no errors}]&\geq \prod_{i=0}^{n-1}\left(1 - \frac{t}{m-i}\right)\geq \prod_{i=0}^{n-1}\left( 1-\frac{\frac{dm\log n}{n}-1}{m-i}\right)\\
&\geq  \prod_{i=0}^{n-1}\left( 1-\frac{d\log n}{n}\left(\frac{m-n}{m-i}\right)\right)\geq \prod_{i=0}^{n-1}\left( 1-\frac{d\log n}{n}\right) \\
&= \left(1-\frac{d\log n}{n}\right)^n  = \left(\left(1-\frac{d\log n}{n}\right)^{\frac{n}{d\log n}}\right)^{d\log n}\geq \frac{1}{4^{d\log n}} = \frac{1}{n^{2d}}
\end{align*}
Thus, the expected number of iterations in step 2 is $n^{2d}$.  We assume all matrix operations can be completed in time $n^3\log q$, giving expected running approximately $n^{2d}(n^3 \log q)$.
\end{proof}

\section{Parameter Settings for \consref{cons:informal construction}}
\label{sec:parameter settings}
In this section, we explain the different parameters that go into our construction.  In \thref{thm:lossless secure sketch log} we give lossless sketches from a security parameter $n$ and an error $t$.  In this section, we discuss constraints imposed by 1) efficient decoding 2) maintaining security of the LWE instance and 3) ensuring no entropy loss of the construction.  In this section, we will assume that the inverter runs in time proportional to the number of errors~(\lemref{lem:i t poly time}).  We begin by reviewing the parameters that make up our construction:

\begin{itemize}
\item $|W|$: The length of the source.  
\item $t$: Number of errors that can be supported.  
\item $n$: LWE security parameter (i.e., number of field elements in $X$), which must be greater than some minimum value $n_0$ for security.
\item $q$: The size of the field.  
\item $\rho$: The fraction of the field needed for error sampling.  
\item $m$: The size of each number of samples in the LWE instance.  
\item $k$: The number of hardcore bits in $X$~(from \lemref{lem:conversion to unpredictability}).
\end{itemize}
We will split the source $|W|$ into $m$ blocks each of size $\rho q$~(that is, $|W| = m\log \rho q$).  We will ignore the parameter $|W|$ and focus on $t, n, q, \rho,$ and $m$.  As stated above we have three constraints:
\begin{itemize}
\item Maintain security of LWE.  If we assume GAPSVP and SIVP are hard to approximate within polynomial factors then \lemref{lem:uniform LWE decision} says that we get security for all $n$ greater than some minimum $n_0$ and $q = \poly(n)$ and $\rho q \geq 2 n^{1/2 + 3\gamma} m = \poly(n)$.  The only reason to increase $\rho q$ over this minimum amount (other than security) is if the number of errors in $W$ decreases with a slightly larger block size.  We ignore this effect and assume that $\rho q = 2n^{1/2+3\gamma}m$.
\item Maintain efficient decoding of Construction~\ref{cons:decoding algorithm}.  Using \lemref{lem:i t poly time}, this means that $t\leq d\log n(m/n-1)$.
\item Minimize entropy loss of the construction.  We will output $X_{1,...,k}$ so the entropy loss of the construction is $|W|-|X_{1,..., k}|$.  We want the entropy loss to be zero, that is, $|W| = |X_{1,..., k}|$.  Substituting, one has $m\log \rho q = k \log q$.
\end{itemize}
Collecting constraints we can support any setting where $t, n, q, \rho, m, k$ satisfy the following constraints~(for constants $d, f$):
\begin{align*}
n_0&< n -k \\
t&\leq d \log n\left(\frac{m}{n}-1\right)\\
q &= n^f\\
\rho q  &= 2n^{1/2+3\gamma}m\\
m\log \rho q &= k \log q
\end{align*}
Substituting $q = n^f$ and $\rho q = 2n^{1/2+3\gamma}m$ yields the following system of equations:
\begin{align*}
n_0&< n - k\\
t&\leq d\log n\left(\frac{m}{n}-1\right)\\
m \log 2n^{1/2+3\gamma}m &= k \log n^f
\end{align*}
%\xnote{$ m \log 2n^{1/2+3\gamma}m = n \log n^f$}
This is the most general form of our construction, we can support any $n, t, m$ that satisfy these equations for constants $d, f$.  However, the last equation may have no solution for $f$ constant.  Putting the last equation in terms of $f$ one has:
\begin{align*}
n_0&< n\\
t&\leq d\log n\left(\frac{ m }{n} -1\right)\\
%f \log n &= \frac{m}{n}\log 2n^2m\\
f &= \frac{m}{n}\frac{\log 2n^{1/2+3\gamma} m}{\log n}
\end{align*}
To ensure $f$ is a constant, we set $t = c \log n$ for some constant $c$ and that $k = n /g$ for some constant $g$.  Finally we assume that $t = d \log n(m/n-1)$~(this means we have only as many dimensions as necessary for polynomial time recovery):
\begin{align*}
n_0&< n\\
m &= \frac{(c+d)n \log n}{d \log n} = \frac{(c+d)n}{d}\\
f &= \frac{m}{k}\frac{\log 2n^{1/2+3\gamma}m}{\log n} = \frac{g(c+d)}{d}\frac{\log \frac{2(c+d)}{d} n^{3/2+3\gamma}}{\log n}
\end{align*}

%\xnote{$f = \frac{m}{n}\frac{\log 2n^{1/2+3\gamma}m }{\log n} = \frac{c+d}{d}\frac{\log 2(c+d)/d n^{3/2+3\gamma}}{\log n}$}
Note that $f$ is at most a constant in $n$.

Assuming $n_0 < n$ and let $t= c\log n$ we get the following setting:
\begin{align*}
m &= \frac{(c+d)n}{d}\\
q & = n^f = n^{\frac{m}{n}\frac{\log 2n^{1/2+3\gamma}m}{\log n}} = \poly(n)\\
\rho q &= 2n^{1/2+3\gamma}m = \frac{2(c+d)n^{3/2+3\gamma}}{d}
\end{align*}
Furthermore, we get decoding in time $O(n^{2d}(n^3\log n+s_{ver}))$.  We will set $g = 2$, that is the first half of $X$ our sketch will output the first half of $X$. This is the setting of parameters in \thref{thm:lossless secure sketch log}.

\ignore{
\subsection{Old parameter setting}
The question then becomes whether there is a setting 
Our construction has two goals: 1) maximizing correcting capability~(while retaining polynomial time decoding, and 2) maximizing the unpredictability entropy of $W$ conditioned on the construction~($\tilde{m}$ in \defref{def:comp secure sketch}).  Our parameters will differ significantly depending on what setting of $t$ is used.  We will consider both settings where $t\leq m/n-1$ and $t\leq c\log n (m/n-1)$.  Recall these corresponding to Construction~\ref{cons:decoding algorithm} running either in fixed polynomial time or time approximately $n^{2c}$.  We consider each of these settings:

$\mathbf{t \leq m/n-1}$

We first find the appropriate range of $n$.  First, we need $n>n_0$ for LWE security. Increasing $n$, up to the point when $H_\infty(X)=H_\infty(W)$, i.e., $n\log q = m\log\rho q$, increases the unpredictability entropy of the construction~(see \assref{assume:entropy LWE}). However, this decreases the number of errors $t$ we can correct. There is no advantage to increasing $n$ further\footnote{We primarily consider the construction with no entropy drop, if a drop is unpredictability entropy is acceptable, more errors can be corrected.  However, if the remaining unpredictability entropy is worse than a known information theoretic construction~(see~\cite{DBLP:journals/siamcomp/DodisORS08}), an information theoretic construction should be used.
}.

We first analyze the lossless construction, when  $H_\infty(X)=H_\infty(W)$ and thus $n\log q = m\log\rho q$.
Substituting $m \ge (t+1)n$ above one has:
\[
n \log q  = m \log \rho q \geq (t+1)n \log \rho q
\]
Thus, the lossless construction can support settings where $t+1\leq \frac{\log q}{\log \rho q}$.  In order to apply \lemref{lem:uniform LWE}, we need that $\rho q\geq 2n^2m \geq 2(t+1)n^3$.  We consider the setting $q = n^d$ for some constant $d$ (larger $q$ is unlikely to improve parameters unless 
%In Appendix~\ref{sec:parameters q expo}, we review parameters where $q=2^n$, these parameters are worse unless 
lattice problems are hard to approximate within exponential factors.)  
Dividing the above equations above we have:
\[
t+1\leq \frac{\log q}{\log \rho q}\leq \frac{\log q}{\log 2(t+1) n^3}\leq \frac{\log n^d}{\log 2t n^3} \approx d/3
\]
(the last step follows because the second-to-last step already implies $t\le d\log n$).
Thus, we can support a constant $d/3-1$ number of errors where $q = n^d$ is the size of our field.  Because we need $\rho q\geq 2n^2m \geq 2(t+1)n^3$ (to apply \lemref{lem:uniform LWE}), we require $\rho > n^{3-d}$.  Thus, our construction is secure if SIVP and GAPSVP are hard within approximation factors of 
\[
\tilde{O}(n^{5/2}m/\rho) = \tilde{O}\left(\frac{n^{5/2} m }{\rho}\right)
= \tilde{O}\left(\frac{n^{5/2}t}{n^{3-d}}\right)
= \tilde{O}\left(n^{d-1/2}m\right)
\]

%As an example if $\gamma = n^{c}$, this yields:
%\[
%t\leq \frac{n}{n + \log n + \log t - \log \gamma} = \frac{n}{n+\log n +\log t  -\log n} = \frac{n}{n + \log t}
%\]
%Alternatively, if $\gamma = n^{\log n}$, this yields:
%\[
%t\leq \frac{n}{n + \log n + \log t - \log \gamma} = \frac{n}{n+\log n +\log t  -\log^2 n} = \frac{n}{n+\log n - \log^2 n + \log t}
%\]
%Thus, as $q$ grows we can support a larger number of errors.  However, the growth of $q$ with a fixed size $\rho q$ leads to the approximation factor in \lemref{lem:uniform LWE} no longer being a harder problem.  Thus, $q$ can only be increased while the lattice problems in \lemref{lem:uniform LWE} is still believed to be hard.  Furthermore, in the decision formulation of \lemref{lem:uniform LWE decision} requires that $q(n) = poly(n)$.  This means that $t\leq O(\log n)$.

$\mathbf{t\leq c\log n(m/n -1)}$

We repeat the above analysis.  Recall we now consider the case where $m\geq (t+1)n/(c\log n)$.  We seek to satisfy:
\[
n\log q = m\log \rho q \geq (t+1)n \log \rho q /(c\log n)
\]
Again, we assume that $q = n^d$ for some constant $d$.  Then 
\[
t+1\leq \frac{c \log n \log q }{\log \rho q}\leq \frac{cd\log^2 n}{ \log \frac{2(t+1) n^3}{c\log n}}\leq \frac{cd \log^2 n}{3 \log n} \approx \frac{cd \log n}{3}.
\]
This gives $\rho > n^{3-d}/\log n$ and security if SVIP and GAPSVP are hard within approximation factors of $\tilde{O}(n^{d-1/2}m/\log n)$.

\textbf{Formulation from $W, t$:}

In a standard application, a source $W$ and number of errors $t$ will be given\footnote{Parameter settings will change the alphabet size and thus might change $t$, we ignore these effects and assume $t$ is given.}.  Thus, the goal is to maximize security subject to correcting $t$ errors.  This means the following parameters must be set:
\begin{itemize}
\item How large should $X$ be?  The size of $X$ is the limiting factor for the resulting unpredictability entropy.  Thus, $n$ should be as large as possible while allowing decoding.  We must also ensure that $n>n_0$ for security.
\item How large a field should we operate over?  This is the parameter $q$.  Increasing this parameter allows correcting more errors~(subject to the underlying lattice problems still being hard to approximate).
\item How length block should $W$ be split into?  This is the parameter $\rho q$.  Setting this parameter also sets $m = |W|/\log \rho q$.
\end{itemize}

Unfortunately, the parameters $m, n$ and depend on each other and cannot be set independently.  As before, we will consider the fixed poly time decoder and variable poly time decoder in turn.

$\mathbf{t\leq (m/n-1)}$

We set parameters in the following order:
\begin{itemize}
\item Let $m, n$ be integer solutions to the following equations that maximizes $n$~(if the maximum solution is less than $n_0$, we cannot support a secure construction):
\begin{align*}
|W| &= m\log 2n^2m\\
n&\leq m/(t+1)
\end{align*}
We assume that the solution for $m,n$ exists and is tight~(in the sense that $n = m/(t+1)$).  If not, there is an entropy loss in the construction due to size mismatches~(in the entropy analysis below) and padding~(to ensure that $|W|$ is an integer number of blocks).
\item Set $\rho q = 2n^2m $.
\item Set $q = n^{(t+1)\log( 2(t+1)n)/\log n}$.  Note that although $n$ appears in the exponent if $t$ is a fixed constant~(does not depend on $|W|$), this value is $q =\poly(n)$.  
\end{itemize}  Given these parameters, we can calculate the sizes of $X$ and $W$~(as we show in the \secref{sec:security of LWE cons} $\tilde{m} =|W|$):
\begin{align*}
H^{\unp}_{\epsilon, s} ( W | AX+W) &= \min\{ H_\infty(W), H_\infty(E)\}\\
H_\infty(X) = |X| &= n \log q= \frac{m}{t+1}\log n^{(t+1)\log 2(t+1)n/\log n} \\
&= \frac{m}{t+1} \frac{(t+1)\log 2(t+1)n}{\log n} \log n = m \log 2(t+1)n^3 \approx m\log 2n^2m\\
H_\infty(W) = |W| & = m \log \rho q = m \log 2n^2 m\,.
\end{align*}

$\mathbf{t \leq c\log n(m/n-1)}$
We set parameters in the following order:
\begin{itemize}
\item Let $m, n$ be integer solutions to the following equations that maximizes $n$~(if the maximum solution is less than $n_0$, we cannot support a secure construction):
\begin{align*}
|W| & = m\log 2n^2m\\
 \frac{nt}{c\log n} + n&\leq m
\end{align*}
We assume that the solution for $m,n$ exists and is tight~(in the sense that $ \frac{nt}{c\log n} + n=m$).  If not, there is an entropy loss in the construction due to size mismatches~(in the entropy analysis below) and padding~(to ensure that $|W|$ is an integer number of blocks).
\item Set $\rho q = 2n^2m $.
\item Set $q = (2n^2m)^{m/n}$.  Note if $t = O(\log n)$, then $m/n = \frac{t}{c\log n} +1 = O(1)$ and thus $q$ is polynomial in $n$.
\end{itemize}  Given these parameters, we can calculate the sizes of $X$ and $W$~(as we show in the \secref{sec:security of LWE cons} $\tilde{m} =|W|$):
\begin{align*}
H^{\unp}_{\epsilon, s} ( W | \vA X+W) &= \min\{ H_\infty(W), H_\infty(E)\}\\
H_\infty(X) = |X| &= n \log q= n \log (2n^2m)^{m/n} = m \log 2n^2m\\
H_\infty(W) = |W| & = m \log \rho q = m \log 2n^2 m\,.
\end{align*}

We'll now consider a formulation for a fixed length biometric and a number of block errors.  We assume that $n$ is given as a security parameter and thus we ask how many errors can be corrected for a noisy uniform distribution of length $w>>n$.  We denote the number of errors by $t$~(we'll be able to correct block errors over the alphabet $[-\rho q, \rho q]$).  
Let $n$ be a security parameter and let $W$ be the uniform distribution of length $w>>n$.  Let $m$ as the minimum integer such that $m\log 4m^2 n\geq w$.  Set $q = 2^{m/n}4m^2n$.  Then split $W=( W_1,..., W_m)$ where each $W_i$ is of length $\log 4m^2 n$ bits.  Then, for $W' =( W_1',.., W_m')$ if  $t = |\{i |  W_i \neq W_i'\}| \leq m/n$ block errors, Construction~\ref{cons:decoding algorithm} allows for decoding in expected polynomial time.  Furthermore, $H^{\unp}_{\epsilon, s}(W | \vA X+W)\geq |W|$~(stating \lemref{lem:uniform LWE decision} in the language of Assumption~\ref{assume:entropy LWE}).  

\begin{theorem}
\label{thm:security of secure sketch}
Let $n_0$ be a security parameter and let $W$ be uniform over $\mathcal{M}$ and let $t$ be a constant.  Let $m,n $ be the solution to the following equations that maximizes $n$.
\begin{align*}
|W| &= m\log 2n^2m\\
n&\leq m/t
\end{align*}
If $n>n_0$ and $n = m/t$ and \assref{assume:entropy LWE} holds, then for 
\begin{align*}
q &= n^{3t\log( \sqrt[3]{2t}n)/\log n}\\
\rho q &= 2n^2m,
\end{align*}  Construction~\ref{cons:LWESecureSketch} with uniform error over the interval $[-2n^2m, 2n^2m]$ is a $(\mathcal{M}, |W|, |W|, \epsilon, s, s_{sketch}, s_{rec}, t)$-computational secure sketch for $s = \poly(n), s_{sketch} = O(m\times n )$ and $s_{rec}$ is expected polynomial time.
\end{theorem}

\bnote{Not sure what to do with this}
\begin{theorem}
\label{thm:security of block sketch}
Let $n_0$ be a security parameter and let $t$ be a constant.  Let $W\in \{0, 1\}^{(m+\alpha)\times \log 2n^2m}$ be an $\alpha$-symbol fixing source where $m,n $ are the solution to the following equations that maximizes $n$.
\begin{align*}
|W| &= (m+\alpha)\log 2n^2m\\
(n+\alpha)&\leq (m+\alpha)/t
\end{align*}
If $n>n_0$ and $n+\alpha = (m+\alpha)/t$ and \assref{assume:entropy LWE} holds, then for 
\begin{align*}
q &= n^{3t\log( \sqrt[3]{2t}n)/\log n}\\
\rho q &= 2n^2m,
\end{align*}  Construction~\ref{cons:LWESecureSketch} with uniform error over the interval $[-2n^2m, 2n^2m]$ is a $(\mathcal{M}, \Hoo(W), \Hoo(W), \epsilon, s, s_{sketch}, s_{rec}, t)$-computational secure sketch for $s= \poly(n)$,  $s_{sketch} = O(m\times n )$ and $s_{rec}$ is expected polynomial time under .
\end{theorem}

}

\ignore{
\section{Fuzzy Extractor Statement of Construction~\ref{cons:LWESecureSketch}}
\label{sec:fuzzy extractor phrasing}
In this section we restate Construction~\ref{cons:LWESecureSketch} as a computational fuzzy extractor instead of a computational secure sketch:
\begin{construction}[Computational Fuzzy Extractor based on LWE] 
\label{cons:LWEFuzzyExtractor} Let $n$ be a security parameter and let $m = m(n) = \poly(n), q = q(n)\geq 2$ be integers, furthermore let $\chi$ be a distribution $\Fq$ that can be sampled with a fixed number of bits $s_{err}$.
Let $\mathcal{I}_t$ be an algorithm~(not necessarily efficient) that inverts an LWE instance when no more than $t$ of $m$ dimensions have non-zero error.  Furthermore, let $\rext$ be a reconstructive extractor.  Let $W$ be a distribution over $\{0,1\}^{s_{err}\times m}$.

\textbf{\gen}
\begin{enumerate}
\item Input $w\leftarrow W$.
\item Sample $A\in\Fq^{m\times n}, x\in\Fq^n$ uniformly at random.
\item Use $w$ as the randomness for the sampling algorithm, $\sample$, for $\chi$.  Set $E\leftarrow  \sample(w)$.
\item Sample $seed\leftarrow U$ as required for \rext.  Compute $r\leftarrow \rext(E, seed)$
\item Set $p = (A, AX+E, seed)$.
\item Output $(r, p)$.
\end{enumerate}

\textbf{\rep}
\begin{enumerate}
\item Input $(w', p)$
\item Parse $p$ as $(A, C, seed)$
\item Compute $E' \leftarrow \sample (w')$.
\item Set $X' = \mathcal{I}_t(A, C-E') $. 
\item Compute $r\leftarrow \rext (C-AX', seed)$.
\item Output $r$.
\end{enumerate}
\end{construction}

\begin{theorem}[Security of Construction~\ref{cons:LWEFuzzyExtractor}]\label{thm:LWEFuzzyExtractor}
Fix $(n, m, q, \chi)$ such that Assumption~\ref{assume:general LWE} holds for $\mathcal{I}$ of size at most $s$ with success $\epsilon$.  Furthermore assume that a fixed number of bits $s_{err}$ are necessary to sample from $\chi$.  Let $\rext: \chi^m \times\{0,1\}^\ell\rightarrow \{0,1\}^{k_{len}}\times \{0,1\}^\ell$ be an extractor with $(\log 1/\epsilon - \log 1/\delta, \delta)$-reconstruction $(\cons,\decons)$.  Let $\mathcal{I}_t$ be an inverter as described in Construction~\ref{cons:LWEFuzzyExtractor} of size $s_{\mathcal{I}_t}$.  Then Construction~\ref{cons:LWEFuzzyExtractor} is a $(\{0,1\}^{m\times s_{err}}, m\times s_{err}, k_{len}, t, s_{rec}, s/(|\cons|+|\decons|), 5\delta)$ computational fuzzy extractor where $s_{rec, t} = s_{\mathcal{I}_t}+ |\sample| + O(m\times n\times \log q) + |\rext|$.
\end{theorem}
}

\ignore{
\section{Min-entropy error LWE}
Throughout this section we will assume that \sample uses $\ell$ bits of randomness to produce the required distribution in a single dimension.  Usually, this is the normal (Gaussian) distribution with some mean.  Note that for the uniform distribution sampling requires $\log q$ bits where $q$ is the size of the range.  By information theory we know that in expectation, the number of random bits is smaller for an distribution that is not uniform, but this provides us no guarantee about the worst case number of bits.  We will begin with noting a few cases that seem significantly easier to show.  Recall that the distribution $W$ is drawn from is specified and known by the adversary.
\begin{itemize}
\item Case 1: $\Hoo(W)\leq \ell (m- n)$.  This case is not secure.  Let $W\in\{0,1\}^{\ell m}$ and we further specify $W=W_1,...,W_m$ where each $W_i\in\{0,1\}^\ell$.  We then set $W_1,...,W_n$ equal to some fixed value, $0$ without loss of generality, and $W_{n+1},...,W_m$ to the uniform distribution.  Then it is clear for the first $n$ equations, the adversary is solving a system $Ax=b$ without any error and this can be done in $P$.
\item Case 2: Full or no entropy.  As before, let $W\in\{0,1\}^{\ell m}$ and we further specify $W=W_1,...,W_m$ where each $W_i\in\{0,1\}^\ell$.  We further restrict to the case where each $W_i$ either $W_i\overset{d}=U_\ell$ or $\Hoo(W_i) = 0$.  Thus, for each dimension either, one of the two occurs: the error is properly generated or the adversary knows the error exactly. So, for each dimension, given the $W_i\overset{d}=U_\ell$, the adversary can only guess the correct random variable not better than $2^{\ell}$. Similarly, if given the $\Hoo(W_i) = 0$, the adversary can always guess the correct variable.(\xnote{This is trivial, since if we give $\Hoo(W_i) = 0$, we are actually giving the a random variable has probably of 1.})

We are considering the case that the adversary will be given the ensemble $\{W_1,...,W_n\}$, where each $W_i$ has some entropy.

\item Case 3: General case.
\item \textbf{Hypothetical worry case}  Suppose for convenience we are working with the normal distribution that has variance $\frac{2}{\pi}$ (centered around 0).  That is 
\[
\Pr[X=x] = \frac{1}{\sqrt{\frac{2}{\pi}}\sqrt{2\pi}}e^{-\frac{1}{2}{\frac{2}{\pi}x^2}}=\frac{1}{2}e^{-\pi x^2}
\]
Thus, $\Pr[X=0] =1/2$.  One can easily design a procedure $\sample'$ that generates this distribution using a maximum of some number $\ell$ bits.  Let $w_1,...,w_\ell$ be the bits that $\sample'$ takes as input.  Then when $w_1=0$ the sampler outputs $0$.  The generation for other values does not matter (only that generating some value takes $\ell$ bits).  Thus, to generate $n$ samples we need at most $\ell n$ bits.  However, there is a distribution with min-entropy $\Hoo(W)=(\ell -1) m$, that produces always produces the all zero error.  This distribution has the first bit of each $W_i$ 0 and the remaining bits truly random.  Thus, it seems impossible for our scheme to be secure for an arbitrary procedure \sample, but we will need to specify a specific \sample.  Another possibility is to use a different distribution that looks like the normai distribution but requires a ``flatter'' number of coins to be used for each distribution.  Is there a definition of an algorithm that uses the same number of random bits for each invocation?  Would this be enough to get rid of this problem?
\end{itemize}
Questions: 
\begin{enumerate}
\item How we sample this error vector $\vect{e}$?  Can we do something other than Gaussian to remove some worries?
\item Given the two biometric data which have small distance between each other, will we get the small distance between the error vector after sampling?
\textbf{Solved, each dimension is sampled independently.  Thus, our distance blowup is only as large as the number of bits needed to sample each dimension.}
\item When is LWE easy and when is LWE hard?  Are we going to be able to create a sufficient gap between these two cases?
\item Does the parallel sampler of Peikert allow for the distance in two randomness being small, creating small output distance?
\textbf{Solved, was trying to do the wrong thing.  Just Gaussian in each dimension.}
\item Does the sampler of Peikert allow for nonuniform input distribution?
\textbf{Solved, was trying to do the wrong thing (was trying to sample from lattice points, instead of adding noise around a lattice point).}
\subitem If not how does applying an extractor change the distance of two distributions?
\item How if it all can this be used concurrently?  Seems very one time right now.
\item How efficient will our decoding procedure be?  Efficient enough for practical usage?
\end{enumerate}

\textbf{What security do we need: }
}

\ignore{
\section{Robustness of LWE to error with min-entropy}
\label{sec: lwe min-entropy}

In this section we explore the robustness of the LWE assumption when the errors are drawn from a deficient distribution.  We provide a short introduction to the Learning with Errors problem in \secref{sec:fuzzyCompLWE}.  For a more complete introduction see the survey by Regev~\cite{regevLWEsurvey}.  The tools presented in this section are used to show the security of the computational secure sketch presented \subsecref{subsec:fuzzyExtLWE}.  This work is also of independent interest and can be viewed in a similar way to the work of Goldwasser et. al.~\cite{goldwasserRobustLWE}.  In their work, they consider the robustness of an LWE instance $A, Ax+e$ where $x$ is sampled from a distribution that has min-entropy but is not uniform.  They then use this result to show LWE-based cryptosystems are leakage-resilient.  Our theorem statement will follow a similar structure to the theorem statement of Goldwasser et. al. so we first present their informal theorem:
\begin{theorem}~(\cite[Theorem 1 (Informal)]{goldwasserRobustLWE})
For any super-polynomial modulus $q=q(n)$, any $k\geq \log q$, and any distribution $\mathcal{D}=\{D\}_{n\in\mathbb{N}}$ over $\{0,1\}^n$ with min-entropy $k$ , the (non-standard) LWE assumption, where the secret is drawn from distribution $\mathcal{D},$ follows from the (standard) LWE assumption with secret size $\ell\overset{\Delta}=\frac{k-\omega(\log n)}{\log q}$ (where the ``error rate'' is super-polynomially small and the adversaries run in time $\poly(n)$).
\end{theorem}

Before considering high entropy error, we must be clear about what this means.  The $x$ in an LWE instance is sampled uniformly from $\{0,q\}^n$, thus the sampling procedure requires $\lceil \log q\rceil\times n$ bits.  However, $e$ is sampled from some error distribution $\chi$.  $\chi$ is normally the normal distribution centered around $0$ with some variance $\sigma^2$ and then rounded to the nearest integer between $[-q/2, q/2]$.  Sampling from a rounded normal distribution requires a variable number of random bits for each dimension.    Indeed, using the normal distribution there are events with nonzero but negligible probability.  These events necessarily take a large number of bits to sample.  

\textbf{This is internal discussion and needs to be taken care of before presentation:}  There are two ways to provide these bits: 1) break the random string into chunks where each chunk is as long as the maximum number of random bits needed 2) provide the random bits needed to each dimensions.  One might hope that this problem is avoided by rounding to the nearest integer, but there is still some weight assigned to integers far from $0$.  We could consider a distribution whose statistical distance to the normal distance is exponentially small and gives weight 0 to all integers outside some radius.

For the moment we will ignore these problems and assume that a fixed number of bits is used to sample each dimension.  This would be the case if we substituted the uniform distribution on a significantly smaller domain.  \bnote{We need to be sure this produces a secure LWE instance}.

Thus, we assume that there exists a $\sample$ that uses $\ell$ bits of randomness to produce an error term in one dimension.  We are now ready to present the LWE assumption:
\begin{assumption}[Learning with Errors]
Let $k$ be a security parameter and define $q=\poly(k)$\bnote{Do we want the poly or exponential case?}and $n = \poly( k)$.
Let $x\overset{\$}\leftarrow \{0,q\}^n$.   An LWE sample is of the form $(a,b) = (a, a_1x_1+...+a_nx_n+\sample(e) \mod q)$ where $a_1,...,a_n\overset{\$}\leftarrow\{0,q\}^n$ and $e\overset{\$}\leftarrow\{0,1\}^\ell$.  The $(m,n,q,k)$-LWE assumption is that no algorithm given $m = \poly(k)$ LWE samples running in time $\poly(k)$ can recover $x$ with probability greater than negligible in $k$.
\end{assumption}

We then achieve our non-standard LWE assumption by sampling $e$ from a high-entropy distribution instead of the uniform distribution.

\subsection{Affine min-entropy distributions}
In the previous section, we investigated the security of the LWE assumption with deficient distributions.  Namely, we saw that the LWE assumption for block-fixing sources was implied by the LWE assumption with fewer dimensions and samples.  We now ask: what other types of min-entropy distributions can be shown to reduce to the LWE assumption?  Using a reduction to standard LWE seems to require that the errors be considered homomorphically.  This is because recovering the error terms would break LWE.  Furthermore, these errors are first put through the \sample procedure, so this needs homomorphic properties as well.  Thus, the class we can hope for in this reduction is a linear set of errors.

Recall, a function, $f:D^n \mapsto R$, over field $K$ is a \textbf{linear transformation} if $\forall x,y\in D$ and $\forall a, b\in K, f(ax+by) = af(x) + bf(y)$.

We will consider the \textbf{LWE assumption w/ linear sources}.  That is, we consider an error distribution $E = E_1||...||E_k$ where $\Hoo(E) = qn$ and $E = \mathbf{T}(U_1||...||U_n)$ for rank $n$ matrix $\mathbf{T}: \Fq^n\mapsto \Fq^k$.
\vspace{.1in}
\begin{theorem}\label{thm:linearLWE}
Let $k$ be a security parameter and define $q, n, m = \poly(k)$.  Let $E$ be a linear source over $\{0,1\}^{\ell(m+\alpha)}$ where $\Hoo(E) \geq m\ell$.  Then the $(m,n,q,k)$-LWE assumption where \sample is a linear transformation implies the $(m+\alpha, n+\alpha, q, k)$-LWE w/ linear sources assumption.
\end{theorem}
\begin{proof}
The main difference between \thref{thm:blockLWE} and \thref{thm:linearLWE} is how we produce the errors terms.  We will use the same basic strategy of introducing equations and randomizing, however, the equations will errors will now be involved in the randomization.  As before, let $\mathcal{A}$ be an algorithm that accepts $m+\alpha$ LWE samples each of length $n+\alpha$ where the errors are generated $(E_1,..., E_{m+\alpha})^T = \mathbf{T} (U_1,..., U_m)^T$ for some matrix $\mathbf{T}$.  Further, assume that $\mathcal{A}$ returns $x$ with noticeable probability.  Our algorithm $\mathcal{A'}$ will make a single call to $\mathcal{A}$ and we will focus on properly preparing the LWE w/ linear sources instance.  We use $A',A,C,D,F,G,b,b'$ with the same meaning as before.
\end{proof}
}



\ignore{
\subsection{LWE search reduction}
\begin{proof}
Let $\mathcal{A}$ be an algorithm that accepts $m+\alpha$ LWE samples each of length $n+\alpha$ where $\alpha$ of the equations have a known error (and the remainder of the errors are uniformly generated) and returns $x$ with noticeable probability.  Denote this probability as $\epsilon$.  We will show how to convert $\mathcal{A}$ into an algorithm $\mathcal{A'}$ that accepts $m$ LWE samples of length $n$ and returns $x'$ with noticeable probability.  We begin by making the following assumptions for clarity:
\begin{itemize}
\item $\mathcal{A}$ asks for all $m+\alpha$ samples and asks for them simultaneously.  Since $\mathcal{A}$ has no effect on the samples, this does not limit $\mathcal{A}$'s power.
\item For the known error samples, the added error is $0$.  It is clear in our reduction where the added error would be added, but exposition is clearer without these errors.
\item The last $\alpha$ samples are the known error samples.  For other positioning of the known error samples, we can provide a permutation from our input, but this form is clearer.
\end{itemize}
The algorithm $\mathcal{A'}$ will make a single call to the algorithm $\mathcal{A}$ and thus the entire analysis will be showing that $\mathcal{A'}$ is able to properly prepare the instance that $\mathcal{A}$ is expecting and convert the result of $\mathcal{A}$ into a solution for the input of $\mathcal{A'}$.  We will use the matrix notation for the remainder of the proof.  Thus, the job of $\mathcal{A'}$ is to perform the following transformation:
\begin{align*}
\begin{array}{c | c}
\mathcal{A'}\text{ receives:        } & \mathcal{A}\text{ expects: }\\\hline
\left(A'\right) , b' = A' x' + e' & A = \left( \begin{array}{c | c}C & D \\ \hline F & G\end{array} \right), b=Ax+e
\end{array}
\end{align*}
Where the matrices have the dimensions as shown in \figref{fig:matrixDimension}: 
\begin{figure}[b]
\[
\begin{array}{c | c | c | c | c | c}
A & A' & C & D & F & G\\\hline
m\times n & (m+\alpha)\times( n+\alpha) & m\times n & m \times \alpha & \alpha \times n & \alpha \times \alpha
\end{array}
\]
\caption{Dimensions of matrices in reduction.}
\label{fig:matrixDimension}
\end{figure}
Thus, we have two major jobs, produce an augmented matrix $A$ that is a truly random matrix and to produce an augmented $b$ that given a solution for $x$ will yield a solution for $x'$.  The code of $\mathcal{A'}$ is presented in \figref{fig:imperfectLWEreduction}.  The main idea is to generate $\alpha$ new LWE samples and use $\alpha$ new variables to exactly solve these equations.  These last $\alpha$ equations then have no errors and can be used to randomize the instance to produce a random matrix without augmenting the error in the $b$ vector.
\begin{figure}
\begin{enumerate}
\item Input $A', b' = A'x' +e'$.
\item Randomly generate $F$.
\item Set $b = (b'_1,..., b'_n, \$, ..., \$)$ that is set the first $n$ values of $b$ equal to $b'$ and randomly generate the remainder.
\item Initialize $A = \left(\begin{array}{c | c}C = A' & D = \mathbf{0}\\\hline F & G = I\end{array}\right)$.
\item For $i=1...m+\alpha$, perform the following randomization:\label{step:randomizationSearch}
\subitem Randomly generate $\gamma_{i,1},.., \gamma_{i,\alpha}$.
\subitem Set $A_{i, \cdot} = A_{i, \cdot} +\sum_{j=1}^\alpha \gamma_{i,j}A_{m+j, \cdot}$.
\subitem Set $b_i = b_i +\sum_{j=1}^\alpha \gamma_{i,j} b_j$.
\item Run $\mathcal{A}$ on input $A, b$.
\item Receive output $x$ from $\mathcal{A'}$
\item Output $x' = x_1,..., x_n$.
\end{enumerate}
\caption{Code for $\mathcal{A'}$ to generate LWE w/ symbol fixing source from standard LWE instance}
\label{fig:imperfectLWEreduction}
\end{figure}
\begin{claim}\label{cl:randomMatrix}
The matrix $A$ generated after Step \ref{step:randomizationSearch} is a random matrix.
\end{claim}
\begin{proof}
First, the submatrices $D, G$ are clearly truly random after step \ref{step:randomizationSearch}.  For the matrix $D, D_{i, j}$ is assigned the value $\gamma_{(i,j-n)}$.  Similarly, for the matrix $G$, each entry is assigned as $\gamma_{(i-n, j-n)}$.  This is due to the original construction of the matrix $G$.  Then before step \ref{step:randomizationSearch} the matrices $C, F$ are random submatrices ($C=A'$ by assumption and $F$ by construction).  Thus, it remains to show that $C, F$ remain random after step 5.  We consider a single row vector of $C$.  The new row vector $C_{(i,\cdot)} = C_{(i,\cdot)} +\sum_{j=1}^\alpha \gamma_{i,j}F_{(j, \cdot)}$.  That is, $C_{(i,\cdot)}$ is the sum of a random vector and a random linear combination independent vectors.  That is, $C_{(i,\cdot)}$ is a random vector.  

Similarly, consider a single row vector $F$. The new row vector $F_{(i,\cdot)} = \sum_{j=1}^\alpha \gamma_{i+n,j}F_{(i,\cdot)}$ is a random linear combination of independent vectors and thus truly random (since all $\gamma$ are chosen independently and randomly).  \bnote{I feel these claims are obvious but I would like to have a citation.}

Thus, the entire matrix $A=\left(\begin{array}{c | c}C & D \\\hline F & G\end{array}\right)$ is a truly random matrix.  This completes the claim.
\end{proof}
\begin{claim}\label{cl:goodLWEinstance}
The inputs $A, b$ provided to $\mathcal{A}$ is a random LWE w/ symbol fixing sources instance (provided that $A', b'$ was a random instance).
\end{claim}
\begin{proof}
By \clref{cl:randomMatrix} $A$ provided to $\mathcal{A}$ is a truly random matrix.  Thus, we must show that $b$ is of the proper form.  That is, we must show there exists an $x$ such that $b_i = A_{(i , \cdot)} x + e_i$ for $i=1,...,m$ and $b_i = A_{(i, \cdot)} x$ for $i=m+1,...,m+\alpha$.

By assumption there exists an $x'$ such that $A'x'+e' = b'$.  Before step \ref{step:randomizationSearch}, we have added $\alpha$ equations as well as $\alpha$ unknowns.  This means there no solutions have been eliminated.  Namely, $x^*_i = x'_i$ for $i=1,...,n$ and $x^*_i = b_i - \sum_{j=1}^m F_{i, j} x_j'$ for $i=n+1,..., n+\alpha$.  The case where $F$ does not have full row rank is also handled because the $G$ is the identity matrix and thus $F|G$ always has full row rank.  Furthermore, note that the last $\alpha$ equations have no error.  Thus, before step \ref{step:randomizationSearch} there exists a solution $x^*$ of the proper form.  It remains to show that the randomization in step \ref{step:randomizationSearch} retains the solution $x^*$.

The newly randomized system $A,b$ retains the same solution $x^*$  because we are not adding rows that have any error terms.  More precisely, let $A^*, b^*$ be the system before randomization (where $A^*x^*+e^* = b^*$).  Then for each row vector 
\begin{align*}
A_{(i, \cdot)} &= A^*_{(i, \cdot)}+\sum_{j=1}^\alpha \gamma_{i, j} A^*_{(m+j, \cdot)}, \\b_i &= b^*_i + \sum_{j=1}^\alpha \gamma_{i,j}b_j^*.
\end{align*}
Thus, we have the following for all $i=1,..., m$:
\begin{align*}
A_{(i, \cdot)} x^* +e_i^*&= (A^*_{(i, \cdot)}+\sum_{j=1}^\alpha \gamma_{i, j} A^*_{(m+j, \cdot)})x^*+e_i^*\\
&= A^*_{(i, \cdot)}x^* + \sum_{j=1}^\alpha \gamma_{i,j} A^*_{(m+j, \cdot)}x^*+e_i^*\\
&=\left( A^*_{(i, \cdot)}x^* + e_i^*\right)+ \sum_{j=1}^\alpha \gamma_{i,j} A^*_{(m+j, \cdot)}x^* \\
&= b^*_i + \sum_{j=1}^\alpha \gamma_{i,j}b_j^* = b_i
\end{align*}

The same equations follow without the term $e_i^*$ for $i=m+1,..., m+\alpha$.  
Thus, $x^*$ is a solution to the randomized LWE instance as well.  It remains to show that $x^*$ is a random vector.  This is clear for $x^*_i, i=1,...,n$.  For the remaining $x^*_i, i=m+1, ..., m+\alpha$ this follows since $x^*_i = b_i - \sum_{j=1}^m F_{i, j} x_j'$ where $b_i, F_{i, j}$ are randomly and independently chosen.  This completes the claim.
\end{proof}
\begin{claim}
If $x$ is a ``good'' solution output by $\mathcal{A}$ then $x'_i = x_i$ for $i=1,...,n$.  
\end{claim}
This claim follows by the analysis in \clref{cl:goodLWEinstance}.  The added equations and randomization do not affect the solution to the original $A', A'x'+e'$ instance provided.
\begin{claim}
$\mathcal{A'}$ outputs a ``good'' solution with probability only negligibly different  than the probability $\mathcal{A}$ outputs a good solution.
\end{claim}
The only time that $\mathcal{A'}$ does not produce the correct distribution is when the rows of $F$ are not linearly independent.  However, this occurs with negligible probability~\cite{something}.

Thus, we have constructed an algorithm $\mathcal{A'}$ that is able to recover a secret $x'$ of a standard LWE instance by adding samples  and equations that have no error.  Notice it was critical to add a variable for each new sample being provided to $\mathcal{A}$.  Furthermore, adding a degree of freedom for each new sample also allowed for construction of an equation with no error.  This completes the proof.

\end{proof}

\ignore{
\section{Parameters for Construction~\ref{cons:LWESecureSketch} when $q=2^n$}
\label{sec:parameters q expo}
The results of~\lemref{lem:uniform LWE decision} only support $q = \poly(n)$.  However, the reductions of Peikert may extend~\lemref{lem:uniform LWE decision} to exponential $q$.  

$q = O(2^n)$. In this setting, we are no longer constrained by field size.  We need $\rho$ be large enough that the underlying lattice problems are still hard to approximate within a $\tilde{O}(n^{5/2}m/\rho)$ factor.  Unfortunately, even correcting a constant $c$ number of errors is difficult in this domain.  Let $\gamma$ be the maximum approximation factor where the underlying lattices problems are still hard.  Then, $\rho \leq \frac{n^{5/2}m}{\gamma}$.  To correct $c$ errors we need that $\log q/\log \rho q \geq c$.  That is,
\begin{align*}
c&\leq \frac{\log q}{\log \rho q}\\
&\leq \frac{n}{\log \frac{n^{5/2}mq}{\gamma}}\\
&\leq \frac{n}{\log \frac{n^{5/2}m2^n}{\gamma}}\\
&\leq \frac{n}{n + \log n + \log m - \log \gamma}
\end{align*}
Ignoring the $\log n$ and $\log m$ terms this yields that:
\begin{align*}
c&\leq \frac{n}{n-\log \gamma}\\
\frac{n(c-1)}{c}&\leq \log \gamma\\
2^{n(c-1)/c} &\leq \gamma
\end{align*}
Thus, supporting even a constant number of errors for $q=2^n$ requires lattice problems are exponentially hard to approximate, which would be quite surprising.  
}
}
\end{document}











