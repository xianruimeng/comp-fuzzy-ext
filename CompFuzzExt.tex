\documentclass[11pt]{article}
%\documentclass{llncs}
\def\shownotes{1}

\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}      % [top=2cm, bottom=2cm, left=2cm, right=2cm]
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    
\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{color}
\usepackage{framed}
\usepackage{algpseudocode} 

\mathchardef\mhyphen="2D

\newcommand{\secref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\subsecref}[1]{\mbox{Subsection~\ref{#1}}}
\newcommand{\apref}[1]{\mbox{Appendix~\ref{#1}}}
\newcommand{\thref}[1]{\mbox{Theorem~\ref{#1}}}
\newcommand{\exref}[1]{\mbox{Example~\ref{#1}}}
\newcommand{\defref}[1]{\mbox{Definition~\ref{#1}}}
\newcommand{\corref}[1]{\mbox{Corollary~\ref{#1}}}
\newcommand{\lemref}[1]{\mbox{Lemma~\ref{#1}}}
\newcommand{\assref}[1]{\mbox{Assumption~\ref{#1}}}
\newcommand{\probref}[1]{\mbox{Problem~\ref{#1}}}
\newcommand{\clref}[1]{\mbox{Claim~\ref{#1}}}
\newcommand{\propref}[1]{\mbox{Proposition~\ref{#1}}}
\newcommand{\remref}[1]{\mbox{Remark~\ref{#1}}}
\newcommand{\consref}[1]{\mbox{Construction~\ref{#1}}}
\newcommand{\figref}[1]{\mbox{Figure~\ref{#1}}}
\DeclareMathOperator*{\expe}{\mathbb{E}}


\newcommand{\class}[1]{{\ensuremath{\mathsf{#1}}}}
\newcommand{\gen}{\ensuremath{\class{Gen}}\xspace}
\newcommand{\rep}{\ensuremath{\class{Rep}}\xspace}
\newcommand{\sketch}{\ensuremath{\class{SS}}\xspace}
\newcommand{\rec}{\ensuremath{\class{Rec}}\xspace}
\newcommand{\enc}{\ensuremath{\class{Enc}}\xspace}
\newcommand{\dec}{\ensuremath{\class{Dec}}\xspace}
\newcommand{\prg}{\ensuremath{\class{prg}}\xspace}
\newcommand{\zo}{\ensuremath{\{0, 1\}}}
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\zq}{\ensuremath{\mathbb{Z}_q}}
\newcommand{\Fq}{\ensuremath{\mathbb{F}_q}}
\newcommand{\sample}{\ensuremath{\class{Sample}}\xspace}
\newcommand{\neigh}{\ensuremath{\class{Neigh}}\xspace}
\newcommand{\dis}{\ensuremath{\mathsf{dis}}}
\newcommand{\decode}{\ensuremath{\mathsf{Decode}}}

\newcommand{\A}{\mathcal{A}}


\newcommand{\metric}{\ensuremath{\mathtt{Metric}}\xspace}
\newcommand{\hill}{\ensuremath{\mathtt{HILL}}\xspace}
\newcommand{\hillrlx}{\ensuremath{\mathtt{HILL\mhyphen rlx}}\xspace}
\newcommand{\yao}{\ensuremath{\mathtt{Yao}}\xspace}
\newcommand{\unp}{\ensuremath{\mathtt{unp}}\xspace}
\newcommand{\unprlx}{\ensuremath{\mathtt{unp\mhyphen rlx}}\xspace}
\newcommand{\metricstar}{\ensuremath{\mathtt{Metric}^*}\xspace}
\newcommand{\metricd}{\ensuremath{\mathtt{Metric}^*\mathtt{-d}}\xspace}
\newcommand{\hillstar}{\ensuremath{\mathtt{HILL}^*}\xspace}
\newcommand{\hillprime}{\ensuremath{\mathtt{HILL'}}\xspace}
\newcommand{\metricprime}{\ensuremath{\mathtt{Metric'}}\xspace}
\newcommand{\metricprimestar}{\ensuremath{\mathtt{Metric'}^*}\xspace}
\newcommand{\hillprimestar}{\ensuremath{\mathtt{HILL'}^*}\xspace}
\newcommand{\poly}{\ensuremath{\mathtt{poly}}\xspace}
\newcommand{\rank}{\ensuremath{\mathtt{rank}}\xspace}
\newcommand{\ngl}{\ensuremath{\mathtt{ngl}}\xspace}
\newcommand{\Hoo}{\mathrm{H}_\infty}
\newcommand{\Hav}{\tilde{\mathrm{H}}_\infty}
\newcommand{\Dom}{\mathsl{Dom}}
\newcommand{\Range}{\mathsl{Rng}}
\newcommand{\Keys}{\mathsl{Keys}}
\def\col{\mathrm{Col}}

\newcommand{\ddetbin}{\ensuremath{\mathcal{D}^{det,\{0,1\}}}}
\newcommand{\drandbin}{\ensuremath{\mathcal{D}^{rand,\{0,1\}}}}
\newcommand{\ddetrange}{\ensuremath{\mathcal{D}^{det,[0,1]}}}
\newcommand{\drandrange}{\ensuremath{\mathcal{D}^{rand,[0,1]}}}

\newcommand{\expinfo}{\ensuremath{\mathcal{E}}}
\newcommand{\ext}{\ensuremath{\mathtt{ext}}}
\newcommand{\rext}{\ensuremath{\mathtt{rext}}}
\newcommand{\cons}{\ensuremath{\mathtt{cons}}}
\newcommand{\decons}{\ensuremath{\mathtt{decons}}}


\newcommand{\lwe}{\class{LWE}}
\newcommand{\LWE}{\class{LWE}}
\newcommand{\distLWE}{\ensuremath{\class{dist\mbox{-}LWE}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{construction}[theorem]{Construction}

\newcounter{ctr}
\newcounter{savectr}
\newcounter{ectr}

\newenvironment{newitemize}{%
\begin{list}{\mbox{}\hspace{5pt}$\bullet$\hfill}{\labelwidth=15pt%
\labelsep=5pt \leftmargin=20pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{3pt} }}{\end{list}}


\newenvironment{newenum}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=17pt%
\labelsep=5pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{tiret}{%
\begin{list}{\hspace{2pt}\rule[0.5ex]{6pt}{1pt}\hfill}{\labelwidth=15pt%
\labelsep=3pt \leftmargin=22pt \topsep=3pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt}}}{\end{list}}


\newenvironment{blocklist}{\begin{list}{}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{20pt}}}{\end{list}}

\newenvironment{blocklistindented}{\begin{list}{}{\labelwidth=0pt%
\labelsep=30pt \leftmargin=30pt\topsep=5pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt}}}{\end{list}}

\newenvironment{onelist}{%
\begin{list}{{\rm (\arabic{ctr})}\hfill}{\usecounter{ctr} \labelwidth=18pt%
\labelsep=7pt \leftmargin=25pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{twolist}{%
\begin{list}{{\rm (\arabic{ctr}.\arabic{ectr})}%
\hfill}{\usecounter{ectr} \labelwidth=26pt%
\labelsep=7pt \leftmargin=33pt \topsep=2pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{2pt} }}{\end{list}}

\newenvironment{centerlist}{%
\begin{list}{\mbox{}}{\labelwidth=0pt%
\labelsep=0pt \leftmargin=0pt \topsep=10pt%
\setlength{\listparindent}{\saveparindent}%
\setlength{\parsep}{\saveparskip}%
\setlength{\itemsep}{10pt} }}{\end{list}}

\newenvironment{newcenter}[1]{\begin{centerlist}\centering%
\item #1}{\end{centerlist}}

\newenvironment{codecenter}[1]{\begin{small}\begin{centerlist}\centering%
\item #1}{\end{centerlist}\end{small}}

\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\textcolor{red}{\textsf{#1 notes: }\textcolor{blue}{ #2}}\marginpar{\textcolor{red}{\textbf{!!!!!}}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\bnote}[1]{{\authnote{Ben}{#1}}}
\newcommand{\lnote}[1]{{\authnote{Leo}{#1}}}
\newcommand{\xnote}[1]{{\authnote{Xianrui}{#1}}}

\newcommand{\ve}{\vect{e}}
\newcommand{\vm}{\vect{m}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vE}{\vect{E}}
\newcommand{\vS}{\vect{S}}
\newcommand{\vA}{\vect{A}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vQ}{\vect{Q}}
\newcommand{\vR}{\vect{R}}
\newcommand{\vU}{\vect{U}}
\newcommand{\vT}{\vect{T}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vB}{\vect{B}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vd}{\vect{d}}
\newcommand{\vs}{\vect{s}}
\newcommand{\vx}{\vect{x}}
\newcommand{\va}{\vect{a}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vgamma}{\mathbf{\Gamma}}
\newcommand{\vt}{\vect{t}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vF}{\vect{F}}
\newcommand{\recout}{x}
\newcommand{\ignore}[1]{}
\newcommand{\M}{\mathcal{M}}

\title{Computational Fuzzy Extractors}
\author{Benjamin Fuller 
\footnote{Email: {\tt bfuller@cs.bu.edu}.  Work done in part while the author was at MIT Lincoln Laboratory.}
~~~~~~~~Xianrui Meng\footnote{Email: {\tt xmeng@cs.bu.edu}.}~~~~~~~~~Leonid Reyzin\footnote{Email: {\tt reyzin@cs.bu.edu}.} \\ Boston University}
%\author{}
\begin{document}
\maketitle


\begin{abstract} 
Fuzzy extractors derive strong keys from noisy sources.  Their
security is defined information-theoretically, which limits the length
of the derived key, sometimes making it too short to be useful. We ask
whether it is possible to obtain longer keys by considering
computational security, and show the following.

\begin{itemize}
\item\textbf{Negative Result:} Noise tolerance in fuzzy extractors is usually
achieved using an information reconciliation component called a ``secure
sketch.'' The security of this component, which directly affects the
length of the resulting key, is subject to lower bounds from coding
theory.  We show that, even when defined computationally, secure
sketches are still subject to lower bounds from coding theory. Specifically, we consider
two computational relaxations of the information-theoretic security requirement of secure sketches, using conditional HILL entropy and unpredictability entropy. For both cases we show  that computational secure sketches cannot outperform the best information-theoretic secure sketches in the case of high-entropy Hamming metric sources.

\item\textbf{Positive Result:} We show that the negative result can be overcome by
analyzing computational fuzzy extractors directly.  Namely, we show
how to build a computational fuzzy extractor whose output key length
equals the entropy of the source (this is impossible in the
information-theoretic setting). Our construction is based on the
hardness of the Learning with Errors (LWE) problem, and is secure when
the noisy source is uniform or symbol-fixing (that is, each dimension
is either uniform or fixed). As part of the security proof, we show a result of independent interest, namely
that the decision version of LWE is secure even when a small number of
dimensions has no error.
\end{itemize}
\textbf{Keywords} Fuzzy extractors, secure sketches, key derivation, Learning with Errors, error-correcting codes, computational entropy, randomness extractors.
\end{abstract}


\section{Introduction}\label{sec:introduction}

Authentication generally requires a secret drawn from some high-entropy source.  One of the primary building blocks for authentication is reliable key derivation.  Unfortunately, many sources that contain sufficient entropy to derive a key are  noisy, and provide similar, but not identical secret values at each reading (examples of such sources include biometrics~\cite{daugman2004}, human memory~\cite{zviran1993comparison}, pictorial passwords~\cite{brostoff2000passfaces}, measurements of capacitance~\cite{tuyls2006puf}, timing~\cite{suh2007physical}, motion~\cite{castelluccia2005shake},  quantum information~\cite{bennett1988privacy} etc.).  

Fuzzy extractors~\cite{DBLP:journals/siamcomp/DodisORS08} achieve reliable key derivation from noisy sources~(see \cite{Boyen05secureremote,dodisWichs2009,chandran2010privacy} for applications of fuzzy extractors).  The setting 
consists of  two algorithms: Generate (used once) and Reproduce (used subsequently).  The Generate ($\gen$) algorithm takes an input $w$ and produces a key $r$ and a public value $p$.  This information allows
the Reproduce ($\rep$) algorithm to reproduce $r$ given $p$ and some value $w'$ that is close to $w$ (according to some predefined metric, such as Hamming distance). 
Crucially for security,  knowledge of $p$ should not reveal $r$; that is, $r$ should be uniformly distributed conditioned on $p$.  This feature is needed because $p$ is not secret: for example, in a single-user setting (where the user wants to reproduce the key $r$ from a subsequent reading $w'$), it would be stored in the clear; and in a key agreement application~\cite{Boyen05secureremote} (where two parties have $w$ and $w'$, respectively), it would be transmitted between the parties.

Fuzzy extractors use ideas from information-reconciliation~\cite{bennett1988privacy} and are defined (traditionally) as information-theoretic objects.  The entropy loss of a fuzzy extractor is the difference between the entropy of $w$ and the length of the derived key $r$.  In the information-theoretic setting, some entropy loss is necessary as the value $p$ contains enough information to reproduce $r$ from any close value $w'$. 
A goal of fuzzy extractor constructions is to minimize the entropy loss, increasing the security of the resulting application.  Indeed, if the entropy loss is too high, the resulting secret key may be too short to be useful. 

We ask whether it is possible to obtain longer keys by considering
computational, rather than information theoretic, security.

\paragraph {Our Negative Results}
We first study (in  \secref{sec:impossCompSecSketch}) whether it could be fruitful to relax the definition of the main building block of a fuzzy extractor, called a \emph{secure sketch}.  A secure sketch is a one-round information reconciliation protocol: it  produces a public value $s$ that allows recovery of $w$ from any close value $w'$.  The traditional secrecy requirement of a secure sketch is that $w$ has high min-entropy conditioned on $s$.  This allows the fuzzy extractor of~\cite{DBLP:journals/siamcomp/DodisORS08}  to form the key $r$ by applying a randomness extractor~\cite{nisan1993randomness} to $w$, because randomness extractors produce random strings from strings with conditional min-entropy. We call this the \emph{sketch-and-extract} construction.

The most natural relaxation of the min-entropy requirement of the secure sketch is to require HILL entropy~\cite{DBLP:journals/siamcomp/HastadILL99}~(namely, that the distribution of $w$ conditioned on $s$ be \emph{indistinguishable} from a high-min-entropy distribution).  Under this definition, we could still use a randomness extractor to obtain $r$ from $w$, because it would yield a pseudorandom key.  Unfortunately, it is unlikely that such a relaxation will yield fruitful results: we prove in Theorem~\ref{thm:impSketchArbitraryW} that the entropy loss of such secure sketches is subject to the same coding bounds as the ones that constrain information-theoretic secure sketches.  

Another possible relaxation is to require that the value $w$ is unpredictable conditioned on $s$. This definition would also allow the use of a randomness extractor to get a pseudorandom key, although it would have to be a special extractor---one that has  a reconstruction procedure (see \cite[Lemma 6]{DBLP:conf/eurocrypt/HsiaoLR07}).  Unfortunately, this relaxation is also unlikely to be fruitful:  we prove in \thref{thm:imp of unp entropy} that the unpredictability is at most $\log$ the size of the metric space minus $\log$ the volume of the ball of radius $t$.  For high-entropy sources of $w$ over the Hamming metric, this bound matches the best information-theoretic security sketches.

\paragraph {Our Positive Results}

Both of the above negative results arise because a secure sketch functions like a decoder of an error-correcting code.  To avoid them, we give up on building computational secure sketches and focus directly on the entropy loss in fuzzy extractors.  Our goal is to decrease the entropy loss in a fuzzy extractor by allowing the key $r$ to be pseudorandom conditioned on $p$.  

By considering this computational secrecy requirement, we construct the first \emph{lossless} computational fuzzy extractors (\consref{cons:informal construction}), where the derived key $r$ is as long as the entropy of the source $w$. Our construction is for the Hamming metric and uses the code-offset construction~\cite{JW99},\cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} used in prior work, but with two crucial differences.  First, the key $r$ is not extracted from $w$ like in the sketch-and-extract approach; rather $w$ ``encrypts'' $r$ in a way that is decryptable with the knowledge of some close $w'$ (this idea is similar to the way the code-offset construction is presented in 
\cite{JW99} as  a ``fuzzy commitment''). Our construction uses private randomness, which is allowed in the fuzzy extractor setting but not in noiseless randomness extraction.  Second, the code used is a random linear code, which allows us to use the Learning with Errors~(LWE) assumption due to Regev~\cite{regev2005LWE, regevLWEsurvey} and derive a longer key $r$.


Specifically,  we use the  recent result of D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012}, which shows the hardness of decoding random linear codes when the error vector comes from the uniform distribution, with each coordinate ranging over a small interval. This allows us to use $w$ as the error vector, assuming it is uniform.  We also use a result of Akavia, Goldwasser, and Vaikuntanathan~\cite{akavia2009},  which says that LWE has many hardcore bits, to hide $r$.

Because we use a random linear code,  our decoding is limited to reconciling a logarithmic number of differences.  Unfortunately, we  cannot utilize the results that improve the decoding radius through the use of trapdoors (such as \cite{regev2005LWE}), because in a fuzzy extractor, there is no secret storage place for the trapdoor. If improved decoding algorithms are obtained for random linear codes, they will improve error-tolerance of our construction.  Given the hardness of decoding random linear codes~\cite{berlekamp1978}, we do not expect significant improvement in the error-tolerance of our construction.

In \secref{sec:LWE block fixing sources}, we are able to relax the assumption that $w$ comes from the uniform distribution, and instead allow $w$ to come from a symbol-fixing source~\cite{KZ07} (each dimension is either uniform or fixed). This relaxation follows from our results about the hardness of LWE when samples have a fixed~(and adversarially known) error vector, which may be of independent interest~(\thref{thm:blockLWE}). 

\paragraph{An Alternative Approach}  Computational extractors~\cite{krawczyk2010cryptographic, barak2011leftover, dachman2012computational} have the same goal of obtaining a pseudorandom key $r$ from a source $w$ in the setting without errors.  They can be constructed, for example, by applying a pseudorandom generator to the output of an information-theoretic extractor.  One way to build a computational \emph{fuzzy} extractor is by using a computational extractor instead  of the information-theoretic extractor in the sketch-and-extract construction of  \cite{DBLP:journals/siamcomp/DodisORS08}.   However, this approach is possible only if conditional min-entropy of $w$ conditioned on the sketch $s$ is high enough.  Furthermore, this approach does not allow the use of private randomness; private randomness is a crucial ingredient in our construction.
We compare the two approaches in \secref{sec:prg based comparison}.

\section{Preliminaries}
\label{sec:preliminaries}
For a random variable $X = X_1||...|| X_n$ where each $X_i$ is over some alphabet $\mathcal{Z}$, we denote by $X_{1,..., k} = X_1||...|| X_k$.  The {\em min-entropy} of $X$ is $\Hoo(X) = -\log(\max_x \Pr[X=x])$, 
and the {\em average (conditional)} min-entropy of $X$ given $Y$ is  $\Hav(X|Y) = -\log(\expe_{y\in Y} \max_{x} \Pr[X=x|Y=y])$~\cite[Section 2.4]{DBLP:journals/siamcomp/DodisORS08}.  
The {\em statistical distance} between random variables $X$ and $Y$ with the same domain is $\Delta(X,Y) = \frac12 \sum_x |\Pr[X=x] - \Pr[Y=x]|$. 
For a distinguisher $D$~(or a class of distinguishers $\mathcal{D}$) we write the \emph{computational distance} between $X$ and $Y$ as $\delta^D(X,Y) = \left| \expe[D(X)]-\expe[D(Y)]\right |$.  We denote by $\mathcal{D}_{s_{sec}}$ the class of randomized circuits which output a single bit and have size at most $s_{sec}$.
For a metric space $(\mathcal{M}, \dis)$, the \emph{(closed) ball of radius $t$ around $x$} is the set of all points within radius $t$, that is, $B_t(x) = \{y| \dis(x, y)\leq t\}$.  If the size of a ball in a metric space does not depend on $x$, we denote by $|B_t(\cdot)|$ the size of a ball of radius $t$.  For the Hamming metric over $\mathcal{Z}^n$, $|B_t(\cdot)| = \sum_{i=0}^t {n \choose i} (|\mathcal{Z}|-1)^i $.  $U_n$ denotes the uniformly  distributed random variable on $\{0,1\}^n$.
Usually, we use bold letters for vectors or matrices, capitalized letters for random variables, and lowercase letters for elements in a vector or samples from a random variable. 

\subsection{Fuzzy Extractors and Secure Sketches}
\label{sec:fuzzy extractors}

We now recall definitions and lemmas from the work of Dodis et. al.~\cite[Sections 2.5--4.1]{DBLP:journals/siamcomp/DodisORS08}, adapted to allow for a small probability of error, as discussed in \cite[Sections 8]{DBLP:journals/siamcomp/DodisORS08}.  Let $\mathcal{M}$ be a metric space with distance function $\dis$.

\begin{definition}
\label{def:fuzzy extractor}
An $(\mathcal{M}, m, \ell, t, \epsilon)$-\emph{fuzzy extractor} with error $\delta$ is a pair of randomized procedures, ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$, with the following properties: 
\begin{enumerate}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $r\in\{0,1\}^\ell$ and a helper string $p\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in \mathcal{M}$ and a bit string $p\in\{0,1\}^*$ as inputs.  The \emph{correctness} property of fuzzy extractors guarantees that for $w$ and $w'$ such that $\dis(w,w')\leq t$, if $R,P$ were generated by $(R,P)\leftarrow\gen(w)$, then $\rep(w',P)=R$ with probability~(over the coins of $\gen, \rep$) at least $1-\delta$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W$ on $\mathcal{M}$ of min-entropy $m$, the string $R$ is nearly uniform even for those who observe $P$:  if $(R,P)\leftarrow\gen (W)$, then $\mathbf{SD}((R,P),(U_\ell,P))\leq \epsilon$.
\end{enumerate}
A fuzzy extractor is efficient if $\gen$ and $\rep$ run in expected polynomial time.
\end{definition}

Secure sketches are the main technical tool in the construction of fuzzy extractors.  Secure sketches produce a string $s$ that does not decrease the entropy of $w$ too much, while allowing recovery of $w$ from a  close $w'$:
\begin{definition}
\label{def:secure sketch}
An $(\mathcal{M},m, \tilde{m}, t)$-\emph{secure sketch} with error $\delta$ is a pair of randomized procedures, ``sketch'' $(\sketch)$ and ``recover'' $(\rec)$, with the following properties:
\begin{enumerate}
\item The sketching procedure \sketch on input $w\in\mathcal{M}$ returns a bit string $s\in\{0,1\}^*$.
\item The recovery procedure \rec takes an element $w'\in\mathcal{M}$ and a bit string $s\in\{0,1\}^*$.  The \emph{correctness} property of secure sketches guarantees that if $\dis(w,w')\leq t$, then $\Pr[\rec(w',\sketch(w))=w]\geq 1-\delta$ where the probability is taken over the coins of $\sketch$ and $\rec$.  If $\dis(w,w')>t$, then no guarantee is provided about the output of \rec.
\item The \emph{security} property guarantees that for any distribution $W$ over $\mathcal{M}$ with min-entropy $m$, the value of $W$ can be recovered by the adversary who observes $w$ with probability no greater than $2^{-\tilde{m}}$.  That is, $\Hav(W|\sketch(W))\geq \tilde{m}$.
\end{enumerate}
A secure sketch is \emph{efficient} if \sketch and \rec run in expected polynomial time. 
\end{definition}

Note that in the above definition of secure sketches (resp., fuzzy extractors), the errors are chosen before $s$ (resp., $P$) is known: if the error pattern between $w$ and $w'$ depends on the output of $\sketch$ (resp., $\gen$), then there is no guarantee about the probability of correctness.


A fuzzy extractor can be produced from a \emph{secure sketch} and an \emph{average-case randomness extractor}. An average-case extractor is a generalization of a strong randomness extractor \cite[Definition 2]{nisan1993randomness}) (in particular, Vadhan~\cite[Problem 6.8]{Vad12} showed that all strong extractors are average-case extractors with a slight loss of parameters):
\begin{definition}
Let $\chi_1$, $\chi_2$ be finite sets.
A function $\ext: \chi_1\times \{0,1\}^d \rightarrow \{0,1\}^\ell$ a \emph{$(m, \epsilon)$-average-case extractor} if for all pairs
of random variables $X, Y$ over $\chi_1, \chi_2$ such that
$\tilde{H}_\infty(X|Y) \ge m$, we have $\Delta((\ext(X, U_d), U_d, Y), U_\ell\times
U_d \times Y) \le \epsilon$.
\end{definition}

\begin{lemma}
\label{lem:fuzzy ext construction}
Assume $(\sketch, \rec)$ is an $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch with error $\delta$, and let $\ext:\mathcal{M}\times \zo^d \rightarrow \zo^\ell$ be a $(\tilde{m}, \epsilon)$-average-case extractor.  Then the following $(\gen, \rep)$ is an $(\mathcal{M}, m, \ell, t, \epsilon)$-fuzzy extractor with error $\delta$:
\begin{itemize}
\item $\gen(w):$ generate $x\leftarrow \zo^d$, set $p=(\sketch(w), x), r=\ext(w;x)$, and output $(r,p)$.
\item $\rep(w', (s, x)):$ recover $w=\rec(w',s)$ and output $r=\ext(w;x)$.
\end{itemize}
\end{lemma}
The main parameter we will be concerned with is the entropy loss of the construction.  In this paper, we ask whether a smaller entropy loss can be achieved by considering a fuzzy extractor with a computational security requirement.  We therefore relax the security requirement of \defref{def:fuzzy extractor} to require a pseudorandom output instead of a truly random output.  Also, for notational convenience, we modify the definition so that we can specify a general class of sources for which the fuzzy extractor is designed to work, rather than limiting ourselves to the class of sources that consists of all sources of a given min-entropy $m$, as in definitions above (of course, this modification can also be applied to prior definitions of information-theoretic secure sketches and fuzzy extractors).

\begin{definition}[Computational Fuzzy Extractor]\label{def:comp fuzzy extractor}
Let $\mathcal{W}$ be a family of probability distributions over $\mathcal{M}$. A pair of randomized procedures ``generate'' $(\gen)$ and ``reproduce'' $(\rep)$ is a $(\mathcal{M}, \mathcal{W}, \ell, t)$-\emph{computational fuzzy extractor} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if \gen and \rep satisfy the following properties:
\begin{itemize}
\item The generate procedure \gen on input $w\in \mathcal{M}$ outputs an extracted string $R\in\{0,1\}^\ell$ and a helper string $P\in\{0,1\}^*$.
\item The reproduction procedure \rep takes an element $w'\in\mathcal{M}$ and a bit string $P\in\{0,1\}^*$ as inputs.  The \emph{correctness} property guarantees that if $\dis(w, w')\leq t$ and $(R, P)\leftarrow \gen(w)$, then $\Pr[\rep( w', P) = R] \geq 1-\delta$ where the probability is over the randomness of $(\gen, \rep)$.  
If $\dis(w, w') > t$, then no guarantee is provided about the output of \rep.
\item The \emph{security} property guarantees that for any distribution $W\in \mathcal{W}$, the string $R$ is pseudorandom conditioned on $P$, that is $\delta^{\mathcal{D}_{s_{sec}}}((R, P), (U_\ell, P))\leq \epsilon$.
\end{itemize}
\end{definition}
Any efficient fuzzy extractor is also a computational fuzzy extractor with the same parameters.

\textbf{Remark}  Fuzzy extractor definitions make no guarantee about \rep behavior when the distance between $w$ and $w'$ is larger than $t$.  In the information-theoretic setting this seemed inherent as the ``correct'' $R$ should be information-theoretically unknown conditioned on $P$.  However, in the computationally setting this is not true.  Looking ahead, in our construction $R$ is information-theoretically determined conditioned on $P$~(with high probability over the coins of \gen).  Our $\rep$ algorithm will never output an incorrect key~(with high probability over the coins of \gen) but may not terminate.  However, it is not clear this is the desired behavior.  For this reason, we leave the behavior of \rep ambiguous when $\dis(w, w')>t$.

\section{Impossibility of Computational Secure Sketches}
\label{sec:impossCompSecSketch}
In this section, we consider whether it is possible in build a secure sketch that retains significantly more computational than information-theoretic entropy.  We consider two different notions for computational entropy, and for both of them show that corresponding secure sketches are subject to the same upper bounds as those for information-theoretic secure sketches. Thus, it seems that relaxing security of sketches from information-theoretic to computational does not help.

In particular, for the case of the Hamming metric and inputs that have full entropy, our results are as follows.  In  \secref{sec:imp HILL sketch} we show that a sketch that retains HILL entropy implies a sketch that retains nearly the same amount of min-entropy.  In \secref{sec:imp unp sketch}, we show that the computational unpredictability of a sketch is at most $\log |\mathcal{M}| - \log |B_t(\cdot)|$. Dodis et al. \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08}  construct sketches with essentially the same information-theoretic security\footnote{The security in  \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08}  is expressed in terms of entropy of the error rate; recall that $\log B_t(\cdot)\approx H_q(t/n)$, where $n$ is the number of symbols, $q$ is the alphabet size, and $H_q$ is the $q$-ary entropy function.} . 
In \secref{ssec:avoiding bounds}, we discuss mechanisms for avoiding these bounds.

\subsection{Bounds on Secure Sketches using HILL entropy}
\label{sec:imp HILL sketch}
HILL entropy is a commonly used computational notion of entropy \cite{DBLP:journals/siamcomp/HastadILL99}.  It was extended to the conditional case by Hsiao, Lu, Reyzin~\cite{DBLP:conf/eurocrypt/HsiaoLR07}. Here we recall a weaker definition due to Gentry and Wichs~\cite{gentry2011separating}~(the term relaxed HILL entropy was introduced in~\cite{reyzin2011some}); since we show impossibility even for this weaker definition, impossibility for the stronger definition follows immediately.

\begin{definition}
\label{def:relaxed hill}
Let $(W, S)$ be a pair of random variables.  $W$ has 
\emph{relaxed HILL entropy} at least $k$ conditioned on $S$,
denoted $H^{\hillrlx}_{\epsilon, s_{sec}}(W|S)\geq k$ if there exists a joint distribution $(X, Y)$, such that $\tilde{H}_\infty(X|Y)\geq k$ and $\delta^{\mathcal{D}_{s_{sec}}} ((W, S),(X,Y))\leq \epsilon$.
\end{definition}

Intuitively, HILL entropy is as good as average min-entropy for all computationally-bounded observers.  Thus, redefining secure sketches using HILL entropy is a  natural relaxation of the original information-theoretic definition; in particular, the sketch-and-extract construction in \lemref{lem:fuzzy ext construction} would yield pseudorandom outputs if the secure sketch ensured high HILL entropy.  
We will consider secure sketches that retain relaxed HILL entropy: that is, we say that $(\sketch, \rec)$ is a  \emph{HILL-entropy~$(\mathcal{M}, m, \tilde{m}, t)$ secure sketch} that is $(\epsilon,s_{sec})$-hard with error $\delta$ if it satisfies \defref{def:secure sketch}, with the security requirement replaced by $H^{\hillrlx}_{\epsilon, s_{sec}}(W|\sketch(W))\geq \tilde{m}$. 

Unfortunately, we will show below that such a secure sketch implies an error correcting code with approximately $2^{\tilde{m}}$ points that can correct $t$ random errors (see  \cite[Lemma C.1]{DBLP:journals/siamcomp/DodisORS08} for a similar bound on information-theoretic secure sketches). For the Hamming metric, our result essentially matches the bound on information-theoretic secure sketches of \cite[Proposition 8.2]{DBLP:journals/siamcomp/DodisORS08}.  In fact, we show that, for the Hamming metric, HILL-entropy secure sketches imply information-theoretic ones with similar parameters, and, therefore, the HILL relaxation gives no advantage. 

The intuition for building error-correcting codes from HILL-entropy secure sketches is as follows.  In order to have  $H^{\hillrlx}_{\epsilon, s_{sec}}(W|\sketch(W))\ge \tilde{m}$, there must be a distribution $X, Y$ such that $\Hav(X | Y)\geq \tilde{m}$ and $(X, Y)$ is computationally indistinguishable from $(W, \sketch(W))$.  Sample a sketch $s\leftarrow \sketch(W)$. We know that $\sketch$ followed by $\rec$ likely succeeds on $W|s$  (i.e., $\rec (w', s) = w$ with high probability for $w\leftarrow W|s$ and $w'\leftarrow B_t(w)$).  %So, by indistinguishability, it must also succeed on $Y$. 
 Consider the following experiment: 1) sample $y\leftarrow Y$, 2) draw $x\leftarrow X|y$ and 3) $x'\leftarrow B_t(x)$. By indistinguishability, $\rec (x',y) = x$ with high probability.
 This means we can construct a large set $\mathcal{C}$ from the support of $X|y$.  $\mathcal{C}$ will be an error correcting code and $\rec$ an efficient decoder.  We can then use standard arguments to turn this code into an information theoretic sketch.  

To make this intuition precise, we need an additional technical condition:  sampling a random neighbor of a point is efficient.
\begin{definition}
\label{def:neighborhood samplable}
We say a metric space $(\mathcal{M}, \dis)$ is $(s_{neigh}, t)$-\emph{neighborhood samplable} if there exists a randomized circuit $\neigh$ of size $s_{neigh}$ that for all $t'\leq t$, $\neigh (w, t')$ outputs a random point at distance $t'$ of $w$.  
\end{definition}

We review the definition of a Shannon code~\cite{shannon1949mathematical}:
\begin{definition}
\label{def:shannon-code}
Let $\mathcal{C}$ be a set over space $\mathcal{M}$.  We say that $\mathcal{C}$ is an $(t,\epsilon)$-\emph{Shannon code} if there exists an efficient procedure $\rec$ such that for all $t'\le t$ and for all $c\in \mathcal{C}$, $\Pr[\rec(\neigh(c, t')) \neq c]\le \epsilon$. To distinguish it from the average-error Shannon code defined below, we will sometimes call it a \emph{maximal-error} Shannon code.
\end{definition}
This is a slightly stronger formulation than usual, in that for every size  $t'<t$ we require the code to correct $t'$ random errors\footnote{In the standard formulation, the code must correct a random error of size up to $t$, which may not imply that it can correct a random error of a much smaller size $t'$, because the volume of the ball of size $t'$ may be negligible compared to the volume of the ball of size $t$.  For codes that are monotone~(if decoding succeeds on a set of errors, it succeeds on all subsets), these formulations are equivalent.  However, we work with an arbitrary recover functionality that is not necessarily monotone.}.  
Shannon codes work for all codewords. We can also consider a formulation that works for an ``average'' codeword. 

 \begin{definition}
Let $C$ be a distribution over space $\mathcal{M}$.  We say that $C$ is an $(t,\epsilon)$-\emph{average error Shannon code} if there exists an efficient procedure $\rec$ such that for all $t'\le t$
$\Pr_{c\leftarrow C}[\rec(\neigh(c, t')) \neq c]\le \epsilon$.
\end{definition}
An average error Shannon code is one whose average probability of error is bounded by $\epsilon$.  See~\cite[Pages 192-194]{cover2006elements} for definitions of average and maximal error probability.  An average-error Shannon code is convertible to a maximal-error Shannon code with a small loss.  We use the following pruning argument from~\cite[Pages 202-204]{cover2006elements} (we provide a proof in \secref{sec:proof of average to maximal error}):
\begin{lemma}
\label{lem:averageToMaximalError}
Let $C$ be a $(t, \epsilon)$-average error Shannon code with recovery procedure $\rec$ such that $\Hoo(C)\geq k$.  There is a set $\mathcal{C}'$ with $|\mathcal{C}'|\ge2^{k-1}$ that  is a $(t, 2\epsilon)$-(maximal error) Shannon code with recovery procedure $\rec$.
\end{lemma}

We can now formalize the intuition above and show that a sketch that retains $\tilde{m}$-bits of relaxed HILL entropy implies a good error correcting code with nearly $2^{\tilde{m}}$ points~(proof in \secref{sec:proof of thm sketch implies code}).
\begin{theorem}\label{thm:impSketchArbitraryW}
Let $(\mathcal{M}, \dis)$ be a metric space that is $(s_{neigh}, t)$-neighborhood samplable.  Let $(\sketch, \rec)$ be an HILL-entropy $(\mathcal{M}, m, \tilde{m}, t)$-secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$.  Let $s_{rec}$ denote the size of the circuit that computes $\rec$.  If $s_{sec}\geq (t(s_{neigh}+s_{rec}))$,  then there exists a value $s$ and a set $\mathcal{C}$ with $|\mathcal{C}|\geq 2^{\tilde{m}-2}$  that is a $(t, 4(\epsilon+t\delta))$-Shannon code with recovery procedure $\rec(\cdot, s)$.
\end{theorem}

For the Hamming metric, any Shannon code (as defined in Definition~\ref{def:shannon-code}) can be converted into an information-theoretic secure sketch~(as described in \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08} and references therein).  The idea is to use the code offset construction, and convert worst-case errors to random errors by randomizing the order of the symbols of $w$ first, via a randomly chosen  permutation $\pi$  (which  becomes part of the sketch and is applied to $w'$ during $\rec$). The formal statement of this result  can be expressed in the following Lemma (which is implicit in \cite[Section 8.2]{DBLP:journals/siamcomp/DodisORS08}).
\begin{lemma}
\label{lem:shannon to sketch}
For an alphabet $\mathcal{Z}$, let $\mathcal{C}$ over $\mathcal{Z}^n$ be a $(t, \delta)$ Shannon code.  Then there exists a $(\mathcal{Z}^n, m, m-(n\log|\mathcal{Z}|-\log |\mathcal{C}|), t)$ secure sketch with error $\delta$ for the Hamming metric over $\mathcal{Z}^n$. 
\end{lemma}
Putting together \thref{thm:impSketchArbitraryW} and \lemref{lem:shannon to sketch} gives us the negative result for the Hamming metric: a HILL-entropy secure sketch (for the uniform distribution) implies an information-theoretic one with similar parameters:
\begin{corollary}
\label{cor:rec yields sketch}
Let $\mathcal{Z}$ be an alphabet. Let $(\sketch', \rec')$ be an $(\epsilon,s_{sec})$-HILL-entropy $(\mathcal{Z}^n, n\log |\mathcal{Z}|, \tilde{m}, t)$-secure sketch with error $\delta$ for the Hamming metric over $\mathcal{Z}^n$, with $\rec'$ of circuit size $s_{rec}$.
If $s_{sec}\geq t(s_{rec} + n\log |\mathcal{Z}|)$, then there exists a   $(\mathcal{Z}^n, n\log |\mathcal{Z}|, \tilde{m}-2,t)$ (information-theoretic) secure sketch with error
$4(\epsilon+t\delta)$. 
\end{corollary}
\textbf{Note} In \corref{cor:rec yields sketch} we make no claim about the efficiency of the resulting  $(\sketch, \rec)$, because the proof of \thref{thm:impSketchArbitraryW} is not constructive.  

\corref{cor:rec yields sketch} extends to non-uniform distributions: if there exists a distribution whose HILL sketch retains $\tilde{m}$ bits of entropy, then for all distributions $W$, there is an information theoretic sketch that retains $\Hoo(W) - (n\log |\mathcal{Z}|-\tilde{m})-2$ bits of entropy.

\subsection{Bounds on Secure Sketches using Unpredictability Entropy}
\label{sec:imp unp sketch}
In the previous section, we showed that any sketch that retained HILL entropy could be transformed into an information theoretic sketch.  However, HILL entropy is a strong notion.  In this section, we therefore ask whether it is useful to consider a sketch that satisfies a minimal requirement: the value of the input is computationally hard to guess given the sketch.  We begin by recalling the definition of conditional unpredictability entropy~\cite[Definition 7]{DBLP:conf/eurocrypt/HsiaoLR07}, which captures the notion of ``hard to guess'' (we relax the definition slightly, similarly to the relaxation of HILL entropy described in the previous section).

\begin{definition}
\label{def:unp entropy}
Let  $(W, S)$ be a pair of random variables. We say that $W$ has \emph{relaxed unpredictability entropy} at least $k$ conditioned on $S,$ denoted by $H^{\unprlx}_{\epsilon, s_{sec}} (W|S) \geq k$, if there exists a pair of distributions $(X, Y)$ such that $\delta^{\mathcal{D}_{s_{sec}}}((W, S),(X, Y))\leq \epsilon$, and for all circuits $\mathcal{I}$ of size $s_{sec}$,
\[
\Pr[\mathcal{I}(Y) = X ] \leq 2^{-k}
.\]
\end{definition}

We will say that a pair of procedures $(\sketch, \rec)$ is a \emph{unpredictability-entropy $(\mathcal{M}, m, \tilde{m}, t)$ secure sketch} that is $(\epsilon, s_{sec})$-hard with error $\delta$ if it satisfies \defref{def:secure sketch}, with the security requirement replaced by $H^{\unprlx}_{\epsilon, s_{sec}}(W| \sketch(W))\geq \tilde{m}$.  
Note this notion is quite natural: combining such a secure sketch in a sketch-and-extract construction of  \lemref{lem:fuzzy ext construction} with a particular type of extractor (called a \emph{reconstructive} extractor~\cite{barak-computational}), would yield a computational fuzzy extractor (per \cite[Lemma 6]{DBLP:conf/eurocrypt/HsiaoLR07}).  

Unfortunately, the conditional unpredictability entropy $\tilde{m}$ must decrease as $t$ increases, as the following theorem states.  (The proof of the theorem, generalized to more metric spaces, is in \secref{sec:proof of imp unp entropy}.)

\begin{theorem}
\label{thm:imp of unp entropy}
Let $\mathcal{Z}$ be an alphabet $(\sketch, \rec)$ be an unpredictability-entropy $(\mathcal{Z}^n, m, \tilde{m}, t)$-secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$.  If $s_{sec} \geq t(|\rec|+n\log |\mathcal{Z}|)$, then $\tilde{m}\leq n\log |\mathcal{Z}| - \log |B_t(\cdot)| + \log(1-\epsilon -t\delta)$.
\end{theorem}
In particular, if the input is uniform, the entropy loss is about $\log |B_t(\cdot)|$.  As mentioned at the beginning of~\secref{sec:impossCompSecSketch}, essentially the same entropy loss can be achieved with information-theoretic secure sketches, by using the randomized code-offset construction. However, it is conceivable that unpredictability entropy secure sketches could achieve lower entropy loss with greater efficiency for some parameter settings.

\subsection{Avoiding sketch entropy upper bounds}
\label{ssec:avoiding bounds}

The lower bounds of \corref{cor:rec yields sketch} and \thref{thm:imp of unp entropy} are strongest for high entropy sources.  
This is necessary, if a source contains only codewords (of an error correcting code), no sketch is needed, and thus there is no (computational)~entropy loss.  
This same situation occurs when considering lower bounds for information-theoretic sketches~\cite[Appendix C]{DBLP:journals/siamcomp/DodisORS08} .

Both of lower bounds arise because \rec must function as an error-correcting code for many points of any indistinguishable distribution.  It may be possible to avoid these bounds if \rec outputs a fresh random variable\footnote{If some efficient algorithm can take the output of $\rec$ and efficiently transform it back to the source $W$, the bounds of \corref{cor:rec yields sketch} and \thref{thm:imp of unp entropy} both apply.  This means that we need to consider constructions that are hard to invert~(either information-theoretically or computationally).}.  Such an algorithm is called a computational fuzzy conductor.  See~\cite{KanukurthiR09} for the definition of a fuzzy conductor.  To the best of our knowledge, a computational fuzzy conductor has not been defined in the literature, the natural definition is to replace the pseudorandomness condition in \defref{def:comp fuzzy extractor} with a HILL entropy requirement.  

Our construction~(in \secref{sec:fuzzyCompExt}) has pseudorandom output and immediately satisfies definition of a computational fuzzy extractor~(\defref{def:comp fuzzy extractor}).  It may be possible to achieve significantly better parameters with a construction that is a computational fuzzy conductor~(but not a computational fuzzy extractor) and then applying an extractor.  We leave this as an open problem.

\section{Computational Fuzzy Extractor based on \class{LWE}}
\label{sec:fuzzyCompExt}

In this section we describe our main construction.  Security of our construction depends on the source $W$. We first consider  a uniform source $W$; we consider other distributions in \secref{sec:LWE block fixing sources}.  Our construction uses the code-offset construction~\cite{JW99}, \cite[Section 5]{DBLP:journals/siamcomp/DodisORS08} instantiated with a random linear code over a finite field $\Fq$.   Let $\decode_t$ be an algorithm that decodes a random linear code with at most $t$ errors (we will present such an algorithm later, in \secref{sec:time main construction}). 

\begin{construction}
Let $n$ be a security parameter and let $m\ge n$.  Let $q$ be a prime. 
Define $\gen, \rep$ as follows:
\begin{center}
\begin{tabular}{c|c}
\begin{minipage}{3in}
\textbf{\gen}
\begin{enumerate}
\item \underline{Input}: $w\leftarrow W$ (where $W$ is some distribution over $\Fq^m$).
\item Sample $\vA\in\Fq^{m\times n}, \vx\in\Fq^n$ uniformly.
\item Compute $p = (\vA, \vA \vx+w)$, \\\ $r = \vx_{1,...,n/2}$.
\item Output $(r, p)$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}{3in}
\textbf{\rep}
\begin{enumerate}
\item \underline{Input}: $(w', p)$ (where the Hamming distance between $w'$ and $w$ is at most $t$).
\item Parse $p$ as $(\vA, \vect{c})$; let $\vb=\vect{c}-w'$.
\item Let $x = \decode_t(\vA, \vb)$\\
\item Output $r = x_{1,...,n/2}$.
\end{enumerate}
\end{minipage} 
\end{tabular}
\end{center}
\label{cons:informal construction}
\end{construction}


Intuitively, security comes from the computational hardness of decoding random linear codes with a high number of errors (introduced by $w$).  
In fact, we know that decoding a random linear code is NP-hard~\cite{berlekamp1978}; however, this statement is not sufficient for our security goal, which is to show  $\delta^{\mathcal{D}_{s_{sec}}}((X_{1,..., n/2},P), (U_{n/2 \log q}, P))\leq \epsilon$.  Furthermore, this construction is only useful if $\decode_t$ can be efficiently implemented. 

The rest of this section is devoted to making these intuitive statements precise.
 We describe the \class{LWE} problem and the security of our construction in \secref{subsec:LWE}.
We describe one possible polynomial-time $\decode_t$ (which corrects more errors than is possible by exhaustive search) in \secref{sec:time main construction}.  In \secref{sec:lossless extractor}, we describe parameter settings that allow us to extract as many bits as the input entropy, resulting in a lossless construction.  In \secref{sec:prg based comparison}, we compare \consref{cons:informal construction} to using a sketch-and-extract approach (\lemref{lem:fuzzy ext construction}) instantiated with a computational extractor. 

\subsection{Security of \consref{cons:informal construction}}
\label{subsec:LWE}
The $\LWE$ problem was introduced by Regev \cite{regev2005LWE, regevLWEsurvey} as a generalization of ``learning parity with noise." For a complete description of the $\LWE$ problem and related lattices problems~(which we do not define here) see~\cite{regev2005LWE}.  We now recall the decisional version of the problem. 


\begin{definition}[Decisional $\lwe$]\label{def:dist-LWE}
Let $n$ be a security parameter.  
Let $m = m(n) = \poly(n)$ be an integer and $q = q(n) = \poly(n)$ be a prime\footnote{%
Unlike in common formulations of LWE, where $q$ can be any integer, we need $q$ to be prime for decoding.}.
%
Let $\vA$ be the uniform distribution over  $\Fq^{m\times n}$, $X$ be the uniform distribution over $\Fq^n$ and $\chi$ be an arbitrary distribution on $\Fq^m$.
 The decisional version of the $\LWE$ problem, denoted \class{dist}-$\LWE_{n, m, q, \chi}$, is to distinguish the distribution
$(\vA, \vA X+\chi)$ from
 the uniform distribution over $(\Fq^{m\times n}, \Fq^m)$.

We say that $\distLWE_{n, m, q, \chi}$ is $(\epsilon, s_{sec})$-secure if no (probabilistic) distinguisher of size $s_{sec}$ can distinguish the $\lwe$ instances from uniform except with probability $\epsilon$.  If for any $s_{sec} = \poly(n)$, there exists   $\epsilon  = \ngl(n)$ such that  $\distLWE_{n, m, q, \chi}$ is $(\epsilon, s_{sec})$-secure, then we say  it is \emph{secure}.
\end{definition}

 Regev\cite{regev2005LWE} and Peikert \cite{peikert2009latticereduction} show that $\class{dist}$-$\lwe_{n, m, q, \chi}$ is secure when the distribution $\chi$ of errors is Gaussian, as follows.
Let $\bar{\Psi}_\rho$ be the discretized Gaussian distribution with variance $(\rho q)^2/2\pi$, where $\rho \in (0,1)$ with $\rho q > 2\sqrt{n}$.  If GAPSVP and SIVP are hard to approximate~(on lattices of dimension $n$) within polynomial factors for quantum algorithms, then $\distLWE_{n, m, q, \bar{\Psi}_\rho^m}$ is secure.  (A recent result of Brakerski et al.~\cite{brakerski2013classical} shows security of $\LWE$ based on hardness of approximating lattices problems for classical algorithms.  We have not considered how this result can be integrated into our analysis.)

The above formulation of $\LWE$ requires the error term to come from the discretized Gaussian distribution, which makes it difficult to use it for constructing fuzzy extractors (because using $w$ and $w'$ to sample Gaussian distributions will increase the distance between the error terms and/or reduce their entropy).
Fortunately, recent work D\"{o}ttling and M\"{u}ller-Quade~\cite{dottling2012} shows the security of $\LWE$, under the same assumptions, when errors come from the uniform distribution over a small interval\footnote{Micciancio and Peikert provide a similar formulation in~\cite{micciancio2013hardness}.  The result D\"{o}ttling and M\"{u}ller-Quade provides better parameters for our setting.}.  This allows us to directly encode $w$ as the error term in an $\LWE$ problem by splitting it  into $m$ blocks.  The size of these blocks is dictated by the following result of D\"{o}ttling and M\"{u}ller-Quade:
\begin{lemma}~\protect{~\cite[Corollary 1]{dottling2012}}
\label{lem:uniform LWE decision}
Let $n$ be a security parameter.  Let $q = q(n) = \poly(n)$ be a prime and $m = m(n) = \poly(n)$ be an integer with $m\ge 3n$. Let $\sigma \in (0, 1)$ be an arbitrarily small  constant and let $\rho=\rho(n)\in (0,1/10)$ be such that $\rho q \geq 2n^{1/2+\sigma}m$. If the approximate decision-version of the shortest vector problem (GAPSVP) and the shortest independent vectors problem (SIVP) are hard within a factor of $\tilde{O}(n^{1+\sigma}m/\rho)$ for quantum algorithms in the worst case, then, for $\chi$ the uniform distribution over $[-\rho q, \rho q]^m$,  $\distLWE_{n, m, q, \chi}$ is secure.
\end{lemma}

To extract pseudorandom bits, we use a result of Akavia, Goldwasser, and Vaikuntanathan~\cite{akavia2009} to show that $X$ has simultaneously many hardcore bits.  The result says that if $\distLWE_{(n-k, m, q, \chi)}$ is secure then any $k$ variables of $X$ in a $\distLWE_{(n, m, q, \chi)}$ instance are hardcore.  We state their result for a general error distribution~(noting that their proof does not depend on the error distribution):
\begin{lemma}\protect{\cite[Lemma 2]{akavia2009}}
\label{lem:many hardcore bits}
 If $\distLWE_{(n-k, m, q, \chi)}$ is $(\epsilon, s_{sec})$ secure, then
\[\delta^ {\mathcal{D}_{s_{sec'}}} ((X_{1,\dots, k}, \vA, \vA X+\chi) , (U, \vA, \vA X+\chi)) \le \epsilon\,,\]
where $U$ denotes the uniform distribution over $\Fq^k$,  $\vA$ denotes the uniform distribution over $\Fq^{m\times n}$, $X$ denotes the uniform distribution over $\Fq^n$, $X_{1,\dots, k}$ denote the first $k$ coordinates of $x$, and $s_{sec}' \approx s_{sec} - n^3$.
\end{lemma}
The security of  \consref{cons:informal construction} follows from Lemmas \ref{lem:uniform LWE decision} and~\ref{lem:many hardcore bits} when parameters are set appropriately (see \thref{thm:lossless secure extractor log}),  because we use the hardcore bits of $X$ as our key.  

\subsection{Efficiency of \consref{cons:informal construction}}
\label{sec:time main construction}
\consref{cons:informal construction} is useful only if $\decode_t$ can be efficiently implemented.  We need a decoding algorithm for a random linear code with $t$ errors that runs in polynomial time.  We present a simple $\decode_t$ that runs in polynomial time and can correct $O(\log n)$ errors (note that this corresponds to a superpolynomial number of possible error patterns).
This algorithm is a proof of concept, and neither the algorithm nor its analysis have been optimized for constants. An improved decoding algorithm can replace our algorithm, which will increase our correcting capability and improve \consref{cons:informal construction}.

\begin{construction}
\label{cons:decoding algorithm} We consider a setting of $(n, m, q, \chi)$ where $m\geq 3n$.  We describe $\decode_t$:
\begin{enumerate}
\item Input $\vA , \vb = \vA \vx + w - w'$
\item Randomly select rows without replacement $i_1,..., i_{2n}\leftarrow [1,m]$.  
\item Restrict $\vA, \vb$ to rows $i_1,...,i_{2n}$; denote these $\vA_{i_1,...,i_{2n}}, \vb_{i_1,...,i_{2n}}$.
\item Find $n$ rows of $\vA_{i_1,..., i_{2n}}$ that are linearly independent.  
If no such rows exist, output $\perp$ and stop.
\item Denote by $\vA', \vb'$ the restriction of $\vA_{i_1,..., i_{2n}}, \vb_{i_1,..., i_{2n}}$ (respectively) to these rows. Compute $\vx' = (\vA')^{-1}\vb'$.  
\item If $\vb- \vA \vx'$ has more than $t$ nonzero coordinates, go to step (2).
\item Output $\vx'$.
\end{enumerate}
\end{construction}

Each step is computable in time $O(n^3)$. 
For $\decode_t$ to be efficient, we need $t$ to be small enough so that  with probability at least $\frac{1}{\poly(n)}$, none of the $2n$ rows  selected  in step 2 have errors (i.e., so that $w$ and $w'$ agree on those rows).  If this happens, and $\vA_{i_1,...,i_{2n}}$ has rank  $n$ (which is highly likely), then $\vx'=\vx$, and the algorithm terminates.  However, we also need to ensure correctness: we need to make sure that if $\vx'\neq \vx$, we detect it in step 6.  This detection will happen if $\vb-\vA \vx' = \vA (\vx-\vx')+(w-w')$ has more than $t$ nonzero coordinates.  It suffices to ensure that $\vA (\vx-\vx')$ has at least $2t+1$ nonzero coordinates (because at most $t$ of those can be zeroed out by $w-w'$), which happens whenever the code generated by $\vA$ has distance $2t+1$.

Setting $t = O(\frac{m}{n}\log n)$ is sufficient to ensure efficiency.    Random linear codes have distance at least $O(\frac{m}{n}\log n)$ with probability $1-e^{-\Omega(n)}$ (the exact statement is in \corref{cor:code high distance}), so this also ensures correctness.
The formal statement is below~(proof in \secref{sec:proof lem i t poly time}):
\begin{lemma}[Efficiency of $\decode_t$ when $t\leq d (m/n-2)\log n$]
\label{lem:i t poly time}
Let $d$ be a positive constant and assume that $\dis(W, W')\leq t$ where $t\leq d(\frac{m}{n}-2)\log n$.  Then $\decode_t$ runs in expected time $O(n^{4d+3})$ operations in $\Fq$~(this expectation is over the choice of random coins of $\decode_t$, regardless of the input, as long as $\dis(w, w')\le t$).  It outputs $X$ with probability $1-e^{-\Omega(n)}$ (this probability is over the choice of the random matrix  $\vA$ and random choices made by $\decode_t$).
\end{lemma}

\subsection{Lossless Computational Fuzzy Extractor}
\label{sec:lossless extractor}
We now state a setting of parameters that yields a lossless construction.  The intuition is as follows.  We are splitting our source into $m$ blocks each of size $\log \rho q$~(from \lemref{lem:uniform LWE decision}) for a total input entropy of $m\log \rho q$.  Our key is derived from hardcore bits of $X$: $X_{1,\dots, k}$ and is of size $k \log q$~(from \lemref{lem:many hardcore bits}). Thus, to achieve a lossless construction we need $k \log q = m\log \rho q$.
In other words, in order to decode a meaningful number of errors, the vector $w$ is of higher dimension than the vector $X$, but each coordinate of $w$ is sampled using fewer bits than each coordinate of $X$.    Thus, by increasing the size of $q$~(while keeping $\rho q$ fixed) we can set $k\log q = m\log \rho q$, yielding a key of the same size as our source.    The formal statement is below. 

\begin{theorem}
\label{thm:lossless secure extractor log}
Let $n$ be a security parameter and let the number of errors $t = c\log n$ for some positive constant $c$.    Let $d$ be a positive constant (giving us a tradeoff between running time of $\rep$ and $|w|$). Consider the Hamming metric over the alphabet $\mathcal{Z}=[-2^{b-1},2^{b-1}]$, where  $b = \log 2(c/d+2) n^2 =O(\log n)$.  Let $W$ be uniform over $\mathcal{M}=\mathcal{Z}^m$, where $m={(c/d+2)n}=O(n)$.  If GAPSVP and SIVP are hard to approximate within polynomial factors using quantum algorithms, then there is a setting of $q = \poly(n)$ such that for any polynomial $s_{sec}=\poly(n)$ there exists $\epsilon=\ngl(n)$ such that the following holds: \consref{cons:informal construction} is a $(\M, W, m\log |\mathcal{Z}|, t)$-computational fuzzy extractor that is $(\epsilon, s_{sec})$-hard with error $\delta = e^{-\Omega(n)}$.
The generate procedure $\gen$ takes $O(n^2)$ operations over $\Fq$, and the reproduce procedure $\rep$ takes expected time $O(n^{4d+3})$ operations over $\Fq$.
\end{theorem}
\begin{proof}
Security follows by combining Lemmas~\ref{lem:uniform LWE decision} and~\ref{lem:many hardcore bits}; efficiency follows by \lemref{lem:i t poly time}. For a more detailed explanation of the various parameters and constraints see \secref{sec:parameter settings}.  
\end{proof}


\thref{thm:lossless secure extractor log} shows that a computational fuzzy extractor can be built without incurring any entropy loss.  We can essentially think of $\vA X+W$ as an encryption of $X$ that where decryption works from any close $W'$.

\subsection{Comparison with computational-extractor-based constructions}
\label{sec:prg based comparison}
As mentioned in the introduction, an alternative approach to building a computational fuzzy extractor is to use  a computational extractor (e.g.,~\cite{krawczyk2010cryptographic, barak2011leftover, dachman2012computational}) in place of the information-theoretic extractor in the sketch-and-extract construction.  We will call this approach \emph{sketch-and-comp-extract}.  (A simple example of a computational extractor is a pseudorandom generator applied to the output of an information-theoretic extractor; note that LWE-based pseudorandom generators exist~\cite{applebaum2006pseudorandom}.)

This approach (specifically, its analysis via \lemref{lem:fuzzy ext construction}) works as long as the amount of entropy $\tilde{m}$ of $w$ conditioned on the sketch $s$ remains high enough to run a computational extractor.  However, as discussed in \secref{sec:impossCompSecSketch}, $\tilde{m}$ decreases with the error parameter $t$ due to coding bounds, and it is conceivable that, if $W$ has  barely enough entropy to begin with, it will have too little entropy left to run a computational extractor once $s$ is known.

In contrast, our approach does not require the entropy of $w$ conditioned on $p=(\vA, \vA X+w)$ to  be high enough for a computational extractor. Instead, we require that $w$ is not computationally recoverable  given $p$.  This requirement is weaker---in particular, in our construction, $w$ may have no information-theoretic entropy conditioned on $p$.  The key difference in our approach is that instead of extracting from $w$, we hide secret randomness using $w$. Computational extractors are not allowed to have private randomness \cite[Definition 3]{krawczyk2010cryptographic}.

The main advantage of our analysis (instead of sketch-and-comp-extract) is that security need not depend on the error-tolerance $t$.  In our construction, the error-tolerance depends only on the best available decoding algorithm for random linear codes, because decoding algorithms will not reach the information-theoretic decoding radius.

Unfortunately, LWE parameter sizes require relatively long $w$. Therefore, in practice, sketch-then-comp-extract will beat our construction  if the computational extractor is instantiated efficiently based on assumptions other than LWE (for example, a cryptographic hash function for an extractor and a block cipher for a PRG). However, we believe that our conceptual framework can lead to better constructions.  Of particular interest are  other codes that are easy to decode up to $t$ errors but become computationally hard as the number of errors increases.

To summarize, the advantage of \consref{cons:informal construction} is that the security of our construction does not depend on the decoding radius $t$.  
The disadvantages of \consref{cons:informal construction} are that it supports a limited number of errors and only a uniformly distributed source.  We begin to address this second problem in the next section.

\section{Computational Fuzzy Extractor for Nonuniform Sources}
\label{sec:LWE block fixing sources}
While showing the security of~\consref{cons:informal construction} for arbitrary high-min-entropy distributions is an open problem, in this section we show it for a particular class of distributions called symbol-fixing.   First we recall the notion of a symbol fixing source~(from~\cite[Definition 2.3]{KZ07}): 
\begin{definition}
Let $W = (W_1,..., W_{m+\alpha})$ be a distribution where each $W_i$ takes values over an alphabet $\mathcal{Z}$.  We say that it is a $(m+
\alpha, m, |\mathcal{Z}|) $ \emph{symbol fixing source} if for $\alpha$ indices $i_1, \dots, i_\alpha$, the symbols $W_{i_\alpha}$ are fixed, and the remaining $m$  symbols are chosen uniformly at random.  Note that $H_\infty(W)=m\log |\mathcal{Z}|$.
\end{definition}

Symbol-fixing sources are a very structured class of distributions.  However, extending \consref{cons:informal construction} to such a class is not obvious.  Although symbol-fixing sources are deterministically extractible~\cite{KZ07}, we cannot first run a deterministic extractor before using \consref{cons:informal construction}.  This is because we need to preserve distance between $w$ and $w'$ and an extractor must not preserve distance between input points.  We present an alternative approach, showing security of $\LWE$ directly with symbol-fixing sources.

The following theorem states the main technical result of this section, which is of potential interest outside our specific setting. The result is that $\distLWE$ with symbol-fixing sources is implied by standard $\distLWE$ (but for $n$ and $m$ reduced by the amount of fixed symbols).  
\begin{theorem}
\label{thm:blockLWE}
Let $n$ be a security parameter, $m, \alpha$ be polynomial in $n$, and $q=\poly(n)$ be a prime and $\beta\in\mathbb{Z^+}$ be such that $q^{-\beta} = \ngl(n)$. 
Let $U$ denote the uniform distribution over $\mathcal{Z}^m$ for an alphabet $\mathcal{Z}\subset \Fq$, and let $W$ denote an $(m+\alpha, m, |\mathcal{Z}|)$ symbol fixing source over $\mathcal{Z}^{m+\alpha}$.
If $\distLWE_{n, m,q, U}$ is secure, then $\distLWE_{n+\alpha+\beta, m+\alpha, q, W}$ is also secure.
\end{theorem}

\thref{thm:blockLWE} also holds for an arbitrary error distribution~(not just uniform error) in the following sense.  Let $\chi'$ be an arbitrary error distribution.  Define $\chi$ as the distribution where $m$ dimensions are sampled according to $\chi'$ and the remaining dimensions have some fixed error.  Then, security of $\distLWE_{n, m, q, \chi'}$ implies security of $\distLWE_{n+\alpha+ \beta, m+\alpha, q, \chi}$.  We show this stronger version of the theorem in \apref{sec:proof of block theorem}.

The intuition for this result is as follows.  Providing a single sample with no error ``fixes'' at most a single variable.  Thus, if there are significantly more variables than samples with no error,  search $\LWE$ should still be hard.  We are able to show a stronger result that $\distLWE$ is still hard.  The nontrivial part of the reduction is using the additional $\alpha+ \beta$ variables  to ``explain'' a random value for the last $\alpha$ samples, without knowing the other variables.  The $\beta$ parameter is the slack needed to ensure that the ``free'' variables have influence on the last $\alpha$ samples.  A similar theorem for the case of a single fixed dimension was shown in concurrent work by Brakerski et al.~\cite[Lemma 4.3]{brakerski2013classical}.  The proof techniques of Brakerski et al. can be extended to our setting with multiple fixed dimensions, improving the parameters of \thref{thm:blockLWE}~(specifically, removing the need for $\beta$).

\thref{thm:blockLWE} allows us to construct a lossless computational fuzzy extractor from block-fixing sources: 

\begin{theorem}
\label{thm:lossless block sketch log}
Let $n$ be a security parameter and let $t = c\log n$ for some positive constant $c$.  Let $d\le c$ be a positive constant and consider the Hamming metric over the alphabet $\mathcal{Z}=[-2^{b-1},2^{b-1}]$, where $b \approx \log 2(c/d+2)n^2 = O(\log n)$.  Let $\mathcal{M} = \mathcal{Z}^{m+\alpha}$ where $m= (c/d+2)n=O(n)$ and $\alpha \leq n/3$. 
Let $\mathcal{W}$ be the class of all $(m+\alpha, m, |\mathcal{Z}|)$-symbol fixing sources.  If GAPSVP and SIVP are hard to approximate within polynomial factors 
using quantum algorithms, then  there is a setting of $q = \poly(n)$ such that for any polynomial $s_{sec} = \poly(n)$
 there exists $\epsilon = \ngl(n)$ 
such that the following holds: \consref{cons:informal construction} is a $(\M, \mathcal{W}, m\log |\mathcal{Z}|, t)$-computational fuzzy extractor that is $(\epsilon, s_{sec})$-hard with error $\delta = e^{-\Omega(n)}$.
 The generate procedure $\gen$ takes $O(n^2)$ operations over $\Fq$, and the reproduce procedure $\rep$ takes expected time $O(n^{4d+3} \log n)$ operations over $\Fq$.
\end{theorem}

\begin{proof} Security follows by Lemmas~\ref{lem:uniform LWE decision} and~\ref{lem:many hardcore bits} and  \thref{thm:blockLWE} .  
Efficiency follows by \lemref{lem:i t poly time}.  For a more detailed explanation of parameters see \secref{ssec:block params}. 
\end{proof}

\section*{Acknowledgements}
The authors are grateful to Jacob Alperin-Sheriff, Ran Canetti, Yevgeniy Dodis, Nico D\"{o}ttling, Danielle Micciancio, J\"{o}rn M\"{u}ller-Quade, Christopher Peikert, Oded Regev, Adam Smith, and Daniel Wichs for helpful discussions, creative ideas, and important references.  In particular, the authors thank Nico D\"{o}ttling for describing his result on LWE with uniform errors.  

This work is supported in part by National Science Foundation grants 0831281, 1012910, and 1012798. The work of Benjamin Fuller is sponsored in part by the United States Air Force under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.  

\bibliographystyle{alpha}
\bibliography{crypto}
\appendix
\section{Properties of Random Linear Codes}
For efficient decoding of \consref{cons:informal construction}, we need the $\LWE$ instance to have high distance with overwhelming probability.  We will use the $q$-ary entropy function, denoted $H_q(x)$ and defined as $H_q(x) = x\log _q(q-1) - x\log_q x - (1-x)\log_q (1-x)$.  Note that $H_2(x) = -x\log x - (1-x)\log (1-x)$.  In the region $[0, \frac{1}{2}]$ for any value $q'\geq q$, $H_{q'}(x)\leq H_{q}(x)$.  The following theorem is standard in coding theory:

\begin{theorem}~\cite[Theorem 8]{venkatLecture}
\label{thm:random code good distance}
For prime $q, \delta\in [0, 1-1/q), 0<\epsilon< 1-H_q(\delta)$ and sufficiently large $m$, the following holds for $n = \lceil (1-H_q(\delta) - \epsilon)m\rceil$ .  If $\vA \in \Fq^{m\times n}$ is drawn uniformly at random, then the linear code with $\vA$ as a generator matrix has rate at least $(1-H_q(\delta) -\epsilon)$ and relative distance at least $\delta$ with probability at least $1-e^{-\Omega(m)}$.
\end{theorem}
Our setting is the case where $m = poly(n)\geq 2n$ and $\delta = O (\log n /n)$.  This setting of parameters satisfies \thref{thm:random code good distance}:
\begin{corollary}
\label{cor:code high distance}
Let $n$ be a parameter and let $m = \poly(n)\geq 2n$.  
Let $q$ be a prime and $\tau = O(\frac{m}{n}\log n )$.  For large enough values of $n$, when $\vA\in \Fq^{m\times n}$ is drawn uniformly, the code generated by $\vA$ has distance at least $\tau$ with probability at least $1-e^{-\Omega(m)}\geq 1-e^{-\Omega(n)}$.
\end{corollary}
\begin{proof}
Let $c$ be some constant.  Let $\delta = \tau/m = \frac{c\log n}{n}$.  We show the corollary for the case when $m = 2n$~(increasing the size of $m$ only increases the relative distance).  It suffices to show that for sufficiently large $n$, there exists $\epsilon>0$ where $1- H_q(\frac{c\log n}{n}) - \epsilon = 1/2$ or equivalently that $H_q(\frac{c\log n}{m})< 1/2$ as then setting $\epsilon = 1/2-H_q(\frac{c\log n}{n})$ satisfies  \thref{thm:random code good distance}.  For sufficiently large $n$:
\begin{itemize}
\item $\frac{c\log n}{n}< 1/2$, so we can work with the binary entropy function $H_2$.  
\item $\frac{c\log n}{n}< .1 < 1/2$ and thus $H_q(\frac{c\log n}{n})< H_q(.1)$. 
\end{itemize}  Putting these statements together, for large enough $n$, $H_q(\frac{c\log n}{n})< H_q(.1) < H_2(.1)< 1/2$ as desired.  This completes the proof.
\end{proof}

We also need that random matrices are full rank with high probability~(to allow us to decode).  We use the following claim~(techniques from Cooper~\cite{cooper2000rank}):
\begin{claim}
\label{cl:full rank matrix}
Let $q\ge 2$ be a prime.  Let $\alpha, \beta$ be integers and let 
let $\vS \overset{\$}\leftarrow \Fq^{\alpha \times (\alpha+\beta)}$ be uniformly generated.  Then $\Pr[\rank(\vS)=\alpha] >  1- q^{-\beta}$.
\end{claim}
\begin{proof}
Let $p_i$ be the probability that the $i$th
row is linearly dependent on the previous $i-1$ rows. 
By the union bound, the probability that $\alpha$ rows are linearly dependent is bounded by 
$\sum_{i=1}^\alpha p_i$.
Since $i-1$ rows can span a space of size at most $q^{i-1}$, the probability $p_i$ that a randomly chosen $i$th row is in that space is at most $q^{i-1}/q^{\alpha+\beta}$. So
\begin{align*}
\Pr[\rank(\vS) < \alpha] &=\sum_{i=1}^{\alpha} \frac{q^{i-1}}{q^{\alpha+\beta}}
= \frac{q^{\alpha}-1}{q-1}\frac{1}{q^{\alpha+\beta} }< q^{-\beta}.
\end{align*}
\end{proof}

\section{Proof of Theorem \ref{thm:blockLWE}}
\label{sec:proof of block theorem}

\begin{proof}
We assume that all of the fixed blocks are located at the end and their fixed value is $0$.  If the blocks are fixed to some other value, the reduction is essentially the same.
 In the reduction, the distinguisher is allowed to depend on the source and can know the positions of the fixed blocks and their values.  For a matrix $\vA$ we will denote the $i$-th row by $\va_i$.  For a set $T$ of column indices, we denote by $\vA_T$ the restriction of the matrix $\vA$ to the columns contained in $T$.  Similarly, for a vector $\vx$ we denote by $\vx_T$ the restriction of $\vx$ to the variables contained in $T$.  We use similar notation for the complement of $T$, denoted $T^c$.  For a matrix or vector we use $\mathsf{T}$ to denote the transpose.  We use $i$ as a index into matrix rows and the error vector and $j$ as an  index into columns and the solution vector.

Let $n$ be a security parameter, $m ,q , \alpha= \poly(n)$.  Let $\beta$ be such that $q^{-\beta} = \ngl(n)$.  All operations are computed modulo $q$, and we omit $``\bmod q$'' notation.  Let $\chi'$ be some error distribution over $\Fq^m$ and let $\chi$ over $\Fq^{m+n}$ be defined by sampling $\chi'$ to obtain values on dimensions $1,..., m$ and then appending $\alpha$ 0s.  

Let $D$ be a distinguisher that breaks $\distLWE_{(m+\alpha), (n+\alpha+\beta), q, \chi}$ with advantage $\epsilon>1/\poly(n)$.
Let  $\vA$ denote the uniform distribution over $\Fq^{(m+\alpha)\times(n+\alpha+\beta)}$, $X$ denote the uniform distribution over $\Fq^{(n+\alpha+\beta)}$, and $U$ denote the uniform distribution over $\Fq^{m+\alpha}$ . Then
\[
|\Pr[D(\vA, \vA X+\chi) = 1] - \Pr[D(\vA, U )=1]|> \epsilon.
\]

We build a distinguisher that breaks $\distLWE_{m, n, q, \chi}$.  Let $\vA'$ denote the uniform distribution over $\Fq^{m\times n}$, $X'$ denote the uniform distribution over $\Fq^n$, and $U'$ denote the uniform distribution over $\Fq^{m}$ .  We will build a distinguisher $D'$ of polynomial size for which
\begin{align}
\label{eq:block LWE dist}
|\Pr[D'(\vA', \vA'X'+\chi') = 1] - \Pr[D'(\vA', U') =1]|> (\epsilon - \ngl(n))(1-\ngl(n)) \approx \epsilon.
\end{align}
$D'$  will make a single call to $D$, so we focus on how to prepare a random block-fixing instance for $D$ from the random instance that $D'$ is given.  The code for $D'$ is given in \figref{fig:perfectLWEreduction}.

\begin{figure}[p]
\begin{framed}
\begin{enumerate}
\item Input $\vA', \vb'$, where $\vA' \overset{\$} \leftarrow \Fq^{m\times n}$ and $\vb'$ is either uniform over $\Fq^m$ or $\vb' = \vA'\vx' +\ve'$ for $\ve'\overset{\$} \leftarrow \chi'$ and uniform $\vx'\  \overset{\$} \leftarrow \Fq^n$.
\item Choose $\vect{R} \overset{\$}\leftarrow \Fq^{\alpha \times n}$ uniformly at random. Initialize $\vQ \in \Fq^{m\times (\alpha+\beta)}$  to be the zero matrix.
\item Let $\vb^* = (\vb', b^*_{m+1}, \ldots,b^*_{m+\alpha})$, for uniformly chosen $(b^*_{m+1}, \ldots, b^*_{m+\alpha} )\overset{\$} \leftarrow \Fq^\alpha$.\label{step:b generation}
\item Choose $\vect{S} \overset{\$}\leftarrow \Fq^{\alpha \times (\alpha+\beta)}$ uniformly at random.
		\subitem If $\rank(\vect{S})<\alpha$, stop and output a random bit.
\item Find a set of $\alpha$ linearly independent columns in $\vS$.  Let $T$ be the set of indices of these columns.\label{step:find columns}
\item For all $1\le j \le \alpha+\beta$, $j\notin T$:
\label{step:fill in matrix}
\subitem Choose $x_{n+j}\overset{\$}\leftarrow \Fq$ uniformly at random.  
\subitem For $i=1,..., m$:
\subsubitem Choose $\vQ_{i,j}\overset{\$}\leftarrow \Fq$ uniformly at random.
\subsubitem Set $b_i^* = b_i^* + \vQ_{i,j} x_{n+j}$.
\item Initialize $\vA^*  = \left(\begin{array}{c | c}\vA' & \vQ\\\hline \vR & \vS\end{array}\right)$.
\item \label{step:randomization}
For {$i=1,..., m$}:
\subitem Choose a row vector $\gamma_i \leftarrow \Fq^{1 \times \alpha}$ uniformly at random.
\subitem Set $\va_{i} \leftarrow \va^*_{i}+\gamma_i (\vR||\vS)$
\subitem Set $b_i \leftarrow b^*_i + \gamma_i (b^*_{m+1},..., b^*_{m+\alpha})^{\mathsf{T}}$
\item For $i=m+1,\dots, m+\alpha$:
\subitem Set $\va_i \leftarrow \va^*_i$
\subitem Set $b_i = b_i^*$.
\item Output $D(\vA, \vb)$. 
\end{enumerate}
\end{framed}
\caption{A PPT $D'$ that distinguishes LWE using distinguisher for LWE w/ block fixing source}
\label{fig:perfectLWEreduction}
\end{figure}

The distinguisher $D'$ has an advantage when $\vS$ is of rank $\alpha$.  This occurs with overwhelming probability:
\begin{claim}
\label{cl:full rank matrix 5.2}
Let $\vS \overset{\$}\leftarrow \Fq^{\alpha \times (\alpha+\beta)}$ be randomly generated.  Then $\Pr[\rank(\vS)=\alpha]\geq  1- \ngl(n)$.
\end{claim}
\begin{proof}
Direct result of \clref{cl:full rank matrix} because $q^{-\beta} = \ngl(n)$.
\end{proof}
The probability that a random $\vS$ is not full rank is $\ngl(n)$ so the distinguisher $D$ must still have an advantage when the matrix $\vS$ is full rank.  That is,
\[
|\Pr[D(\vA, \vA X+\chi) = 1  | \rank(\vS) = \alpha] - \Pr[D(\vA, U) =1 | \rank(\vS) = \alpha]|> \epsilon - \ngl(n).
\]

It suffices to show that $D'$ prepares a good instance for $D$ conditioned on $\vS$ being full rank. We show this in the following three claims: 
\begin{enumerate}
\item If $\vA'$ is a random matrix then $\vA$ is a random matrix subject to the condition that $\rank(\vS) = \alpha$.
\item If $\vb' = \vA'\vx'+\ve'$ for uniform $\vA'$ and $\vx'$, then $\exists \vx$~(uniformly distributed and independent of $\vA$ and $\ve'$) such that $\vb = \vA \vx + \ve$, where $\ve_i = \ve_i'$ for $1\leq i\leq m$ and $\ve_i = 0$ otherwise.
\item If the conditional distribution $\vb'\,|\,\vA'$ is uniform, then the conditional distribution $\vb\,|\,\vA$ is also uniform.
\end{enumerate}

\begin{claim}
\label{cl:randomMatrixDist}
The matrix $\vA$ is distributed as a uniformly random choice from the set of all matrices whose bottom-right $\alpha\times (\alpha+\beta)$ submatrix $\vS$ satisfies $\rank(\vS) = \alpha$.
\end{claim}
\begin{proof}
The bottom $\alpha$ rows of $\vA$ (namely, $\vR|\vS$) are randomly generated~(conditioned on $\rank(\vS) =\alpha$).  The top left $m\times n$ quadrant of $\vA$ is also random, because it is produced as a sum of a uniformly random $\vA'$ with some values that are uncorrelated with $\vA'$.
The submatrix of the top-right $m\times (\alpha+\beta)$ quadrant corresponding to $\vQ_{T^c}$~(recall this is the restriction of $\vQ$ to the columns not in $T$) is also random, because it is initialized with random values to which some uncorrelated values are then added. It is important to note that all these values are independent of $\gamma_i$ values.

Thus, we restrict attention to the $m\times \alpha$ submatrix of $\vA$ that corresponds to $\vQ_T$ in $\vA^*$ (note that these values are $0$ in $\vA^*$).  Consider a particular row $i$. That row is computed as $\gamma_i \vS_{T}$.  Since $\vS_T$ is a full rank square matrix and $\gamma_i$ is uniformly and independently generated, that row is also uniform and independent of other entries in $\vA$.
\end{proof}
\begin{claim}
\label{cl:random ax+e}
If $D'$ is provided with input distributed as $\vA', \vb' = \vA'\vx'+\ve'$ then $\vb = \vA \vx+\ve$, where
\begin{itemize}
\item $e_i = e_i'$ for $1\leq i\leq m$,
\item $e_i = 0$ for $m<i\leq m+\alpha$,
\item $x_j = x_j'$ for $1\leq j \leq n$,
\item and $x_j$ is uniform and independent of $\vA$ and $\ve'$ for $n<j\le n+\alpha+\beta$,
\end{itemize}
\end{claim}
\begin{proof}
Partially define $\vx$ as $x_j = x_j'$ if $1\leq j \leq n$ and $x_j$ as the value generated in step~\ref{step:fill in matrix} for $j>n $ and $j\not\in T$.  Define the remaining variables $\vx_T$ as the solution to the following system of equations.
\begin{eqnarray}
\vS_T  \vx_T = \begin{pmatrix} b_{m+1}^*  \\ \vdots \\b_{m+\alpha}^* \end{pmatrix}  - \vR \vx' - \vS_{T^c} \vx_{T^c}  \label{eq:x t solution}
\end{eqnarray}
A solution $\vx_T$ exists as $\vS_T$ is full rank. Moreover, it is uniform and independent of $\vA$ and $\ve$, because $b^*_{m+1}, \dots, b^*_{m+\alpha}$ are uniform and independent of $\vA$ and $\ve$. 

We now show that $\vb^* = \vA^* \vx+\ve$.  All entries in matrix $\vQ$ corresponding to variables in $T$ are set to zero.  Thus, the values of $\vx^T$ do not affect $b_i^*$ for $1\le i \le m$.  The values of $\vx_{T^c}$ are manually set, and $\vQ_{i, j} \vx_{j}$ is added to the corresponding $b_i^*$.  Thus, for $1 \leq i \le m$, we have $\vb^* = \vA^*\vx+\ve$.   For $m< i$, this constraint is also satisfied by the values of $\vx_T$ set in Equation~\ref{eq:x t solution}.  

Thus, it remains to show that step~\ref{step:randomization} preserves this solution.
We now show that for all rows $1\leq i\leq m$, if $b_i^* = \va^*_i \vx+e_i$  then $b_i = \va_i \vx + e_i$.
Recall the other rows are not modified.  We have the following for $1\leq i\leq m$:
\begin{align*}
\va_i \vx + e_i &= \left(\va_{i}^*+ \gamma_i(\vR || \vS)\right) \vx + e_i\\
&=\va_i^* \vx + e_i +\gamma_i(\vR||\vS) \vx\\&= b_i^* + \gamma_i (\vR||\vS)\vx
\end{align*}
Recall that $b_i =b_i^* + \gamma_i(b_{m+1}^*,..., b_{m+k}^*)$.  We consider the product $(\vR|| \vS) \vx$.  It suffices to show that $(\vR|| \vS) \vx = (b_{m+1}^*,..., b_{m+\alpha}^*)$,
\begin{align*}
(\vR|| \vS) \vx &= \vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}+  \vS_{T^c} \vx_{T^c}  + \vS_T \vx_T \\
&=\vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}+  \vS_{T^c} \vx_{T^c}   + \begin{pmatrix} b_{m+1}^*  \\ \vdots \\b_{m+\alpha}^* \end{pmatrix}  - \vR \begin{pmatrix} \vx_1  \\ \vdots \\\vx_n \end{pmatrix}- \vS_{T^c} \vx_{T^c}  
\\&=  \begin{pmatrix} b_{m+1}^*  \\ \vdots \\b_{m+\alpha}^* \end{pmatrix}
\end{align*}
This completes the proof of the claim.
\end{proof}
\begin{claim}\label{clm:random b}
If the conditional distribution $\vb'\,|\,\vA'$ is uniform, then $\vb\,|\,\vA$ is also uniform.
\end{claim}
\begin{proof}
Since $\vR, \vS$, and $\vQ$ are chosen independently of $\vb'$, the distribution $\vb'\,|\,\vA^*$ is uniform.
Let $\vb^*$ be the vector generated after step~\ref{step:fill in matrix}. Its first $m$ coordinates are  computed by adding the uniform vector $\vb'$ to values that are independent of $\vb^*$, and its remaining $\alpha$ coordinates $b^*_{m+1},\dots,b^*_{m+\alpha}$ are   chosen uniformly.  Thus $\vb^*\,|\,\vA^*$ is uniform. 

Let $\vgamma$ represent the matrix formed by $\gamma_{i}$.  It is independent of $\vb^*$ and $\vA^*$, so $\vb^*\,|\,(\vA^*, \vgamma)$ is uniform.    Let $\vgamma'=\left(\begin{array}{c | c}\vect{I_m} & \vgamma \\\hline \vect{0} & \vect{I_\alpha}\end{array}\right)$.
Note that $\vb=\vgamma' \vb^*$.  Since $\vb^*\,|\,(\vA^*, \vgamma)$ is uniform, and $\vgamma'$ is invertible, $\vb\,|\,(\vA^*, \vgamma)$ must also be uniform.
Since $\vA$ is a deterministic function of $\vA^*$ and $\vgamma$ (assuming Step~\ref{step:find columns} is deterministic---if not, we can fix the coins used), the distribution $\vb\,|\,\vA$ is the same  as $\vb\,|\,(\vA^*, \vgamma)$ and is thus also uniform.
\end{proof}

Finally, the reduction runs in polynomial time and together Claims~\ref{cl:randomMatrixDist},~\ref{cl:random ax+e}, and~\ref{clm:random b} show that when $\rank(\vS) = \alpha$ the distinguisher $D'$ properly prepares the instance thus, 
\begin{align*}
&\left|\Pr[D'(\vA, \vA X+\chi) = 1] - \Pr[D'(\vA, U) =1] \right|\\
&\, = \left| \Pr\left[D'(\vA', \vu') = 1 | \rank(\vS) = \alpha \right]- \Pr\left[D'(\vA', \vb'=\vA'\vx + \ve)=1 | \rank(\vS) = \alpha\right]\right| \Pr[\rank(\vS) = \alpha] \\
&\, =\left|\Pr[D(\vA, \vA X+\chi) = 1  | \rank(\vS) = \alpha] - \Pr[D(\vA, U) =1 | \rank(\vS) = \alpha]  \right| \Pr[\rank(\vS) = \alpha] \\
&\, \geq (\epsilon - \ngl(n))(1-\ngl(n)) \approx \epsilon
\end{align*}
Where the second line follows because we can detect when $\rank(\vS)<\alpha$ and output a random bit in this case.
Thus, Equation~(\ref{eq:block LWE dist}) is satisfied, this completes the proof.
\end{proof}
\section{Additional Proofs}

\subsection{Proof of \lemref{lem:averageToMaximalError}}
\label{sec:proof of average to maximal error}
\begin{proof}
Let $C$ be the  $(t,\epsilon)$-average error Shannon code with recovery procedure $\rec$ such that  $\Hoo(C)\geq k$.  Then for all $t'\le t$
\[
\sum_{c\in C} \Pr[C=c]\Pr[ c'\leftarrow \neigh (c, t') \wedge \rec(c') \neq c]\leq \epsilon.
\]
For $c$ denote by $\epsilon_c = \Pr[c'\leftarrow \neigh(c, t') \wedge \rec(c') \neq c]$.  
Then by Markov's inequality:
\[
\Pr_{c\in C}[ \epsilon_c \leq 2\expe_{c\leftarrow C} [\epsilon_c ] ] = \Pr_{c\in C} [\epsilon_c \le 2\epsilon ] \geq \frac{1}{2}
\]
Let $C'$ denote the of  set all $c\in C$ where $\epsilon_c\leq 2\epsilon$.  Note that $\Pr_{c\leftarrow C}[c\in C']\geq 1/2$.  Since $H_\infty(C)\geq k$, we know $|C'|\geq 2^{k-1}$~(otherwise $\Pr_{c\leftarrow C}[c\in C']=\sum_{c\in C'}\Pr[C=c]$ would be less than $2^{k-1}\frac{1}{2^k} = 1/2$).  This completes the proof of the~\lemref{lem:averageToMaximalError}.
\end{proof}

\subsection{Proof of \thref{thm:impSketchArbitraryW}}
\label{sec:proof of thm sketch implies code}
\begin{proof}
  Let $W$ be an arbitrary distribution of min-entropy $m$.  Let $(X, Y)$ be a joint distribution such that $\Hav(X | Y)\geq k$ and
\[ 
\delta^{\mathcal{D}_{s_{sec}}}((W, \sketch(W)), (X, Y))\le \epsilon\, ,
\]  
where  $s_{sec} \geq t(s_{neigh}+s_{rec})$.  One such $(X, Y)$ must exist by the definition of conditional HILL entropy. 
Define $D$ as:
\begin{enumerate}
\item Input $w\in\mathcal{M}, z \in\{0, 1\}^*, t$.
\item For all $1\leq t'\leq t$: 
\subitem  $w'\leftarrow \neigh(w, t')$.
\subitem If $\rec(w', z) \neq  w$ output $0$.
\item Output $1$.
\end{enumerate}
 By correctness of the sketch $ \Pr[D(W, \sketch(W)) =1]\ge 1-t\delta$.  Since 
$\delta^D((W, \sketch(W)), (X, Y))\le \epsilon$, we know $\Pr[D(X, Y) = 1]\ge 1-\epsilon-t\delta$.  Let $X_y$ denote the random variable $X|Y=y$.  By Markov's inequality,  there exists a set $S_Y$ such that $\Pr[Y\in S_Y]\ge 1/2$ and for all $y\in S_Y$, $\Pr[ D(X_y, y) =1]\ge 1- 2(\epsilon + t\delta)$.  

Because $\Hav(X | Y)\geq k$, we know that $\expe_{y\leftarrow Y} \max_x \Pr[X_y=x]\leq 2^k$.  Applying Markov's inequality to the random variable $\max_x \Pr[X_y=x]$, there exists a set $S'_Y$ such that $\Pr[y\in S'_Y]> 1/2$, and for all $y\in S'_Y$, $\Hoo(X_y)\ge k-1$ (we can use the strict version of Markov's inequality here, because the random variable $\max_x \Pr[X_y=x]$ is positive).  Fix one value $y \in S_Y\cap S'_Y$ (which exists because the sum of probabilities of $S_Y$ and $S'_Y$ is greater than 1).  
Thus, for all such that $t', 1\leq t'\leq t$, 
\[ \Pr_{x\leftarrow X_y}[x'\leftarrow \neigh(x, t') \wedge \rec(x',z) = x]\ge  1-2(\epsilon+t\delta).\]  
Thus,  $X_y$ is a $(t, 2(\epsilon+t\delta))$-average error Shannon code with recovery $\rec(\cdot,y)$ and $2^{k-1}$ points.  The statement of the theorem follows by application of \lemref{lem:averageToMaximalError}.  
\end{proof}

\subsection{Proof of \thref{thm:imp of unp entropy}}
\label{sec:proof of imp unp entropy}
Instead of proving the result just for Hamming metric over $\mathcal{Z}^n$, we will prove the result for any metric space that is both neighborhood samplable~(\defref{def:neighborhood samplable}) and where picking a random point in the space is easy.  We now define this second condition:
\begin{definition}
A metric space space $(\mathcal{M}, \dis)$ is $s_{sam}$-\emph{efficiently-samplable} if there exists a randomized circuit $\sample$ of size $s_{sam}$ that outputs a uniformly random point in $\mathcal{M}$.
\end{definition}

\begin{theorem}
Let $W$ be a distribution over a metric space $(\mathcal{M}, \dis)$ that is $s_{sam}$ samplable and $(s_{neigh}, t)$ neighborhood samplable.  Furthermore, assume that the number of points within distance $t$ in $\mathcal{M}$ is at least some fixed value $B_t(\cdot)$.  Let $(\sketch, \rec)$ be an unpredictability-entropy $(\mathcal{M}, \Hoo(W), \tilde{m}, t)$ secure sketch that is $(\epsilon, s_{sec})$-secure with error $\delta$.  If $s_{sec} \geq \max\{ t(|\rec| +s_{neigh}), |\rec| + s_{sam}\}$, then $\tilde{m}\leq \log |\mathcal{M}| - \log |B_t(\cdot)| + \log(1-\epsilon -t\delta)$.
\end{theorem}
\begin{proof}
Let $(X, Y)$ be two random variables such that $\delta^{\mathcal{D}_{s_{sec}}}((W, \sketch(W)), (X, Y))\leq \epsilon$.  It suffices to show that $\exists \mathcal{I}$ of size $s_{sec}$ such that $\Pr[\mathcal{I}(Y) = X]\geq |\mathcal{M}| (1-\epsilon -t\delta) / |B_t(\cdot)|$.  

Let $B_t(x)$ denote the random variable representing a random neighbor of distance at most $t$ from $x$ (note that $B_t$ may not be efficiently samplable, because we are assuming only that a neighbor a fixed distance is efficiently samplable).
We begin by showing that \rec must recover points of $X$.  
\begin{claim}
\label{clm:y is recoverable}
\begin{align*}
\Pr[\rec(B_t(X), Y) = X]&=\\
\Pr[(x, y)\leftarrow (X, Y) \wedge x'\leftarrow B_t(x) \wedge \rec(x', y) = x] &\geq 1-\epsilon -t\delta.
\end{align*}
\end{claim}
\begin{proof}
Suppose that $\Pr[\rec(B_t(X), Y) = X]<1-\epsilon -t\delta$.  We construct the following distinguisher $D\in\mathcal{D}_{s_{sec}}$ (the distinguisher design is slightly complicated by the fact that we don't know at which particular distance $t'$ the recover procedure is most likely to fail, so we have to try all distances):
\begin{itemize}
\item Input $w\in \mathcal{M}, s\in\zo^*$.
\item For all $1\leq t'\leq t$: 
\subitem  $w'\leftarrow \neigh(w, t')$.
\subitem If $\rec(w', z) \neq  w$ output $0$.
\item Output $1$.
\end{itemize}
First note that $|D| = t( |\rec|+ s_{neigh} )$.  Since $(\sketch, \rec)$ has error $\delta$ we know that $\forall w, w'\in \mathcal{M}$ where $\dis(w, w')\leq t$ \[ \Pr[s\leftarrow \sketch(w) \wedge \rec(w', s) =  w] \geq 1-\delta.\]  This implies that for all $1\leq t'\leq t$, $\Pr[\rec(\neigh(W, t'), \sketch(W) )= W)  ]\geq 1-\delta$ and thus $\Pr[D(W, \sketch(W)) = 1]\geq 1-t\delta$.  If $\Pr[\rec(B_t(X), Y) = X] < 1-\epsilon -t\delta$ there must exist at least one $1\leq t'\leq t$ for which $\Pr[\rec(\neigh(X, t'), Y) = X] < 1-\epsilon -t\delta$.  Then 
\begin{align*}
\Pr[D(W, \sketch(W)) = 1]  - \Pr[D(X, Y)=1] &\geq\\
(1-t\delta) - \Pr[\rec(\neigh(X, t'), Y) = X] &> (1-t\delta)-(1-t\delta - \epsilon)>\epsilon.
\end{align*}
This is a contradiction and the statement of the claim follows.
\end{proof}

Now define $\mathcal{I}$ as follows:
\begin{itemize}
\item Input $y\in\zo^*$.
\item Sample $x'\leftarrow \sample$.
\item Output $\rec(x', y)$.
\end{itemize}
Note that $|\mathcal{I}| =  |\rec|+ s_{sam}$. 
We now show that $\mathcal{I}$ predicts $X$:
\begin{align*}
\Pr[\mathcal{I}(Y) = X] & = \\
&= \sum_{x, y\in \mathcal{M}} \Pr[(x, y)\leftarrow (X, Y)] \Pr[\mathcal{I}(y) = x]\\
&= \sum_{x, y\in \mathcal{M}} \Pr[(x, y)\leftarrow (X, Y)] \sum_{x'\in\mathcal{M}} \Pr[\sample = x' ] \Pr[\rec(x', y) =x]\\
&\ge \sum_{x, y\in \mathcal{M}} \Pr[(x, y)\leftarrow (X, Y)] \sum_{x' \mathrm{ s. t. } \dis(x', x)\le t} \Pr[\sample = x']\Pr[\rec(x', y) =x]\\
&\ge \sum_{x, y\in \mathcal{M}} \Pr[(x, y)\leftarrow (X, Y)] \sum_{x'\mathrm{ s. t. } \dis(x', x)\le t} \frac{|B_t(\cdot)|}{|\mathcal{M}|} \Pr[B_t(x) = x'] \Pr[\rec(x', y) =x]\\
&\geq \frac{|B_t(\cdot)|}{|\mathcal{M}|}(1-\epsilon - t\delta)
\end{align*}
(the last step follows by  \clref{clm:y is recoverable}).

\thref{thm:imp of unp entropy} follows by noting that $\mathcal{Z}^n$ can be sampled in time $n\log |\mathcal{Z}|$ and neighborhood sampled in time $n\log |\mathcal{Z}|$.
\end{proof}

\subsection{Proof of \lemref{lem:i t poly time}}
\label{sec:proof lem i t poly time}
\begin{proof}
Note that $\decode_t$ will stop if $w$ and $w'$ agree on all the rows selected in Step 2 (it may also stop for other reasons---namely, in step 4; but we do not use this fact to bound the expected running time).
The probability of each selected row having an error is at most $\frac{t}{m - i}$ where $i$ is the number of rows already selected.  That is,
\begin{align*}
\Pr[i_1,..., i_{2n}\text{ have no errors}]&\geq \prod_{i=0}^{2n-1}\left(1 - \frac{t}{m-i}\right)\geq \prod_{i=0}^{2n-1}\left( 1-\frac{d\left(\frac{m}{n}-2\right)\log n}{m-i}\right)\\
&\geq  \prod_{i=0}^{2n-1}\left( 1-\frac{d\log n}{n}\left(\frac{m-2n}{m-i}\right)\right)\geq \prod_{i=0}^{2n-1}\left( 1-\frac{d\log n}{n}\right) \\
&= \left(1-\frac{d\log n}{n}\right)^{2n}  = \left(\left(1-\frac{d\log n}{n}\right)^{\frac{n}{d\log n}}\right)^{2d\log n}\geq \frac{1}{4^{2d\log n}} = \frac{1}{n^{4d}}\,.
\end{align*}
(The second-to-last step holds as long as $n\ge 2d\log n$.) Because at each iteration, we select $2n$ rows independently at random, the expected number of iterations is at most $n^{4d}$; each iteration takes $O(n^3)$ operations in $\Fq$, which gives us the expected running time bound.

The probability that $\decode_t$ outputs $\perp$ is bounded by 
\begin{eqnarray*}
\Pr[\decode_t\rightarrow \perp]& \le & \sum_{j=1}^\infty \Pr[\decode_t\rightarrow \perp \text{ in $j$th iteration of step 4}]\\
&= &\sum_{j=1}^\infty  \Pr[\decode_t \text{ has not stopped after $j-1$ iterations} \wedge \rank(\vA_{i_1,\dots, i_{2n}})<n]\\
&\le &\sum_{j=1}^\infty  \Pr[i_1,..., i_{2n}\text{ had errors $j-1$ times} \wedge \rank(\vA_{i_1,\dots, i_{2n}})<n]\\
&= &\sum_{j=1}^\infty  \Pr[i_1,..., i_{2n}\text{ had errors $j-1$ times}]\cdot \Pr[\rank(\vA_{i_1,\dots, i_{2n}})<n]\\
&\le &\sum_{j=1}^\infty  \left(1-\frac{1}{n^{4d}}\right)^{j-1} \cdot q^{-n}\\
& = & n^{4d} e^{-\Omega(n)} = e^{-\Omega(n)}\,.
\end{eqnarray*}
The third line from the bottom follows from the fact that the locations of the errors are assumed to be independent of the sketch, and therefore independent of the matrix $\vA$.
The second line from the bottom follows from \clref{cl:full rank matrix} when $\beta = n$; note that, because we use the union bound and evaluate the probability separately for each value of $j$,  we can treat $\vA_{i_1,\dots, i_{2n}}$ as a randomly chosen $2n\times n$ matrix, ignoring the fact that these matrices are correlated.

We claim that if the code generated by $\vA$ 
has distance at least $2t+1$, then $\decode_t$ will output $\perp$ or the correct $\vx'=\vx$.
Indeed, suppose $\vx'\neq \vx$. Since $\vA (\vx-\vx')$ has at least $2t+1$ nonzero coordinates by the minimum distance of the code generated by $\vA$, and at most  $t$ of those can be zeroed out by the addition of  $w-w'$, such an $\vx'$ will not pass Step 6. 

The probability that the code generated by $\vA$ has distance lower than $2t+1$ is at most $e^{-\Omega(n)}$ (see \corref{cor:code high distance}), the probability of outputting $\perp$ is also $e^{-\Omega(n)}$~(computed above).  This gives the correctness bound for $\decode_t$.
\end{proof}

\section{Parameter Settings for \consref{cons:informal construction}}
\label{sec:parameter settings}
In this section, we explain the different parameters that go into our construction.  In \thref{thm:lossless secure extractor log} we give a lossless fuzzy extractor from a security parameter $n$ and an error $t$.  In this section, we discuss constraints imposed by 1) efficient decoding 2) maintaining security of the LWE instance and 3) ensuring no entropy loss of the construction.  We begin by reviewing the parameters that make up our construction:

\begin{itemize}
\item $|W|$: The length of the source.  
\item $t$: Number of errors that can be supported.  
\item $n$: LWE security parameter (i.e., number of field elements in $X$), which must be greater than some minimum value $n_0$ for security.
\item $q$: The size of the field.  
\item $\rho$: The fraction of the field needed for error sampling.  
\item $m$: The size of each number of samples in the LWE instance.  
\item $k$: The number of hardcore bits in $X$~(from \lemref{lem:many hardcore bits}).
\end{itemize}
We will split the source $|W|$ into $m$ blocks each of size $2\rho q+1$~(that is, $|W| = m\log (2\rho q+1)$).  We will ignore the parameter $|W|$ and focus on $t, n, q, \rho,$ and $m$.  As stated above we have three constraints:
\begin{itemize}
\item Maintain security of LWE.  If we assume GAPSVP and SIVP are hard to approximate within polynomial factors then \lemref{lem:uniform LWE decision} says that we get security for all $n$ greater than some minimum $n_0$ and $q = \poly(n)$ and $\rho q \geq 2 n^{1/2 + \sigma} m = \poly(n)$.  The only reason to increase $\rho q$ over this minimum amount (other than security) is if the number of errors in $W$ decreases with a slightly larger block size.  We ignore this effect and assume that $\rho q = 2n^{1/2+\sigma}m$.
\item Maintain efficient decoding of Construction~\ref{cons:decoding algorithm}.  Using \lemref{lem:i t poly time}, this means that $t\leq d\log n(m/n-2)$.
\item Minimize entropy loss of the construction.  We will output $X_{1,...,k}$ so the entropy loss of the construction is $|W|-|X_{1,..., k}|$.  We want the entropy loss to be zero, that is, $|W| = |X_{1,..., k}|$.  Substituting, one has $m\log 2\rho q+1 = k \log q$.
\end{itemize}
Collecting constraints we can support any setting where $t, n, q, \rho, m, k$ satisfy the following constraints~(for constants $d, f$):
\begin{align*}
n_0&< n -k \\
t&\leq d \log n\left(\frac{m}{n}-2\right)\\
q &= n^f\\
\rho q  &= 2n^{1/2+\sigma}m\\
m\log (2\rho q +1)&= k \log q
\end{align*}
Substituting $q = n^f$ and $\rho q = 2n^{1/2+\sigma}m$ yields the following system of equations:
\begin{align*}
n_0&< n - k\\
t&\leq d\log n\left(\frac{m}{n}-2\right)\\
m \log (4n^{1/2+\sigma}m +1)&= k \log n^f
\end{align*}
This is the most general form of our construction, we can support any $n, t, m$ that satisfy these equations for constants $d, f$.  However, the last equation may have no solution for $f$ constant.  Putting the last equation in terms of $f$ one has:
\begin{align*}
n_0&< n -k \\
t&\leq d\log n\left(\frac{ m }{n} -2\right)\\
f &= \frac{m}{k}\frac{\log 4n^{1/2+\sigma} m+1}{\log n}
\end{align*}
To ensure $f$ is a constant, we set $t = c \log n$ for some constant $c$ and that $k = n /g$ for some constant $g> 1$.  Finally we assume that $m$ is the minimum value such that $t \leq d \log n(m/n-2)$~(that is, there are only as many dimensions as necessary for decoding using \lemref{lem:i t poly time}):
\begin{align*}
n_0&< n -k \\
m &= \frac{(c/d+2)n \log n}{\log n} = (\frac{c}{d}+2)n\\
f &= \frac{m}{k}\frac{\log 4n^{1/2+\sigma}m+1}{\log n} = \frac{g(c+2d)}{d}\frac{\log (\frac{4(c+2d)}{d} n^{3/2+\sigma}+1)}{\log n}
\end{align*}
Note that $f$ is at a constant in $n$.
Assuming $n-k = n(1-1/g) > n_0$ and letting $t= c\log n$ we get the following setting:
\begin{align*}
m &= (\frac{c}{d}+2)n\\
q & = n^f = n^{\frac{m}{k}\frac{\log (4n^{1/2+\sigma}m+1)}{\log n}} = \poly(n)\\
\rho q &= 2n^{1/2+\sigma}m = 2(\frac{c}{d}+2)n^{3/2+\sigma}
\end{align*}

Note, that $f> \frac{m}{k}\geq \frac{m}{n} \geq \frac{(c/d+2)n}{n} \geq 3$ as long as $d<c$~(this also ensures that $m\geq 3n$, as required for \lemref{lem:i t poly time} to hold).  Since $\rho q = 2n^{1/2+\rho }m = O(n^{5/2})$ in our setting $\rho = O(n^{-1/2})$.  Thus, for large enough settings of parameters $\rho$ is less than $1/10$ as required by \lemref{lem:uniform LWE decision}.

Furthermore, we get decoding using $O(n^{4d+3})$ $\Fq$ operations.  We can output a $k$ fraction of $X$ and the bits will be pseudorandom~(conditioned on $\vA, \vA X+W$).  The parameter $g$ allows is a tradeoff between the number of dimensions needed for security and the size of the field $q$.  In \thref{thm:lossless secure extractor log}, we set $g=2$ and output the first half of $X$.  Setting $1<g<2$ achieves an increase in output length~(over the input length of $W$).   We also (arbitrarily) set $\sigma=1/2$ to simplify the statement of \thref{thm:lossless secure extractor log}, making $\rho q = 2(c/d+2) n^2$.

\subsection{Parameter Settings for \thref{thm:lossless block sketch log}}
\label{ssec:block params}
We repeat parameter settings for block fixing sources.  We now have $m+\alpha$ as the number of samples, while $n + \alpha+\omega(1)$ is the number of variables.  We can support any setting where $t, n, q, \rho, m, k, \alpha$ satisfy the following constraints~(for $\beta = \omega(1)$ and constants $d, f$):
\begin{align*}
n_0&< n -k  -\alpha -\beta\\
t&\leq d \log n\left(\frac{m}{n}-2\right)\\
q &= n^f\\
\rho q  &= 2n^{1/2+\sigma}m\\
m\log (2\rho q+1)&= k \log q
\end{align*}
Substituting $q = n^f$ and $\rho q = 2n^{1/2+\sigma}m$ yields the following system of equations:
\begin{align*}
n_0&< n - k - \alpha -\beta\\
t&\leq d\log n\left(\frac{m}{n}-2\right)\\
m \log (4n^{1/2+\sigma}m +1)&= k \log n^f
\end{align*}
As before we can support any setting any $n, t, m, \alpha$ that satisfy these equations for $\beta = \omega(1)$ and constants $d, f$.  However, the last equation may have no solution for $f$ constant.  Putting the last equation in terms of $f$ one has:
\begin{align*}
n_0&< n -k  - \alpha - \beta \\
t&\leq d\log n\left(\frac{ m }{n} -2\right)\\
%f \log n &= \frac{m}{n}\log 2n^2m\\
f &= \frac{m}{k}\frac{\log (4n^{1/2+\sigma} m+1)}{\log n}
\end{align*}
To ensure $f$ is a constant, we set $t = c \log n$ for some constant $c$ and that $k, \alpha = n/3$ and $\beta = \log n$.  Finally we assume that $m$ is the minimum value such that $t \leq  d \log n(m/n-2)$~(that is, there are only as many dimensions as necessary for decoding using \lemref{lem:i t poly time}):
\begin{align*}
n_0&< n/3 -  \log n\\
m &= \frac{(c/d+2)n \log n}{ \log n} = (\frac{c}{d}+2)n\\
f &= \frac{m}{k}\frac{\log (4n^{1/2+\sigma}m+1)}{\log n} = \left(3(\frac{c}{d}+2)\right)\frac{\log (4(\frac{c}{d}+2) n^{3/2+\sigma}+1)}{\log n} = O(1)
\end{align*}

Assuming $n/3-\log(n)> n_0$ and letting $t= c\log n$ we get the following setting:
\begin{align*}
m &= (\frac{c}{d}+2)n\\
q & = n^f = n^{\frac{m}{n}\frac{\log (4n^{1/2+\sigma}m+1)}{\log n}} = \poly(n)\\
\rho q &= 2n^{1/2+\sigma}m = 2(\frac{c}{d}+2)n^{3/2+\sigma}
\end{align*}

As before we arbitrarily set $\sigma = 1/2$, giving $\rho q = 2(\frac{c}{d}+2)n^2$.  Also, if $c<d$ then we get efficient decoding and $\rho = o(1)$ satisfying the condition of \lemref{lem:uniform LWE decision}.
\end{document}











